Potential for Discrimination in Online Targeted Advertising Recently online targeted advertising platforms like Facebook have been criticized for allowing advertisers to discriminate against users belonging to sensitive groups ie to exclude users belonging to a certain race or gender from receiving their ads Such criticisms have led for instance Facebook to disallow the use of attributes such as ethnic affinity from being used by advertisers when targeting ads related to housing or employment or financial services In this paper we show that such measures are far from sufficient and that the problem of discrimination in targeted advertising is much more pernicious We argue that discrimination measures should be based on the targeted population and not on the attributes used for targeting We systematically investigate the different targeting methods offered by Facebook for their ability to enable discriminatory advertising We show that a malicious advertiser can create highly discriminatory ads without using sensitive attributes Our findings call for exploring fundamentally new methods for mitigating discrimination in online targeted advertising Keywords Discrimination advertising Facebook INTRODUCTION Much recent work has focused on detecting instances of discrimination in online services ranging from discriminatory pricing on e-commerce and travel sites like Staples Mikians et al and Hotelscom Hannák et al to discriminatory prioritization of service requests and offerings from certain users over others in crowdsourcing and social networking sites like TaskRabbit Hannák et al In this paper we focus on the potential for discrimination in online advertising which underpins much of the Internets economy Specifically we focus on targeted advertising where ads are shown only to a subset of users that have attributes features selected c T Speicher et al Potential for Discrimination in Online Targeted Advertising by the advertiser Targeted ads stand in contrast to non-targeted ads such as banner ads on websites that are shown to all users of the sites independent of their attributes The targeted advertising ecosystem comprises of i advertisers who decide which users an ad should not be shown to ii ad platforms such as Google and Facebook that aggregate data about their users and make it available to advertisers for targeting and iii users of ad platforms that are consumers of the ads The potential for discrimination in targeted advertising arises from the ability of an advertiser to use the extensive personal demographic behavioral and interests data that ad platforms gather about their users to target their ads An intentionally malicious or unintentionally ignorant advertiser could leverage such data to preferentially target ie include or exclude from targeting users belonging to certain sensitive social groups eg minority race religion or sexual orientation Recently the Facebook ad platform was the target of intense media scrutiny Angwin and Parris Jr and a civil rights lawsuit for allowing advertisers to target ads with an attribute named ethnic affinity After clarifying that a users ethnic affinity does not represent the users ethnicity but rather represents how interested the user is in content related to different ethnic communities Facebook agreed to not allow ads related to housing employment and financial services be targeted using the attribute Facebook and renamed it to multicultural affinity In this paper we conduct a systematic study of the potential for discriminatory advertising on the Facebook advertisement platform We focus on Facebook because it is one of the largest online advertising platforms in terms of number of users reached by ads the number of advertisers and the amount of personal data gathered about the users that is made available to advertisers Furthermore Facebook is an innovator in introducing new methods for targeting users such as cus Unfortunately Facebook was found half a year later to still accept discriminatory ads despite the fixes it claims were put in place Angwin et al a tom audience and look-alike audience targeting that are then subsequently adopted by other online social media and social networking platforms like Twitter Pinterest LinkedIn and YouTube Thus many of our findings may also be applicable to these other online ad targeting platforms as well Our study here is driven by the following high-level question What are all the different ways in which a Facebook advertiser out of malice or ignorance can target users in a discriminatory manner ie include or exclude users based on their sensitive attributes like race To answer this question we begin by proposing an intuitive measure to quantify discrimination in targeted ads We then systematically investigate three different targeting methods attribute-based targeting PII-based targeting and lookalike audience targeting offered by Facebook for their ability to enable discriminatory advertising At a high-level we find that all three methods enable advertisers to run highly discriminatory ads Worse we show that the existing solution approaches of banning the use of certain attributes like ethnic affinity in targeting is not only inadequate but does not even apply in two out of the three ad targeting methods While our findings primarily serve to demonstrate the perniciousness of the problem of discriminatory advertising in todays ad platforms it also lays the foundations for solving ie detecting and mitigating ad discrimination QUANTIFYING AD DISCRIMINATION Next we begin by outlining different ad targeting methods offered by Facebook We then discuss the current approach to determining whether an ad is discriminatory and argue why it is Potential for Discrimination in Online Targeted Advertising Finally we propose a new and intuitive approach to quantify discrimination Methods for targeted ads Facebook gathers and infers several hundreds of attributes for all of its users covering their demographical behavioral and interest features Andreou et al Some of those attributes such as gender or race are considered sensitive meaning targeting ie including or excluding people based on those attributes is restricted by law for certain types of advertisements eg those announcing access to housing or employment or financial services Barocas and Selbst Facebook allows advertisers to select their target audience in three ways Attribute-based targeting Advertisers can select audiences that have or do not have a certain attribute or a combination of attributes eg select users who are men aged and are interested in tennis PII-based custom audience targeting Advertisers can directly specify who should be targeted by providing a list of personally identifiable information such as phone numbers or email addresses Look-alike audience targeting Advertisers can ask Facebook to target users who are similar to ie look like their existing set of customers specified using their Quantification approaches Next we discuss three basic approaches to quantifying discrimination and their trade-offs Based on advertisers intent An intuitive moralized way to quantify discrimination would be to base it on the advertisers intent However not only is such a measure challenging to operationalize ie to measure from empirical observations but it also overlooks the harmful effects of unintentionally discriminatory ads that may be placed by a well-meaning but ignorant or careless advertiser In this paper we do not consider such approaches Based on ad targeting process Another approach to determine whether an ad is facebook-ads-choose-audience discriminatory is based on the process used to target the ads Any ads placed using the right process would be non-discriminatory by definition while those using a wrong process would be declared discriminatory by definition Existing approaches such as those that determine whether an ad is discriminatory based on the use of sensitive attributes eg ethnic affinity in targeting fall under this category As we show in this paper attempting to quantify discrimination based on the process means or methods of targeting is quite difficult when there exist multiple different processes for targeting users Instead in this work we advocate for a third approach Based on targeted audience outcomes We propose to quantify discrimination based on the outcomes of the ad targeting process ie the audience selected for targeting Put differently we do not take into account how users are being targeted but only who they are Outcomebased approaches to quantifying discrimination have the advantage that they can be generally applied to all scenarios independently of the employed method of targeting We discuss one such method in the next section Outcome-based discrimination To formalize our discrimination measure we will assume that an ad platform like Facebook keeps track of a database D of where each user is represented by a vector of boolean attributes ie We denote the sensitive attribute eg race or gender that we are interested in a particular situation by s m and its value for a user u by us The corresponding sensitive group S is the set of all users that have the sensitive attribute ie S u D us To measure outcome- ie targeted audience-based discrimination we define a metric for how discriminatory an advertisers targeting is It is inspired by the disparate impact measure that is frequently used to detect discrimination in selecting candidates from a pool of applicants in recruiting and housing allotment scenarios Barocas and Selbst Our key observation is that ad targeting like recruiting involves selecting the target audience TA from a much larger pool of relevant audience RA The relevant audience of an ad is the set of all users in the database D who would find the ad useful and interesting and thus might interact with it Intuitively the discrimination measure should capture the extent to which the target audience selection is biased based on sensitive group membership of relevant users We define the representation ratio measure for sensitive attribute s to capture how much more likely a relevant user u is to be targeted when having the sensitive attribute compared to not having it More specifically it is the ratio between the fraction of relevant audience with attribute s that are selected for targeting and the fraction of relevant audience without attribute s that are selected ie where u RA us and u RA us Based on the representation ratio we define a measure that we call disparity in targeting defined for a sensitive attribute s as max Note that it is important to compute disparity based on the relevant audience RA because RA may have a very different composition in terms of attribute s than the whole database D For example an ad for mens clothes may have a relevant audience RA with a gender-ratio highly skewed towards men A random selection of users from RA would be non-disparate with respect to RA but might be highly disparate with respect toD Similarly for the same targeted audience including mostly males some ads could be non-discriminatory eg ads for mens clothes while others could be highly discriminatory eg ads for high-paying jobs depending on the corresponding relevant audience Throughout the paper we implicitly assume that for the sensitive attributes considered the relevant audience has the same distribution as the global population and we show that the advertiser can include or exclude certains groups based on the sensitive the ad targeting is discriminatory We propose to detect discriminatory targeting using our disparity measure as follows we declare a targeting formula as discriminatory when its disparity for some sensitive attribute value group exceeds a certain threshold ie the group is over- or under-represented For instance a reasonable threshold value may be mimicking the popular disparate impact rule Biddle to declare a group over- or underrepresented In addition to disparity we would be interested in the recall of an ad which quantifies how many of the relevant users with the sensitive attribute the discriminatory ad targets or excludes It can be defined as TARA RA where RA might be the restriction of RA to or depending on whether the discriminatory advertiser wants to target or exclude users with the sensitive attribute s PII-based Targeting In this section we show how the audience targeting mechanism based on personally identifiable information recently introduced by Facebook can be exploited by advertisers to covertly implement discriminatory advertising We first briefly describe the PII-based audience targeting feature of Facebook we then explain how this feature can be exploited to implement discriminatory advertising Next we explain how public data sources have data that advertisers can use to implement discriminatory advertising and finally demonstrate the feasibility of such an attack by using information from public records to create audiences for advertising that are discriminatory PII-based audience selection While Facebook traditionally allowed advertisers to select audiences to advertise to by specifying attributes of the audience eg age gender etc Facebook recently introduced custom audiences This feature allows advertisers to specify exactly which users they want to target by specifying personally identifying information that uniquely identifies those users Facebook allows different types of to be used including phone numbers email addresses and combinations of name with other attributes such as date of birth or ZIP code The advertiser uploads a file containing a list of Facebook then matches these to Facebook accounts to create a custom audience Custom audiences can be viewed as implementing a linking function that allows advertisers to link the large amounts of external personal data available today with Facebooks user information The linking function that custom audiences provide to advertisers is not a one-to-one function ie advertisers cannot determine the exact Facebook account of a given person but rather it is an aggregate function that maps to a group of Facebook users In the next section we show that despite this limitation custom audiences can be abused to covertly implement discriminatory advertising by exploiting external data to create lists that selectively include only people with the sensitive attribute Potential for discrimination To implement discriminatory advertising using custom audiences an advertiser could simply create a list of corresponding selectively to people who have the sensitive attribute uploading this list of to create a custom audience and then advertising to that custom audience Since the advertiser does not upload the sensitive attribute instead uploading only a list of and since the advertising platform itself may not have the sensitive user attribute such targeting becomes difficult to detect Most advertisers already possess significant amounts of customer information eg customer data information from data brokers however even if they do not have such data there are many other sources of data including public records data brokers and web data that can be accessed for free or at low cost We next describe public sources of data from which one can get sensitive attributes for large sets of people we then demonstrate how these data sources can be used in combination with custom audiences to implement discriminatory advertising Public data sources An increasing amount of information about people is publicly available we now briefly discuss how advertisers could obtain large amounts of external personal information Race age and gender Most US states release voter records that contain the personal information of all registered voters names phone numbers addresses etc along with other sensitive attributes such as race age and gender For example date of birth and gender are available in the records released by and states respectively Minkus et al race information is available in the records of eight states North Carolina New Mexico Louisiana Tennessee Alabama Georgia Florida and South Carolina Ansolabehere and Hersh Even when the race or gender is not available they can often be predicted with reasonable precision from other attributes Mislove et al For example Tang et al Tang et al propose a technique to infer the gender of a person from their name with an accuracy of while covering more than of users Other companies such as Catalist aggregate voter records from states and infer missing values of gender from the first name and race from the name and address the resulting race attributes matched voters self-reported race of the time Ansolabehere and Hersh Criminal history People with criminal records even those who have completed their sentence are often victims of discrimination We quickly survey the US and find that more than states in the US make criminal records available online and that states offer free access to their state-wide criminal record databases these records often contain significant amounts of personal information such as name race gender and date of birth along with the specific criminal record Thus advertisers can easily create custom audiences consisting only of users in this vulnerable population Discriminatory audience creation We briefly demonstrate how it is possible to create discriminatory custom audiences on todays advertising platforms Note that we did not actually advertise to these users or affect them in any way Rather our goal here is simply to demonstrate that using only public sources of data advertisers can target protected classes and vulnerable populations with little effort Table Results from experiment creating custom audiences using only users with certain attributes from the North Carolina voter records For each sensitive attribute we created and uploaded a custom audience of random voters with that attribute Shown is the total number of records per attribute the number of Facebook users in the resulting Targetable custom audience and the percentage of Targetable users who match the sensitive attribute as per Facebooks estimates Voter Records Facebook Users Validation of Custom Audience Attribute Number Percent Targetable Targetable matching sensitive attribute Male Female White Black Asian Hispanic Age Age Age We downloaded the public voter records from North Carolina giving us M records Using data from the voter records we then created custom audiences on Facebook for each sensitive attribute selecting a random subset of users from the voter file with each attribute For example we created a custom audience of women by uploading a list of voters listed as female we created a custom audience of white users by uploading a list of voters listed as white We created these custom audiences by uploading records containing the following fields last name first name city state zip code phone number and country We then examine how many of these records match to Facebook accounts that can be targeted with advertisements and then evaluate whether the created audiences are indeed discriminatory Whenever we target an audience either based on attributes or by specifying a custom audience Facebook provides an estimate of the number of users in the audience who can be targeted with advertisements this estimate is called the potential reach We first target only the custom audiences created without any additional targeting attributes specified and use the potential reach estimate to measure how many records in the audience are Targetable Facebook previously defined the potential reach as the number of daily active people on Facebook that match the audience you defined through your audience targeting selections Finally in order to validate that advertisements targeted to these custom audiences would indeed be discriminatory we take each custom audience and then target users with the corresponding sensitive attribute eg for the male voter records audience we target the Male attribute we then measure the potential reach and use the potential reach to measure what percentage of the Targetable users in the audience actually have that sensitive attribute according to Facebook Ideally the percentage of Targetable users with the sensitive attribute would be however Facebook may not know the attributes of some users may have errors in their matching algorithm or there may be errors in the user-provided data making this percentage smaller It is important to note that definitions in our data sources the voter file and census data do not always line up with the targeting options that Facebook presents For race Facebook does not provide race directly but instead provides ethnic affinity this is the same targeting parameter by which Facebook was accused of allowing discriminatory advertising Angwin and Parris Jr Results The results of this experiment are shown in Table and we make a number of interesting observations First the fraction of voter records that are Targetable ie online on a daily basis is both significantly high over for most audiences we create and fairly consistent across custom audiences The only notable outliers are the Age audience with only matching Second we observe that the fraction of the Targetable audience that matches the sensitive attribute although it varies fairly widely across the different sensitive attributes is consistently much higher than the fraction of the general adult population that has those sensitive attributes assuming the voter records to be representative of the general adult population In particular for many sensitive attributes including gender most races and all ages the percentage of Targetable audience that matches the sensitive attribute is higher than We suspect that the reason this fraction is low for the Asian attribute is due to the fact that race is an attribute that users typically do not upload to Facebook directly however we leave determining the source of this inconsistency to future work We also note that even for these cases the fraction of the Targetable audience that matches the sensitive attribute is significantly higher than the fraction of the voter records with the sensitive attribute Taken together our results show that advertisers can exploit public records to easily target discriminatory advertisements to a large number of people Summary We explored the inherent risks that custom audiences induce for end users by allowing the linking of external information with Facebooks user data We demonstrated the ease with which malicious advertisers could leverage the custom audience feature now present on advertising platforms like Facebook to implement discriminatory advertising In fact the wide variety of sources of public data available today means that even if an advertiser does not possess customer records of its own it can easily find data sources to feed into custom audience creation Attribute-based Targeting In this section we examine how Facebooks attribute-based targeting mechanism can be used to launch discriminatory ads First we briefly explain how attribute-based targeting works and then examine the potential for abusing it Attribute-based audience selection In brief attribute-based targeting refers to the process of selecting an ad audience by specifying that recipients need to have a certain attribute or a combination of attributes this is the traditional way of targeting ads on Facebook For each user in the US Facebook tracks a list of over binary attributes spanning demographic behavioral and interest categories that we refer to as curated attributes Additionally Facebook tracks users interests in entities such as websites apps and services as well as topics ranging from food preferences eg pizza to niche interests eg space exploration We refer to these as free-form attributes as they number at least in hundreds of thousands It is unclear how exactly Facebook infers these attributes but from their own description this information can be gathered in many different ways such as user activity on Facebook pages apps and services check-ins with Facebook and accesses to external webpages that use Facebook ad technologies Beyond specifying a target region language age and gender for their ad advertisers can choose that an ad should be shown to people that have some of these curated or free-form attributes turned on or off Potential for discrimination The potential for discrimination on the Facebook ad platform was first publicly highlighted when researchers discovered the ability to exclude people based on their ethnic affinity a curated attribute when targeting ads related to housing Angwin and Parris Jr Facebook responded by banning the use of ethnic affinity attribute for certain types of ads Facebook More recently researchers discovered the ability to target people interested in or holding anti-semitic viewpoints via free-form attributes like jew haters Angwin et al b These findings raise several questions about the potential for discriminatory targeting using Facebooks curated and free-form attributes First given that ethnic affinity-based targeting was disallowed for its potential correlation with ethnicity race of users are there other demographic behavioral or interest attributes that are similarly correlated if not more Second given that there exist hundreds of thousands of free-form attributes can malicious advertisers productad_preferences Table Most inclusive and exclusive curated attributes for each race In parentheses are the recall and representation ratio for a population from North Carolina These were obtained by uploading voter records filtered to contain only a single race and then measuring the size of the subaudience targeted by each attribute Attributes present in less than of the population are not considered Race Most inclusive Most exclusive Asian US Politics Liberal US Politics Very Conservative Frequent travelers African American affinity Interest Vegetarianism Interest Country music Black African American affinity US Politics Very Conservative US Politics Very Liberal US Politics Conservative Interest Online games Interest Mountain biking Indian Interest Motorcycles US Politics Very Conservative Interest Online games Away from hometown Interest Ecotourism Primary OS Mac OS X White US Politics Very Conservative African American affinity US Politics Conservative US Politics Very Liberal Interest Hiking Interest Online games find facially neutral free-form attributes that disproportionately target or exclude users of a sensitive group For example an advertiser seeking to create an audience excluding certain ethnic groups may choose to select her target audience from users interested in particular news media sites or magazines To answer these questions and understand how vulnerable the Facebook ad platform is to these kinds of indirect discrimination we investigate how strongly curated attributes other than ethnic affinity correlate with ethnicity and whether free-form attributes that are facially unrelated to sensitive attributes can be used as proxies for sensitive attributes We executed these experiments by automatically querying the Facebook ad interface for the number of people belonging or not belonging to sensitive groups that have a certain curated or free-form attribute Discriminatory audience creation We now explore how both curated and free-form attributes are correlated with ethnicity Curated attributes We conduct our analysis in the way described in Section We use the custom audience mechanism to create groups of people from the North Carolina voter records that only contain particular ethnicities White African-American Asian and Hispanic We then create sub-audiences by choosing to only target users matching each curated attribute and observe the size estimates of these sub-audiences The percentage of users from each audience for whom Facebook inferred a curated attribute reveals how prevalent the attribute is within the audiences of different ethnicities The top three inclusive and exclusive attributes per ethnicity are shown in Table The results point out that ethnic affinity is by far not the only and in many cases not even the most disparate feature with respect to ethnicity For example when targeting Asians on Facebook it is more effective to do so based on political leaning or eating habits The tradeoffs between representation ratio and recall for members outside the sensitive group which an advertiser has to consider when aiming to exclude sensitive group members can also be gauged from the table In particular there are a number of curated attributes with low representation ratio ie high disparity some of which achieve high recall for members not belonging to the sensitive group Free-form attributes We begin our investigation by gathering an extensive though not exhaustive list of free-form attributes that are supported by the Facebook marketing API The API provides two useful calls that we exploit i given a piece of text the API provides a list of free-form attributes that match the given text and ii given an attribute the API provides a marketing-api Table Free-form attributes that may be used for discriminatory targeting We show the percentage of the attribute audience that are members of the sensitive group as well as the fraction of the US Facebook population that are members of the sensitive group as a reference Free-form Attribute Potential Target PT PT Audience US Audience Marie Claire Female myGayTripcom Man interested in Man BlackNewscom African American affinity hoc Magazine Asian American affinity Nuestro Diario Hispanic affinity Table Examples of free-form attributes that can be targeted by advertisers In the parenthesis we show the number of audience that can be targeted or excluded with the attribute Topic Free-form attributes Religion Islam M Catholic Church M Evangelicalism M LGBT LGBT Gay pride M Same-sex Vulnerable people Addicted REHAB AA Support group list of other related attribute suggestions For instance the list of related attributes for The New York Times includes The Washington Post The Wall Street Journal and The Economist We start with a seed set of names of news outlets extracted from three different sources Google News Leskovec et al List of Newspapers and the top newspapers from Alexa We first identify around free-form attributes that exactly match with the names of the news outlets We then execute a snowball sampling on these attributes using Facebooks related attribute suggestions recursively starting from them This process resulted in retrieving nearly free-form attributes We begin by trying to find attributes from the above set of attributes that can be used to primarily target or exclude people belonging to sensitive groups Table shows example free-form attributes that could be exploited for discriminatory targeting For example the attribute Marie Claire has an audience with of women a much larger fraction than the proportion of US women in Facebook Similarly the attribute myGayTripcom has an audience of men interested in men while only of the US population in Facebook consists of men interested in men We also News/Newspapers a number of attributes with very biased audiences in terms of racial affinities For example BlackNewscom has an audience with of the users with African American affinity in contrast with of African American affinity in the reference population the audience of hoc Magazine is composed of users with Asian American affinity which corresponds to times more in comparison with the reference population Similarly Nuestro Diario has an audience with of Hispanic affinity on the reference population These results suggest that a malicious advertiser could easily find free-form attributes to launch discriminatory ads based on gender race and sexual orientation More worryingly some free-form attributes allow a malicious advertiser to target people based on their beliefs Table presents a few sensitive free-form attributes from our dataset along with their potential audience in the US These attributes correspond to a large audience with specific religious beliefs including islam M catholic church M and evangelicalism M Thus although it is not possible to target religion using curated attributes one can use free-form attribute targeting to narrow the audience to people who are interested in a specific religion Finally we note that it is possible to target or exclude gay and LGBT users or people sympathetic to their causes via attributes like LGBT community M Gay Table Suggestions for the most conservative news outlets The left column shows a set of freeform attributes for conservative news outlets and the right column shows the corresponding free-form attributes suggested by Facebook The percentage of very conservative users in the audience of each of these free-form attributes is shown in parentheses Very Conservative US Facebook Population Input Attribute Attribute Suggestions Townhallcom The Daily Caller RedState TheBlaze Hot Air news site The American Spectator The Daily Caller Townhallcom The American Conservative National Review Weekly Standard Human Events Commentary RedState Harpers Magazine US News World Report The Patriot Post American Patriot Patriot Nation Patriot Update NewsBustersorg Guns Patriots RedEye Americas Conservative Voice American Thinker National Review Fox Nation The Cullman Times Montgomery Advertiser The Huntsville Times The Tuscaloosa News alcom pride M Same-sex marriage M as well as groups of vulnerable people including Addicted REHAB AA While this last set of free-form attributes might be useful for example for an advertiser to prevent addicted people to receive ads about alcoholic beverages a discriminatory advertiser could explicitly exclude them Using Facebooks attribute suggestions We first investigate the free-form attributes suggested by Facebook to better understand the criteria used to select these suggestions Table shows the suggestions returned by the Facebook Marketing API right column given a freeform attribute left column We selected attributes associated with news outlets biased towards conservative audience to check whether their respective suggestions are also similarly biased For instance almost of the audience of Townhallcom are very conservative Facebook users whereas the average amount of very conservative US users of Facebook is about We note that the audiences corresponding to most of the suggested attributes also exhibit a strong bias towards conservative audience From all suggestions presented only Harpers Magazine and RedEye have a less conservative audience in comparison with the US distribution A malicious advertiser could exploit free-form attribute suggestions from Facebook in two different ways First a malicious advertiser could exploit the Facebooks attribute suggestions to discover attributes that are facially neutral but are similarly biased as a given free-form attribute For example one suggestion from Facebook for myGayTripcom is the free-form attribute Matt Dallas who is a gay actor of the audience for Matt Dallas are men interested in men which is times more than the US distribution Thus a malicious advertiser may use Matt Dallas as a facially neutral proxy for targeting or excluding gay users Second an advertiser can use the suggestion mechanism to search for extremely biased free-form attributes For example suppose an advertiser is interested in conservative leaning audiences and the most biased free-form attribute they know is Fox with of conservative audience The advertiser can start with Fox and keep choosing more and more conservative attribute suggestions until she reaches attributes with extreme conservative audience bias Below we show a sequence of suggested attributes starting from Fox that leads to The Sean Hannity Show a free-form attribute with conservative audience Fox Fox News Channel Sean Hannity Mark Levin Rush Limbaugh The Rush Limbaugh Show The Sean Hannity Show Summary In this section we demonstrated that many curated attributes beyond ethnic affinity exhibit correlations with sensitive attributes like race which makes them potential vectors for discrimination We also investigated whether the freeform attribute targeting mechanism allows advertisers to target or exclude sensitive groups of users in a discriminatory manner Specifically we showed that advertisers can circumvent existing limitations on targeting users based on their interests in sensitive topics like religion and sexual orientation Furthermore we show that malicious advertisers can exploit Facebooks suggestions to discover new facially neutral free-form attributes that allow extremely biased targeting Look-Alike Audience Targeting In this section we show how the recently introduced look-alike audience targeting mechanism can be exploited by advertisers to covertly implement discriminatory advertising We first briefly describe the look-alike audience targeting feature of Facebook we then explain how this feature can be exploited to implement discriminatory advertising Look-alike audience selection Recently Facebook introduced the look-alike audience targeting feature to help advertisers reach people that are similar to ie look like their existing set of customers Look-alike audiences are a particularly useful feature for advertisers who have limited data about their customers and want to grow their customer base Advertisers can use it to outsource the job of marketing ie identifying the attributes of their potential customers and finding them to Facebook To select look-alike audiences advertisers need to first provide Facebook with information about their existing initial set of customers called the source audience An advertiser can choose source audience users in a variety of ways including by ing a custom audience or by specifying them to be the fans of their Facebook page After specifying a source audience Facebook allows advertisers to specify a geographical region either countries or groups of countries from which the look-alike audience should be chosen Facebook orders ranks all users in the geographical region based on their similarity to ie how closely they look like the source audience and allows advertisers to select look-alike audiences by specifying a percentile range eg or over these ordered users from the geographical regions population Thus an advertiser can select X to Y percentile of closest matching users from any countrys population to target In practice Facebook limits Y to Potential for discrimination Our concern is that a malicious advertiser seeking to place discriminatory advertisements could exploit look-alike audiences as follows they could start by creating a highly biased highly discriminatory source audience and use the look-alike audience feature to find a larger set of users that is similarly biased effectively scaling the bias to much larger populations Put differently our concern is that when the source audience is discriminatory its look-alike audience would also be discriminatory In the following sections we first investigate whether biases in source audience selection propagate to look-alike audience selection Later we show how an advertiser seeking to selectively target people of a particular race could simply create a small in the order of a few thousands but highly biased source audience consisting primarily of people of a particular race as described in Section and use it to effectively target a large in the order of tens of millions yet similarly or worse exaggeratedly-biased lookalike audience Bias in look-alike audience selection In this section we construct several highly biased source audiences and check if and how the selection biases in source audience propagate to look-alike audiences Similar to what we did in Sections and we use the North Carolina voter database to Table Top most over-represented and under-represented attributes in a source audience of African Americans and its two closest look-alike audiences In parentheses we show the value of the representation bias of each attribute Over-represented Attributes Under-represented Attributes Source Audience African American affinity US politics very liberal Liberal content engagement Interest Gospel music Interest Dancehalls Asian American affinity Hispanic Spanish dominant affinity Expats Mexico Hispanic all affinity Expats all countries Look-Alike Audience African American affinity Liberal content engagement US politics very liberal Interest Gospel music Interest Soul music Hispanic Spanish dominant affinity Expats Mexico Asian American affinity Hispanic all affinity Expats all countries Look-Alike Audience African American affinity Liberal content engagement US politics very liberal Interest Gospel music Interest Dancehalls Asian American affinity Hispanic Spanish dominant affinity Expats Mexico Hispanic all affinity Expats all countries construct several groups of randomly selected people based on their ethnicity Asian Black White Hispanic gender political affiliation registered Democrat or Republican and age We construct a source audience corresponding to each group and for each source audience we ask Facebook to construct look-alike audiences from the US in five percentile ranges closest matching and of the US population Note that the audiences in the different percentile ranges do not overlap with one another and each subsequent percentile range becomes less similar ie less closely matching to the source audience Each of the five look-alike audiences we create for every source audience of people consist of approximately million people thus totaling to an approximate of million unique people in the US Thus look-alike audiences allow expansion of the source audience by over three orders of magnitude The key remaining question is whether the look-alike audience selection reflects the biases in source audience selection To capture the biases in our audience selection we define a measure that we call representation bias of a target audience for every user attribute maintained by Facebook Simply put representation bias captures how disproportionately an attribute is observed amongst the target audience TA compared to the people in the geographic location from where the look-alike audience is being selected the geographic location is the US in our scenario and we refer to people in the US as the relevant audience RA More formally the representation bias of an attribute in an audience is defined as TARA TA RA where similar to the representation ratio Equation are the subsets of people with attribute in TA and RA respectively We leave out attributes with very low prevalence in Facebook from our analysis ie attributes for which Knowing the representation bias of each attribute allows us to construct a ranking of attributes from most to least biased we refer to attributes at the top of the ranking as overrepresented and those at the bottom to be underrepresented in a target audience Table shows the top over-represented and under-represented attributes for the source audience of African Americans and its and the most similar two look-alike audiences The table shows that a majority of attributes that were found to be overrepresented in the source audience remain so for the look-alike audiences similar behavior can be observed for the underrepresented attributes These results particularly the presence of multicultural affinity attributes suggest that Facebook is using its extensive set of attributes to likely infer the biases that we introduced into our source audience Moreover it is propagating these biases to the selection of look-alike audiences constantly over-representing African Americans and under-representing Hispanics and Asian Americans compared to their proportions in the national population To further validate our findings above we take the top over-represented and underrepresented attributes in the source audience and computed their average rank in the look-alike audiences We performed these computations for differently biased source audiences selected along the basis of gender age ethnicity and political affiliation Figure shows how the average rank changes across the look-alike audiences given by Facebook We can see that attributes that were most over- and under-represented in source audience tend to stay on average amongst the most over- and under-represented in the look-alike audiences respectively These results strengthen our inference that the look-alike audience feature in Facebook is able to both capture the biases in a source audience and propagate the biases to the larger audiences it helps construct Discriminatory audience creation Having observed that the look-alike audience selection mimics the biases of the source audience selection we now check whether the bias propagation is sufficiently strong to lead to discriminatory audience creation To answer this question we compute the disparity of the sensitive attribute on which the source audience was biased and observe how disparate that attribute remains in the look-alike audiences made by Facebook Note that since we are observing look-alike audiences built from source audiences where the sensitive attribute was severely exaggerated we expect the disparity measure to reflect the disparity in favor of the attribute Source Look-alike audience er ag e ra nk Politics Republicans Ethnicity White Age Gender Men Figure Comparison of the average ranks of top over-represented and under-represented attributes in look-alike audiences built from different types of biased source audiences Average ranks for over-represented attributes are indicated by upward triangles downward triangles are used for the average ranks of underrepresented attributes Figure shows how source audiences that were disparate in favor of an ethnic group tend to produce look-alike audiences also disparate in favor of that ethnicity although as the audiences become less similar the disparity tapers off Only one of these audiences the look-alike audience for White has a disparity below the threshold obtained from the disparate impact rule Biddle These results show that Source Look-alike audience D is pa ri ty Asian Black Hispanic White Figure Disparity in favor for each ethnicity when the look-alike audiences are created from an audience biased on that ethnicity look-alike audiences selected using highly biased source audiences can be highly discriminatory Summary In this section we investigated whether it is possible to start with a small discriminatory source audience and then leverage Facebooks look-alike audience feature to construct a considerably larger discriminatory audience We show that in order to select a look-alike audience Facebook tries to infer the attributes that distinguish the audience from the general population and propagates these biases in the selection of look-alike audiences Such bias propagation can amplify the explicit intentionally created or implicit unintentionally overlooked biases in a source audience of a few thousand to a lookalike audience of tens of millions As Facebook is actively involved in the selection of the look-alike audience one might argue that Facebook needs to be more accountable for the selection of such a discriminatory audience Concluding Discussion Recently concerns have been raised about the potential abuse of online advertising platforms to target ads related to housing employment and financial services only to users of a particular race in violation of anti-discrimination laws In this paper we set out to investigate the following high-level question can a malicious advertiser leverage the different targeting methods offered by platforms like Facebook to target users in a discriminatory manner At a high-level our study makes the following contributions i We argue that the determination of whether a targeted ad is discriminatory should not be made based on the use or non-use of specific user attributes by advertisers Rather inspired by the notion of disparate impact Feldman et al we propose a simple outcome-based measure for discriminatory targeting that is computed independently of the user attributes used in targeting ii Next using public voter record data in the US we conduct an empirical study demonstrating that several user attributes in Facebook beyond the much-criticized ethnic affinity show strong positive and negative correlations with users belonging to different races Worse Facebooks related attribute suggestions can be exploited by advertisers to discover facially-neutral attributes that can be used for highly discriminatory audience targeting Thus simply banning certain attributes is insufficient to solve the problem iii Finally we explore the vulnerability of two previously overlooked methods of targeting supported by Facebook namely PII-based custom audience targeting and look-alike audience targeting We show that both these methods can be exploited by a malicious advertiser to include or exclude users with certain sensitive features at scale ie in the order of tens of millions of users Future work Towards detecting and mitigating ad discrimination Our study here has largely focussed on understanding the problem of discriminatory advertising rather than proposing solutions for detecting or mitigating discriminatory targeting However in the process we lay the foundations for the future solutions First the discrimination measure proposed here could be used when designing procedures to detect discrimination in the future Second we argue that the look-alike audience selection feature also presents a promising solution to the problem of mitigating discrimination in audience selection Specifically ad platform providers could expand the targeted audience to include look-alike most similar users that belong to under-represented groups rather than select all look-alike audience We plan to explore the effectiveness of this approach in mitigating discrimination in future work