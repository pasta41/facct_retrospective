Gender Shades Intersectional Accuracy Disparities in Commercial Gender Classification Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender In this work we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups Using the dermatologist approved Fitzpatrick Skin Type classification system we characterize the gender and skin type distribution of two facial analysis benchmarks IJB-A and Adience We find that these datasets are overwhelmingly composed of lighter-skinned subjects for IJB-A and for Adience and introduce a new facial analysis dataset which is balanced by gender and skin type We evaluate commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group with error rates of up to The maximum error rate for lighter-skinned males is The substantial disparities in the accuracy of classifying darker females lighter females darker males and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair transparent and accountable facial analysis algorithms Keywords Computer Vision Algorithmic Audit Gender Classification INTRODUCTION Artificial Intelligence AI is rapidly infiltrating every aspect of society From helping determine Download our gender and skin type balanced PPB dataset at gendershadesorg who is hired fired granted a loan or how long an individual spends in prison decisions that have traditionally been performed by humans are rapidly made by algorithms ONeil Citron and Pasquale Even AI-based technologies that are not specifically trained to perform highstakes tasks such as determining how long someone spends in prison can be used in a pipeline that performs such tasks For example while face recognition software by itself should not be trained to determine the fate of an individual in the criminal justice system it is very likely that such software is used to identify suspects Thus an error in the output of a face recognition algorithm used as input for other tasks can have serious consequences For example someone could be wrongfully accused of a crime based on erroneous but confident misidentification of the perpetrator from security video footage analysis Many AI systems eg face recognition tools rely on machine learning algorithms that are trained with labeled data It has recently been shown that algorithms trained with biased data have resulted in algorithmic discrimination Bolukbasi et al Caliskan et al Bolukbasi et al even showed that the popular word embedding space WordVec encodes societal gender biases The authors used WordVec to train an analogy generator that fills in missing words in analogies The analogy man is to computer programmer as woman is to X was completed with homemaker conforming to the stereotype that programming is associated with men and homemaking with women The biases in WordVec are thus likely to be propagated throughout any system that uses this embedding c Buolamwini T Gebru Gender Shades Although many works have studied how to create fairer algorithms and benchmarked discrimination in various contexts Kilbertus et al Hardt et al ba only a handful of works have done this analysis for computer vision However computer vision systems with inferior performance across demographics can have serious implications Esteva et al showed that simple convolutional neural networks can be trained to detect melanoma from images with accuracies as high as experts Esteva et al However without a dataset that has labels for various skin characteristics such as color thickness and the amount of hair one cannot measure the accuracy of such automated skin cancer detection systems for individuals with different skin types Similar to the well documented detrimental effects of biased clinical trials Popejoy and Fullerton Melloni et al biased samples in AI for health care can result in treatments that do not work well for many segments of the population In other contexts a demographic group that is underrepresented in benchmark datasets can nonetheless be subjected to frequent targeting The use of automated face recognition by law enforcement provides such an example At least million Americans are included in law enforcement face recognition networks A yearlong research investigation across police departments revealed that African-American individuals are more likely to be stopped by law enforcement and be subjected to face recognition searches than individuals of other ethnicities Garvie et al False positives and unwarranted searches pose a threat to civil liberties Some face recognition systems have been shown to misidentify people of color women and young people at high rates Klare et al Monitoring phenotypic and demographic accuracy of these systems as well as their use is necessary to protect citizens rights and keep vendors and law enforcement accountable to the public We take a step in this direction by making two contributions First our work advances gender classification benchmarking by introducing a new face dataset composed of unique individuals that is more phenotypically balanced on the basis of skin type than existing benchmarks To our knowledge this is the first gender classification benchmark labeled by the Fitzpatrick TB six-point skin type scale allowing us to benchmark the performance of gender classification algorithms by skin type Second this work introduces the first intersectional demographic and phenotypic evaluation of face-based gender classification accuracy Instead of evaluating accuracy by gender or skin type alone accuracy is also examined on intersectional subgroups darker females darker males lighter females and lighter males The evaluated commercial gender classifiers have the lowest accuracy on darker females Since computer vision technology is being utilized in high-stakes sectors such as healthcare and law enforcement more work needs to be done in benchmarking vision algorithms for various demographic and phenotypic groups RELATED WORK Automated Facial Analysis Automated facial image analysis describes a range of face perception tasks including but not limited to face detection Zafeiriou et al Mathias et al Bai and Ghanem face classification Reid et al Levi and Hassner a Rothe et al and face recognition Parkhi et al Wen et al Ranjan et al Face recognition software is now built into most smart phones and companies such as Google IBM Microsoft and Face have released commercial software that perform automated facial analysis IBM Microsoft Face Google A number of works have gone further than solely performing tasks like face detection recognition and classification that are easy for humans to perform For example companies such as Affectiva Affectiva and researchers in academia attempt to identify emotions from images of peoples faces Dehghan et al Srinivasan et al Fabian Benitez-Quiroz et al Some works have also used automated facial analysis to understand and help those with autism Leo et al Palestra et al Controversial papers such as Kosinski and Wang claim to determine the sexuality of Caucasian males whose profile pictures are on Facebook or dating sites And others such as Wu and Zhang and Israeli based company Faception Faception have developed software that purports to determine an individuals characteristics eg propensity towards crime IQ terrorism solely from their faces The clients of such software include governments An article by Aguera Y Arcas et al details the dangers and errors propagated by some of these aforementioned works Face detection and classification algorithms are also used by US-based law enforcement for surveillance and crime prevention purposes In The Perpetual Lineup Garvie and colleagues provide an in-depth analysis of the unregulated police use of face recognition and call for rigorous standards of automated facial analysis racial accuracy testing and regularly informing the public about the use of such technology Garvie et al Past research has also shown that the accuracies of face recognition systems used by US-based law enforcement are systematically lower for people labeled female Black or between the ages of than for other demographic cohorts Klare et al The latest gender classification report from the National Institute for Standards and Technology NIST also shows that algorithms NIST evaluated performed worse for female-labeled faces than male-labeled faces Ngan et al The lack of datasets that are labeled by ethnicity limits the generalizability of research exploring the impact of ethnicity on gender classification accuracy While the NIST gender report explored the impact of ethnicity on gender classification through the use of an ethnic proxy country of origin none of the locations used in the study were in Africa or the Caribbean where there are significant Black populations On the other hand Farinella and Dugelay claimed that ethnicity has no effect on gender classification but they used a binary ethnic categorization scheme Caucasian and non-Caucasian Farinella and Dugelay To address the underrepresentation of people of African-descent in previous studies our work explores gender classification on African faces to further scholarship on the impact of phenotype on gender classification Benchmarks Most large-scale attempts to collect visual face datasets rely on face detection algorithms to first detect faces Huang et al Kemelmacher-Shlizerman et al Megaface which to date is the largest publicly available set of facial images was composed utilizing Head Hunter Mathias et al to select one million images from the Yahoo Flicker M image dataset Thomee et al Kemelmacher-Shlizerman et al Any systematic error found in face detectors will inevitably affect the composition of the benchmark Some datasets collected in this manner have already been documented to contain significant demographic bias For example LFW a dataset composed of celebrity faces which has served as a gold standard benchmark for face recognition was estimated to be male and White Han and Jain Although Taigman et al s face recognition system recently reported accuracy on the LFW dataset its performance is not broken down by race or gender Given these skews in the LFW dataset it is not clear that the high reported accuracy is applicable to people who are not well represented in the LFW benchmark In response to these limitations Intelligence Advanced Research Projects Activity IARPA released the IJB-A dataset as the most geographically diverse set of collected faces Klare et al In order to limit bias no face detector was used to select images containing faces In comparison to face recognition less work has been done to benchmark performance on gender classification In the Adience gender and age classification benchmark was released Levi and Hassner b As of The National Institute of Standards and Technology is starting another challenge to spur improvement in face gender classification by expanding on the study Intersectional Benchmark An evaluation of gender classification performance currently requires reducing the construct of gender into defined classes In this work we use the sex labels of male and female to define gender classes since the evaluated benchmarks and classification systems use these binary labels An intersectional evaluation further requires a dataset representing the defined genders with a range of phenotypes that enable subgroup accuracy analysis To assess the suitability of existing datasets for intersectional benchmarking we provided skin type annotations for unique subjects within two selected datasets and compared the distribution of darker females darker males lighter females and lighter males Due to phenotypic imbalances in existing benchmarks we Figure Example images and average faces from the new Pilot Parliaments Benchmark PPB As the examples show the images are constrained with relatively little variation in pose The subjects are composed of male and female parliamentarians from countries On average Senegalese subjects are the darkest skinned while those from Finland and Iceland are the lightest skinned created a new dataset with more balanced skin type and gender representations Rationale for Phenotypic Labeling Though demographic labels for protected classes like race and ethnicity have been used for performing algorithmic audits Friedler et al Angwin et al and assessing dataset diversity Han and Jain phenotypic labels are seldom used for these purposes While race labels are suitable for assessing potential algorithmic discrimination in some forms of data eg those used to predict criminal recidivism rates they face two key limitations when used on visual images First subjects phenotypic features can vary widely within a racial or ethnic category For example the skin types of individuals identifying as Black in the US can represent many hues Thus facial analysis benchmarks consisting of lighter-skinned Black individuals would not adequately represent darker-skinned ones Second racial and ethnic categories are not consistent across geographies even within countries these categories change over time Since race and ethnic labels are unstable we decided to use skin type as a more visually precise label to measure dataset diversity Skin type is one phenotypic attribute that can be used to more objectively characterize datasets along with eye and nose shapes Furthermore skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals Roth Poorly exposed images that result from sensor optimizations for lighter-skinned subjects or poor illumination can prove challenging for automated facial analysis By labeling faces with skin type we can increase our understanding of performance on this important phenotypic attribute Existing Benchmark Selection Rationale IJB-A is a US government benchmark released by the National Institute of Standards and Tech LightestDarkest Figure The global distribution of skin color Most Africans have darker skin while those from Nordic countries are lighter-skinned Image from Encyclopedia Britannica Encyclopedia Britannica NIST in We chose to evaluate this dataset given the governments involvement and the explicit development of the benchmark to be geographically diverse as mentioned in Sec At the time of assessment in April and May of the dataset consisted of unique subjects who are public figures One image of each unique subject was manually labeled with one of six Fitzpatrick skin types TB Adience is a gender classification benchmark released in and was selected due to its recency and unconstrained nature The Adience benchmark contains unique individual subjects of those subjects had reference images that were discernible enough to be labeled by skin type and gender Like the IJB-A dataset only one image of each subject was labeled for skin type Creation of Pilot Parliaments Benchmark Preliminary analysis of the IJB-A and Adience benchmarks revealed overrepresentation of lighter males underrepresentation of darker females and underrepresentation of darker individuals in general We developed the Pilot Parliaments Benchmark PPB to achieve better intersectional representation on the basis of gender and skin type PPB consists of individuals from three African countries Rwanda Senegal South Africa and three European countries Iceland Finland Sweden selected for gender parity in the national parliaments Property PPB IJB-A Adience Release Year Subjects Avg IPD pixels Size avg IM Width IM Height Table Various image characteristics of the Pilot Parliaments Benchmark compared with prior datasets Subjects denotes the number of unique subjects the average bounding box size is given in pixels and IM stands for image Figure shows example images from PPB as well as average faces of males and females in each country represented in the datasets We decided to use images of parliamentarians since they are public figures with known identities and photos available under non-restrictive licenses posted on government websites To add skin type diversity to the dataset we chose parliamentarians from African and European countries Fig shows an approximated distribution of average skin types around the world As seen in the map African countries typically have darker-skinned individuals whereas Nordic countries tend to have lighter-skinned citizens Colonization and migration patterns nonetheless influence the phenotypic distribution of skin type and not all Africans are darker-skinned Similarly not all citizens of Nordic countries can be classified as lighter-skinned The specific African and European countries were selected based on their ranking for gender parity as assessed by the Inter Parliamentary Union Inter Parliamentary Union Ranking Of all the countries in the world Rwanda has the highest proportion of women in parliament Nordic countries were also well represented in the top nations Given the gender parity and prevalence of lighter skin in the region Iceland Finland and Sweden were chosen To balance for darker skin the next two highest-ranking African nations Senegal and South Africa were also added Table compares image characteristics of PPB with IJB-A and Adience PPB is highly constrained since it is composed of official profile photos of parliamentarians These profile photos are taken under conditions with cooperative subjects where pose is relatively fixed illumination is constant and expressions are neutral or smiling Conversely the images in the IJB-A and Adience benchmarks are unconstrained and subject pose illumination and expression by construction have more variation Intersectional Labeling Methodology Skin Type Labels We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer TB The six-point Fitzpatrick classification system which labels skin as Type I to Type VI is skewed towards lighter skin and has three categories that can be applied to people perceived as White Figure Yet when it comes to fully representing the sepia spectrum that characterizes the rest of PPB IJB-A Adience Darker Female Darker Male Lighter Female Ligher Male Figure The percentage of darker female lighter female darker male and lighter male subjects in PPB IJB-A and Adience Only of subjects in Adience are darker-skinned and female in comparison to in PPB the world the categorizations are fairly coarse Nonetheless the scale provides a scientifically based starting point for auditing algorithms and datasets by skin type Gender Labels All evaluated companies provided a gender classification feature that uses the binary sex labels of female and male This reductionist view of gender does not adequately capture the complexities of gender or address transgender identities The companies provide no documentation to clarify if their gender classification systems which provide sex labels are classifying gender identity or biological sex To label the PPB data we use female and male labels to indicate subjects perceived as women or men respectively Labeling Process For existing benchmarks one author labeled each image with one of six Fitzpatrick skin types and provided gender annotations for the IJB-A dataset The Adience benchmark was already annotated for gender These preliminary skin type annotations on existing datasets were used to determine if a new benchmark was needed More annotation resources were used to label PPB For the new parliamentarian benchmark annotators including the authors provided gender and Fitzpatrick labels A board-certified surgical dermatologist provided the definitive labels for the Fitzpatrick skin type Gender labels were determined based on the name of the parliamentarian gendered title prefixes such as Mr or Ms and the appearance of the photo Set n M Darker Lighter DF LF LM All Subjects Africa South Africa Senegal Rwanda Europe Sweden Finland Iceland Table Pilot Parliaments Benchmark decomposition by the total number of female subjects denoted as total number of male subjects M total number of darker and lighter subjects as well as female darkerlighter and male darkerlighter subjects The group compositions are shown for all unique subjects Africa Europe and the countries in our dataset located in each of these continents Dataset Lighter Darker IV V VI Total PPB IJB-A Adience Table The distributions of lighter and darker-skinned subjects according to the Fitzpatrick classification system in PPB IJB-A and Adience datasets Adience has the most skewed distribution with of the subjects consisting of lighter-skinned individuals whereas PPB is more evenly distributed between lighter and darker subjects Fitzpatrick Skin Type Comparison For the purposes of our analysis lighter subjects will refer to faces with a Fitzpatrick skin type of III or III Darker subjects will refer to faces labeled with a Fitzpatrick skin type of IVV or VI We intentionally choose countries with majority populations at opposite ends of the skin type scale to make the lighterdarker dichotomy more distinct The skin types are aggregated to account for potential off-by-one errors since the skin type is estimated using images instead of employing a standard spectrophotometer and Fitzpatrick questionnaire Table presents the gender skin type and intersectional gender by skin type composition of PPB And Figure compares the percentage of images from darker female darker male lighter female and lighter male subjects from Adience IJB-A and PBB PPB provides the most balanced representation of all four groups whereas IJB-A has the least balanced distribution Darker females are the least represented in IJB-A and darker males are the least represented in Adience Lighter males are the most represented unique subjects in all datasets IJB-A is composed of unique lighter males whereas this percentage is reduced to in Adience and in PPB As seen in Table Adience has the most skewed distribution by skin type While all the datasets have more lighterskinned unique individuals PPB is around half light at whereas the proportion of lighterskinned unique subjects in IJB-A and Adience is and respectively PPB provides substantially more darker-skinned unique subjects than IJB-A and Adience Even though Adience has labeled unique subjects which is nearly twice that of the subjects in PPB it has darker subjects nearly half the darker subjects in PPB Overall PPB has a more balanced representation of lighter and darker subjects as compared to the IJB-A and Adience datasets Commercial Gender Classification Audit We evaluated commercial gender classifiers Overall male subjects were more accurately classified than female subjects replicating previous findings Ngan et al and lighter subjects were more accurately classified than darker individuals An intersectional breakdown reveals that all classifiers performed worst on darker female subjects Key Findings on Evaluated Classifiers All classifiers perform better on male faces than female faces difference in error rate All classifiers perform better on lighter faces than darker faces difference in error rate All classifiers perform worst on darker female faces error rate Microsoft and IBM classifiers perform best on lighter male faces error rates of and respectively Face classifiers perform best on darker male faces error rate The maximum difference in error rate between the best and worst classified groups is Commercial Gender Classifier Selection Microsoft IBM Face We focus on gender classifiers sold in API bundles made available by Microsoft IBM and Face Microsoft IBM Face Microsofts Cognitive Services Face API and IBMs Watson Visual Recognition API were chosen since both companies have made large investments in artificial intelligence capture significant market shares in the machine learning services domain and provide public demonstrations of their facial analysis technology At the time of evaluation Google did not provide a publicly available gender classifier Previous studies have shown that face recognition systems developed in Western nations and those developed in Asian nations tend to perform better on their respective populations Phillips et al Face a computer vision company headquartered in China with facial analysis technology previously integrated with some Lenovo computers was thus chosen to see if this observation holds for gender classification Like Microsoft and IBM Face also provided a publicly available demonstration of their gender classification capabilities at the time of evaluation April and May All of the companies offered gender classification as a component of a set of proprietary facial analysis API services Microsoft IBM Face The description of classification methodology lacked detail and there was no mention of what training data was used At the time of evaluation Microsofts Face Detect service was described as using advanced statistical algorithms that may not always be precise Microsoft API Reference IBM Watson Visual Recognition and Face services were said to use deep learning based algorithms IBM API Reference Face Terms of Service None of the commercial gender classifiers chosen for this analysis reported performance metrics on existing gender estimation benchmarks in their provided documentation The Face terms of use explicitly disclaim any warranties of accuracy Only IBM provided confidence scores between and for face-based gender classification labels But it did not report how any metrics like true positive rates TPR or false positive rates FPR were balanced Evaluation Methodology In following the gender classification evaluation precedent established by the National Institute for Standards and Technology NIST we assess Classifier Metric All M Darker Lighter DF LF LM MSFT Error Rate TPR FPR Face Error Rate TPR FPR IBM Error Rate TPR FPR Table Gender classification performance as measured by the positive predictive value error rate true positive rate TPR and false positive rate FPR of the evaluated commercial classifiers on the PPB dataset All classifiers have the highest error rates for darker-skinned females ranging from for Microsoft to for IBM Classifier Metric DF LF LM MSFT Error Rate TPR FPR Face Error Rate TPR FPR IBM Error Rate TPR FPR Table Gender classification performance as measured by the positive predictive value error rate true positive rate TPR and false positive rate FPR of the evaluated commercial classifiers on the South African subset of the PPB dataset Results for South Africa follow the overall trend with the highest error rates seen on darker-skinned females the overall classification accuracy male classification accuracy and female classification accuracy as measured by the positive predictive value Extending beyond the NIST Methodology we also evaluate the true positive rate false positive rate and error rate of the following groups all subjects male subjects female subjects lighter subjects darker subjects darker females darker males lighter females and lighter males See Table in supplementary materials for results disaggregated by gender and each Fitzpatrick Skin Type Audit Results Male and Female Error Rates To conduct a demographic performance analysis the differences in male and female error rates for each gender classifier are compared first in aggregate Table and then for South Africa Table The NIST Evaluation of Automated Gender Classification Algorithms report revealed that gender classification performance on female faces was to lower than performance on male faces for the nine evaluated algorithms Ngan et al The gender misclassification rates on the Pilot Parliaments Benchmark replicate this trend across all classifiers The differences between female and male classification error rates range from to The relatively high true positive rates for females indicate that when a face is predicted to be female the estimation is more likely to be correct than when a face is predicted to be male For the Microsoft and IBM classifiers the false positive rates FPR for females are double or more than the FPR for males The FPR for females is more than times that of males with the Face classifier Darker and Lighter Error Rates To conduct a phenotypic performance analysis the differences in darker and lighter skin type error rates for each gender classifier are compared first in aggregate Table and then for South Africa Table All classifiers perform better on lighter subjects than darker subjects in PPB Microsoft achieves the best result with error rates of on darker subjects and on lighter individuals On darker subjects IBM achieves the worst classification accuracy with an error rate of This rate is nearly times higher than the IBM error rate on lighter faces Intersectional Error Rates To conduct an intersectional demographic and phenotypic analysis the error rates for four intersectional groups darker females darker males lighter females and lighter males are compared in aggregate and then for South Africa Across the board darker females account for the largest proportion of misclassified subjects Even though darker females make up of the PPB benchmark they constitute between to of the classification error Lighter males who make up of the benchmark contribute only to of the total errors from these classifiers See Table in supplementary materials We present a deeper look at images from South Africa to see if differences in algorithmic performance are mainly due to image quality from each parliament In PPB the European parliamentary images tend to be of higher resolution with less pose variation when compared to images from African parliaments The South African parliament however has comparable image resolution and has the largest skin type spread of all the parliaments Lighter subjects makeup n of the images and darker subjects make up the remaining n of images Table shows that all algorithms perform worse on female and darker subjects when compared to their counterpart male and lighter subjects The Microsoft gender classifier performs the best with zero errors on classifying all males and lighter females On the South African subset of the PPB benchmark all the error for Microsoft arises from misclassifying images of darker females Table also shows that all classifiers perform worse on darker females Face is flawless on lighter males and lighter females IBM performs best on lighter females with error rate Examining classification performance on the South African subset of PPB reveals trends that closely match the algorithmic performance on the entire dataset Thus we conclude that variation in performance due to the image characteristics of each country does not fully account for the differences in misclassification rates between intersectional subgroups In other words the presence of more darker individuals is a better explanation for error rates than a deviation in how images of parliamentarians are composed and produced However darker skin alone may not be fully responsible for misclassification Instead darker skin may be highly correlated with facial geometries or gender display norms that were less represented in the training data of the evaluated classifiers Analysis of Results The overall gender classification accuracy results show the obfuscating nature of single Darker Female Darker Male Lighter Female Lighter Male Group C on fid en ce S co re s Gender Female Male Figure Gender classification confidence scores from IBM IBM Scores are near for lighter male and female subjects while they range from for darker females metrics Taken at face value gender classification accuracies ranging from to on the PPB dataset suggest that these classifiers can be used for all populations represented by the benchmark A company might justify the market readiness of a classifier by presenting performance results in aggregate Yet a gender and phenotypic breakdown of the results shows that performance differs substantially for distinct subgroups Classification is worse on female than male subjects and worse on darker than lighter subjects Though helpful in seeing systematic error gender and skin type analysis by themselves do not present the whole story Is misclassification distributed evenly amongst all females Are there other factors at play Likewise is the misclassification of darker skin uniform across gender The intersectional error analysis that targets gender classification performance on darker female lighter female darker male and lighter male subgroups provides more answers Darker females have the highest error rates for all gender classifiers ranging from For Microsoft and IBM classifiers lighter males are the best classified group with and error rates respectively Face classifies darker males best with an error rate of When examining the gap in lighter and darker skin classification we see that even though darker females are most impacted darker males are still more misclassified than lighter males for IBM and Microsoft The most improvement is needed on darker females specifically More broadly the error gaps between male and female classification along with lighter and darker classification should be closed Accuracy Metrics Microsoft and Face APIs solely output single labels indicating whether the face was classified as female or male IBMs API outputs an additional number which indicates the confidence with which the classification was made Figure plots the distribution of confidence values for each of the subgroups we evaluate ie darker females darker males lighter females and lighter males Numbers near indicate low confidence whereas those close to denote high confidence in classifying gender As shown in the box plots the API is most confident in classifying lighter males and least confident in classifying darker females While confidence values give users more information commercial classifiers should provide additional metrics All evaluated APIs only provide gender classifications they do not output probabilities associated with the likelihood of being a particular gender This indicates that companies are choosing a threshold which determines the classification if the prediction probability is greater than this threshold the image is determined to be that of a male or female subject and viceversa if the probability is less than this number This does not give users the ability to analyze true positive TPR and false positive FPR rates for various subgroups if different thresholds were to be chosen The commercial classifiers have picked thresholds that result in specific TPR and FPR rates for each subgroup And the FPR for some groups can be much higher than those for others By having APIs that fail to provide the ability to adjust these thresholds they are limiting users ability to pick their own TPRFPR trade-off Data Quality and Sensors It is well established that pose illumination and expression PIE can impact the accuracy of automated facial analysis Techniques to create robust systems that are invariant to pose illumination expression occlusions and background have received substantial attention in computer vision research Kakadiaris et al Ganguly et al Ahmad Radzi et al Illumination is of particular importance when doing an evaluation based on skin type Default camera settings are often optimized to expose lighter skin better than darker skin Roth Underexposed or overexposed images that present significant information loss can make accurate classification challenging With full awareness of the challenges that arise due to pose and illumination we intentionally chose an optimistic sample of constrained images that were taken from the parliamentarian websites Each country had its peculiarities Images from Rwanda and Senegal had more pose and illumination variation than images from other countries Figure The Swedish parliamentarians all had photos that were taken with a shadow on the face The South African images had the most consistent pose and illumination The South African subset was also composed of a substantial number of lighter and darker subjects Given the diversity of the subset the high image resolution and the consistency of illumination and pose our finding that classification accuracy varied by gender skin type and the intersection of gender with skin type do not appear to be confounded by the quality of sensor readings The disparities presented with such a constrained dataset do suggest that error rates would be higher on more challenging unconstrained datasets Future work should explore gender classification on an inclusive benchmark composed of unconstrained images Conclusion We measured the accuracy of commercial gender classification algorithms on the new Pilot Parliaments Benchmark which is balanced by gender and skin type We annotated the dataset with the Fitzpatrick skin classification system and tested gender classification performance on subgroups darker females darker males lighter females and lighter males We found that all classifiers performed best for lighter individuals and males overall The classifiers performed worst for darker females Further work is needed to see if the substantial error rate gaps on the basis of gender skin type and intersectional subgroup revealed in this study of gender classification persist in other human-based computer vision tasks Future work should explore intersectional error analysis of facial detection identification and verification Intersectional phenotypic and demographic error analysis can help inform methods to improve dataset composition feature selection and neural network architectures Because algorithmic fairness is based on different contextual assumptions and optimizations for accuracy this work aimed to show why we need rigorous reporting on the performance metrics on which algorithmic fairness debates center The work focuses on increasing phenotypic and demographic representation in face datasets and algorithmic evaluation Inclusive benchmark datasets and subgroup accuracy reports will be necessary to increase transparency and accountability in artificial intelligence For human-centered computer vision we define transparency as providing information on the demographic and phenotypic composition of training and benchmark datasets We define accountability as reporting algorithmic performance on demographic and phenotypic subgroups and actively working to close performance gaps where they arise Algorithmic transparency and accountability reach beyond technical reports and should include mechanisms for consent and redress which we do not focus on here Nonetheless the findings from this work concerning benchmark representation and intersectional auditing provide empirical support for increased demographic and phenotypic transparency and accountability in artificial intelligence