Decoupled Classifiers for Group-Fair and Efficient Machine Learning When it is ethical and legal to use a sensitive attribute such as gender or race in machine learning systems the question remains how to do so We show that the naÄ±ve application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups We provide a simple and efficient decoupling technique which can be added on top of any black-box machine learning algorithm to learn different classifiers for different groups Transfer learning is used to mitigate the problem of having too little data on any one group INTRODUCTION As algorithms are increasingly used to make decisions of social consequence the social values encoded in these decision-making procedures are the subject of increasing study with fairness being a chief concern Pedreschi et al Zliobaite et al Kamishima et al Dwork et al Friedler et al Angwin et al Chouldechova Kleinberg et al Hardt et al Joseph et al Kusner et al Berk Classification and regression algorithms are one particular locus of fairness concerns Classifiers map individuals to outcomes applicants to accept/reject/waitlist adults to credit scores web users to advertisements felons to estimated recidivism risk Figure No linear classifiers can achieve greater than accuracy on both groups formally the concern is whether individuals are treated fairly however this is defined Still speaking informally there are many sources of unfairness prominent among these being training the classifier on historically biased data and a paucity of data for under-represented groups leading to poor performance on these groups which in turn can lead to higher risk for those such as lenders making decisions based on classification outcomes Should ML systems use sensitive attributes such as gender or race if available The legal and ethical factors behind such a decision vary by time country jurisdiction culture and downstream application Still speaking informally it is known that ignoring these attributes does not ensure fairness both because they may be closely correlated with other features in the data and because they provide context for C Dwork N Immorlica AT Kalai M Leiserson Decoupled Classifiers for Group-Fair and Efficient Machine Learning ing the rest of the data permitting a classifier to incorporate information about cultural differences between groups Dwork et al Using sensitive attributes may increase accuracy for all groups and may avoid biases where a classifier favors members of a minority group that meet criteria optimized for a majority group as illustrated visually in Figure of In this paper we consider how to use a sensitive attribute such as gender or race to maximize fairness and accuracy assuming that it is legal and ethical A data scientist wishing to fit say a simple linear classifier may use the raw data upweight/oversample data from minority groups or employ advanced approaches to fitting linear classifiers that aim to be accurate and fair No matter what he does and what fairness criteria he uses assuming no linear classifier is perfect he may be faced with an inherent tradeoff between accuracy on one group and accuracy on another As an extreme illustrative example consider the two group setting illustrated in Figure where feature x perfectly predicts the binary outcome y For people in group where x the majority group y ie y when x and otherwise However for the minority group where x exactly the opposite holds y Now if one performed classification without the sensitive attribute x the most accurate classifier predicts y so the majority group would be perfectly classified and the minority group would be classified as inaccurately as possible However even using the group membership attribute x it is impossible to simultaneously achieve better than random accuracy on both groups This is due to limitations of a linear classifier b since the same is used across groups In this paper we define and explore decoupled classification systems in which a separate classifier is trained on each group Training a classifier involves minimizing a loss function that penalizes errors examples include mean squared loss and absolute loss In decoupled classification In the case of linear classifiers training separate classifiers is equivalent to adding interaction terms between the sensitive attributes and all other attributes More generally the separate classifiers can equivalently be thought of as a single classifier that branches on the group attribute The decoupling technique is a simple way to add branching to any type of classifier one first obtains for each group separately a collection of classifiers differing in the numbers of positive classifications returned for the members of the given group Let this set of outputs for group be denoted The output of the decoupled training step is an element of C that is a single classifier for each group The output is chosen to minimize a joint loss function that can penalize differences in classification statistics between groups Thus the loss function can capture group fairness properties relating the treatment of different groups eg the false positive respectively false negative rates are the same across groups the demographics of the group of individuals receiving positive negative classification are the same as the demographics of the underlying population the positive predictive value is the same across groups By pinning down a specific objective the modeler is forced to make explicit the tradeoff between accuracy and fairness since often both cannot simultaneously be achieved Finally a generalization argument relates fairness properties captured by the joint loss on the training set to similar fairness properties on the distribution from which the data were drawn We broaden our results so as to enable the use of transfer learning to mitigate the problems of low data volume for minority groups The following observation provides a property essential for efficient decoupling A profile is a vector specifying for each group a number of positively classified examples from the training set For a given profile p pK the most accurate classifier also simultaneously minimizes the false positives and false negatives It is the choice of profile that is determined by the joint loss criterion We show that as long as the joint loss function satisfies a weak form of monotonicity one can use off-the-shelf classifiers to find a decoupled solution that minimizes joint loss The monotonicity requirement is that the joint loss is non-decreasing in error rates for any fixed profile This sheds some light on the thought-provoking impossibility results of Chouldechova and Kleinberg et al on the impossibility of simultaneously achieving three specific In contrast individual fairness Dwork et al requires that similar people are treated similarly which requires a task-specific culturally-aware similarity metric notions of group fairness see Observation in Section Finally we present experiments on datasets downloaded from The experiments are semi-synthetic in the sense that the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features We find that on many data sets our algorithm improves performance while much less often decreasing performance Remark The question of whether or not to use decoupled classifiers is orthogonal to our work which explores the mathematics of the approach and a comprehensive treatment of the pros and cons is beyond our expertise Most importantly we emphasize that decoupling together with a poor choice of joint loss could be used unfairly for discriminative purposes Furthermore in some jurisdictions using a different classification method or even using different weights on attributes for members of demographic groups differing in a protected attribute is illegal for certain classification tasks eg hiring Even barring legal restrictions the assumption that group membership is an input bit is an oversimplification and in reality the information may be obscured and the definition of the groups may be ambiguous at best Logically pursuing the idea behind the approach it is not clear which intersectionalities to consider or how far to subdivide Nonetheless we believe decoupling is valuable and applicable in certain settings and thus merits investigation The contributions of this work are a showing how when using sensitive attributes the straightforward application of many machine learning algorithms will face inherent tradeoffs between accuracy across different groups b introducing an efficient decoupling procedure that outputs separate classifiers for each class using transfer learning c modeling fair and accurate learning as a problem of minimizing a joint loss function and d presenting experimental results showing the applicability and potential benefit of our approach Related Work Group fairness has a variety of definitions including conditions of statistical parity class balance and calibration In contrast to individual fairness these conditions constrain in various ways the dependence of the classifier on the sensitive attributes The statistical parity condition requires that the assigned label of an individual is independent of sensitive attributes The condition formalizes the legal doctrine of disparate impact imposed by the Supreme Court in Griggs v Duke Power Company Statistical parity can be approximated by either modifying the data set or by designing classifiers subject to fairness regularizers that penalize violations of statistical parity see Feldman et al and references therein Dwork et al propose a fair affirmative action methodology that carefully relaxes between-group individual fairness constraints in order to achieve group fairness Zemel et al introduce a representational approach that attempts to forget group membership while maintaining enough information to classify similar individuals similarly this approach also permits generalization to unseen data points To our knowledge the earliest work on trying to learn fair classifiers from historically biased data is by Pedreschi et al see also Zliobaite et al and Kamishima et al The class-balanced condition called error-rate balance by Chouldechova or equalized odds by Hardt et al similar to statistical parity requires that the assigned label is independent of sensitive attributes but only conditional on the true classification of the individual For binary classification tasks a class-balanced classifier results in equal false positive and false negative rates across groups One can also modify a given classifier to be class-balanced while minimizing loss by adding label noise Hardt et al The well-calibrated condition requires that conditional on their label an equal fraction of individuals from each group have the same true classification A well-calibrated classifier labels individuals from different groups with equal accuracy Hebert-Johnson et al extend calibration to multi-calibration which requires the classifier to be well calibrated on a collection of sets of individuals eg all those described by circuits of a given size The class-balanced solution Hardt et al also fails to be well-calibrated Chouldechova and Kleinberg et al independently showed that except in cases of perfect predictions or equal base rates of true classifications across groups there is no class-balanced and well-calibrated classifier A number of recent works explore causal approaches to defining and detecting unfairness Nabi and Shpitser Kusner et al Bareinboim and Pearl Kilbertus et al See the beautiful primer of Pearl et al for an introduction to the central concepts and machinery Finally we mention that sensitive attributes are used in various real-world systems As one example Hassidim et al describe using such features in an admissions matching system for masters students in Israel PRELIMINARIES Let X X X be the set of possible examples partitioned by group The set of possible labels is Y and the set of possible classifications is Z A classifier is a function c X Z We assume that there is a fixed family C of classifiers For simplicity we restrict our analysis the case of binary classification Y Z but many of the results extended directly to regression or randomized classification R We suppose that there is a joint distribution D over labeled examples x y X Y and we have access to n training examples x y X Y drawn independently from D We denote by the group number to which x belongs and gi so xi Finally as is common we consider the loss Dc for an application-specific loss function Y Z R where y z accounts for the cost of classifying as z an example whose true label is y The loss for D c is defined to be or if D assigns probability to The standard approach in ML is to minimize Dc over c C Common loss functions include the L loss y z y z and L loss y z y z In Section we provide a methodology for incorporating a range of fairness notions into loss Decoupling and the cost of coupling For a vector of classifiers c c c the decoupled classifier X Z is defined to be The set of decoupled classifiers is denoted c Some classifiers such as decision trees of unbounded size over X d are already decoupled ie C As we shall see however in high dimensions common families of classifiers in use are coupled to avoid the curse of dimensionality The cost of coupling of a family C of classifiers with respect to is defined to be the worst-case maximum of the difference between the loss of the most accurate coupled and decoupled classifiers over distributions D max min cC Dc min Here S denotes the set of probability distributions over set S To circumvent measure-theoretic nuisances we require Y to be finite sets Note that numbers on digital computers are all represented using a fixed-precision bounded number of bits representation and hence all these sets may be assumed to be of finite but possibly exponentially large size We now show that the cost of coupling is related to fairness across groups Lemma Suppose Then there is a distribution D such that no matter which classifier c C is used there will always be a group and a classifier c C whose loss is at least smaller than that of c ie Proof Let be a decoupled classifier with minimal loss where c c c This loss is a weighted average weighted by demography of the average loss on each group Hence for any c there must be some group on which the loss of is less than that of c Hence if the cost of coupling is positive then the learning algorithm that selects a classifier faces an inherent tradeoff in accuracy across groups The following theorem shows that the cost of coupling is large a constant for linear classifiers and decision trees similar arguments exist for other common classifiers All remaining proofs are deferred to the full version Theorem Fix X d Y and groups encoded by the last bit of x Then the cost of coupling is at least for Linear regression Z R C x b Rd b R and y z y z Linear separators Z C x b Rd b R and y z y z Bounded-size decision trees For Z C being the set of binary decision trees of size s leaves and y z y z We note that it is straightforward to extend the above theorem to generalized linear models ie functions x for monotonic functions u R R which includes logistic regression as one common special case It is also possible though more complex to provide a lower bound on the cost of coupling of neural networks regression forests or other complex families of functions of bounded representation size s In order to do so one needs to simply show that the sizes functions are sufficiently rich in that there are two different size-s classifiers c c c such that has loss say over the uniform distribution on X but that every single size-s classifier has significant loss Joint loss and monotonicity As discussed the classifications output by an ML classifier are often evaluated by their empirical loss n i To account for fairness we generalize loss to joint classifications across groups In particular we consider an application-specific joint loss L Y Z R that assigns a cost to a set of classifications where indicates the group number for each example A joint loss might be for parameter L gi n n i n i The above L trades off accuracy for differences in number of positive classifications across groups For this is simply L loss while for the best classifications would have an equal number of positives in each group Joint loss differs from a standard ML loss function in two ways First joint loss is aware of the sensitive group membership Second it depends on the complete labelings and is not simply a sum over labels Even with only group this captures situations beyond what is representable by the sum A simple example is when one seeks exactly P positive examples L gi if P otherwise Since n the ensures that the loss minimizer will have exactly P positives if such a classifier exists in C for the data In this section we denote joint loss L with the hat notation indicating that it is an empirical approximation In the next section we will define joint loss L for distributions We denote classifications by rather than the standard notation which suggests predictions because as in the above loss one may choose classifications z y even with perfect knowledge of the true labels For the remainder of our analysis we henceforth consider binary labels and classifications Y Z Our approach is general however and our experiments include regression For a given xi and for any group and all y z recall that the groups are gi and define counts nk i gi n profile pk group losses Note that the normalization is such that the standard loss is nk n and the fraction of positives within any class is n nk pk We note many studied fairness notions including numerical parity demographic parity and false-negative-rate parity can be represented in a joint loss function For example demographic parity is pk pk n nk In many applications there is a different cost for false positives where y z and false negatives where y z The fractions of false positives and negatives are defined below for each group They can be computed based on the fraction of positive labels in each group pk n nk pk n nk While minimizing group loss in general does not minimize false positives or false negatives on their own the above implies that for a fixed profile pk the most accurate classifier on group simultaneously minimizes false positives and false negatives The above can be derived by adding or subtracting the equations since every error is a false positive or a false negative and n nk pk since every positive classification is either a false positive or true positive and the fraction of true positives from group are We also define the false negative rate False positive rates can be defined similarly Equations and imply that if one desires fewer false positives and false negatives all other things being fixed then greater accuracy is better That is for a fixed profile the most accurate classifier simultaneously minimizes false positives and false negatives This motivates the following monotonicity notion Definition Monotinicity Joint loss L is monotonic if for any fixed gi Y L can be written as where c R is a function that is nondecreasing in each fixing all other inputs to c That is for a fixed profile increasing can only increase joint loss To give further intuition behind monotonicity we give two other equivalent definitions Definition Monotonicity Joint loss L is monotonic if for any gi Z and any i where gi gj and swapping and can only increase loss ie where z is the same as z except and We can see that if then swapping and does not change the loss because the condition can be used in either order This means that the loss is semi-anonymous in the sense that it only depends on the numbers of true and false positives and negatives for each group The more interesting case is when where it states that the loss when is no greater than the loss when Finally monotonicity can also be defined in terms of false positives and false negatives Definition Monotonicity Joint loss L is monotonic if for any gi Z and any alternative classifications z z n such that in each group the same profile as z but all smaller or equal false positive rates and all smaller or equal false negative rates the loss of classifications is no greater than that of Lemma Definitions and of Monotonicity are equivalent One may be tempted to consider a simpler notion of monotonicity such as requiring the loss with to be no greater than that of fixing everything else However this would rule out many natural monotonic joint losses L such as demographic parity Discussion fairness metrics versus objectives The monotonicity requirement admits a range of different fairness criteria but not all We do not mean to imply that monotonicity is necessary for fairness but rather to discuss the implications of minimizing a non-monotonic loss objective The following example helps illustrate the boundary between monotonic and non-monotonic Observation Fix The following joint loss is monotonic if and only if The loss in the above lemma trades off accuracy for differences in loss rates between groups What we see is that monotonic losses can account to a limited extent for differences across groups in fractions of errors and related statements can be made for combinations of rates of false positive and false negative inspired by equal odds definitions of fairness However when the weight on the fairness term exceeds then the loss is non-monotonic and one encounters situations where one group is punished with lower accuracy in the name of fairness This may still be desirable in a context where equal odds is a primary requirement and one would rather have random classifications eg a lottery than introduce any inequity What is the goal of an objective function We argue that a good objective function is one whose optimization leads to favorable outcomes and should not be confused with a fairness metric whose goal is quantify unfairness Often a different function is appropriate for quantifying unfairness than for optimizing it For example the difference in classroom performance across groups may serve as a good metric of unfairness but it may not be a good objective on its own The root cause of the unfairness may have begun long before the class Now suppose that the objective from the above observation was used by a teacher to design a semester-long curriculum with the best intention of increasing the minority groups performance to the level of the majority If there is no curriculum that in one semester increases one groups performance to the level of another groups performance then optimizing the above loss for leads to an undesirable outcome the curriculum would be chosen so as to intentionally misteach students the higher-performing group of students so that their loss increases to match that of the other group This can be see by rewriting the loss as follows This rewriting illuminates why is necessary for monotonicity otherwise there is a negative weight on the total loss corresponds to maximizing the minimum performance across groups while means teaching to the average and in between allows interpolation However putting too much weight on fairness leads to undesirable punishing behavior Minimizing joint loss on training data Here we show how to use learning algorithm to find a decoupled classifier in that is optimal on the training data In the next section we show how to generalize this to imperfect randomized classifiers that generalize to examples drawn from the same distribution potentially using an arbitrary transfer learning algorithm Our approach to decoupling uses a learning algorithm for C as a black box A C-learning algorithm A X Y C returns one or more classifiers from C with differing numbers of positive classifications on the training data ie for any two distinct c c A xi i i c xi In ML it is common to simultaneously output classifiers with varying number of positive classifications eg in computing ROC or precision-recall curves Davis and Goadrich Also note that a classifier that purely minimizes errors can be massaged into one that outputs different fractions of positive and negative examples by reweighting or subsampling the positive- and negative-labeled examples with different weights Our analysis will be based on the assumption that the classifier is in some sense optimal but importantly note that it makes sense to apply the reduction to any off-the-shelf learner Formally we say A is optimal if for every achievable number of positives P i c C it outputs exactly one classifier that classifies exactly P positives and this classifier has minimal error among all classifiers which classify exactly P positives Theorem shows that an optimal classifier can be used to minimize any monotonic joint loss Theorem For any monotonic joint loss function L any C and any optimal learner A for C Algorithm Decouple A L xi Xi Minimize training loss L using learner A For to A xi Learner outputs a set of classifiers return that minimizes L gi The simple decoupling algorithm partitions data by group and runs the learner on each group Within each group the learner outputs one or more classifiers of differing numbers of positives the Decouple procedure from Algorithm returns a classifier in of minimal joint loss L For constant Decouple runs in time linear in the time to run A and polynomial in the number of examples n and time to evaluate L and classifiers c C Implementation notes Note that if the profile is fixed as in Lp then one can simply run the learning algorithm once for each group targeted at pk positives in each group Otherwise also note that to perform the slowest step which involves searching over losses of combinations of classifiers one can pre-compute the error rates and profiles of each classifier In the big data regime of very large n the evaluations of a simple numeric function of profile and losses will not be the rate limiting step Generalization and transfer learning We now turn to the more general randomized classifier model in which Z but still with Y and we also consider generalization loss as opposed to simply training loss We will define loss in terms of the underlying joint distribution D over X Y from which training examples are drawn independently We define the true error true profile and true probability pk E E y z x Joint loss L is defined on the joint distribution on g y z Y Z induced by D and a classifier c X Z A distributional joint loss L is related to empirical joint loss L in that L limn EL ie the limit of the empirical joint loss as the number of training data grows without bound if it exists Fixing the marginal distribution over joint loss L R can be viewed as a function of p pK in addition to group probabilities which are independent of the classification In addition to requiring monotonicity namely L being nondecreasing in fixing all other parameters we will assume that L is continuous with a bound on the rate of change of the form L p pK L p p R pk pk for parameter R and all pk p Note that the in the above bound is necessary for our analysis because a loss that depends on without may require exponentially large quantities of data to estimate and optimize over if is exponentially small Of course alternatively could be removed from this assumption by imposing a lower bound on all Many losses such as L and above can be shown to satisfy this continuity requirement for R and R respectively We also note that the reduction we present can be modified to address certain discontinuous loss functions For instance for a given target allocation ie a fixed fraction of positive classifications for each group one simply finds the classifier of minimal empirical error for each group which achieves the desired fraction of positives as closely as possible A transfer learning algorithm for C is A X X C where A takes in-group examples xi and out-group examples xi i both from X This is also called supervised domain adaptation The distribution of out-group examples is different from but related to the distribution of in-group samples The motivation for using the out-group examples is that if one is trying to learn a classifier on a small dataset accuracy may be increased using related data Algorithm GD T L xi Xi For to nk i n xi T xi xi Run transfer learner output is a set For all c n Estimate profile kc nk Estimate error rates return for c L The general decoupling algorithm uses a transfer learning algorithm T In the next section we describe and analyze a simple transfer learning algorithm that downweights samples from the out-group For that algorithm we show Theorem Suppose that for any two groups and any classifiers c c C jc jc kc kc For algorithm with the transfer learning algorithm described in Section with probability over the n training data the algorithm outputs c with Lc at most min cC Lc R min where the run-time of the algorithm is polynomial in n and the runtime of the optimizer over C The assumption in states that the performance difference between classifiers is similar across different groups and is weaker than an assumption of similar classifier performance across groups Note that it would follow from a simpler but stronger requirement that jc kc by the triangle inequality Parameter settings see Lemma and tighter bounds can be found in the next section However we can still see qualitatively that as n grows the bound decreases roughly like as expected We also note that for groups with large as we will see in the next section the transfer learning algorithm places weight on and hences ignores the out-group data For small the algorithm will place significant weight on the out-group data A transfer learning algorithm T In this section we describe analyze a simple transfer learning algorithm that down-weights out-group examples by parameter To choose we can either use cross-validation on an independent held-out set or can be chosen to minimize a bound as we now describe The cross-validation which we do in our experiments is appropriate when one does not have bounds at hand on the size of set of classifiers or the difference between groups as we shall assume or when one simply has a black-box learner that does not perfectly optimize over C We now proceed to derive a bound on the error that will yield a parameter choice Consider to be fixed For convenience we write nk as the number of samples from other groups Define and analogously to and for out-of-group data xi Instead of outputting a set of classifiers one for each different number of positives within group it will be simpler to think of the profile pk P as being specified in advance and we hence focus our attention on the subset of classifiers c C n P which depends on the training data The bounds in this section will be uninteresting of course when is empty eg in the unlikely event For very small the term is negative making the left side of the above min imaginary in which case we define the min to be the real term on the right If the learning algorithm doesnt support weighting subsampling can be used instead that x x the only realizable pk of interest are and The general algorithm will simply run the subroutine described in this section nk n times once for each possible value of pk Of course C As before we will assume that the underlying learner is optimal meaning that given a weighted set of examples x y with total weight wi it returns a classifier c that has minimal weighted error wi among all classifiers in In Appendix A we derive a closed-form solution for the approximately optimal downweighting of out-group data for our transfer learning algorithm This solution depends on a bound on the difference in classifier ranking across different groups For small the difference in error rates of each pair of classifiers is approximately the same for in-group and outgroup data In this case we expect generalization to work well and hence For large outgroup data doesnt provide much guidance for the optimal in-group classifier and we expect For a fixed and let c be a classifier that minimizes the empirical loss when out-of-group samples are down-weighted by ie c min nk kc and c be an optimal classifier that minimizes the true loss ie c min kc We would like to choose such that kc is close to kc In order to derive a closed-form solution for in terms of we use concentration bounds to bound the expected error rates of c and c in terms of and and then choose to minimize this expression We find that as long as nk the optimal choice of will be strictly in between and Experiment For this experiment we used data that is semisynthetic in that the datasets are real In practice classification learning algorithms generally learn a single real-valued score and consider different score thresholds decoupled loss/blind loss co u p le d l o lin d l o Comparison of joint loss across datasets Figure Comparing the joint loss of our decoupled algorithm with the coupled and blind baselines Each point is a dataset A ratio less than means that the loss was smaller for the decoupled or coupled algorithm than the blind baseline ie that using the sensitive feature resulted in decreased error Points above the diagonal represent datasets in which the decoupled algorithm outperformed the coupled one downloaded from openmlorg but an arbitrary binary attribute was used to represent a sensitive attribute so The base classifier was chosen to be least-squares linear regression for its simplicity no parameters speed and reproducibility In particular each dataset was a univariate regression problem with balanced loss for squared error ie LB where To gather the datasets we first selected the problems with twenty or fewer dimensions Classification problems were converted to regression problems by assigning y to the most common class and y to all other classes Regression problems were normalized so that y Categorical attributes were similarly converted to binary features by assigning to the most frequent category and to others The sensitive attribute was chosen to be the first binary feature such that there were at least examples in both groups both and values Further large datasets were truncated so that there were at most examples in each group If there was no appropriate sensitive at decoupled loss/blind loss d e co u p le d l o lin d l o it o u t tr a n sf e r Comparison of joint loss across datasets Figure Comparing the joint loss of our decoupled algorithm with the decoupled algorithm with and without transfer learning Each point is a dataset A ratio less than means that the loss was smaller for the decoupled algorithm than the blind baseline Points above the diagonal represent datasets in which transfer learning improved performance compared to decoupling without transfer learning tribute then the dataset was discarded We also discarded a small number of trivial datasets in which the data could be perfectly classified less than error with linear regression The openml ids and detailed error rates of the remaining datasets are listed in the appendix All experiments were done with five-fold crossvalidation to provide an unbiased estimate of generalization error on each dataset Algorithm was implemented where we further used five-fold cross validation within each of the outer folds to choose the best down-weighting parameter for each group Hence least-squares regression was run times on each dataset to implement our algorithm The baselines were considered the blind baseline is least-squares linear regression that has no access to the sensitive attribute the coupled baseline is least-squares linear regression that can take into account the sensitive attribute Figure compares the loss of the coupled baseline x-axis and our decoupled algorithm yaxis to that of the blind baseline In particular the log ratio of the squared errors is plotted as this quantity is immune to scaling of the y values Each point is a dataset Points to the left of x represent datasets where the coupled classifier outperformed the blind one Points below the horizontal line y represent points in which the decoupled algorithm outperformed the indiscriminate baseline Finally points above the diagonal line x y represent datasets where the decoupled classifier outperformed the coupled classifier Figure compares transfer learning to decoupling without any transfer learning ie just learning on the in-group data or setting As one can see on a number of datasets transfer learning significantly improves performance In fact without transfer learning the coupled classifiers significantly outperform decoupled classifiers on a number datasets Image retrieval experiment In this section we describe an anecdotal example that illustrates the type of effect the theory predicts where a classifier biases towards minority data that which is typical of the majority group We hypothesized that standard image classifiers for two groups of images would display bias towards the majority group and that a decoupled classifier could reduce this bias More specifically consider the case where we have a of images and want to learn a binary classifier c X We hypothesized that a coupled classifier would display a specific form of bias we call majority feature bias such that images in the minority group would rank higher if they had features of images in the majority group We tested this hypothesis by training classifiers to label images as suit or no suit We constructed an image dataset by downloading the suit suit of clothes synset as a set of positives and male person and female person synsets as the negatives from ImageNet Deng et al We manually removed images in the negatives that included suits or were otherwise outliers and manually classified suits as male or female removing suit images that were neither We used the pre-trained BVLC CaffeNet model which is similar to the AlexNet mode from Krizhevsky et al to generate features for the images and clean the dataset We Linear classifier Decoupled linear classifiers Figure Differences between image classifications of suit using standard linear classifiers and decoupled classifiers trained using standard neural network image features The females selected by the linear classifier are wearing a tuxedo and blazer more typical of the majority male group used the last fully connected of layer fc of the CaffeNet model as features and removed images where the most likely label according to the CaffeNet model was envelope indicating that the image was missing or suit suit of clothes or bow tie bow-tie bowtie from the negatives The dataset included suit images male female and no suit images male female We then trained a coupled and decoupled standard linear support vector classifier SVC on this dataset and provide anecdotal evidence that the decoupled classifier displays less majority feature bias than the coupled classifier We trained the coupled SVC on all images and then ranked images according to the predicted class We trained decoupled SVCs with one SVC trained on the male positives and all negatives and the other on female positives and all negatives Both classifiers agreed on eight of the top ten females predicted as suit and Fig shows the four images two per classifier that differed One of the images found by the coupled classifier is a woman in a tuxedo typically worn by men which may be an indication of majority feature bias adding a binary gender attribute to the coupled classifier did not change the top ten predictions for female suit We further note that we also tested both the coupled and decoupled classifier on out-of-sample predictions using -fold cross-validation and that both were highly accurate both had accuracy with the coupled classifier predicting one additional true positive We emphasize that we present this experiment to provide an anecdotal example of the potential advantages of a decoupled classifier and we do not make any claims on generalizability or effect size on this or other real world datasets because of the small sample size and the several manual decisions we made Conclusions In this paper we give a simple technical approach for a practitioner using ML to incorporate sensitive attributes Our approach avoids unnecessary accuracy tradeoffs between groups and can accommodate an application-specific objective generalizing the standard ML notion of loss For a certain family of weakly monotonic fairness objectives we give a black-box reduction that can use any off-the-shelf classifier to efficiently optimize the objective In contrast to much prior work on ML which first requires complete fairness this work requires the application designer to pin down a specific loss function that trades off accuracy for fairness Experiments demonstrate that decoupling can reduce the loss on some datasets for some potentially sensitive features