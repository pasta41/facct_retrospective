From Soft Classifiers to Hard Decisions How fair can we be A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a calibrated non-binary scoring classifier and then to post-process this score to obtain a binary decision We study various fairness or error-balance properties of this methodology when the non-binary scores are calibrated over all protected groups and with a variety of post-processing algorithms Specifically we show First there does not exist a general way to post-process a calibrated classifier to equalize protected groups positive or negative predictive value PPV or NPV For certain nice calibrated classifiers either PPV or NPV can be equalized when the postprocessor uses different thresholds across protected groups Still when the post-processing consists of a single global threshold across all groups natural fairness properties such as equalizing PPV in a nontrivial way do not hold even for nice classifiers Second when the post-processing stage is allowed to defer on some decisions that is to avoid making a decision by handing off some examples to a separate process then for the non-deferred decisions the resulting classifier can be made to equalize PPV NPV false positive rate FPR and false negative rate FNR across the protected groups This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al which preclude equalizing all of these measures simultaneously We also present different deferring strategies and show how they affect the fairness properties of the overall system We evaluate our post-processing techniques using the COMPAS data set from CONCEPTS Computing methodologies Machine learning KEYWORDS algorithmic fairness classification post-processing INTRODUCTION The concept of fairness is deeply ingrained in our psyche as a fundamental essential ingredient of Human existence Indeed the perception of fairness broadly construed as accepting each others equal right for well being is arguably one of the most basic tenets of cooperative societies of individuals in general However as fundamental as this concept may be it is also elusive different cultures have developed very different notions of fairness and equality among individuals subject to religious ethical and social beliefs in particular the intricate interplay between fairness and justice is often left to subjective interpretation In the context of decision processes fairness is further complicated by the fact that decisions are often made with incomplete information and limited resources These two factors have become increasingly prominent as society grows and decision processes become more complex and algorithmic One way that researchers are responding to these growing concerns is by attempting to formulate precise notions for fairness of decisions processes eg While these notions do not intend to capture the complexities of the ethical socio-economic or religious aspects of fairness they do consider the fairness aspects of statistical decision-making processes with incomplete information Essentially these notions accept the fact that a decision process with incomplete information will inevitably make errors relative to the desired full-information notion which is treated as a given and provide guidelines on how to balance the errors fairly across individuals or groups of individuals These definitions have proven to be meaningful and eye opening in particular it has been demonstrated that some very natural measures of fair distribution of error are mutually inconsistent No decision mechanism with incomplete information can satisfy all except in trivial cases Faced with this basic impossibility we aim to better understand the process of decision making with incomplete information and propose ways to relax the known measures so as to regain feasibility Specifically we concentrate on the task of post-processing a calibrated soft classifier to obtain a binary decision under group fairness constraints for the case of several disjoint protected groups That is we consider the following two-stage mechanism The first stage consists of constructing a classifier S that outputs for each individual x a score s that is related to the chance that x has property B The only requirement we make of S is group-wise calibration For each group and for each s the fraction of individuals in that get score s and have the property out of all individuals in that get score s is s The second stage takes as input the output s of the first stage and the group to which x belongs and outputs a binary decision interpreted as its guess at whether x has property B The first stage is aimed at gathering information and providing the best accuracy possible with only minimal regard to fairness ie only group-wise calibration The second stage extracts a decision from the information collected in the first stage while making sure that the errors are distributed fairly To further focus our study we take the first stage as a given and concentrate on the second That is we consider the problem of post-processing the scores given by the calibrated soft classifier S into binary predictions A representative example is a judge making a bail decision based on a score provided by a software package Following we consider the following four performance measures for the resulting binary classifier the positive predictive value PPV namely the fraction of individuals that have the property among all individuals that the classifier predicted to have the property The false positive rate FPR namely the fraction of individuals that were predicted to have the property among all individuals that dont have the property The negative predictive value NPV and false negative rate FNR which are defined analogously Ideally we would like to equalize each one of the four measures across the groups ie the measure will have the same value when restricted to samples from each group Unfortunately however we know that this is impossible in general This leads us to a broad question that motivates our work Under what conditions can we post-process a calibrated soft classifiers outputs so that the resulting hard classifier equates a subset of FNR FPR across a set of protected groups How can we balance these conflicting goals Results Post-Processing With Thesholds In a first set of results we consider the properties obtained by post-processing via a threshold mechanism Naively a threshold post-processing mechanism would return for individual x whenever the calibrated score is above some fixed threshold and return otherwise We somewhat extend this mechanism by allowing the post-processor fine-tune its decision by choosing the output probabilistically whenever the result of the soft classifier is exactly the threshold We first observe that the popular and natural pos-t-procesing method of using a single threshold across all groups has some inherent deficiency No such mechanism can in general guarantee equality of either PPV or NPV across the protected groups We then show that when using different thresholds for the different groups one can equalize either PPV or NPV but not both across the two groups assuming the profile of S has some nondegeneracy property The combination of the impossibility of single threshold and the possibility of per-group threshold also stands in contrast to the belief that a soft classifier that is calibrated across both groups allows ignoring group-membership information in any post-processing decision Indeed the conversion to a binary decision loses information in different ways for the two groups and so group membership becomes relevant again after post-processing Results Adding deferrals For the second set of results we consider post-processing strategies that do not always output a decision Rather with some probability the output is or I dont know which means that the decision is deferred to another hopefully higher quality even if more expensive process Let us first present our technical results and then discuss potential interpretations and context The first strategy is a natural extension of the per-group threshold we use two thresholds per group returning above the right threshold below the left threshold and between the thresholds We show that there always exists a way to choose the thresholds such that conditioned on the decision not being both the PPV and NPV are equal across groups Next we show a family of post-processing strategies where conditioned on the decision not being all four quantities PPV NPV FPR FNR are equal across groups All strategies in this family have the following structure Given an individual x the strategy first makes a randomized decision whether to defer on x where the probability depends on and the group membership of x If not deferred then the decision is made via another post-processing technique One method for determining the probabilities of deferral is to make sure that the profiles of scores returned by the calibrated soft classifier conditioned on not deferring is equal for the two groups That is let ps denote the probability restricted to group that an element gets score s conditioned on not deferring Then for any s we choose deferral probabilities so that ps ps The resulting classifier can then be post-processed in any group-blind way say via a single threshold mechanism as described above Of course the fact that all four quantities are equalized conditioned on not deferring does not in and of itself provide any guarantees regarding the fairness properties of the overall decision process which includes also the downstream decision mechanism For one it would be naive to simply assume that fairness composes Furthermore the impossibility of says that the overall decision-making process cannot possibly equalize all four measures However in some cases one can provide alternative non-statistical justification for the fairness of the overall process For instance if the downstream decision process never errs the overall process might be considered procedurally fair We present more detailed reflections on our deferral-based approach in Section We note that deferring was considered in machine learning in a number of contexts including the context of fairness-preservation In these works the classifier typically defers only when its confidence regarding some decision is low By contrast we use deferrals in order to equalize the probability mass functions of the soft classifier over the two groups which may involve deferring on individuals for whom there is higher confidence Furthermore our framework allows for a wide range of deferral strategies which might be used to promote additional goals Pursuing alternate strategies for deferral is an interesting direction for future work Experimental results We demonstrate the validity of our methodology on the Broward county dataset with COMPAS scores made public by ProPublica Indeed it has been shown that the COMPAS scoring mechanism is an approximately calibrated soft classifier We first ran our two-threshold post-processing mechanism and obtained a binary decision algorithm which equalizes both PPV and NPV across Caucasians and African-Americans We then ran our post-processing mechanism with deferrals to equalize all four of PPV NPV FPR FNR across the two groups with three different methods for deciding how to defer In the first method decisions are deferred only for Caucasians in the second decisions are deferred only for African Americans in the third method decisions are deferred for an equal fraction of Caucasians and of African Americans This fraction is precisely equal to the statistical total variation distance between the profiles of scores produced by the soft classifier on the two groups More details about the results are given in Section Related work We briefly describe the works most closely related to ours though both the list of works and their summaries are inevitably too short Our work fits in a research program on group fairness notions following the work of Chouldechova and Kleinberg et al Our work considers the notions of calibration as formalized in and those of PPV NPV FPR and FNR from and The power of post-processing calibrated scores into decisions using threshold classifiers in the context of fairness has been previously studied by Corbett-Davies Pierson Feller Goel and Huq As in our work they show that it is feasible to equalize certain statistical fairness notions across groups using possibly different thresholds They additionally show that these thresholds are in some sense optimal Whereas focuses on statistical parity conditional statistical parity and false positive rate our most comparable results consider PPV In our work we further show that in some cases thresholds fail to equalize both PPV and NPV called predictive parity by unless we also allow our post-processor to defer on some inputs Our work also studies methods of post-processing that are much more powerful than thresholding especially when allowing deferrals Using deferrals to promote fairness was also considered by Zemel Madras and Pitassi Specifically they consider how deferring on some inputs may promote a combination of accuracy and fairness especially when taking explicit account of the downstream decision maker They make use of two-threshold deferring post-processors like those discussed in Section takes a more experimental approach and focuses on minimizing the disparate impact a measure of total difference in classification error between groups while maximizing accuracy One important difference between our works is that Madras et al distinguish between rejecting and deferring Rejecting is oblivious as to properties of the downstream decision maker while deferring tries to counteract the biases of the decision maker Our work considers only the former notion but uses the term defer instead of reject Our work inherits both the strengths and weakness of the group fairness research program The clear definitions and goals of group fairness have have catalyzed the field and caused rapid progress early infeasibility results positive results for complex and intersecting collections of groups and extensions to the basic model including and this work The formalization of group fairness has fostered precise discussion and greater understanding including of its shortcomings Group fairness notions have been criticized for not fully capturing the complex social goals that motivate our communitys interest in fairness failure to compose failure to adequately capture peoples wellbeing and failure to prevent against certain social evils However we are optimistic that improving our understanding of group fairness will contribute to the successful study of algorithmic fairness generally PRELIMINARIES We study the problem of binary classification An instance is an element usually denoted x of a universe X We restrict our attention to instances sampled uniformly at random from the universe denoted X X Our theory extends directly to any other distribution on X that distribution does not need to be known to the classifiers Each instance x is associated with a true type Y x Each instance x is also associated with a group G where G is the set of groups We restrict our attention to sets G that form a partition of the universe X We denote by the set of instances x in group and by the random variable distributed uniformly over Note that for any events E and E E E Definition Base rate The base rate of a group G is PrY When X is finite is simply the fraction of individuals x in the group for whom Y x A classifier is a randomized function with domain X G A hard classifier denoted Y outputs a prediction in interpreted as a guess of the true type Y x A soft classifier denoted S outputs a score s interpreted as a measure of confidence x We restrict our attention to soft classifiers with finite image We call a classifier group blind if its output is independent of the input group For all groups G we call a hard classifier Y non-trivial on if PrY and PrY Hard classifiers are trivial on if they are not non-trivial on A post-processor is a randomized function with domain G As with classifiers a post-processor can be hard or soft A hard post-processor denoted D outputs a prediction in A soft post-processor denoted outputs a score s Observe that for a soft classifier S D S is a hard classifier and S is x X s y soft post-processor hard Y soft S hard post-processor D Figure We call a classifier that returns results in a soft classifier to differentiate it from those which return results in which we call hard classifiers We refer to classifiers that take as input the output of a soft classifier as post-processors a soft classifier As with classifiers we call a post-processor group blind if its output is independent of the group and we restrict our attention to post-processors with finite image The restriction to finite image is for mathematical convenience and also because digital memory leads to discrete universes our results generalize to infinite images as well In Section we expand the definitions of both classifier and post-processors to allow an additional input or output the special symbol indicating a deferral Calibration Several works concerning algorithmic fairness focus on various notions of calibration The following calibration notions are defined only over soft classifiers Definition Calibration Soft We say a soft classifier S is calibrated if s for which s Pr Y X s E Y X s s The probability above is taken over the sampling of X as well as random choices made by S at classification time Definition Groupwise Calibration Soft We say that a soft classifier S is groupwise calibrated if it is calibrated within all groups That is G and s for which s we have that PrY s s Groupwise calibration is essentially the same notion as multi-calibration with the difference that in their case the true types are values in We use a different term to emphasize that we restrict our attention to collections of groups G that form a partition of the universe X The two definitions above are stated for soft classifiers whose output distribution is discrete since we must be able to condition on the event s or s That said it extends naturally to classifiers with continuously-distributed outputs provided that the conditional probabilities are well defined Accuracy Profiles Throughout this work we make repeated reference to the probability mass function of the random variable for a calibrated soft classifier S acting on a randomly distributed input We call this distribution on calibrated scores an accuracy profile Definition Accuracy Profile The accuracy profile of a calibrated soft classifier S for a group denoted by is the PMF of That is for s s Abusing notation we denote by the collection and call it the of S We denote by the support of the namely the set s x r st r s An accuracy profile is a distribution of scores for a calibrated classifier S Because S is calibrated the conveys information about the performance of S and is constrained by properties of the underlying distribution on X For example the expectation is exactly the base rate for the population Proposition Proof in For any groupwise calibrated soft classifier S for all groups G Accuracy profiles also provide useful geometric intuition for reasoning about the effects of post-processing calibrated scores We elaborate on this in Section see Figure Group Fairness Measures Several well-studied measures of statistical fairness eg look at how the following key performance measures of a classifier differ across groups We define these statistics formally Definition For a hard classifier Y and group we define the false positive rate of Y for PrY Y false negative rate of Y for PrY Y positive predictive value of Y for PrY Y negative predictive value of Y for PrY Y The probability statements in the definitions above reflect two sources of randomness the sampling of from the group and any random choices made by the classifier Y Among previous works some focus on equalizing only one or both of the false positive rates and false negative rates across groups called balance for the negative and positive classes respectively Equalizing positive and negative predictive value across groups is often combined into one condition called predictive parity We split the value out to be a separate condition for the positive and negative predictive classes Predictive parity appears to be a hard-classifier analogue of calibration both can be interpreted as saying that the output of the classifier hard or soft contains all the information contained in group membership Our results highlight that the relationship between these notions is more subtle than it first appears see Section for further discussion THE LIMITS OF POST-PROCESSING Suppose throughout this section that S is a groupwise calibrated soft classifier Our goal in this section is to make binary predictions based on and possibly the subject to equalizing PPV among groups That is we wish to make a prediction using a hard post-processor D such that Y D S equalizes PPV among groups We chose to concentrate first on the limitations of equalizing PPV rather than FPR and FNR due to the conceptual similarity of PPV to calibration and we address NPV in Also the case of equalizing false positive rates with thresholds is addressed in Figure Accuracy profiles definition yield useful geometric intuitions which come from the calibration property definition The dashed line is the multiplied by the y x line the region below this line shows the expected positives and above shows the expected negatives With a threshold the expected PPV NPV FPR and FNR can be seen visually Fairness Conditions for Post-Processors We begin by making a simple observation about post-processing that provides some geometric intuition for the rest of this section Just as in Proposition we can express succinctly in terms of conditional expectations over the We state this formally in Proposition Proposition Proof in Let Y D S be a hard classifier that is non-trivial for all G where S is groupwise calibrated with respect to G For any G we have Y This characterization of PPV and NPV in terms of conditional expectations lets us geometrically see how certain post-processing decision rules will interact with the for a group For example Figure shows the expected true positives true negatives false positive and false negatives when using a threshold General impossibility of equalizing PPV It is not always possible to directly post-process a soft groupwise calibrated classifier into a hard one with equalized PPV or NPV for all groups as we demonstrate by counterexample in Proposition Before proceeding we note that our counterexample is somewhat contrived in particular the induced by the soft classifier S in the proof of Proposition takes only one value on each group When the of S is more nicely structured on each group we will see that there are general methods to equalize PPV or NPV Proposition Fix two disjoint groups and with respective base rates and such that Then there exists a soft classifier S that is groupwise calibrated but for which there is no post-processor D G such that D S equalizes PPV unless for i or Proof of Proposition Consider the classifier S such that if x and if x This classifier is trivially groupwise calibrated Since for i and we conclude that is well-defined for and The proof now follows from the characterization of PPV in Proposition This is because is equal to the expectation of where X is drawn from a distribution with support contained in and hence it is equal to and A Niceness Condition for Motivated by the above proof we define a non-degeneracy condition on Definition Niceness of Let G be a set of groups An accuracy profile is nice if is the same for all G Note that this condition rules out the counterexample given by Proposition since the in the counterexample had different in fact disjoint supports for different groups Equalizing PPV or NPV by Thresholding We pay special attention to thresholds because they are simple to understand and therefore very widely used We use one slight modification to deterministic thresholds that adds an element of randomness if a score is at the threshold we randomly determine which side of the threshold it falls on according to a distribution defined below Definition Threshold Post-Processor A threshold post-processor R G is a function from a score s and a group G parameterized by and R The threshold parameter G specifies the threshold for the group and R G is the probability of returning when the input score s is on the threshold It returns the following outputs s s else s In the setting of an infinite number of scores and a continuous domain ie scores are represented by a probability density function instead of a probability mass function we can use purely deterministic threshold functions in which R and achieve very similar results for the rest of this section We now study the effectiveness of thresholds for post-processing soft classifiers with nice The main takeaways are If the is nice threshold post-processors can equalize PPV However group blind threshold post-processors are rather limited in their ability to equalize PPV Furthermore equalizing PPV with thresholds group blind or otherwise may have undesirable social consequences see Figure Thresholds cannot always equalize PPV and NPV simultaneously even for nice Proposition Results also apply to NPV We delegate formal statements and proofs for Results to the full version Result shows that threshold post-processors are inherently limited even when they factor in groups Proposition Proof in Fix groups and There exists a soft classifier S with a nice such that no threshold postprocessor can simultaneously equalize PPV and NPV between groups and Equalizing Accuracy Profiles While thresholding is a conceptually simple approach to postprocessing a soft classifier its power is limited We now consider a very different approach using soft post-processors to equalize the across groups of a soft classifier The intuition is that if the Figure The PPV for both groups is However the threshold for dark blue is higher than the threshold for orange even though has a higher base rate are equal across groups then any hard post-processor that is group blind should result in equal PPV NPV FPR and FNR We formalize this intuition in Claim Let S be a soft classifier and for each group G let be the of S for group For a soft post-processor let S S and let be the corresponding for group Our goal is to find a soft post-processor such that S is groupwise calibrated and for all G In this section we describe only one approach to constructing which we call mass averaging The approach of equalizing has a fundamental weakness if and both are calibrated then This severely limits applicability of this approach However this limitation will removed in Section by allowing deferrals Claim Proof in If the are equal for two groups then PPV NPV FPR and FNR are equalized by any hard post-processor D satisfying group blindness The group-blindness requirement in the claim is necessary consider the not group blind post-processor that outputs on one group and on the other PPV will not be equalized Mass Averaging The mass-averaging technique is best illustrated with an example Suppose that is uniform over and is uniform over It is easy to define a soft postprocessor which equalizes these two On we leave the score unchanged s On we compute the output as s The for groups and of the resulting soft classifier S S are equal and are equal to In the example the probability mass is being redistributed by averaging the scores This can be equivalently viewed as adding noise to the scores and then recalibrating the scores something discussed in More generally a mass-averaging post processor assigns to each possible pair a distribution over possible output scores s Such a is fully specified by G parameters where is the number of possible values of s and is the number of possible values of s Given a soft classifier S and a mass-averaging post processor the constraint that the resulting are equalized across groups is linear in these parameters Such classifiers therefore may be found by a linear program We do not explore the choice of mass-averaging post-processors further POST-PROCESSING CALIBRATED CLASSIFIERS WITH DEFERRALS In the first part of the paper we considered the problem of postprocessing calibrated soft classifiers which output a score s into fair hard classifiers which output a decision in y subject to a number of group fairness conditions In the remainder of this work we reconsider this problem but with one important change we allow classifiers to refuse to decide by outputting the special symbol We call such classifiers deferring classifiers borrowing the nomenclature from The output is the deferring classifiers way of refusing to make a decision and deferring to a downstream decision maker For example a risk assessment tool might aid a parole board to make a decision by categorizing an individual as high risk or low risk or it might output providing no advice and deferring to the judgment of the board We now modify our notation appropriately Instances x are still associated with a true type Y x and a group G A deferring hard classifier Y is a randomized function Y X A deferring soft classifier is a randomized function S X A deferring hard soft post-processor is a randomized function D G G that takes as input the output of a deferring soft and post-processes it into a deferring hard soft classifier We also introduce new versions of the FPR and FNR conditioned on not deferring Definition The conditional false positive rate and conditional false negative rate of a deferring hard classifier Y for a group are respectively PrY Y Y PrY Y Y We additionally consider a version of the accuracy profile conditioned on not deferring which we call the conditional For non-deferring soft classifiers Definitions and coincide Definition The conditional of a classifier S for a group is the PMF of conditioned on not outputting That is for s s Note that the conditional is undefined if Abusing notation we denote by the collection and call it the conditional of S The conditional error rates are applicable generally but they can be difficult to interpret The consequences of using the conditional FPR and FNR are discussed further in Section along with a discussion of different deferral models They are also amenable to the consideration of additional goals which we will briefly address For example one could seek to minimize the total deferral rate equalize the deferral rate among groups or prefer deferrals on positive instances Figure Threshold post-processors with deferrals defer between the thresholds Thresholding with deferrals We return now to the problem of post-processing of calibrated soft classifiers but now with the extra power of deferring on some inputs We revisit the two approaches discussed in Section thresholding and equalizing accuracy profiles Proposition stated PPV and NPV cannot both be equalized across groups in general when using only a single threshold per group By using two thresholds per groups and deferring on some inputs PPV and NPV can always be equalized across groups We post-process using two thresholds per group as follows return when s is lower than the first threshold return between the thresholds and return above the second threshold as shown in Figure This buys us more degrees of freedom when equalizing binary constraints and it has the useful property that we say on the instances where we are the least confident about the predicted type We adapt our notation as follows Definition Deferring Threshold Post-Processor A deferring threshold post-processor assigns to each group two thresholds and two probabilities with the following requirements for all G for all G for which This corresponds to the case where the two thresholds are the same and therefore individuals with that score must be mapped to with probability and to with probability with the remainder mapped to The corresponding threshold post-processor is defined as follows s s s else s else s else s s Using two thresholds allows the equalization of both PPV and NPV across groups in general whereas without deferrals we could only equalize one or the other In our full paper we first demonstrate the existence of near-trivial classifiers that equalize PPV and NPV by deferring on all but the highest and lowest scores We now claim the existence of meaningful non-trivial threshold deferring post-processors that equalize PPV and NPV across groups Proposition Proof in Let S be a soft classifier with nice that is groupwise calibrated for a set of groups G Suppose that for all G Then there exists a non-trivial threshold post-processor such that the hard classifier Y S equalizes and for all G The following example demonstrates that it is sometimes possible to equalize PPV NPV FPR and FNR using deferrals but without equalizing the themselves Example Eqalizing PPV NPV and with Thresholds This example is presented with continuous support for simplicity Consider two one for group and one for Let the for be uniform with density give by the line and let the for group have density given by the parabola s s as shown in Figure Consider the post-processor Let let and let as shown in Figure The PPV and NPV of both groups is both is This example is somewhat unsatisfactory because the base rates are equal in the two groups We did not find a similar example without equal base rates Figure This threshold post-processor equalizes PPV NPV and as described in Example Equalizing with deferrals As with Claim equalizing the conditional between groups renders trivial the task of downstream decision-making subject to equality of PPV NPV and Importantly unlike in Section equalizing the conditional between groups does not require the groups to have equal base rates greatly increasing the applicability of this approach Claim If the conditional are equal for two groups then PPV NPV and are equalized or simultaneously undefined by any hard deferring post-processor D satisfying group blindness and The additional condition that D defers on input is necessary if D output on all inputs even on then PPV would remain unequal as long as the base rates differed The proof is similar to the proof of Claim and is included in the full version Deferrals are a powerful tool for manipulating and thereby equalizing conditional Consider a function Q Q if s else s otherwise If S is a calibrated classifier the soft deferring classifier S Q S is still calibrated For a group let be the of S and be the of S There is a simple graphical intuition for the shape of as shown in Figure The following theorem proved in the full version states that any conditional can be transformed into almost any other conditional by appropriate choice of Q Theorem Let be a conditional of a soft classifier S on group and let be any probability mass function such that Then there exists Q for which the calibrated of Q S is equal to Together Theorem and Claim suggest a general framework for using deferrals to post-process a soft possibly deferring classifier S which is groupwise calibrated into a hard deferring classifier which simultaneously equalizes PPV NPV and across groups as follows For each G let be the conditional of S for group Let be any conditional such that Use Theorem to equalize the conditional for all G Then use any hard post-processor D satisfying the requirements of Claim to make the ultimate deferring hard classifier This method is shown in Figure This framework allows for enormous flexibility in the choice of both and D even when considering just two In Figure we illustrate the first step of the framework on a COMPAS dataset using min as where is African-Americans and  is Caucasians In Figure in Section we also use and as Figure Choosing deferrals appropriately allows transforming one into another conditional In this example the solid orange line is the original s By deferring at the rates indicated by the shaded region the resulting conditional s S is represented by the dark blue line The area of the shaded region is One can design to achieve additional goals For example the choice min results in equal deferral rate across each group equal to the total variation distance between the two initial conditional The framework can be further expanded by combining deferrals with other methods for manipulating conditional including the mass-averaging discussed in Section A better understanding of these techniques is left for future work EXPERIMENTS ON COMPAS DATA We demonstrate the validity of our methodology on the Broward County data containing the recidivism risk decile scores of the COMPAS tool We restrict our attention to the subset of defendants whose race is recorded as either African-American or Caucasian Our code can be found at It has been shown that the COMPAS scoring mechanism is an approximately calibrated soft classifier with possible outcomes across the two race groups of African-Americans and Caucasians We note here that the distribution of the COMPAS scores differs significantly across the two groups In particular the scores for African-Americans are more evenly distributed as opposed to the skewed distribution seen with Caucasians Thresholding with Deferrals We first ran our two-threshold postprocessing mechanism Section and obtained a binary decision algorithm with deferrals which equalizes both PPV and NPV across Caucasians and African-Americans See Figure We observe that the percent of deferrals in total is smaller than of the decisions to be made which shows that a fairly large number of the defendants can be classified in this manner without having to defer to a downstream decision maker Thresholds for African-American scores PPV NPV Deferrals Decile Score ro ba bi lit y D en si ty Thresholds for Caucasian scores PPV NPV Deferrals Decile Score ro ba bi lit y D en si ty approximately equalizing PPV and NPV In the left right plot we show the thresholds for the African American Caucasian group Next we look at our post-processing mechanisms to equalize all four quantities PPV NPR FPR and NPR using deferrals Section As was noted earlier in the paper equalizing the of the two groups post-deferral achieves the goal of equalizing all four of the above quantities We implement two methods for doing so Converting one into Another In the first method decisions are deferred only on one group so as to convert its into that of the other group First we consider deferring only on African-Americans to convert their into that of Caucasians left side of Figure next decisions are deferred only for Caucasians right side of Figure Equalizing Alternately we have a second method where decisions are deferred for an equal fraction of Caucasians and of African Americans Figure This fraction is precisely the total variation distance between the two Converting African-American into Caucasian Deferral rate Decile Score ro ba bi lit y D en si ty Non deferrals Deferrals Converting Caucasian into African-American Deferral rate Decile Score ro ba bi lit y D en si ty Non deferrals Deferrals Figure Two instances of our conditional equalization method applied to COMPAS data from On the left right plot we use deferrals to create a conditional for African-Americans Caucasians that matches the for Caucasians African-Americans Equalizing using the minimum method Total deferral rate in each group Decile Score ro ba bi lit y D en si ty African-American non deferrals African-American deferrals Caucasian non deferrals Caucasian deferrals Figure A version of our conditional equalization method The conditional has the same distribution as the pointwise minimum of the two The total deferral rate is equalized across the two groups equals the TV distance between the two but the distribution of deferrals across scores is not Observations We observe several interesting phenomena on the COMPAS data set First using the method of deferring only on African-Americans we defer on roughly of the total decisions This number goes down to roughly when we defer only on Caucasians This seems to suggest as a general heuristic to try and use deferrals on the group with smaller size The total deferral fraction is also roughly when we defer on an equal fraction of Caucasians and African-Americans Second for all three methods that equalize the accuracy profiles for this particular dataset deferrals happen more on the extremes namely on individuals with respect to which the classifier had relatively high confidence either close to or close to This stands in sharp contrast to how the two-threshold method Figure distributes its deferrals they occur in the middle of the distribution examples on which the classifier is unsure While it may seem somewhat counter-productive to defer on these individuals any method that seeks to first equalize the accuracy profiles will have to defer most on the scores which appear in different probabilities across the two groups which for the COMPAS predictor is at the extremes Furthermore deferring on such scores may make sense from a social point of view When a score appears at drastically different rates for different groups perhaps deferring to another decision mechanism can be used to check for systemic bias in the present one Alternatively one can view the above observation as proposing a design criterion for calibrated soft classifiers That is if one wants to have a classifier that defers only on individuals for which the classifier has less confidence and still guarantee that the for the protected groups are equal conditioned on not deferring then one should design the soft classifier so that the for the protected groups are the same or almost the same close to and close to Indeed if the COMPAS classifier had these properties then our post-processing algorithm would have deferred only or mainly on individuals with low certainty namely medium risk individuals MODELS OF DEFERRING Whether or not a classifier is thought of as promoting fairness depends on the context this is true for both deferring and nondeferring classifiers In addition to the myriad considerations present for non-deferring classifiers deferring classifiers and downstream decision makers introduce some additional axes for consideration Cost to the individual Even though it is not intended to be a final decision a deferral may impose burdensome costs to an individual being classified It may mean that a defendant remains in jail while additional hearings are scheduled that invasive and expensive medical tests are ordered or that continued investigation engenders social stigma These costs may not be borne equally by all individuals and may depend on their group membership their true type or other factors For example a delay in granting a loan to a applicant may overly burden poorer applicants even those very likely to repay Cost to the decider Allowing deferrals might make the decision process more cost-effective Given that in most cases making a determination is cheap one may now invest more in the deferred cases For instance a team of trained moderators might be hired to manually review content on which an automated content filter defers or an expensive investigation might be required to adjudicate insurance claims that are not cut-and-dry Accuracy of downstream decision One reason to defer is to introduce a delay that will allow for a more accurate decision Thus the usefulness of allowing a classifier to defer depends on the accuracy of the downstream decision maker Additional medical tests might allow for highly accurate diagnoses But a judge deciding bail will be prone to a variety of errors and biases Fairness of downstream decision and of composed classifier Similar to the above the fairness of the downstream decision maker however one wants to interpret that will impact our interpretation of the deferring classifier Here one should take into account also the procedural aspect of the two-step evaluation here it is important that the downstream classifier will be deemed as more fair and more knowledgeable than the first stage Exploring fairness criteria for systems of deferring classifiers and downstream decision-makers eg as done in did for non-deferring classifiers is an interesting direction for future work Frequency of decisions In many settings the deferring classifier is a fast automated test eg automated risk assessment while the downstream decision maker is a slow manual process eg parole board However we anticipate situations in which there may be repeated deferring classifiers chained together which comprise the complete decision making pipeline For example a doctor might have a sequence of diagnostic tests at her disposal as needed or a bank might allow many rounds of appeal for loan applications but with lengthy delays Some applications might even permit hundreds or thousands of near-continuous deferring classifiers As an example consider a live video streaming platform that passively monitors streams for inappropriate content in real time The automated passive monitor might decide the content is inappropriate and shut it down appropriate and continue passive monitoring or suspicious by deferring and begin active monitoring by devoting more computing resources or bringing in a human moderator Technical implications of deferral model The contextual considerations discussed above directly impact the appropriate application of a deferring classifier and its goals An obvious goal is to minimize the overall rate of deferrals while maintaining the best possible FPR FNR PPV and NPV for the classifier conditioned on not deferring and without considering the distribution of deferrals However one might desire very different properties from the distribution of deferrals in different contexts The deferrals may be distributed differently among individuals with different true type group membership or soft-classifier scores while the burden imposed by deferrals and errors may differ greatly between different populations In a medical diagnosis scenario a false negative ie failing to diagnose a disease may have serious consequences and deferring to run additional non-invasive and inexpensive additional tests may be generally acceptable On the other hand an insurance provider may prefer to minimize expensive investigations by paying out more false claims The context may also affect the way one defines the deferral analogues of FPR and FNR While calibration PPV and NPV apply directly to deferring classifiers it is not clear how best to generalize the definitions of error rates For example consider false positive rate by Definition the false positive rate of a non-deferring hard classifier Y for a group is PrY Y The approach we take in Section is to condition on not deferring Definition A deferring classifier Y that output on half of true negative instances within a would have conditional false positive rate as low as if it never output on true negatives or as high as if it never output on true negatives The conditional false positive rate is agnostic towards the downstream decision maker It codifies no value judgements as to whether a deferral is desirable or undesirable as an individual nor whether deferrals ultimately result in accurate or inaccurate decisions This is itself a value judgement A second approach is to leave the original definition unchanged The same deferring hard classifier as above would have unconditional false positive rate This would be true regardless of whether Y output or on the other half of true negative instances We call this the unconditional false positive rate The unconditional false positive rate effectively categorizes deferrals as correct outputs This may be appropriate if the downstream decision maker has very high accuracy If for example a doctor orders an additional more accurate diagnostic test in response to a deferral the unconditional false positive rate might be appropriate Finally a third approach is to base our measure of inaccuracy on true negatives instead of false positives a reverse of the above Just as in the case of non-deferring classifiers the relationships among these contrasting group statistics their meaningfulness in different settings and their application in different settings are not well understood and deserve further study