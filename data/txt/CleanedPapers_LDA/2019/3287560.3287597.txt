The Disparate Effects of Strategic Manipulation When consequential decisions are informed by algorithmic input individuals may feel compelled to alter their behavior in order to gain a systems approval Models of agent responsiveness termed strategic manipulation analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to trick a published classifier In cases of real world classification however an agents ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification but is bound up in a complex web of social factors that affect her ability to pursue certain action responses In this paper we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation We nd that whenever one groups costs are higher than the others the learners equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group while erroneously excluding some members of the disadvantaged group We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group lowering their costs in order to improve her own classification performance Here we encounter a paradoxical result there exist cases in which providing a subsidy improves only the learners utility while actually making both candidate groups worse the group receiving the subsidy Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individuals quality when agents capacities to adaptively respond differ CONCEPTS Computing methodologies Machine learning Theory of computation Algorithmic game theory KEYWORDS fairness in machine learning strategic classification INTRODUCTION The expanding realm of algorithmic decision-making has not only altered the ways that institutions conduct their day-to-day operations but has also had a profound impact on how individuals interface with these institutions It has changed the ways we communicate with each other receive crucial resources and are granted important social and economic opportunities Theoretically algorithms have great potential to reform existing systems to become both more efficient and equitable but as exposed by various high-profile investigations prediction-based models that make or assist with consequential decisions are in practice highly prone to reproducing past and current patterns of social inequality While few algorithmic systems are explicitly designed to be discriminatory there are many underlying forces that drive such socially biased outcomes For one since most of the features used in these models are based on proxy rather than causal variables outputs often reeffects the various structural factors that bear on a persons life opportunities rather than the individualized characteristics that decision-makers often seek Much of the previous work in algorithmic fairness has examined a particular undesirable proxy effect in which a classifiers features may be linked to socially significant and legally protected attributes like race and gender interpreting correlations that have arisen due to centuries of accumulated disadvantage as genuine attributes of a particular category of people But algorithmic models do not only generate outcomes that passively correlate with social advantages or disadvantages These tools also provoke a certain type of reactivity in which agents see a classifier as a guide to action and actively change their behavior to accord with the algorithms preferences On this view classifiers both evaluate and animate their subjects transforming static data into strategic responses Just as an algorithms use of certain features differentially advantages some populations over others the room for strategic response that is inherent in many automated systems also naturally favors social groups of privilege Admissions procedures that heavily weight SAT scores motivate students who have the means to take advantage of test prep courses and even take the exam multiple times Loan approval systems that rely on existing lines of credit as an indication of creditworthiness encourage those who can to apply for more credit in their name Thus an algorithm that scores applicants to determine how a resource should be allocated sets a standard for what an ideal candidates features ought to be A responsive subject would look to alter how she appears to a classifier in order to increase her likelihood of gaining the systems approval But since reactivity typically requires informational and material resources that are not equally accessible to all even when an algorithm draws on features that seem to arise out of individual effort these metrics can be skewed to favor those who are more readily able to alter their features In the machine learning literature agent reactivity to a classifier is termed strategic manipulation Since previous work in strategic classification has typically depicted agent-classifier interactions as antagonistic such actions are usually viewed as distortions that aim to undermine a learners classifier As shown in Hardt et al a learner who anticipates these responses can under certain formulations of agent costs adapt to protect against the misclassification errors that would have resulted from manipulation recovering an accuracy level that is arbitrarily close to the theoretical maximum These results are welcome news for a learner who correctly assesses agents best-responses Indeed in most strategic manipulation models agents are depicted as equally able to pursue manipulation allowing the learner who knows their costs to accurately preempt strategic responses While there are occasions in which agents do largely face homogenous costs an even playing many other social use cases of machine learning tools agents do not encounter the same costs of altering the attributes that are ultimately observed and assessed by the classifier As such in this paper we ask What are the effects of strategic classification and manipulation in a world of social stratification As in previous work in strategic classification we cast the problem as a Stackelberg game in which the learner moves rst and publishes her classifier before candidates best-respond and manipulate their features But in contrast with the models in Brückner Scheer and Hardt et al we formalize the setting of a society comprised of social groups that not only may differ in terms of distributions over unmanipulated features and true labeling functions but also face different costs to manipulation This extra set of differences brings to light questions that favor an analysis that focuses on the welfares of the candidates who must contend with these classifiers Do classifiers formulated with strategic behavior in mind impose disparate burdens on different groups If so how can a learner mitigate these adverse effects The altered gameplay and outcomes of strategic classification beg questions of fairness that are intertwined with those of optimality Though our model is quite general we obtain technical results that reveal important social ramifications of using classification in systems marked by deep inequalities and a potential for manipulation Our analysis shows that under our model even when the learner knows the costs faced by different groups her equilibrium classifier will always act to reinforce existing inequalities by mistakenly excluding qualified candidates who are less able to manipulate their features while also mistakenly admitting those candidates for whom manipulation is less costly perpetuating the relative advantage of the privileged group We delve into the cost disparities that generate such inevitable classification errors Next we consider the impact of providing subsidies to lighten the burden of manipulation for the disadvantaged group We nd that such an intervention can improve the learners classification performance as well as mitigate the extent to which her errors are inequality-reinforcing However we show that there exist cases in which providing subsidies enforces an equilibrium learner strategy that actually makes some individual candidates worse-o without making any better-o Paradoxically in these cases paying a subsidy to the disadvantaged group actually benefits only the learner while both candidate groups experience a welfare decline Further analysis of these scenarios reveals that in many cases all parties would have preferred a world in which manipulation of features was not possible for any candidates Our papers agent-centric analysis views data points as representing individuals and classifications as impacting those individuals welfares This orientation departs from the dominant perspective in learning theory which privileges a vendors predictive accuracy and instead evaluates classification regimes in light of the social consequences of the outcomes they issue By incorporating insights and techniques from game theory and economics domains that consider deeply the effects of various policies on agents behaviors and outcomes we hope to broaden the perspective that machine learning takes on socially-oriented tools Presenting more democratically-inclined analysis has been central to the eld of algorithmic fairness and we hope our work sheds new light on this generic setting of classification with strategic agents Related Work While many earlier approaches to strategic classification in the machine learning literature have tended to view learner-agent interactions as adversarial our work does not assume inherently antagonistic relationships and instead shares the Stackelberg game-theoretic perspective akin to that presented in Brückner Scheer and built upon by Hardt et al Departing from these models focus on static prediction and homogeneous manipulation costs Dong et al propose an online setting of strategic classification in which agents appear sequentially and have individual costs for manipulation that are unknown to the learner Unlike our work they take a traditional learner-centric view whereas our concerns are with the welfare of the candidates Agent features and potential manipulations in the face of a learner classifier can also be interpreted as serving informational purposes In the economics literature on signaling theory agents interact with a principal the counterpart to our learner via signals that convey important information relevant to a particular task at hand Classic works such as Spences paper on job-market signaling focus their analysis on the varying quality of information that signals provide at equilibrium The emphasis in our analysis on different group costs shares features with a recent update to the signaling literature by Frankel Kartik who also distinguish between natural actions corresponding to unmanipulated features in our model and gaming ability which operate similarly to our cost functions The connection between gaming capacity and social advantage is also explicitly discussed in work by Esteban Ray who consider the effects of wealth and lobbying on governmental resource allocation While most works in the economics signaling literature center on the decay of the informativeness of signals as gaming and natural actions become indistinguishable some recent work in computer science has also considered the effect of costly signaling on mechanism design In contrast to both of these perspectives our work highlights the effect of manipulation on a learners action and as a consequence on the agents welfares In independent concurrent work appearing at the same conference Milli et al also consider the social impacts of strategic classification Whereas our model highlights the interplay between a learners Stackelberg equilibrium classifier and agents best-response manipulations at the feature level their work traces the relationship between the learners utility and the social burden a measure of agents manipulation costs They show that an institution must select a point on the outcome curve that trades o its predictive accuracy with the social burden it imposes In their model an agent with an unmanipulated feature vector x has a likelihood x of having a positive label and can manipulate to any vector y with y x at zero cost or to y with y x for a positive cost This assumption called outcome monotonicity allows them to reason about manipulations in one-dimensional likelihood space rather than feature space since the optimal learner strategies amount to thresholds on likelihoods In contrast we allow features to be differently manipulable perhaps a student can boost her SAT score via test prep courses but can do nothing to change her grades from the previous year and cannot freely obtain a higher SAT score in exchange for a worse record of extracurricular activities which affects the forms of both the learners equilibrium classifier and agents best-response manipulations Despite these differences in model and focus their analysis yields results that are qualitatively similar to ours Highlighting the differential impact of classifiers on social groups they also nd that overcoming stringent thresholds is more burdensome on the disadvantaged group MODEL FORMALIZATION As in Brückner Scheer and Hardt et al we formalize the Strategic classification Game as a Stackelberg competition in which the learner moves rst by committing to and publishing a binary classifier Candidates who are endowed with innate features best respond by manipulating their feature inputs into the classifier Formally a candidate is defined by her d-dimensional feature vector x X d and group membership A or B with A signifying the advantaged group and B the disadvantaged Group membership bears on manipulation costs such that a candidate from group m who wishes to move from a feature vector x to a feature vector y must pay a cost of y x We note that these cost function forms are similar to the class of separable cost functions considered in Hardt et al We assume that higher feature values indicate higher quality to the learner and thus restrict our attention to manipulations such that y x where the symbol signifies a component-wise comparison such that y x if and only if i d i xi Throughout this paper we study non-negative monotone cost functions such that the cost of manipulating from a feature vector x to a feature vector y increases as x and y get further apart To motivate this distinction between features and costs consider the use of SAT scores as a signal of academic preparedness in the US college admissions process The high-stakes nature of the SAT has encouraged the growth of a test prep industry dedicated to helping students perform better on the exam Test preparation books and courses while also exposing students to content knowledge and skills that are covered on the SAT promise to hack the exam by training students to internalize test-taking strategies based on the format structure and style of its questions One can view SAT scores as a feature used by a learner building a classifier to select candidates with sufficient academic success according to some chosen standard The existence of test prep resources then presents an opportunity for some applicants to their scores which might trick the tool into classifying the candidates as more highly qualified than they are in actuality In this example a candidates strategic manipulation move refers to her investment in these resources which despite improving her exam score do not confer any genuine benefits to her level of academic preparation for college Just as access to test prep resources tends to fall along income and race lines we view candidates different abilities to manipulate as tied to their group membership We model these group differences with respect to availability of resources and opportunity by enforcing a cost condition that orders the two groups We suppose that for all x d and y x cA y cA x cB y cB x Manipulating from a feature vector x to y is always at least as costly for a member of group B as it is for a member of group A We believe our models inclusion of this cost condition reeffects an authentic aspect of our social world wherein one group is systematically disadvantaged with respect to a task in comparison to another In our setup we also allow groups to have distinct probability distributions DA and DB over unmanipulated features and to be subject to different true labeling functions hA and hB defined as hA x x such that Pd A x such that Pd A hB x x such that Pd B x such that Pd B We assume that hA x hB x for all x Returning to the SAT example research has shown that scores are skewed by race even before factoring in additional considerations such as access to manipulation In such cases the true threshold for the disadvantaged group is lower than that for the advantaged group We leave this generality in our model to acknowledge and account for the influence that various social and historical factors have on candidates unmanipulated features and not we emphasize as an endorsement of a view that groups are fundamentally different in ability A formal description of the Strategic classification Game with Groups is given in the following definition D S C G G In the Strategic classification Game with Groups candidates with features x d and group memberships A or B are drawn from distributions DA The population proportion of each group is given A candidate from group m pays cost x from her original features x to y x There exist true binary classifiers hA and hB for candidates of each group Probability distributions cost functions and true binary classifiers are all common knowledge Gameplay proceeds in the following manner The learner issues a classifier generating outcomes Each candidate observes and manipulates her features x to y x A group m candidate with features x who moves to y earns a y y x The learner incurs a penalty of CF P X m AB x y X m AB x y where CF P and denote the cost of a false positive and a false negative respectively The learner looks to correctly classify candidates with respect to their original features x whereas each candidate hopes to manipulate her features to attain a positive classification expending as little cost as possible in the process Under this setup candidates are only willing to manipulate their features if it their classification from to and if the cost of the manipulation is less than We note that defining the utility of a positive classification to be can be considered a scaling and thus is without loss of generality This learner-candidate interaction is very similar to that studied in Hardt et al However our inclusion of groups with distinct manipulation costs leads to an ambiguity regarding a candidates initial features that does not exist when all candidates have an equal opportunity to manipulate In very few cases can a vendor distinguish among candidates based on their group membership for the explicit purpose of issuing distinct classification policies especially if that group category is a protected class attribute As such in our setup we require that a learner publish a classifier that is not adaptive to different agents based on their group identities It is important to note that the positive results in Hardt et als formulation of the Strategic classification Game wherein for separable cost functions the learner can attain a classification error at test-time that is arbitrarily close to the optimal attainable do not carry over into this setting of heterogeneous groups and costs Even when hA hB the existence of different costs of agent manipulation even when separable as in our model introduces a base uncertainty to the learning problem that generates errors that cannot be extricated so long as the learner must publish a classifier that does not distinguish candidates based on their group memberships Second an analysis of the learners strategy and performance the perspective typically taken in most learning theory papers contributes only a partial view of the total welfare effect of using classification in strategic settings The main objective of this paper is to oer a more thorough and holistic inspection of all agents outcomes paying special heed to the different outcomes experienced by candidates of the two groups Insofar as all social behaviors are impelled by goals interests and purposes we should view data that is strategically generated to be the rule rather than the exception in social machine learning settings Remark on the assumption that hA and hB are known Our assumption that the learner has knowledge of groups true labeling functions is not central to our analysis We make such an assumption to highlight the pure effect of groups differential costs of manipulation on equilibrium gameplay and consequent welfares rather than the potential side effects due to a learners noisy estimation of the true classifiers Our general endings do not substantially rely on this feature of the model and the overall results carry through into a setting in which the learner optimizes from samples Remark on unequal group costs The differences in costs cA and cB encoded by the cost condition is not restricted to referring only to differences in the monetary cost of manipulation Instead as is common in information economics and especially signaling theory cost reeffects the multiplicity of factors that bear on the effort exertion required by feature manipulation To demonstrate the generality of our formulation of distinct group costs we show that the cost condition given in is equivalent to a more explicit derivation of the choice that an agent faces when deciding whether to manipulate her feature A rational agent with feature x will only pursue manipulation if her value for a positive classification minus her cost of manipulation exceeds her value for a negative classification x y u c y c x The monotone function u translates the costs borne by a candidate to manipulate from x to y into her utility space ie it reeffects the value that she places on that expenditure We can rewrite the previous inequality to be c y c x u y x Substituting in u y x we have c y c x Since the same cost expenditure is valued more highly by the disadvantaged group than by the advantaged group the function u is more convex for group B than for group A Thus all else equal we have cA y cA x cB y cB x as desired More generally the functions c and u may each be different for the groups As such the disadvantage encoded in the cost condition can arise due to differences in valuations of classifications differences in costs c or differences in valuations of those costs u EQUILIBRIUM ANALYSIS We begin by studying agents best-response strategies in the basic Strategic Manipulation Game with Groups in which candidates belong to one of two and B and the cost condition holds so that group B members face greater costs to manipulation than group A members To build intuition we rst consider best-response strategies in the one-dimensional case in which candidates have features x and group cost functions are of any non-negative monotone form We then move on to consider the d-dimensional case in which candidate features are given as vectors x d and manipulation costs are assumed to be linear One-dimensional Features In the d case the cost condition given in may be written as c are linear in the one-dimensional case they may be written as threshold functions where thresholds A and B are constants in and for agents in group m x if and only if x m A university admissions decision based on a single score is an example of such a classifier Although the SAT does not act as the sole determinant of admissions in the US in countries such as Australia Brazil and China a single exam score is often the only factor of applicant quality that is considered for admissions When the learner has access to A and B and group costs cA and cB satisfy the cost condition the following proposition characterizes the space of undominated strategies for the learner who seeks to minimize any error-penalizing cost function P OD U L S Given group cost functions cA and cB and true label thresholds A and B where B A there exists a space of undominated learner threshold strategies B A where A c A cA A and B cB cB B That is for any error penalties CF P and the learners equilibrium classifier is based on a threshold B A such that for all manipulated features To understand this result rst notice that if the learner were to face only those candidates from group A she would achieve perfect classification by labeling as only those candidates with unmanipulated feature x A This strategy is enacted by considering candidates best-response manipulations A rational candidate would only be willing to manipulate her feature if the gain she receives in her classification exceeds her costs of manipulation The learner would like to guard against manipulations by candidates with x A but still admit candidates with x A so she considers the maximum manipulated feature that is attainable by a rational candidate with x A who is willing to spend up to a cost of one in order to secure a better classification as illustrated in Figure The maximum such value is A and thus the learner sets a threshold at A admitting all those with A and rejecting all those with A The same reasoning applies to a learner facing only group B candidates and the learner sets a threshold at B admitting all those candidates with B and rejecting all those with B It can be shown that for all valid values of AB cA and cB necessarily B A Then all classifiers with threshold B are dominated by B in the sense that for any arbitrary error penalties CF P and the learner would suffer higher costs by setting her threshold to be rather than B In the same way all thresholds A are dominated by A thus leaving B A to be the space of undominated thresholds For an account of the full proof of this result and all omitted proofs see the appendix Even without committing to a particular learner cost function the space of optimal strategies characterized in Proposition leads to an important consequence A rational learner in the Strategic classification Game always selects a classifier that exhibits the following phenomenon it mistakenly admits unqualified candidates from the group with lower costs and mistakenly excludes qualified candidates from the group with higher costs This result is formalized in Proposition To state the proposition the following definition is instructive Whereas the true thresholds A and B are a function of unmanipulated features the learner only faces candidate features that may have been manipulated In order to make these observed features commensurable with A and B it is helpful for the learner to translate a candidates possibly manipulated feature to its minimum corresponding original unmanipulated value D C For any observed candidate feature the minimum Figure Group cost functions for a one-dimensional feature x A and B signify true thresholds on unmanipulated features for group A and B but a learner must issue a classifier on manipulated features The threshold A perfectly classifies group A candidates B perfectly classifies group B candidates A learner selects an equilibrium threshold B A committing false positives on group a red bracket and false negatives on group B blue bracket corresponding unmanipulated feature is defined as A max cA cA B max cB cB for a candidate belonging to group A and group B respectively The corresponding values A and B are defined such that a candidate who presents feature must have as her true unmanipulated feature x A if she is a group A member and x B if she is a group B member P L C D A learner who employs a classifier based on a threshold strategy B A only commits false positives errors on group A and false negatives errors on group B The cost C of such a classifier is x B B g CF x A A g where false negative errors entail penalty and false positive errors entail penalty CF P A learner who commits to classifying only one of the groups correctly bears costs given by the following corollaries C A classifier based on A perfectly classifies group A candidates and bears cost c A x B B g C A classifier based on B perfectly classifies group B candidates and bears cost c B CF x A A g Notice that the learners errors always cut in the same direction by unduly benefiting group A candidates and unduly rejecting group B candidates these errors act to reinforce the existing social inequality that had generated the unequal group cost conditions in the rst place Since these errors arise out of the asymmetric group costs of manipulation the Strategic classification Game can be viewed as an interactive model that itself perpetuates the relative advantage of group A over group B candidates Within the undominated region B A the equilibrium learner threshold is attained as the solution to the optimization problem argmin BA C In the games greatest generality where candidates are drawn from arbitrary probability distributions groups bear any costs that abide by the cost condition and the learner has arbitrary error penalties one cannot specify the equilibrium learner threshold any further However under some special cases of candidate cost functions and probability distributions the equilibrium threshold can be characterized more precisely specifically when candidates from both groups are assumed to be drawn from a uniform distribution over unmanipulated features in an error-minimizing learner seeks a threshold value that minimizes the length of the interval of errors given by the following quantity argmin BA B A From here one natural assumption of candidate cost functions would have that groups A and B bear costs that are proportional to each other In this case the curvature of the cost functions is determinative of a learners equilibrium threshold P Suppose group cost functions are proportional such that cA x x for q that DA and DB are uniform on and CF P and pA pB Let be the learners equilibrium threshold When cost functions are strictly concave B When cost functions are strictly convex A When cost functions are the learner is indifferent between all B A General d-Dimensional Feature Vectors In the general d-dimensional case of the Strategic classification Game candidates are endowed with features that are given by a vector x d and can choose to manipulate and present any feature y x to the learner In this section we consider optimal learner and candidate strategies when group costs are linear such that they may be written as cA x i cB x i for groups A and B respectively Now the cost condition cA y cA x cB y cB x for all y component-wise as before implies that i d In d dimensions the true classifiers hA and hB have linear decision boundaries such that for a group A candidate with feature x hA x A A Figure The forward simplex A candidate in group A with unmanipulated feature vector x can manipulate to reach any feature vector y A x at a cost of at most and for a group B candidate with feature x hB x B B We assume that all components xi contribute positively to an agents likelihood of being classified as so for all i To ensure that the cost of manipulation is always non-negative all cost coefficients are positive for all i d A candidate may now manipulate any combination of the d components of her initial feature x to reach the feature y that she presents to the learner Despite this increased exibility on the part of the candidate we are still able to characterize the performance of undominated learner classifiers generalizing the result in Proposition All potentially optimal classifiers exhibit the same inequality-reinforcing property inherent within the one-dimensional interval of undominated threshold strategies trading o false positives on group A candidates with false negatives on group B candidates Before we formally present this result we rst describe candidates best-response strategies Here a geometric view of the space of potential manipulations is informative Suppose a candidate endowed with a feature vector x faces i and is willing to expend a total cost of for manipulation Then she can move to any y x contained within the d-simplex with orthogonal corner at x and remaining vertices at x where is the ith standard basis vector This region is given by x x i ti d i ti ti i x depicted in Figure gives the space of potential movement for a candidate with unmanipulated feature x who is willing to expend a total cost of Notice that ti can be interpreted as the cost that a candidate expends on movement in the ith direction Thus Pd i ti gives the total cost of manipulation Moving beyond the range of possible moves in order to describe how a rational candidate will best-respond to a learner we must consider the published classifier Suppose a learner publishes a classifier based on a hyperplane Pd i ii so that y if and only if Pd i ii A best-response manipulation occurs along the direction that generates the greatest increase in the value Pd i i i xi for the least cost As such a candidate will move in any directions i argmax d i This result is formalized in the following lemma L dD C B R Suppose a learner publishes the classifier y if and only if Pd i ii Consider a candidate with unmanipulated feature vector x and linear Figure A perfect classifier for group A Every candidate with unmanipulated feature vector x on or above the true decision boundary for group a is able to manipulate to a point y A x on or above the blue decision boundary depicted here No an unmanipulated feature vector below the true decision boundary is able to do so The kink in the blue decision boundary arises due to the restriction of features to d A perfect classifier for group A does not need to have this kink for example a more lenient perfect classifier can be formed by straightening it out costs Pd i If x or if for all i d x the candidates best response is to set y x Otherwise letting argmax d i her manipulation takes the form x i ti for any t such that ti for all i d ti for all i i i xi ti While in the d-dimensional case a candidate has many more choices of manipulation directions to pursue a best response strategy will always lead her to increase her feature in those components that are most valued by the learner and least costly for manipulation That is she behaves according to a bang for your buck principle in which the optimal manipulations are in the direction or directions where the ratio i is highest Despite the fact that the optimal manipulation may not be unique as in the cases where there are multiple equivalently good directions for a candidate to move in a learner who knows candidates costs can still anticipate best-response manipulations and avoid errors on that group As such we are once again able to construct a perfect classifier for candidates of group A and a perfect classifier for candidates of group B T dD S D L S In the general d-dimensional Strategic classification Game with linear costs there exists a classifier that perfectly classifies group A and a classifier that perfectly classifies group B All undominated classifiers commit no false positive errors on group a and no false negative errors on group B A full exposition of the proof appears in the appendix but here we present an abbreviated explanation of the result For each group m the learner computes an optimal boundary that perfectly classifies all of its members by considering the set of simplices m x anchored at the vectors x that m x m and drawing the strictest hyperplane that intersects each simplex That is for all hyperplanes i Pd i i that are constructed to intersect each simplex then Pd is the strictest if for all x d i i for all i Due to the cost ordering for any x d B x A x and thus wherever a comparison is possible the group A boundary is at least as strict as the group B boundary Figure gives a visualization of a boundary formed by connecting the simplices x the corresponding classifier perfectly classifies the group As in the one-dimensional general costs case learner strategies necessarily entail inequality-reinforcing classifiers a rational learner equipped with any error-penalizing cost function will select an equilibrium strategy that trades o undue optimism with respect to group A for undue pessimism with respect to group B We note that except in the extreme case in which there exists a perfect classifier for all candidates in the population this result implies that the classifier for group A issues false negatives on group B and the classifier for group B issues false positives on group A In order to formalize this result we would like to generalize the idea behind the minimum correspondence unmanipulated features given by A and B in for general d-dimensions and linear costs A learner who observes a possibly manipulated feature vector y must consider the space of unmanipulated feature vectors that the candidate could have had Thus we use of the simplex idea of potential manipulation however in this case the learner seeks to project a simplex backward to undo the potential candidate manipulation Since groups are subject to different costs simplices A y and B depiction is given in Figure which represent the region from where a candidate could have manipulated will differ based on the candidates group membership with A y y i ti d i ti ti i B y y i ti d i ti ti i We can now use these constructs in order to define d-dimensional generalizations of A y and B y D C U dD For any observed candidate feature y d the minimum corresponding unmanipulated feature vectors are given by A y x A y d x A y such that x x B y x B y d x B y such that x x for a candidate belonging to group A and group B respectively Figure The backward simplex A candidate in group a with manipulated feature vector y could have started with any feature vector x A y and paid a cost of at most The corresponding values A y and B y are defined such that a candidate who presents feature y must have had a true unmanipulated feature vector x x for some x A y if she is a group A member and x x for some x B y if she is a group B member For any hyperplane decision boundary containing vectors y the minimum corresponding feature vectors given by A y and B y are helpful for determining the effective thresholds that generates on unmanipulated features for groups A and B L Suppose a learner classifier is based on a hyperplane Pd i Construct the set Lm argmin y i y s t i ii Then a group m agent with feature x can move to some y and y x if and only if x for some Lm By definition for any two Lm X i i i X i i i argmax id i Thus a learner who cares only about the true label of presented features will construct her decision boundary such that all Lm have the same true label A cost-minimizing learner who publishes a classifier based on a hyperplane on manipulated features will commit errors on those candidates with unmanipulated features x d contained within the boundaries given by LA and LB This space can be understood as the d-dimensional generalization of the A B error interval in one-dimension P L C d D A learner who publishes an undominated classifier based on a hyperplane can only commit false positives on group A candidates and false negatives on group B candidates The cost of such a classifier is x kB B CF x Ax A kA where kB argmax d i and kA argmax d i LEARNER SUBSIDY STRATEGIES Since in our setting the learners classification errors are directly tied to unequal group costs we ask whether she would be willing to subsidize group B candidates in order to shrink the manipulation gap between the two groups and as a result reduce the number of errors she commits In this section we formalize subsidies as interventions that a learner can undertake to improve her classification performance Although in many high-stakes classification settings the barriers that make manipulation differentially accessible are non-monetary such as time information and social access in this section we consider subsidies that are monetary in nature to alleviate the financial burdens of manipulation We introduce these subsidies for the purpose of analyzing their effects on not only the learners classification performance but also candidate groups outcomes Since subsidies mitigate the inherent disparities in groups costs and increase access to manipulation one might expect that their implementation would surely improve group overall welfare In this section we show that in some cases optimal subsidy interventions can surprisingly have the effect of lowering the welfare of candidates from both groups without improving the welfare of even a single candidate Subsidy Formalization There are different ways in which a learner might choose to subsidize candidates costs In the main text of this paper we focus on subsidies that reduce each group B candidates costs such that the agent need only pay a fraction of her original manipulation cost D P Under a proportional subsidy plan the learner pays a proportion of each group B candidates cost of manipulation for some As such a group B candidate who manipulates from an initial feature vector x to a feature vector y bears a cost of cB y cB x In the appendix we also introduce at subsidies in which the learner absorbs up to a at amount from each group B candidates costs leaving the candidate to pay max cB x Similar results to those shown in this section hold for at subsidies When considering proportional subsidies the learners strategy now consists of both a choice of and a choice of classifier to issue The learners goal is to minimize her penalty CF P X m AB x y g X m AB x y g cost where cost is the monetary cost of the subsidy CF P denote the cost of a false positive and a false negative respectively as before and is some constant that determines the relative weight of misclassification errors and subsidy costs for the learner For ease of exposition the remainder of the section is presented in terms of one-dimensional features In Section A of the appendix we show that in many cases the d-dimensional linear costs setting can be reduced to this one-dimensional setting As an analog of we define B cB cB giving the minimum corresponding unmanipulated feature x for any observed feature Under the proportional subsidy for a given the group B candidate must have x B From this we define B such that B B B In order to compute the cost of a subsidy plan wemust determine the number of group B candidates who will take advantage of a given subsidy benefit Since manipulation brings no benefit in itself candidates will only choose to manipulate and use the subsidy if it will lead to a positive classification For a published classifier with threshold we then have cost Z B cB cB x x Although the learners optimization problem can be solved analytically for various values of we are primarily interested in taking a welfare-based perspective on the effects of various classification regimes on both the learner and candidate groups In the following section we analyze how the implementation of a subsidy plan can alter a learners classification strategy and consider the potential impacts of such policies on candidate groups Group Welfare Under Subsidy Plans While a learner would choose to adopt a subsidy strategy primarily in order to reduce her error rate offering cost subsidies can also be seen as an intervention that might equalize opportunities in an environment that by default favors those who face lower costs That is if costs are keeping group B down then one might believe that reducing costs will surely allow group B a fairer shot at manipulation and as a result a fairer shot at positive classification Alas we nd that mitigating cost disparities by way of subsidies does not necessarily lead to better outcomes for group B candidates In fact an optimal subsidy plan can actually reduce the welfares of both groups Paradoxically in some cases the subsidy plan boosts only the learners utility whereas every individual candidate from both groups would have preferred that she oer no subsidies at all The following theorem captures the surprising result that subsidies can be harmful to all candidates even those from the group that would appear to benefit T S There exist cost functions cA and cB satisfying the cost conditions learner distributions DA and DB true classifiers with threshold A and B population proportions pA and pB and learner penalty parameters CF P and such that no candidate in either group has higher at the equilibrium of the Strategic classification Game with proportional subsidies compared with the equilibrium of the Strategic classification Game with no subsidies and some candidates from both group A and group B are strictly worse o We note that a slightly weaker version of the theorem holds for at subsidies In particular there exist cases in which some individual candidates have higher at the equilibrium of the Strategic classification Game with at subsidies compared with the equilibrium with no subsidies but both group A and group B candidates have lower pay on average with the subsidies To prove the theorem it suffices to give a single case in which both candidate groups are harmed by the use of subsidies However to illustrate that this phenomenon does not arise only as a rare corner case we provide one such example here plus two in the appendix and discuss general conditions under which this occurs In each example we consider a particular instance of the Strategic classification Game and compare the welfares of candidates at equilibrium when the learner is able to select a proportional subsidy with their welfares at equilibrium when no subsidy is allowed E Suppose that a learner is error-minimizing such that CF P and Suppose that unmanipulated features for both groups are uniformly distributed with pA pB Let group cost functions be given by cA x p x x and cB x p x note that the cost condition c true group thresholds be given by A and B When subsidies are not allowed the learner chooses a classifier with threshold B at equilibrium This threshold perfectly classifies all candidates from group B while permitting false positives on candidates from group A with features x If the learner decides to implement a proportional subsidies plan at equilibrium the learner chooses a classifier with threshold prop A and a subsidy parameter Her new threshold now correctly classifies all members of group A while committing false negatives on group B members with features x Some candidates in group B are thus strictly worse-o while none improve Without the subsidy offering group B members had been perfectly classified but now there exist some candidates who are mistakenly excluded Further one can show that candidates who are positively classified must pay more to manipulate to the new threshold in spite of receiving the subsidy benefit This increased cost is due to the fact that the higher classification threshold imposes greater burdens on manipulation than the subsidy alleviates Group A candidates are also strictly worse-o since the threshold increase eliminates false positive benefits that some members had previously been granted in the no-subsidy regime Moreover all candidates who manipulate must expend more to do so since these candidates do not receive a subsidy payment Only the learner is strictly better o with the implementation of this subsidy plan Additional examples in the appendix show cases in which both groups experience diminished welfare when they bear linear costs Even when the learner has an error function that penalizes false negatives twice as harshly as false positives and thus is explicitly concerned with mistakenly excluding group B candidates an equilibrium subsidy strategy can still make both groups worse-o We thus highlight two consequences of subsidy interventions On the one hand with reduced cost burdens more candidates from the disadvantaged group should be able to manipulate to reach a positive classification However subsidy payments also allow a learner to select a classifier that is at least as strict as the one issued without offering subsidies These are opposing forces and these examples show that without needing to distort underlying group probability distributions or the learners penalty function in extreme ways the effect of mitigating manipulation costs may be outweighed by the overall impact of a stricter classifier This result can also be extended to show that a setup in which candidates are unable to manipulate their features at all can be preferred by all three parties groups A and B as well as the learner to both the manipulation and subsidy regimes We provide an informal statement of this proposition below and defer the interested reader to its formal statement and demonstration in the appendix P There exist general cost functions such that the outcomes issued by a learners equilibrium classifier under a nonmanipulation regime is preferred by all parties the learner group A and group B to outcomes that arise both under her equilibrium manipulation classifier and under her equilibrium subsidy strategy DISCUSSION Social stratification is constituted by forms of privilege that exist along many different axes weaving and overlapping to create an elaborate mesh of power relations While our model of strategic manipulation does not attempt to capture this irreducible complexity we believe this work highlights a likely consequence of the expansion of algorithmic decision-making in a world that is marked by deep social inequalities We demonstrate that the design of classification systems can grant undue rewards to those who appear more meritorious under a particular conception of merit while justifying exclusions of those who have failed to meet those standards These consequences serve to exacerbate existing inequalities Our work also shows that attempts to resolve these negative social repercussions of classification such as implementing policies that help disadvantaged populations manipulate their features more easily may actually have the opposite effect A learner who has offered to mitigate the costs facing these candidates may be encouraged to set a higher classification standard underestimating the deeper disadvantages that a group encounters and thus serving to further exclude these populations However it is important to note that these unintended consequences do not always arise A conscientious learner who offers subsidies to equalize the playing eld can guard against such paradoxes by making sure to classify agents in the same way even when offering to mitigate costs Other research in signaling and strategic classification has considered models in which manipulation is desirable from the learners point of view Though this perspective diverges from the one we consider here we acknowledge that there do exist cases in which manipulation serves to improve a candidates quality and thus leads a learner to encourage such behaviors It is important to note however that although this account may accurately represent some social classification scenarios differential group access to manipulation remains an issue and in fact cases in which manipulation genuinely improves candidate quality may present even more problematic scenarios for machine learning systems As work in algorithmic fairness has shown feedback effects of classification can lead to deepening inequalities that become justified on the basis of features both manipulated and natural The rapid adoption of algorithmic tools in social spheres calls for a range of perspectives and approaches that can address a variety of domain-specific concerns Expertise from other disciplines ought to be imported into machine learning informing and infusing our research in motivation application and technical content As such our work seeks to investigate from a theoretical learning perspective some of the potential adverse effects of what sociology has called quantification a world increasingly governed by metrics In doing so we bring in techniques from game theory and information economics to model the interaction between a classifier and its subjects This paper adopts a framework that tries to capture the genuine unfair aspects of our social reality by modeling group inequality in a population of agents Although this perspective deviates from standard idealized settings of learner-agent interaction we believe that so long as machine learning tools are designed for deployment in the imperfect social world pursuing algorithmic fairness will require us to explicitly build models and theory to address critical issues such as social stratification and unequal access