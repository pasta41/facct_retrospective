The Profiling Potential of Computer Vision and the Challenge of Computational Empiricism Computer vision and other biometrics data science applications have commenced a new project of profiling people Rather than using transaction generated information these systems measure the real world and produce an assessment of the world state in this case an assessment of some individual trait Instead of using proxies or scores to evaluate people they increasingly deploy a logic of revealing the truth about reality and the people within it While these profiling knowledge claims are sometimes tentative they increasingly suggest that only through computation can these excesses of reality be captured and understood This article explores the bases of those claims in the systems of measurement representation and classification deployed in computer vision It asks if there is something new in this type of knowledge claim sketches an account of a new form of computational empiricism being operationalised and questions what kind of human subject is being constructed by these technological systems and practices Finally the article explores legal mechanisms for contesting the emergence of computational empiricism as the dominant knowledge platform for understanding the world and the people within it CONCEPTS Social and professional topics Computing technology policy Surveillance vision Machine learning Applied computing Law KEYWORDS Computer Vision Data Science Biometrics Law and Policy Computational Empiricism INTRODUCTION profiling means different things in different traditions The proliferation of machine learning and data science in profiling however make the European General Data Protection Regulation GDPR definition especially meaningful Under that regime profiling includes the automated evaluation of certain traits about a person reminding us of the legal significance of classification even without subsequent discrimination This definition also applies to an emerging class of automated image-based profiling technologies that evaluate traits about individuals Computer vision techniques make sense of image or video data by measuring using machine learning to transform those measurements into representations and subsequently into decisions and classifications The online profiling discussed over the past several decades typically concerned knowledge production from transaction generated information created by interacting with informational environments framing individuals in terms of proxies and scores Computer vision and other biometric techniques however use extremely granular measurements of the real world and increasingly deploy a logic of exposing or revealing the truth of reality and people within it This article explores the nature of those knowledge claims and what might be the appropriate register of legal intervention to address data sciences dominance as a paradigm for knowing people Computer vision generates classifications and knowledge about scenes objects events and people Computer vision is not the only domain of data science articulating this new epistemological stance Similar ideas are evident in numerous data science applications often those involving biometrics Computer vision does however provides a useful entry point into what is qualitatively new about the empirical and epistemological position being developed When applied to people computer vision has clear legal and social significance Facial recognition for instance one such notorious application links a persons portrait to some form of institutional identity like a drivers licence By connecting an individual to their behaviour in physical space facial recognition couples identity across physical and computational registers enabling automated decisions to be articulated into physical environments Another example is vehicle registration and automated number plate recognition Both the intrinsic limitations and problematic applications of these technologies are well documented with calls for regulation not far behind However there is already much more that computer vision can do when looking at people Beyond identification computer vision can also classify people As well as linking your image and spatial location to The term looking at people is an umbrella for research on computer vision applied to persons see eg an institutional identity computer vision can make decisions about non-visual attributes such as what type of person you are what you are doing what you are feeling and how you are likely to act in the future These decisions may be about identity gender emotional state or future behaviours But more controversial classifiers have also addressed questions of sexuality criminal propensity political position IQ workplace suitability and paedophilic tendencies That type of computational physiognomy is one element of an emerging eld of personality computation often described as Apparent Personality Analysis APA or Apparent Personality Recognition APR Personality computation uses faces postures movements actions gestures interactions and emotions as well as whether those emotions and expressions are real or fake to infer personality traits different analytic techniques operate on different data inputs Some use dynamic information such as what a person does or the way a physical morphology changes Some use static information such as how a person looks Some use multiple modalities like combinations of visual and audio data in different configurations Whatever the particular decision or classification the premise of these techniques is generating knowledge by applying data science to images in a way that transforms measurements of the world in the form of pixel vectors into classifications The scientific merit of computer vision profiling especially when applied to non-visual characteristics of persons has been rigorously challenged But the scientific validity of these projects is not the focus of this article As Ian Hacking reminds us there is a long history of producing social knowledge through new systems of measurement both at the level of groups and individuals and that new classifications and new enumerations are inseparable The purpose of this article is therefore to examine those systems of enumeration and classification try and understand the basis on which they produce knowledge about people and explain why this presents unique challenges The profiling exercises outlined here are thus presented as examples of the shift towards computational empiricism or positivism as a dominant knowledge system likely to have ongoing effects in juridical and political practices While the knowledge claims of computer vision profiling are often modest or tentative in certain papers authors have begun to argue that only through computation are we able to access all the information available in human faces and truly know people In other words only through computation can the excesses of the physical world and the people within it be perceived processed and understood While the trope that technology facilitates greater knowledge of nature is a familiar one it still requires close analysis in the specific context of computer vision That analysis has become more imperative in the context of Trevor Paglens identification that Human visual culture has become a special case of vision and exception to the rule in a situation where actually The overwhelming majority of images are now made by machines for other machines While computer vision images may be increasingly invisible they still operate within the dynamics of representation and knowledge and they still interact with human visuality Western See eg where they discuss the problems of Paul Elkmans Facial Action Coding System which measured the way peoples faces change when they express emotions suggesting the use of dynamic information does not necessarily solve any problems thought has spent a good deal of time reeffecting on the connection between actuality and photo as well as the consequences of living in a world coded by undecipherable technical images This material offers a useful touchstone for understanding the computer vision project of looking at people and its political and epistemological consequences While there has been a great deal of exceptional work on the problematic application of computer vision technologies the goal of this article is thus less about exposing bias or lack of fairness and more about understanding the programs of the computer vision apparatus Rather than challenge computer vision at the level of problematic applications this article instead argues that to avoid becoming a vector of data sciences politics of completion legal narratives should highlight the fundamental contingency of computational empiricism and its knowledge claims This article begins with a historical and technological contextualisation of personality computation It looks at similar exercises from the past and tracks the history of combining photographic portraiture with statistical analyses That section demonstrates the ongoing ideologies behind this type of profiling and how they are serviced by big data epistemology However this is identified as only one of the political problems of this type of classification another being the reverence for the forms of empiricism operationalised Through the historical analysis the idea is to demonstrate not how statistics and mathematics are becoming instruments of real or symbolic violence but rather the nature of the change from visual to statistical forms of knowing The article then seeks to sketch an outline of this emerging computational empiricism by connecting the knowledge claims of computer vision profiling to antecedent media technologies The goal is to understand the ongoing role of representation within the systems of measurement at work in computer vision and the politics of their dissimulation Finally the article explores possibilities for legal and technical intervention In that section it is argued that the narratives animating existing legal protections are becoming deficient in the face of technological practices that see humans in a new way primarily as patterns of information Accordingly the article suggests that when dealing with profiling privacy and data protection now need to focus as much on metaphysics as they do on metadata Rather than the elimination of bias or unfairness or the total prohibition of certain applications both of which have their place within legal armatures the argument here is for legal mechanisms that marginalize purely computational ways of knowing The goal is not to put the genie back in the bottle but rather highlight and contest the ontological power of computer vision when classifying people COMPUTER VISION PROFILING Computer vision is a family of technologies and practices by which computational systems come to understand and make decisions about the physical world and the people within it When looking These terms are intended to follow the definitions provided by Vilém Flusser in the book includes a Glossary At Apparatus pi a plaything or game that simulates thought trans An overarching term for a non-human agency eg the camera the computer and the apparatus of the State or of the market organization or system that enables something to function At Program a combination game with clear and distinct elements trans A term whose associations include computer programs hence the US spelling at people applications range from relatively benign to highly problematic The more problematic applications do however typically reveal more in terms of embedded technical ideological and epistemological assumptions Forms of computational analytics have been used in personality analysis since the s But the history of combining photography with statistics to explore correlations between external appearance and internal personality goes back much further Kate Crawford reminds us that If algorithms present us with a new knowledge logic then it is important to consider the contours of that logic and by which histories and philosophies it is most strongly shaped Connecting computer vision projects to previous photographic ones thus demonstrates what lurks behind these systems of human classification and how previous claims about the mechanical objectivity of the camera have been displaced into the computational objectivity of statistics But rather than demonstrate how data science can be a vector for ideology the goal is to demonstrate what else is at stake in the computer vision project when looking at people in particular the proliferation of computational empiricism the difference between visual and statistical knowledge and the idea that data science can know people better than they know themselves classification and Ideology Criminal portraits were one of the rst datasets systematically produced after the invention of the daguerreotype photographic process Founder of British Eugenics bio-metrician and Charles Darwins cousin Sir Francis Galton used these portraits to create composite images intended to expose the mean appearance of criminality Both phrenology and physiognomy had already been scientifically discredited by the time of Galtons experiments in However belief in photographys mechanical objectivity gave Galton a new though crude statistical tool to investigate his hypothesis of biological degeneration Galtons experiments were ultimately failures He found that the visual similarities of the criminal classes disappeared through the composite process which instead revealed the common humanity in all Despite this failure however and despite the stigma associated with eugenics and Social Darwinism the experiment has been repeated using new photographic techniques statistical methodologies and biological theories One important differentiation however is the movement in knowledge paradigm from qualitative visual searches for similarity amongst groups towards purely quantitative statistical analysis In parallel with ongoing physiognomic experimentation there has been a growing psychological literature exploring how rst impressions are generated from looking at faces including impressions of criminality That material demonstrates how faces are a form of non-verbal communication and part of the intrinsic heuristics we use to navigate daily social interactions However it has also led to the claim that research on appearance-based personality assessment might seem more credible if society were Lorraine Daston and Peter Galison explain the term mechanical objectivity in their text Objectivity as the insistent drive to repress the wilful intervention of the artist-author and to put in its stead a set of procedures that would as it were move nature to the page through a strict protocol if not automatically They note how the camera could quiet the observer so nature could be heard and let nature speak for itself not so enamoured of the idea that because a persons appearance ought not to make a difference it does not blaming the dearth of this research on the naturalistic fallacy that is confusing how things are with how they ought to be In rejecting the naturalistic fallacy the theoretical justifications for this work have at least moved away from the biological or genetic determinisms of Social Darwinism and is justified with other ideas Some explicitly avoid the politics of investigating correspondences between personality and appearance by claiming their work interrogates the pseudoscience of physiognomy in order to understand human subjective judgments ie stereotyping Others deploy Darwins ending of adaptive significance in the capacity to make quick social judgments what is called environment of evolutionary adaptation studies to provide an ecological rather than biological theory On that basis researchers like Valla Ceci and Williams argue that faces even in the absence of dynamic behaviour may be informative In their study of the accuracy of physiognomic interpretation they cite multiple studies showing that accurate physiognomic judgments can be made by humans Whether it is genetic determinism biased social information processing or ecological accounts there has however never emerged a stable and accepted physiognomic theory More than the success of evolutionary adaption theories it is the absence of an accepted or unified theoretical justification that has become the mechanism by which these physiognomic experiments continue Although the relationship between causation and correlation in scientific method is complex these projects seem content in Kuhns terms to travel the road from scientific law to measurement in reverse and focus on correlations entirely unpinned from theoretical accountability To that end explorations of facial morphology through statistical methods have proliferated and gained disciplinary legitimacy with entire fields and competitions sponsored by industry actors like Microsoft Facebook Amazon and even Disney for personality and social interaction typing through image and video data With its capacity to surpass the visual with the quantitative computer vision represents the technological boon that Galton was missing Computational Personality Analysis The success of machine learning and particularly convolutional neural networks in computer vision mean APA and APR have become significant sub-fields in computer vision research Whatever the specific technique this research assumes a stable statistical relationships between a stimuli and social perception of personality in APA or true personality characteristics for APR Whereas See eg where Alley neatly categorises common approaches in physiognomy including a seeking resemblances in appearance between humans and animals assuming similarities in appearance indicate shared psychological qualities eg a person whose face is reminiscent of a fox is believed to be sly and cunning b an inductive process whereby others sharing distinctive facial characteristics with a known person or group are believed to share some of the familiar persons groups psychological characteristics c an approach based on functional analogies whereby inferences are made according to the size shape or presence of facial features and their common functions and d an approach based on the facial expressions engendered by various emotional and cognitive states that looks for traces of these expressive facial postures as clues to the common emotional and cognitive states of an individual One example is a study on inferences of criminality that claimed success in replicating Galtons process Kozenys Experimental investigation of physiognomy utilizing a photographic-statistical method used criminal portraits to create composites for crime categories which were then physionogmically classified from the visual information in Kozeny APA avoids interrogating the accuracy of its claims instead focusing on personality traits attributed by others often Amazon Turk workers APR on the other hand seeks to identify the true personality of the individual Although these disciplines explore multiple stimuli including dynamic facial information handwriting and speech they also include what could be called computational physiognomy Early work on computational physiognomy through computer vision was based on very simplistic models using Euclidean distances the straight-line measurement between points in a geometric plane and geometric angles to classify facial features corresponding with the pioneering physiognomer Lavators division of the face into classes Initially these experiments were framed in terms of investigating whether computers could replicate the trait evaluation performed by humans ie rst impressions analysis There were no complex machine learning methods or assessments of accuracy It was simply a translation of the task of physiognomic measurement into a computer vision system From that point however this type of personality computation became far more sophisticated It was not long until convolutional neural networks were predicting intelligence and other personality characteristics based on images Competitions for computational personality analysis both apparent and true began in with deep learning dramatically improving the early unimpressive results in classifier accuracy Deep learning enables measurement at a radically different scale than earlier classifiers that used engineered low-level image features like facial landmarks or intermediate level attributes like nose glasses moustache hair and forehead height How neural networks change the basis of knowledge production as compared to visual systems is demonstrated very clearly in several computer vision and data science projects for instance Wu and Zhangs paper Automated Inference on Criminality Using Face Images That paper used government ID images about half of which included individuals with a criminal conviction as a dataset for a criminal-propensity classifier The theoretical grounding of this research is similar to other APR exercises ostensibly testing the social inference hypothesis But the research is more revealing in terms of how computer vision systems claim to generate knowledge about individuals Most telling is the authors acknowledgment that the variance between criminal and non-criminal populations is not evident from visual assessments or simple Euclidean measurements In other words visual information in the sense of information that is visually perceivable and interpretable what Galton had unsuccessfully relied on was insufficient for physiognomic purposes Only through the higher dimensional computational analysis of numerical quantitative measurements was this statistical separation discernible This ending represents the premise and utility of neural networks for this application and is highly illustrative of the shift from qualitative visual to quantitative statistical ways of knowing people This change in the nature of knowledge production is made even These advances were typically based on research into object detection using convolutional neural networks in Work of this type often uses the OCEAN model for personality description These authors built a criminality classifier not on the basis of a statistical mean as sought by Galton but by measuring variance from a non-criminal normal ie based on the idea that there is a greater resemblance in non-criminal faces more explicit in Wang and Kosinskis controversial physiognomic work Deep Neural Networks are More Accurate than Humans at Detecting Sexual Orientation from Facial Images That project used facial images extracted from an existing database of which were identifiably gay according to correlation with a Facebook Audience Insights platform and self-declared sexual interest on Facebook A deep neural network called VGGFace originally deployed for facial recognition that transforms facial images into particular scores was used for pattern analysis and classification While the authors begin from the proposition that DNNs are increasingly outperforming humans in visual tasks such as image classification facial recognition or diagnosing skin cancer they ultimately conclude that our faces contain more information about sexual orientation than can be perceived or interpreted by the human brain This ending suggests that if the data sets can be constructed and the results are to be accepted there are few limitations to what characteristics might be inferred from images There is something historically significant in physiognomy being a platform for the advancement of computational empiricism Criminologist Nicole Rafter has brilliantly argued with respect to the earlier analogue practices of physiognomy and phrenology that their scientific invalidity was insignificant when compared to the epistemological paradigm that they ushered in Those pseudosciences shifted how we understand people and their behaviour away from metaphysics and theology and towards analytical empiricism Rafter shows how phrenology the discredited science of the correspondences between the external and internal man the visible and the invisible contents but based on reading character traits from skull morphology therefore produced one of the most radical reorientations in ideas about crime and punishment ever proposed in the Western world It was radical for its providing a measurable explanation of criminal behaviour and other social phenomena on the basis of positivist reasoning with empirical methods and it transformed criminology penology and jurisprudence by changing how we generate knowledge about people It is argued that the profiling techniques described above should be thought of as data points in the movement towards computational empiricism as a dominant knowledge system Beyond laundering various ideological even eugenicist politics through technological neutrality the claim here is that the foundational knowledge claims of these technical systems exhibit their own political even philosophical position Computational physiognomy like its analogue predecessor is not the only application of this empirical paradigm Rather it is best understood as a harbinger of an evolving epistemological environment Other biometric data science applications operate on a similar knowledge-logic For instance automated analysis of voice and speech are being used to assess ethic origins for migration assessments as well as making customs assessments of migrants and tourists There is also a growing health-tech industry where recording of particular bodily functions like sleeping and eating speaking or physical interactions with a computer are being computed into assessments of physical and psychological health The results were post-hoc justified with a prenatal hormone theory that over or under exposure of androgens during gestation affects both facial appearance and sexual orientation which gave a biological foundation to the statistical endings The accuracy of these systems is not going to determine their epistemological validity Projects are being funded and the political will is there to deploy them The empirical sciences accordingly follow building new ways to classify and know persons Data science and its particular systems of measurement and classification are thus becoming the prime-movers in building this new epistemological terrain COMPUTATIONAL EMPIRICISM The claim that nature can only be understood through computation articulates data science as an attempt to access the hidden mathematical sub-structures of reality In many respects this claim is neither unique nor controversial but it remains significant in the context of a technological capacity that is proliferating so rapidly and widely It is also far from universally accepted Some computer scientists sensibly argue that deep learning cant extract information that isnt there and we should be suspicious of claims that it can reliably extract hidden meaning from images that eludes human judges But such admissions seem peculiar when really deriving meaning from measurements too granular for non-computational analysis is the very premise of machine learning Further as argued the accuracy of those applications is unlikely to be the primary determinant of whether their outputs are interpreted as true To that end some have begun to argue that claiming access to the hidden structures of reality has now become the organizing principle of data science Many technologies of representation claim to reveal previously unseen information Photography for instance was understood as a way to see into natures cabinet It challenged the optical unconscious Walter Benjamin describes this access to nature as opening up in this material the physiognomic aspects of the world of images which reside in the smallest details clear and yet hidden enough to have found shelter in daydreams Other optical technologies oer similar narratives The telescope gave access to celestial knowledge the microscope to cellular knowledge X-ray imaging and radiographic measurements of material density afforded a New Sight that could reveal hidden existence Roberta McGrath describes public attitudes around x-rays noting The body itself is thus perceived as literally ghost-like immaterial only flesh as being merely a thin veneer literally a skin covering a hidden deeper reality which will like truth be uncovered revealed Artists and intellectuals like Umberto Boccioni similarly responded Who can still believe in the opacity of bodies since our sharpened and multiplied sensibilities have already penetrated the obscure manifestation of mediums The idea of the body as readable medium and material manifestation of a deeper informational truth is also highlighted in Mark Andrejevics analysis of neuromarketing where he describes how The notion that bodies are for marketing purposes more truthful than the words they utter is emerging as a recurring theme in the promotion of neuromarketing which promises to render obsolete the allegedly quaint and outdated techniques of surveys and focus groups They can although those authors do then give the conflicting argument that On a scientific level machine learning can give us an unprecedented window into nature and human behaviour allowing us to introspect and systematically analyze patterns that used to be in the domain of intuition or folk wisdom thanks to new technologies cut through directly to the underlying truths revealed by the brain Andrejevic places this claim within The appeal of techniques for bypassing discursive forms of representation by cutting straight to the brain against the popularised and reflexive mediated critiques of discursive forms of representation for their potentially deceptive indeterminate and constructed character These approaches can thus be genealogically situated amongst the behavioural sciences wherein the premise of stable relationship between stimuli and character afforded mechanisms for personality and behaviour computation such as digital phenotyping They also represent an historical trajectory of understanding human subjectivity as an information pattern bound within the material container of the body that becomes its expression As William Gibson said data made Computer vision and biometric data science add another dimension to these narratives however Photographs telescopic observations radiographic photograms and even neuromarketing enabled a form of discovery that still requires human interpretation In computer vision the measurement encoding and decoding and knowledge discovery are increasingly automated Through machine learning and neural networks even the selection of representations that is the mechanisms by which the world is presented for analysis is also automated The elements of images from which meaning is derived often called features are selected through automated learning processes instead of the laborious manual process of feature engineering Through these processes the clinical is replaced with the empirical image comprehension is replaced by image computation observation replaced with measurement with the truth the deeper hidden reality only becoming available in the high-dimensionality that neural networks can process This is not the combination of inputs from multiple large data sources to generate new hypotheses about the way the world works and prescriptions for how to act upon that knowledge typically associated with big data and predictive analytics It involves a deeper belief in how the world can be understood premised on new systems of measurement new systems of representation new understandings of human subjectivity and new forms of statistical analysis Measurement and Representation Accessing the hidden mathematical substructure of reality requires numbers Reducing the real world to numbers requires measurement Accordingly computer vision systems do not see they measure They measure visual data x in order to determine a world state As Sheila jasanoff notes Any form of data collection involves to begin with an act of seeing and recording something that was previously hidden and possibly nameless In the same way that photography bypassed the optical unconscious computer vision profiling is about noticing measuring and analysing that which was previously not available to human perception and cognition The process is complex however because the relationship between x and is not one-to-one Photographic images are reductions of the three-dimensional world into a two-dimensional set of See Kichin in where he says Big Data analytics enables an entirely new epistemological approach for making sense of the world rather than testing a theory by analysing relevant data new data analytics seeks to gain insights born from the data measurements There are however multiple configurations of the three-dimensional world that might result in any particular two-dimensional output In other words there may be many real-world configurations that are compatible with the same measurements Thus the chance that any possible world state is present or true is described using probability The probabilistic techniques of statistical clustering used to generate meaning about the world state typically use machine learning But machine learning is computationally demanding and it cannot operate on absolutely all the data that could be derived from an image Instead it is necessary to marshal selections of data into representations When computer vision emerged as a discipline in the s it became apparent that those systems could not respond to the totality of recorded signal an image Image data typically comes as RBG values per pixel inscribed by the translation or transduction of light energy to voltage in a sensor The computational resources required to perform statistical analysis on that amount of data is immense Therefore instead of image level computation computational economy requires transformations into symbolic representations The inability of computer vision systems to respond directly to the totality of registered signal has been acknowledged by some as a failure and others as a fundamental limitation Nevertheless it remains the primary mechanism by which computer vision translates measurements of the world into the computational register That means features of visual data have to be selected for analysis but as noted above that selection is increasingly automated In the last few years significant research has gone into ways to avoid manual assessments of images to identify what might be relevant for building classifiers ie feature engineering in favour of a statistical pattern recognition that can identify a relevant feature on the basis of its probabilistic relationship to other features in the environment This is sometimes called representation learning where a system is fed with raw data to automatically discover the representations needed for detection or classification Deep learning means multiple layers of representation automatically generated where each level of representation is more complex and abstract than the previous one As LeCun Bengio and Hinton explain The key aspect of deep learning is that these layers of features are not designed by human engineers they are learned from data using a general-purpose learning procedure The inherent paradox of this epistemological platform is the use of correlation to create invisible or inaccessible automated representations in order to surpass the use of manual representation In other words overcoming the limits of representation with more representations Framing it this way prompts us to reconsider imagistic representation within data scientific contexts Much has been written about how knowledge is modulated through natural representations like the index and icon But now similar investigations are necessary for mathematical imagistic representations like functional representations where a function is t to the discrete and finite set of measurements that constitute an image such as the pixel coordinates and values linear representations where This is also discussed by Andrejevic in images are unwound into a vector matrices spatial frequency representations that measure the speed at which a particular quality changes across an image surface relational representations where images are represented with graphs and probabilistic representations where mathematical tools are used to estimate the best version of a particular image given a measurement of a corrupted or noisy image Representation in computer vision has changed into a project of ending the target thing ie the pattern from all the information within an image With real consequences for how we derive meaning from the visual world Computational Empiricism as a Dominant Epistemology As society becomes more statistical it makes sense that both representation and knowledge take less deterministic forms However from the above we can begin to draw an outline of new forms of computational empiricism or positivism as applied to understanding people First it operates on the basis that external measurement or observation is a more reliable pathway to knowledge than the symbolic output of a subject This is of course not unique to these practices or technologies it is simply one element of the schema It is also not a uniquely visual phenomenon In addition to the techniques described above the technology of the stethoscope and the practice of auscultation listening to the body at a physical distance offers another useful example of technological mediation making the body an object of knowledge Histories of diagnostic practices with stethoscopes have been invoked to demonstrate the movement from theoretical to perceptual ways of knowing the body achieved through the combination of rationality and empiricism This line of thinking describes the ascendance of empiricism in this medical context as entwined with the construction of a new subject and productive of a new medical epistemology of pathological anatomy The updating of remote sensing in medical applications with new sensors and data science is similarly oriented towards constructing epistemologies of pathological behaviours However empirical data science engages with a different human subject from those older techniques one that is simultaneously interpreted and constructed an informational pattern rather than embodied puzzle A second element to the schematic is a specific type of computational intervention in the relationship between measurement and classification On one hand computational systems differ from other representational technologies because of their capacity to both measure and process quantities of data that are too large for human tabulation too discrete for human perception and too complex for human cognitive analysis This is also not unique to computer vision or data science Where these techniques differ however is the degree of automation in the selection of elements or features or dimensions of a measurement deemed to be meaningful In Note the term computational empiricism was used by Paul Humphries in where he discussed the idea in the context of changing scientific methods associated with the adoption of powerful instrumentation and powerful computational devices His point is to suggest notions of logical empiricism require updating by computationally oriented methods The term computational positivism is best described in Narasimha where he uses the term to describe methodologies in exact sciences and mathematics that focus on matching algorithms to observations rather than drawing conclusions from axioms and models other words decisions about what elements of a measurement inform each layer of representation are increasingly displaced into automated learning systems Those are highly political decisions about how the real world is translated into the symbolic register of computation and are handed o to automated systems Of course humans participate in selecting input data ie what the sensor captures and defining the accuracy of outputs which each participate in tuning the selection of representations and their parameters ie weightings but the process of producing representations is dissimulated into the architecture of the system A third element is a belief that this process is working towards exposing the fundamental substructures of reality The belief that increasingly granular measurement and high dimensionality analysis has the capacity to reveal hidden truths is visible to a greater or lesser degree in research projects using high resolution sensors and learning applications This computational variant of realism has been described by Dan McQuillan as machinic neoplatansim That is a metaphysical commitment to a world of truth form and idea existing behind and only imperfectly imprinting on the world of the humanly sensible accessible only through mathematics Rather than being a specific tool or method for McQuillan data science thus represents an automated and applied philosophy maintaining an epistemological reverence for a hidden layer of reality which is ontologically superior expressed mathematically and apprehended by going against direct experience None of these observations are unique to technologies of computer vision the practices of machine learning or the disciplines of data science However when schematised and directed at understanding people they represent an idiosyncratic system of knowledge production that challenges the way legal narratives as well as many others have been deployed to protect individuals in the context of online profiling LAW IN THE WORLD STATE A classic critique of technology insists that technological mediation inhibits access to the real or the event In the case of photography for instance we are reminded that the images we create while supposed to be windows or maps for understanding the world around us actually operate more like screens Rather than expose the truth of the world our images saturate the world producing a veneer under which the real slowly decays A form of this critique is often levelled at digital profiling wherein data produced through interactions with information environments are used as proxies for defining characteristics about us That is the world of actuarial risk assessment and the scored society Critiques of those technologies describe how such scores inadequately capture or represent individuals Proxies result in reduction distortion and error Justin Clemens and Adam Nash oer a useful account of this process wherein information must rst be digitised to data then modulated between storage and display in an endless protocol-based negotiation that both severs any link to the datas semantic source and creates an ever-growing excess of data weirdly related to but ontologically distinct from its originating data source In other words the mathematical codes and conventions used to analyse and parse already hyper-mediated digital information omit or marginalise their natural starting point Clemens and Nash thus claim that only through modulation into a display register does digital information obtain meaning This intervention frames the harm to persons from profiling in terms of loss It also grounds for instance data protections animating principle of transparency Legal narratives around transparency particularly in the context of data protection emerged to deal with that potential for misrepresentation inaccuracy or distortion in cases where individuals are transcribed into computational systems and categorised ie profiled on the basis of pattern analysis Data subject rights of access and rectification are thus deployed to help individuals maintain the borderlines of meaning about themselves as data is disclosed and processed across contexts However as technical narratives change they also challenge this legal mechanism The scores and proxies used in for example computer vision profiling are of a different class and scale than those used in actuarial analysis and other forms of data mining The scores of behavioural computation and personality computation take the shape of a ledger through which behavioural characteristics can be collated Through its new epistemological program data science then starts to move from a logic of approximation to a logic of revelation From the computational empiricist position in which the quantitative is the path to knowledge the world of infinite possibility now exists in the data rather than in the human spirit The natural starting point is no longer omitted but rather measured in a dimensionality that humans can neither access nor interpret profiling systems claiming deeper knowledge about persons thus challenge the utility of transparency as a legal narrative and dismiss the role of the captured subject in forming their own identity Thinking through how law or any other system of governance might address these technologies accordingly means attending to this re-arrangement of technological practice and how it constructs persons Legal thinking has so far responded to these technologies and techniques in different ways One register seeks improvement of automated systems encoding of the world The goal is to achieve a fairer computational translation of the real world by exposing and limiting bias and prejudice Unfair discrimination can nd its way into automated systems in multiple ways and improving the outcomes of automated decision systems remains critically important But scholars like Frank Pasquale have begun to ask whether this form of accountability adequately considers the question of accountability to whom Without proper attention to that question this form of accountability risks becoming part of the feedback mechanism that continually improves and thus proliferates automated decision making through legal optimisation Yarden Katz similarly comments that If AI runs society then grievances with societys institutions can get reframed as questions of algorithmic accountability This move paves the way for AI experts and entrepreneurs to present themselves as the architects of society Fairness and accountability through decision system optimisation thus fail to address the problem that computational empiricism should be understood as simply one way of knowing especially when applied to persons Rather than challenge the dominance of computational empiricism fairness projects focused exclusively on system improvement risk law becoming enrolled in data sciences politics of completion As Joshua Scannell notes it is not at all clear that biometric accountability accuracy and fairness are mechanisms for achieving justice or even a baseline common humanity That is not what biometric systems do Instead they reeffects the institutional demands of the entities that employ them demands that are always intended to sort people into those with access or without secured by or made insecure by the state capacitated or incapacitated by a credit score Another legal register challenges specific applications of data science on ethical or political grounds This legal mode identifies where data science applications have too pernicious an effect on society according to liberal democratic values This is typically a more traditional privacy mechanism defending the fundamental dignity and opacity of persons against overreaching applications These projects similarly deploy their force at the level of application rather than at the epistemological or political bases of computational empiricism Beyond abandoning or optimising the technologies however there remains an important space for demonstrating the contingency of computational empiricisms knowledge claims and challenging data sciences move from metadata to metaphysics This is what Mireille Hildebrandt for instance means when she talks of speaking law to the power of statistics a program of re-inscribing uncertainty into automated knowledge production Law and Computer Vision profiling There are already important legal limitations on automated decision making and automated profiling in for instance the GDPR Alongside well explored access and rectification rights and the fundamental principles of processing in Article Article gives data subjects rights to not be subject to automated profiling decisions based solely on automated processing where those decisions produce legal or similar effects There are numerous limitations to that latter provision including explicit consent enabling legislation or satisfaction of a contract What constitutes legal or similar effects as well as purely automated processing are also unclear That a human decision maker anywhere in the process might remove an automated system from the purview of the Article seems a problematic limitation especially considering the growing evidence that human decision makers rarely contest the outcomes of decision support systems Further guidance by the Court of Justice of the European Union will be essential to a more complete understanding of these provisions A right to explanation has also been read into Art producing rigorous debate over what the Article truly affords data subjects and how useful it may be While a similar and arguably broader provision has been in place since a broad reading may give data subjects useful mechanisms for not only understanding but properly contesting and challenging automated decisions However it still places a substantial impetus on the data subject to protect its own rights and only indirectly challenges the knowledge logic of computational empiricism Explainability may even be harmful if entrenching automated decision making and narrowing the types of reasons to be given for decisions and thus the grounds for contestation Being subjected to automated decisions See eg Arts access rectification erasure restriction notification data portability These include lawfulness fairness transparency purpose limitation data minimisation accuracy accountability storage limitation integrity and confidentiality Art a b and c without understanding how or why that decision was made may be problematic But receiving inadequate automated explanations without recourse might be worse In that format explanation similarly becomes a legal vector for proliferating decision automation However other approaches appear to more directly challenge data sciences epistemological dominance Hildebrandt for instance outlines several concepts that challenge data sciences program of quantification what she describes as the over-complete datafication of anything and everything based on the idea that the mathematics that grounds all these machines reveals the ultimate layer of a hidden reality She offers a right to human non-computability built on the philosophical principle of indeterminate identity This is not necessarily a return to older opacity paradigms of privacy a relocation of the black box to the level of the individual but rather a mechanism for limiting certain classes of knowledge claims It is achieved in her vision through agonistic machine learning systems capable of demanding that companies or governments that base decisions on machine learning must explore and enable alternative ways of datafying and modelling the same event person or action Agonistic systems would demonstrate how each act of computation relies on a specific system of measurements representations and analytics Rather than challenging human computability wholesale the project highlights how humans can be computed in multiple ways in order to ward omonopolistic claims about the true or the real representation of human beings This is arguably also a form of explanation and accountability but one targeted more appropriately at the decision making process itself rather than the relationship between input and outcome ie counter-factual analysis In other words an explanation mechanism informing how to transcend the solution space of any particular decision This type of mechanism is appealing because it more directly addresses the construction of the human subject as information pattern It also recognises the significance of representations in a manner similar to technical approaches like disentangling the factors of variation in order to produce disentangled representations What this manner of legal intervention would actually look like requires more legal conceptual and technical thinking however But it at least offers a pathway to envisioning new projects built for a technical age that sees humans in a very different way than our existing legal systems insist on seeing them CONCLUSION This article has attempted to outline some emerging challenges produced by a profound new technical capacity Lawmakers now have to contend with a radically extrapolated enlightenment philosophy insisting that measuring everything and drawing knowledge from those measurements is the path to truth Permitting such measurement and classification without limitation risks generating a fair transparent and non-prejudicial totalitarianism This has been described as the insanity proper to logic whereby measuring everything logic conceives a world in which all things are relative makes itself absolute and denying the whole of nature establishes its own artifices Addressing this reality means clarifying how computer vision systems and data science applications are merely apparatuses combining and computing symbols that encode the world a particular way according to particular programs Without intervention at that level digital systems risk simultaneously constituting both the world and the dominant understanding of it As Flusser reminds us if humans cease to decode technical images and instead project them unencoded back onto the world out there the world itself becomes like an image a context of scenes of states of things Critiquing data science therefore means exposing the difference between world and the world state and challenging the idea that such systems access a hidden reality instead of producing and operationalise a para-reality built from para-visual representations Computer vision has been presented as one vector of that knowledge logic and one target of a new kind of legal thinking However this is certainly not the case for computer vision alone It is relevant for any data science application translating the real world into the symbolic register of computation The goal is to open up space for legal ideas that introduce contingency and automated un-decidability into those translations that are building the environments ordering social experience Such projects seem more and more imperative especially in the face of a new form of computational empiricism designed for looking at people