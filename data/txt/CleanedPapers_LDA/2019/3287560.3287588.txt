Fairness-Aware Programming Increasingly programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people The issue of fairness in automated decision-making has thus become a major problem attracting interdisciplinary attention In this work we aim to make fairness a first-class concern in programming Specifically we propose fairness-aware programming where programmers can state fairness expectations natively in their code and have a runtime system monitor decision-making and report violations of fairness We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature as well as others As the decision-making program executes the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated reporting such violations to the developer The advantages of this approach are two fold i Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness as the programmer does not have to write ad hoc code for maintaining statistics ii Compared to existing techniques for checking and ensuring fairness our approach monitors a decision-making program in the wild which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested We describe an implementation of our proposed methodology as a library in the Python programming language and illustrate its use on case studies from the algorithmic fairness literature CONCEPTS Theory of computation Program specifications Software and its engineering Specification languages KEYWORDS Probabilistic specifications Fairness Assertion languages Runtime monitoring Runtime verification INTRODUCTION With algorithmic decision-making becoming the norm the issue of algorithmic fairness has been identified as a key problem of interdisciplinary dimensions Across the vast computer science community the past few years have delivered a range of practical techniques aimed at addressing the fairness question In this work we aim to make fairness a first-class concern in programming Specifically we propose fairness-aware programming where developers can state fairness expectations natively in their code and have a runtime system monitor decision-making and report violations of fairness This programming-language-based approach to fairness yields two advantages The developer can declaratively state their fairness expectations natively in their code and have them checked As such the developer need not write ad hoc code for collecting statistics and checking whether fairness definitions are violated they simply state the fairness definitions that they expect to hold We argue that embedding notions of fairness directly within the programming language reduces the barrier to investigating fairness of algorithmic decision-making The past few years have witnessed numerous techniques for constructing fair classifiers as well as for testing and auditing fairness of existing decision-making procedures Generally these techniques assume a provided dataset or distribution that is representative of the population subject to decision-making Unfortunately the data may be itself biased eg due to historical discrimination or it may simply be unrepresentative of the actual population By monitoring actual algorithmic decisions as they are made we can check whether fairness is violated in the wild which for instance may exhibit different population characteristics than the data used for training and validation in a machine learning setting Fairness Specifications Consider a software developer who constructed a decision-making procedure following the best practices for ensuring certain notions of fairness relevant to their task Once they have deployed their decision-making procedure they want to ensure that their fairness assumptions still hold To enable them to do so we propose treating fairness definitions as first-class constructs of a programming language Thus the developer can specify in their code that a given procedure perhaps a learned classifier satisfies some notion of fairness As the procedure makes the decisions the runtime system monitors the decisions potentially inferring that is violated therefore alerting the developer The approach we propose is analogous to the notion of assertions which are ubiquitous in modern programming languages For instance the developer might assert that x indicating that they expect the value of x to be positive at a certain point in the program If this expectation ends up being violated by some execution of the program the program crashes and the developer is alerted that their assertion has been violated We propose doing the same for fairness definitions The difficulty however is that fairness definitions are typically probabilistic and therefore detecting their violation cannot be done through a single execution as in traditional assertions Instead we have to monitor the decisions made by the procedure and then using statistical tools infer that a fairness property does not hold with reasonably high confidence To enable this idea we propose a rather general and rich language of specifications that can capture a range of fairness definitions from the literature as well as others As an example our specification language can capture notions of group fairness for example that of Feldman et al In this example the developer has specified using the annotation spec that the hiring procedure whose return variable is r has a selection rate for minorities denoted by s that is at least that of non-minorities Generally our specification language allows arbitrary conditional expectation expressions over events concerning the input and return arguments of some procedure Further we extend the language to be able to specify hyperproperties which refer to two different executions of a procedure This allows us to specify for example properties like individual fairness where we have to compare between pairs of inputs to a procedure that are similar Section provides concrete examples Runtime Analysis To determine that a procedure satisfies a fairness specification we need to maintain statistics over the inputs and outputs of as it is being applied Specifically we compile the specification into runtime monitoring code that executes every time is applied storing aggregate results of every probability event appearing in For instance in the example above the monitoring code would maintain the number of times the procedure returned true for a minority applicant Every time is applied the count is updated if needed In the case of hyperproperties like individual fairness the runtime system has to remember all decisions made explicitly so as to compare new decisions with past ones Assuming that inputs to the decision-making procedure are drawn from an unknown distribution of the underlying population we can employ concentration of measure inequalities to estimate the probability of each event in and therefore whether is violated with some prespecified probability of failure If the runtime code detects a violation we may ask it to completely halt applications of the decision-making program until further investigation Python Implementation We have implemented a prototype of our proposed ideas in the Python programming language Fairness specifications are defined as Python decorators which are annotations that wrap Python functions and therefore affect their behavior In our setting we use them to intercept calls to decisionmaking functions and monitor the decisions made To illustrate our approach we consider two case studies drawn from the fairness literature and demonstrate how our approach may flag violations of fairness Contributions We summarize our contributions as follows Fairness-aware programming We propose a programming-language approach to fairness specification where the developer declaratively states fairness requirements for sensitive decision-making procedures in their code Incremental runtime analysis We propose a runtime-checking technique that incrementally checks the provided fairness specifications every time a decision is made The decisionmaking procedure is flagged when a fairness specification is falsified with high confidence after witnessing enough decisions Implementation case studies We describe an implementation of our proposed methodology as a library in the Python programming language using Python decorators and illustrate its use on case studies from the algorithmic fairness literature OVERVIEW EXAMPLES In this section we provide a general overview of our fairness aware programming approach and provide a set of examples to demonstrate its versatility We consider a programming language where the developer can annotate a procedure with a specification which is a probabilistic statement over the behavior of Here is assumed to be a procedure making a potentially sensitive decision and is a fairness definition that we expect to adhere to While our approach is generic we adopt Python-like syntax in our examples due to the popularity of the language for data analysis tasks and the fact that we implemented our approach as a Python library Given a Python procedure the fairness definition will be provided using a decorator of the form that directly precedes Note that may be a machine-learned classifier or manually written code Throughout we assume that the return value of is stored in a local variable r Example A Group Fairness Consider the scenario where a developer has trained a classifier for deciding which job applicants to invite for a job interview Following existing fairness techniques and data available to them the developer ensured that satisfies a form of group fairness where the selection rate from the minority group is very close to that of the majority group assuming for illustration that we divide the population into minority and majority The developer is aware that the fairness definition may not hold once the classifier is deployed perhaps because the data used for training may exhibit historical biases or may be old and not representative of the current population distribution As such the developer annotates the procedure as follows Here takes a set of attributes of the job applicant along with a Boolean sensitive attribute s indicating whether the applicant is a minority The fairness specification indicates that the selection rate of the minority applicants should be at most different from that of the majority applicants by a factor of Now when is deployed and is making interview decisions the programming language incrementally improves an estimate for every probability expression pr in the specification This is performed automatically in the background using concentration inequalities assuming job applications are independently and identically distributed following an unknown distribution D The more decisions made by the more confident the runtime is about whether the specification holds The developer can choose the level of confidence at which a violation of fairness is reported For instance the developer might configure the runtime to only report violations if the probability that the specification is violated is To further ground our example in practice the procedure may be a wrapper for a saved PyTorch model and be applied in real time to a stream of applications that are processed using eg the filter operator in Apache Sparks Python interface which removes applications that do not return true on By annotating with spec we ensure that it is monitored as it is being applied to the stream of job applications Example B Robustness individual fairness The previous example considered selection rates amongst groups Another important albeit less studied class of fairness definitions is individual fairness where the goal is to ensure that similar individuals are treated similarly Individual fairness can also be seen as a measure of robustness of a decision-maker small changes to an individuals record should not change the decision The following definition is an adaptation of the definition put forth by Bastani et al which was proposed for robustness of neural networks Boolean function Continuing our hiring scenario the procedure takes as input an application x and returns a Boolean decision Notice that this specification here is slightly different from what we have considered thus far it talks about two inputs and outputs of identified by a superscript For a partial illustration the expression is interpreted as the probability that two applications drawn from the distribution are similar as defined by the user-defined Boolean function The full specification above says that the probability of making different decisions on two applicants and should be bounded by under the condition that the applicants are similar Notice that in this setting every time a decision is made checking the specification involves comparing the decision to all previously made decisions and updating the statistics This is because the specification is a so-called hyperproperty since it compares multiple program executions Example C Proxy Consider an algorithmic pricing scenario where an e-commerce website presents custom prices depending on the user Suppose that the algorithm uses browser type eg Chrome Firefox etc as an attribute when deciding the price to present to a given user And suppose that the developers of this algorithm by examining their data determined that it is safe to use browser type as it cannot be used as a proxy for minority status ensuring that minorities receive fair pricing To make sure that this is indeed true in practice the developers can use the ϵ-proxy definition proposed by Datta et al This definition ensures that the normalized mutual information between the browser type and the sensitive attribute is bounded above by a small number spec The pricing procedure takes the browser type b and minority status s The function en is conditional entropy the expression on the left side of the inequality measures how strong a proxy is browser type for minority status Note that we use en for brevity it can be translated into an arithmetic expression over probabilities pr Example D Recommendation For our final example consider a movie recommendation system where user data has been used to train a recommender that given a user profile recommends a single movie for simplicity A key problem with recommender systems is isolation of similar users from certain types of recommendations eg left-leaning social media users tend to see only articles from left-leaning websites and vice versa Suppose that the recommender was constructed with the goal of ensuring that male users are not isolated from movies with a strong female lead Then the developer may add the following specification to their recommender code male The above specification ensures that for male users the procedure recommends a movie with a female lead at least of the time Combining Fairness Definitions In the afore-described examples we considered a single fairness definition at a time Our technique is not limited to that A developer can simply combine two different fairness definitions eg and by conjoining them ie using Analogously the developer may want to specify that at least one of the two definition of fairness is not violated in which case they can disjoin the two definitions using A FAIRNESS-AWARE LANGUAGE In this section we formalize a simple abstract programming language that includes a fairness specification language Concretely for the purpose of our discussion we will limit programs in our language to a pair denoting a single decision-making procedure and an associated fairness specification Decision-Making Procedures We define a decision-making procedure as a function X from elements of some to real values in the non-empty range R The procedure is implemented using standard imperative programming constructs assignments conditionals loops etc The details of how is implemented are not important for our exposition and we therefore omit them The only assumption we make is that is a pure function ie its results are not dependent on global variables We shall assume that elements of the type X are vectors and use the variable x to denote the input arguments of procedure x When needed we will expand x as x Fairness Specification Language The language of specifications is presented as a grammar in Figure Intuitively a specification is a Boolean combination of inequalities of the form c where c R and is an arithmetic expression over expectations An expectation E has one of two forms or An expression E is a numerical expression over the input variable x and a special variable r which denotes the value of x Example The expectation Er denotes the probability that returns a positive value The expectation Ex x denotes the expected value of the sum of the first two arguments of x Note that conditional probabilities and expectations can be encoded in our grammar For instance a conditional probability PE E can be written as The hyperexpression is non-standard it is a numerical expression over two annotated copies of program variables ra and rb We assume that the expression is symmetric meaning that if we swap the a and the b variables we still get the same result As an example consider the following expectation Era rb which denotes the probability that two runs of the function result in different answers Notice that the expression ra rb is symmetric since it is always equal to rb ra for any values of ra and rb Another example of a hyperexpression appears in our robustness example from Section We model Boolean procedures by returning R values Our technique can easily handle randomized functions but we assume deterministic functions for conciseness Spec B c Spec Spec Spec Spec B c R Figure Grammar of a specification Spec and are nonterminals The expressions E and are defined in the text where the operator is some symmetric Boolean function indicating whether the two inputs and are similar Thus this denotes the probability that two inputs to are similar Semantics of Specifications We have thus far only provided an intuition for what it means for a specification to be satisfied or violated We will now formalize this notion We assume that there is a probability distribution D over the domain X of the decision-making procedure This distribution characterizes the population from which inputs to are drawn eg population of job or loan applicants Now each expectation is interpreted such that x D and r x In the case of an expectation over a hyperexpression we consider two identical and independent copies of x called and Each expectation is interpreted such that D and D and where ra and rb Now that we have defined interpretations of expectations we say that a specification is satisfied by procedure if it evaluates to true after plugging in the numerical values of the expectations Otherwise we say is violated by Example To give a simple illustrative example let the distribution D be the normal distribution centered at with scale Now consider the identity function x x and the specification Era rb As described above we evaluate this specification over two variables drawn from D D D states that This is not true since is the identity function and drawing two positive values has probability Therefore we say that is violated by RUNTIME FAIRNESS ANALYSIS In this section we describe a program transformation that instruments the decision-making procedure with additional functionality so as to detect a violation of a fairness property The setting we work in is one where is being continuously applied to inputs which are viewed as independent random variables following an unknown distribution D At every step after making the ith decision we want to check if the property is violated using the empirical observations we have made thus far To achieve this we summarize the instrumentation as follows Empirical expectations For every expectation we maintain the sum of observed values of E which we shall denote Es Thus at any point i in the process we can compute the empirical estimate of namely Note that at every step we simply update Es by adding the new observation Thus we do not need to remember past observations just their aggregate With hyperexpressions however we need to remember past observations as every new observation has to be compared to past observations Concentration inequalities Once we have updated our estimates for every expectation we can bound the estimates using a concentration of measure inequality eg Hoeffdings inequality which gives us an -guarantee on the empirical expectations The more observations we make the tighter the bounds get It is important to note though since we are working in an online setting where we are continuously making statistical tests we need to ensure that we handle the multiple comparisons problem Checking violations Finally to check violations of we plug in the empirical estimates of expectations into and evaluate it using the uncertain values we computed Specifically we by appropriately propagating errors across its operators eg etc If we deduce that is violated with a probability greater than some threshold then we halt the program and report a violation Program Instrumentation and Transformation We now precisely describe how we perform a transformation of a procedure to monitor violations of We shall denote the new transformed procedure We begin by demonstrating the case where there are no hyperexpressions in Algorithm shows the instrumented procedure The new procedure begins by calling x and storing the result in a local variable r Then it proceeds to increment a global variable n indicating the number of decisions made thus far The loop considers every expression E in and updates a corresponding global variable Es that holds the sum of all values of E witnessed so notation E x r is E evaluated on given values of x and r The function bound is treated as a black-box for now as one can employ different well-known statistical techniques bound provides an estimate for the expected value of E which we denote as E Specifically E is the value and the two values R denote an additive error and a probability of failure respectively such that PE In other words our estimate E is within an from with a probability of Evaluation over Uncertain Quantities After computing a value E for every E we plug those values into and check if it is violated and with what probability This is implemented using the function which evaluates by propagating the additive errors and failure probabilities across arithmetic/Boolean operators Algorithm Instrumenting for monitoring violations procedure x r x invoke n n update count for E do Es Es E x r increment sum of E E n estimate with every replaced by E if and then Abort with φ-violation return r E E E E where S E E s and S E E Figure Evaluation of uncertain interval arithmetic data type where For if E E then is assumed to immediately return denoting unknown E c E c E c otherwise Figure Evaluation of uncertain Boolean combinations of inequalities data type We now discuss the implementation of in detail it takes a formula which is but with all expectations replaced by their uncertain E values It then proceeds to in two steps simplifying arithmetic terms and appropriately propagating error and probability values and simplifying Boolean connectives by checking inequalities and propagating probability values The result of is one of the following or unknown If the result is and then we know that is violated with a probability of at least the set threshold Figure defines how simplifies arithmetic expressions Effectively implements arithmetic over intervals of the form E For instance consider the rule for addition If we are given E and E this implies that the value of is within an additive error and with a probability following the union bound Intuitively as we perform arithmetic on uncertain values the intervals and failure probabilities both increase After simplifying all arithmetic expressions we proceed to simplify inequalities and Boolean connectives as shown in the rules in Figure The first rule shows how to check if an inequality of the form E c holds If the full interval E is c then we know that the inequality is false with a failure probability of Similarly if E is c then we know that the inequality is true with a failure probability of Otherwise we cannot make a conclusive statement based on the data we have and we therefore propagate the special Boolean value which stands for unknown and has a failure probability of Technically since can take any Boolean value we have false and After applying the simplification rules to exhaustion we arrive at a value of the form where true false If false and then we know that the specification is violated with a probability at least that of the threshold Example Consider the following Suppose we have the following estimates of the two expectations for and for Then applying the rules for division we get The value is the result of dividing the two empirical expectations The failure probability is the result of taking the union bound of the two failure probabilities The error is the largest deviation from possible which occurs by dividing Now using the rule for evaluating inequalities we know that the inequality above holds with probability at least since the lower bound is Instrumentation for Hyperexpressions Thus far we have not discussed how to monitor hyperexpressions Algorithm shows an extension of Algorithm that additionally tracks the empirical mean of hyperexpressions as shown in the second for loop For every hyperexpression the variable s contains the aggregate Recall however that hyperexpressions are over two copies of the variables Therefore unlike expressions every time we invoke x and receive a value r we need to update s with all valuations of s on x r together with every previously seen pair of values x r which we maintain in a list called hist is symmetric we only evaluate it in one direction ie x r x r and not x r x r The empirical mean of is sn since every two decisions made are compared to each other once Defining bound with Concentration Inequalities At this point we have shown how to maintain all the required statistics in All that is left to do is apply a concentration of measure inequality to determine values for the empirical means There are two issues here i we need a concentration inequality that handles hyperexpressions and ii the multiple comparisons problem To handle hyperexpressions we can apply Hoeffdings inequality for U-statistics which applies to cases where we are computing the mean of an k-ary symmetric function over a sequence of random variables In our case hyperexpressions are -ary symmetric Algorithm Instrumenting for monitoring violations including hyperexpressions procedure x r x n n for E do Es Es E x r E n for do s s x r hist x r x r boundH s n Append x r to list hist with every replaced by Y if and then Abort with φ-violation return r functions We therefore use the following instantiation PH where although we can technically get tighter bounds there To deal with the multiple comparisons problem we apply the standard trick of union bounding the probability that is we take the failure probability to be n e This has a closed form by setting appropriately as a function of n Practical Choice of Values In practice since we are given the threshold that we want to achieve we can conservatively pick values for across expressions such that if returns then we know that Example For example suppose we have of the form c and a threshold of We can divide the failure probability of on the two expectations If we bound with and with then using the structure of we know that the simplification rules will result in a value of the and therefore CASE STUDIES In this section we describe a prototype implementation of fairness-aware programming and two case studies designed to exhibit the various features and nuances of our approach Implementation We have implemented a prototype of our approach as a library of decorators in the Python programming language Decorators in Python can be seen as function annotations that wrap the function and can modify its behavior when running In our case a decorator contains a fairness specification which every time is invoked remembers the decisions it makes and performs some statistical reasoning to check whether is violated Our implementation optimizes the estimates for conditional probabilities The naïve approach is to transform a conditional expectation EA B into EA However the bounds Number of decisions E ta n te Upper bound Lower bound Empirical mean a Group fairness Number of decisions E ta n te Upper bound Lower bound Empirical mean b Group fairness with modified classifier Number of decisions E ta n te Upper bound Lower bound Empirical mean c Individual fairness Figure Simulations from our case studies on the estimates of the two expectations compound after applying the division operation as formalized in Figure To work around this loss of precision we employ the standard approach of rejection sampling where we discard invocations of that do not satisfy B when computing EA thus resulting in a finer bound on EA B This improves precision and therefore the number of observations needed to discover a violation of Case Study A Group Fairness For our case studies we consider classifiers trained and used in the FairSquare tool These classifiers were trained on the Adult income dataset which has been a popular benchmark for fairness techniques The task there is to predict someones income as high true or low false Unfairness can therefore be exhibited in scenarios where this is used to for instance assign salaries automatically FairSquare provides the classifiers as Python functions which we annotate with fairness definitions Additionally FairSquare provides a generative probabilistic population model which we used to simulate a stream of inputs to the classifier For our first case study we consider a group fairness property of the following form q q This property specifies that the rate at which qualified females receive a high salary is at least that of qualified males The Boolean qualification attribute q is used to limit the fairness constraint to a subset of the population eg those that meet some minimum requirement for consideration for high salaries We annotated one of the FairSquare classifiers with this property and simulated it using the provided probability distribution We fixed a threshold of meaning that violations should only be reported with confidence We automatically fix the probability for each expectation so that only the error varies decreasing as a function of the number of observations made see Appendix for more details Figure a shows the value of the term q q as a function of the number of observations The figure shows the empirical mean as well as This optimization is only applied to conditional probabilities over expressions but not hyperexpressions as rejection sampling does not easily translate to that setting Available at commit bd its upper and lower bounds ie empirical mean As shown the empirical mean converges to about and after about observations we conclude a violation of the fairness property since the upper bound red goes below indicating that the selection rate for qualified females is less than that of qualified males with a probability To further illustrate we manually modified the classifier to favor males reducing the ratio q q to about The simulation results are shown in Figure b where about observations are made before deducing that the fairness definition is violated Notice that for the modified classifier we required a smaller number of observations to declare a violation this is because the selection ratio is much lower in than and so a larger error suffices to show a violation with the same probability of One observation we can make here is the number of decisions we needed to establish a fairness violation is One may argue that these numbers are too large However consider the case of algorithmic decisions being made at the scale of a large corporation which may receive thousands of job applications per day or algorithmic decisions made during crowdsourcing where a huge number of decisions of who to pay and how much to pay can be made in an instant In those scenarios a few thousand decisions is a small number At a more technical level we could imagine a deployment where the developer can see in real-time the graphs in Figure As such the developer may decide to investigate unfairness as soon as the empirical mean has stabilized before the lower/upper bounds are tight enough to conclusively establish a violation Further in our implementation we have employed a vanilla concentration of measure inequality Hoeffdings but there is room for improvement eg by modifying the parameters of the inequality or investigating other inequalities We leave the investigation of the merits of different statistical tests for future work Case Study B Individual Fairness For our next case study we consider the same classifier from case study A but with a different fairness specification involving hyperexpressions The following definition specifies that the probability that two individuals who are similar receive different outcomes is less than While Figure shows single simulations these are representative of the average number of observations needed to discover a violation x is the input to the classifier and similarity is a function of the education level two individuals are similar if their education levels differ by at most Figure c shows the estimated value of the term The empirical estimate gray line converges to about the probability that a pair of similar individuals are given different salaries After about observations we deduce that is greater than as indicated by the lower bound blue which goes above Recall that estimating an expectationEH is more involved due to the fact that after every decision we update the sum s with x r hist x r x r which compares the current observation x r with all previous observations as recorded in the list hist Therefore the time it takes to update s increases linearly in the number of observations In our case study we have found that time taken to compute the sum is not prohibitive For instance if we keep the simulation running past the point it discovers a violation we observe that the time it takes to compute the sum is s when hist and when hist Of course the time taken is also a function of the complexity of evaluating which in our case is simple Our prototype implementation in Python is not optimal as Python is an interpreted language We envision a number of optimizations if hist grows very large or is complex For instance compiling into low-level code or parallelizing the evaluation of the sum using reducers DISCUSSION Fairness Definitions Our foremost design goal for the specification language is to provide a flexible set of operators that can capture most existing definitions from the literature However there are definitions that we cannot immediately capture in the language and instrumentation in its current form Consider for instance the work of Datta et al where they consider influence of a proxy To demonstrate that a proxy is influential on the decision they intervene on the distribution D by varying the values of variables under test independently of the others This cannot be currently defined in our specification language as we cannot consider hypothetical inputs to the procedure only the ones we have witnessed in the course of execution To handle such property we have to modify the specification language to be able to invoke the procedure We made the decision to restrict the language to be only a monitor of decisions and not a tester that intervenes The same problem holds for other causal definitions of fairness Another prominent property that we cannot capture is individual fairness as formalized by Dwork et al There one considers randomized classifiers and measures that distance between output distributions for two similar inputs Just like with the property discussed above to be able to measure distance between distributions for two distinct inputs the monitoring code has to be able to repeatedly invoke a randomized to get an accurate picture of its output distribution for each input GHz Intel Core i GB RAM Mac OS X Population Distribution Like many other works in the algorithmic fairness space we make the assumption that the underlying population distribution D of the decision-making procedure is fixed and inputs are iid This however may not be always true For instance the decisions made may affect the population eg in a setting like giving loans a problem that has been recently explored by Liu et al In most cases impacts of decision-making are delayed and therefore our work can potentially catch unfairness over a small time-scale where the population distribution is constant Even when the underlying distribution is constantly shifting one could imagine incorporating that fact into the runtime eg by maintaining a sliding window where older less-representative observations are discarded RELATED WORK The algorithmic fairness literature has been rapidly expanding in breadth and depth In this section we focus on the most related works from the fairness literature and relevant works from software engineering and verification Enforcing and Checking Fairness We focus on two types of work on algorithmic fairness i works on enforcing fairness and ii works developing techniques for checking fairness We have shown that our language-based approach can capture a range of properties from the literature These include forms of group fairness which have appeared in a various forms in the literature eg Some notions of fairness like equalized odds work exclusively in the context of supervised learning where we are ensuring fairness with respect to some labeled data In this work we consider the setting in which we are observing decisions as they are being made on unseen data and therefore we are not targeting notions of fairness tied to supervised learning In Section we discussed some of the notions of fairness that our language cannot capture In the context of enforcing fairness much of the work in the machine-learning community has been focused on modifying the learning algorithm to take fairness into account eg by incorporating fairness into the objective function eliminating proxies to sensitive attributes by preprocessing the data or applying a post-processing step to make a classifier fair Works in security and verification have studied approaches that syntactically modify unfair programs to make them fair In the context of checking fairness a number of systems have been proposed For example the work on Themis uses a notion of causal fairness and generates tests to check fairness of a given decision-making procedure The work on FairTest provides a comprehensive framework for investigating fairness in data-driven pipelines Runtime Statistical Verification Our work is reminiscent of statistical verification techniques Most closely related is the work of Sampson et al where probabilistic assertions are added to approximate noisy programs to check that they return answers within reasonable bounds There are numerous differences between our techniques First we consider an online checking of assertions as opposed to an offline testing phase by sampling from a known distribution Second we consider a much richer class of properties where we have arithmetic and Boolean operations over expectations as opposed to a single event of interest There is a rich body of work on various forms of runtime verification which involves monitoring properties at runtime Little work in that area however has targeted probabilistic properties Lee et al proposed applying statistical hypothesis testing at runtime to detect violations of a certain form of temporal properties in real-time systems Like the work of Sampson et al they are restricted to a single probability expression and do not consider hyperexpressions Our work is also distinguished by embedding the probabilistic properties to be checked into the language as opposed to an external specification mechanism CONCLUSION We proposed fairness-aware programming where fairness definitions can be declaratively specified in decision-making code and checked at runtime We argued that embedding notions of fairness directly within the programming language reduces the barrier to investigating fairness of algorithmic decision-making We demonstrated our approach by implementing it in Python and applying it to example classifiers from the literature