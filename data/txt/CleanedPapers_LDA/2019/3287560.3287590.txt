On Human Predictions with Explanations and Predictions of Machine Learning Models A Case Study on Deception Detection Humans are the final decision makers in critical tasks that involve ethical and legal concerns ranging from recidivism prediction to medical diagnosis to fighting against fake news Although machine learning models can sometimes achieve impressive performance in these tasks these tasks are not amenable to full automation To realize the potential of machine learning for improving human decisions it is important to understand how assistance from machine learning models affects human performance and human agency In this paper we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency We propose a spectrum between full human agency and full automation and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions We find that without showing predicted labels explanations alone slightly improve human performance in the end task In comparison human performance is greatly improved by showing predicted labels relative improvement and can be further improved by explicitly suggesting strong machine performance Interestingly when predicted labels are shown explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff CONCEPTS Applied computing Law social and behavioral sciences KEYWORDS human agency human performance explanations predictions INTRODUCTION Machine learning has achieved impressive success in a wide variety of tasks For instance neural networks have surpassed human-level performance in ImageNet classification vs Kleinberg et al demonstrate that in bail decisions machine predictions of recidivism can reduce jail rates by with no increase in crime rates compared to human judges Ott et al show that linear classifiers can achieve accuracy in detecting deceptive reviews while humans perform no better than chance As a result of these achievements machine learning holds promise for addressing important societal challenges However it is important to recognize different roles that machine learning can play in different tasks in the context of human decision making In tasks such as object recognition human performance can be considered as the upper bound and machine learning models are designed to emulate the human ability to recognize objects in an image A high accuracy in such tasks presents great opportunities for large-scale automation and consequently improving our societys efficiency In contrast efficiency is a lesser concern in tasks such as bail decisions In fact full automation is often not desired in these tasks due to ethical and legal concerns These tasks are challenging for humans and for machines but with vast amounts of data machines can sometimes identify patterns that are unsalient unknown or counterintuitive to humans If the patterns embedded in the machine learning models can be elucidated for humans they can provide valuable support when humans make decisions The goal of our work is to investigate best practices for integrating machine learning into human decision making We propose a spectrum between full human agency where humans make decisions entirely on their own and full automation where machines make decisions without human intervention see Figure for an illustration We then develop varying levels of machine assistance along the spectrum using explanations and predictions of machine learning models We build on recent developments in interpretable machine learning that provide useful frameworks for generating explanations of machine predictions Instead of using these explanations to help users debug machine learning models we incorporate the explanations as assistance for humans to improve human performance while retaining human agency in the decision making process Accordingly we directly evaluate human performance in the end task through user studies In this work we focus on a constrained form of decision making where humans make individual predictions Specifically we ask humans to decide whether a hotel review is genuine or deceptive based on the text This prediction problem allows us to focus on the integration of machine learning into human predictions In comparison prior work in decision theory and decision support systems focuses on modeling preferences and utilities as well as building Figure A spectrum between full human agency and full automation illustrating how machine learning can be integrated in human decision making The detailed explanation of each method is in Section knowledge databases and representations to reason about complex decisions Moreover since many policy decisions can be formulated as prediction problems understanding human predictions with assistance from machine learning models constitutes an important step towards empowering humans with machine learning in critical challenging tasks Deception detection as a testbed In this work we use deception detection as our testbed for three reasons First deceptive information is prevalent on the Internet For instance Ott et al find that deceptive reviews are a growing problem on multiple platforms such as TripAdvisor and Yelp Fake news has also received significant attention recently and might have influenced the outcome of the US presidential election in Enhancing humans ability in detecting deception can potentially alleviate these issues Second deception detection is a challenging task for humans and has been extensively studied It is promising that machines show preliminary success in prior work For example machines are able to achieve an accuracy of in distinguishing genuine reviews from deceptive ones while human performance is no better than chance Machines can identify unsalient and counterintuitive signals eg deceptive reviews are less specific about spatial configurations and tend to include less sensorial and concrete language It is worth noting that we should take the high machine accuracy with a grain of salt in the general domain because deception detection is a complex problem The task introduced by Ott et al nevertheless provides an ideal sandbox to understand human predictions with assistance from machine learning models Third full automation is not desired in critical tasks such as deception detection because of ethical and legal concerns The government should not have the authority to automatically block information from individuals eg in the context of fake news Furthermore full automation may not comply with legal requirements For instance in the case of recidivism prediction the Wisconsin Supreme Court ruled that judges be made aware of the limitations of risk assessment tools and a COMPAS risk assessment should not be used to determine the severity of a sentence or whether an offender is incarcerated Similarly the trial judge is required to act as a gatekeeper regarding the evidence from a polygraph lie detector Therefore it is crucial to retain human agency and understand human predictions with assistance from machine learning models For instance one can argue that it is impossible to fully address the issue of deception in online reviews only based on textual information as an adversarial user can copy another users review which becomes a deceptive review but with exactly the same text as a genuine one Organization and Highlights We start by reviewing related work to provide the necessary background for our study Section Our focus in this work is on investigating human predictions with assistance from machine learning models in the context of deceptive review detection To explore the spectrum between full human agency and full automation in Figure we develop varying levels of assistance from machine learning models Section For example the following three levels of machine assistance gradually increase the influence of machine predictions showing only explanations of machine predictions without revealing predicted labels showing predicted labels without revealing high machine accuracy showing predicted labels with an explicit statement of strong machine accuracy In Section we investigate human performance under different experimental setups along the spectrum We show that explanations alone slightly improve human performance while showing predicted labels achieves great improvement relative improvement in human accuracy However this improvement is still moderate compared to full priming with an explicit statement of machine accuracy relative improvement in human accuracy Our findings suggest that there exists a tradeoff between human performance and human agency Interestingly when predicted labels are shown explanations of machine predictions can achieve a similar effect as an explicit statement of machine accuracy We also find that humans tend to trust correct machine predictions more than incorrect ones indicating that they can somewhat identify when machines are correct We further examine the effect of statements of machine accuracy by varying the accuracy numbers Section Surprisingly we find that our participants are not sensitive to statements of machine accuracy and are more likely to trust machine predictions with an accuracy statement than without even if the accuracy statement suggests poor machine performance These observations echo with prior work on numeracy and suggest that it is difficult for humans to interpret and act on numbers We also find that frequency explanations eg out of for explaining can help humans calibrate the accuracy numbers Note that we do not recommend these presentations on the spectrum because they present untruthful information We discuss the limitations of our work and provide concluding thoughts regarding future directions of investigating best practices for integrating artificial intelligence into human decision making in Section RELATED WORK We summarize related work in two areas to put our work in context interpretable machine learning and deception and misinformation Interpretable machine learning Machine learning models remain as black boxes despite wide adoption Blindly following machine predictions may lead to dire repercussions especially in scenarios such as medical diagnosis and justice systems Therefore improving their transparency and interpretability has attracted broad interest dating back to early work on recommendation systems In the case of general automation researchers have also studied issues of appropriate reliance and trust There are two major approaches to providing explanations of machine learning models example-based and feature-based For example an example-based explanation framework is MMD-critic proposed by Kim et al which selects both prototypes and criticisms Ribeiro et al propose a feature-based approach LIME that fits a sparse linear model to approximate non-linear models locally Similarly Lundberg and Lee present a unified framework that assigns each feature an importance value for a particular prediction We would like to emphasize two unique aspects of our work task difficulty and interpretability evaluation First compared to categorizing text into topics and object recognition deception detection is a challenging task for humans and it remains an open question whether humans can leverage help from machine learning models in such settings Second we directly measure human performance in the end task In comparison prior work in interpretable machine learning aims to help humans understand how machine learning models work and/or debug them the evaluation is thus mostly based on either the understanding of the models or the improvement in machine performance Concurrently several recent studies have also examined how explanations relate to human performance Our work also resonates with the seminal work on mixed-initiative user interfaces and intelligence augmentation In addition our work is connected to cognitive studies on understanding effective explanations beyond the context of machine learning Deception and misinformation Deception is a widely studied phenomenon in many disciplines In psychology deception is defined as an act that is intended to foster in another person a belief or understanding which the deceiver considers false To detect deception researchers have examined the role of behavioral emotional and linguistic cues Since people are increasingly relying on online reviews to make purchase decisions machine learning methods have been used to detect deception in online reviews An important challenge in detecting deception in online reviews is to obtain the ground-truth labels of reviews Ott et al create the first sizable dataset in deception detection by asking Amazon Mechanical Turkers to write deceptive reviews Deceptive reviews can also be seen as an instance of spamming and online fraud More recently the issue of misinformation and fake news has drawn much attention from both the public and the research community Most relevant to our work is Zhang et al which explores varying types of credibility annotations specifically designed for news articles In addition Nyhan and Reifler demonstrate the backfire effect which suggests that corrections of misperceptions may enhance peoples false beliefs and Vosoughi et al show that fake news is more innovative and spreads faster than real news It is worth noting that deception detection is a broad and complex issue For instance fake news can be hard to define and may not be easily separated into two classes Moreover detecting fake news is different from detecting deceptive reviews as the former task requires other skills such as fact checking It is important to note that our focus in this work is on investigating how humans interact with assistance from machine learning algorithms in decision making We thus adopt the task of distinguishing genuine reviews from deceptive ones based on textual information in Ott et al as a sandbox Our results on this constrained deception detection task can potentially contribute valuable insights to future solutions of the broader issue of deception detection EXPERIMENTAL SETUP AND HYPOTHESES Our goal is to understand whether machine predictions and their explanations can improve human performance in challenging tasks such as deception detection and how humans interpret assistance from machine learning models In this section we first present our task setup and then develop varying levels of machine assistance along the spectrum introduced in Figure We finally formulate our hypotheses and define our evaluation metrics Experimental setup We employ the deception detection task developed by Ott et al and evaluate human performance in this task with varying levels of machine assistance The dataset in Ott et al includes genuine and deceptive hotel reviews for hotels in Chicago The genuine reviews were extracted from TripAdvisor and the deceptive ones were written by turkers We use of the reviews as training data and the remaining as the heldout test set Since the machine performance with linear SVM in Ott et al already surpasses humans by a wide margin and linear classifiers are generally considered more interpretable we follow Ott et al and use linear SVM with bag-of-words features as our machine learning model The linear SVM classifier achieves an accuracy of on the heldout test set Our main task in this paper is to evaluate human performance with assistance from machine learning models To do that we conduct a user study on Amazon Mechanical Turk Turkers are recruited to determine whether a review in the heldout test set is genuine or deceptive In other words humans are asked to perform the same task as the machine on the test set We follow a between-subject design each turker is assigned a level of machine assistance along the spectrum Figure and labels reviews after going through three training examples and correctly answering an attention-check question To incentivize turkers to perform at their best we provide bonus for each correct prediction in addition to the cent base rate for a review We also solicit our participants estimation of their own performance and basic demographic information such as gender and education background through an exit survey We only allow a turker to participate in the study once to guarantee sample independence across experimental a Heatmap without showing predicted labels an instance of feature-based explanations b Predicted label with accuracy c Predicted label heatmap without accuracy Figure Example interfaces with varying levels of machine assistance Figure a only presents feature-based explanations of machine predictions in the form of heatmap Figure b shows both the predicted label and an explicit statement about machine accuracy Figure c shows the predicted label with heatmap but does not present machine accuracy We crop the Genuine and Deceptive buttons in Figure b and c to save space setups Given that there are test reviews and that we collect five turker predictions for each review each experimental setup has a total of turkers Refer to the appendix for more details regarding our user study and survey questions Varying levels of machine assistance Humans are the main agents in our experiments and make final decisions machines only provide assistance which can be ignored if humans deem it useless An ideal outcome is that human performance can be improved with minimal information from machine learning models so that humans retain their agency in the decision making process To examine how humans perform under different levels of influence from machine learning models we consider the following presentations along the spectrum in Figure we only show three interfaces in Figure for space reasons see the appendix for more Control Humans are only presented a review This setup contains no information from machine learning models and humans have full agency Feature-based explanations Since our machine learning model is linear we present two versions of feature-based explanations by highlighting words based on absolute values of weight coefficients First we highlight the top words in each review with the same color highlight Second we use heatmap to show gradual changes in weight coefficients among the top words The most heavily-weighted words are highlighted in the darkest shade of blue Soft-highlighting heatmap has been shown to improve visual search on targeted areas for humans Note that we do not indicate the sign of features to avoid revealing predicted labels Humans may pay extra attention to the highlighted words and accordingly make decisions on their own Figure a shows an example interface for heatmap Example-based explanations This method examples is inspired by example-based interpretable machine learning Humans are presented two additional reviews from the training data one deceptive and one genuine that are most similar to the review under consideration This setup resonates with nearest Accuracy Predicted label w/ accuracy Predicted label w/o accuracy Heatmap Highlight Examples Control p p p p p a Human accuracy with varying levels of machine assistance Accuracy Machine performance Predicted label w/ accuracy Predicted label heatmap Predicted label examples Predicted label random heatmap Predicted label w/o accuracy p p p p b Human accuracy with predicted labels and other information Figure Human accuracy with varying levels of assistance In Figure a control provides no assistance examples highlight and heatmap present explanations of machine predictions alone predicted label w/o accuracy shows predicted labels predicted label w/ accuracy shows predicted labels and reports machine accuracy that suggests strong machine performance It is clear that showing predicted labels is crucial for improving human accuracy Adding an explicit statement of machine accuracy further improves human accuracy Figure b further investigates the combinations of predicted labels and their explanations and presents machine performance as a benchmark Intriguingly we find that adding explanations achieves a similar effect as adding an explicit statement of machine accuracy All p-values are computed by conducting t-test between the corresponding setup and the first experimental setup in the figure control in Figure a and predicted label w/o accuracy in Figure b neighbor classifiers Humans can potentially make better decisions in this setup than in control by comparing the similarity between reviews Predicted label without accuracy The above two approaches only show explanations of machine predictions but do not reveal any information about predicted labels The next level of priming presents the predicted label If humans fully follow machine predictions they will perform much better than chance and likely lead to an upper bound in this deception detection task for humans However humans may not trust the machine due to algorithm aversion Predicted label with accuracy We may further influence human decisions by explicitly suggesting that machines perform well in this task with accuracy Figure b shows an example for predicted label with accuracy Note that such strong recommendations may not be desired due to ethical and legal concerns see our discussion in the introduction Combinations Finally we combine feature example-based explanations and predicted labels Note that we do not show machine performance to avoid strong priming Figure c shows an example of predicted label heatmap without information about machine performance Hypotheses We formulate the following hypotheses regarding how well humans can perform with machine assistance and how often humans trust machine predictions when predicted labels are available Hypothesis a Feature-based explanations and example-based explanations improve human performance over control Hypothesis b Heatmap is more effective than highlight as gradual changes in weight coefficients can be useful as shown in Kneusel and Mozer for visual search Feature-based explanations are more effective than example-based explanations since the latter requires a greater cognitive load ie reading two more reviews Hypothesis Showing predicted labels significantly improves human performance compared to feature example-based explanations alone Assuming that humans trust the machine and follow its prediction showing predicted labels can likely improve human performance because the machine accuracy is However showing predicted labels reduces human agency so it is important to understand the size of the performance gap and make informed design choices Hypothesis By combining predicted labels and feature example-based explanations the trust that humans place on machine predictions increases as it has been shown that concrete details can influence the level of trust in general automation We evaluate the above hypotheses using two metrics accuracy and trust Accuracy is defined as the percentage of correctly predicted instances by humans trust is defined as the percentage of instances for which humans follow the machine prediction Note that we can only compute trust when predicted labels are available RESULTS In this section we investigate how varying levels of assistance from machine learning models along the spectrum in Figure affect human predictions We first discuss aggregate human performance using human accuracy and trust Our results show that in this challenging task explanations alone slightly improve human performance while showing predicted labels can significantly improve human performance When predicted labels are shown we examine the level of trust that humans place on machine predictions Our results suggest that humans can somewhat differentiate correct machine predictions from incorrect ones Finally we present individual differences among our participants based on information collected in the exit survey Our dataset and demonstration are available at Human Accuracy We first present human accuracy measured by the percentage of correctly predicted instances by humans Our results suggest that Trust Predicted label w/ accuracy Predicted label w/ heatmap Predicted label examples Predicted label heatmap random Predicted label w/o accuracy p p p p a Trust in machine predictions Trust Predicted label w/ accuracy Predicted label w/ heatmap Predicted label examples Predicted label heatmap random Predicted label w/o accuracy Correct machine predictions Incorrect machine predictions b Trust in correct and incorrect machine predictions Figure The trust that humans place on machine predictions Figure a shows that adding feature-based explanations heatmap can effectively increase the trust level compared to predicted label w/o accuracy p-value in Figure a is computed by conducting t-test between the corresponding setup and predicted label w/o accuracy Figure b breaks down the trust based on whether machine predictions are correct or incorrect and shows that humans trust correct machine predictions more than the incorrect ones in all the five experimental setups although the differences are only statistically significant in two setups showing predicted labels is crucial for improving human performance Featured-based explanations coupled with predicted labels are able to induce similar human performance as an explicit statement of strong machine accuracy As such adding feature-based explanations to predicted labels may be more ideal than suggesting strong machine performance as the priming is weaker and may facilitate a higher level of human agency in decision making Explanations alone slightly improve human performance Figure a As Figure a shows human performance in control is no better than chance This finding is consistent with Ott et al and decades of research on deception detection Explanations alone slightly improve human performance over control and the differences are statistically significant for highlight and heatmap not for examples However the best explanations heatmap is not statistically significantly different from highlight p or examples p As a result our findings partially supports Hypothesis a and rejects Hypothesis b These findings suggest that it is difficult for humans to understand explanations on their own This is plausible for example-based explanations since it requires extra cognitive burden and estimating text similarity is a nontrivial task for humans For feature-based explanations it seems that the improvement is driven by the small number of training reviews that we provide to explain the task First-person singular pronouns provide a good example one of the training reviews is deceptive and highlight many occurrences of the word my A participant said I tried to match the pattern from the example In the example the review with the most Mys and Is were deceptive In other words the improvement in heatmap and highlight may not happen at all without the training reviews which indicates the difficulty of interpreting these feature-based explanations and the importance of explaining the explanations One possible direction is to develop automatic tutorials to teach the intuitions behind important features which is related to machine teaching Showing predicted labels significantly improves human performance Figure a and b As Figure a shows showing predicted labels drastically improves human performance for predicted label w/o accuracy a relative improvement over control the difference with heatmap is statistically significant p By presenting machine accuracy as shown in Figure b the performance is further improved to predicted label w/ accuracy in Figure a a relative improvement over control These results are consistent with Hypothesis The big performance gap between showing predicted labels and showing feature example-based explanations alone suggests that when humans interact with machine learning models it makes a significant difference whether predicted labels are shown However this observation also echoes with concerns about humans overly relying on machines To further understand human performance with predicted labels we examine all experimental setups with predicted labels in Figure b Although showing predicted labels seems necessary for achieving sizable human performance improvement the effect of presenting machine accuracy can be moderated by showing feature example-based explanations We find that predicted label examples and predicted label heatmap outperform predicted label w/o accuracy and vs without presenting the machine accuracy In this case we observe that heatmap is more effective than examples and leads to comparable human performance with predicted label w/ accuracy There is still a gap between the best human performance predicted label w/ accuracy and machine performance vs These observations suggest that humans do not necessarily trust machine predictions Trust We further examine the levels of trust that humans place on machine predictions when predicted labels are available Since machine performance surpasses human performance in control by a wide margin in this task higher levels of trust are correlated with higher levels of accuracy in our experiments However these two metrics capture different dimensions of human predictions because trust is tied to machine predictions This becomes clear when we break down human trust by whether machine predictions are correct or not We find that humans tend to trust correct machine predictions more than incorrect ones which suggests that humans can somewhat effectively identify cases where machines are wrong It is important to emphasize that our focus is on understanding how Percentage of participants Predicted label heatmap Predicted label w/o accuracy Heatmap Under estimate Correct estimate Over estimate a Human estimation of their own performance Accuracy Female Male Useful Not useful p p b Gender and hint usefulness in predicted label heatmap Figure Heterogeneity findings among participants in our study Figure a shows performance estimation by participants in three different experimental setups Figure b presents the performance of participants in predicted label heatmap group by two variables hint usefulness and gender human trust varies along the spectrum rather than manipulating the trust of humans in machines Feature example-based explanations increase the trust that humans place on machine predictions Figure aWe further introduce random heatmap by randomly highlighting an equal number of words as in heatmap to examine whether humans are influenced by any explanations including random ones Our results are consistent with Hypothesis both feature-based and example-based explanations increase the trust of humans in machine predictions In fact predicted label heatmap leads to a similar level of trust as predicted label w/ accuracy although the latter explicitly tells humans that the machine learning model has an accuracy of approximately In other words when predicted labels are shown heatmap can nudge humans in decision making without making strong statements of machine accuracy Interestingly random heatmap also increases the trust level significantly suggesting that even irrelevant details can increase the trust of humans in machine predictions The fact that heatmap is significantly more effective than random heatmap vs p indicates that humans can interpret valuable information in weight coefficients beyond the placebo effect Humans tend to trust machine predictions more when machine predictions are correct Figure b We next examine whether humans trust machine predictions more when machine predictions are correct than when they are incorrect Figure b shows that in all the five experimental setups with predicted labels our participants trust correct machine predictions more than incorrect ones However the difference is statistically significant only in predicted label w/ accuracy p and predicted label w/ heatmap random p These results suggest that humans can somewhat differentiate correct machine predictions from incorrect ones Further evidence is required to fully understanding the reasons why humans dont trust incorrect machine predictions Such understandings can improve both machine learning models and their presentations to support human decision making Heterogeneity in Human Perception and Performance We finally discuss the heterogeneity between participants in our study Here we focus on the participants estimation of their own performance and gender differences Refer to the appendix for additional comparisons Human estimation of their own performance Figure aWe ask participants to estimate their own performance in our exit survey Our results are not exactly aligned with the previous finding that humans tend to overestimate their capacity of detecting lying In fact of the participants correctly predicted their performance Among the remaining overestimated their performance while underestimated their performance Figure a shows the breakdown for three experimental setups In general it seems difficult for humans to estimate their performance One participant who overestimated his performance estimated but got correct said I enjoyed this hit When I was a young man I was a manager in the hotel business and got to read a lot of comment cards from guests I hope that I was pretty accurate in my answers Another participant who underestimated his performance estimated but got correct said It was difficult to determine if they were genuine or deceptive I dont feel certain on any of my choices Heterogeneity in performance across individuals Figure b We have so far focused on average human performance comparisons between different experimental setups It is important to recognize that the performance of individuals can vary Exit survey responses allow us to study such heterogeneity We focus on two properties in the interest of space Refer to the appendix for a complete discussion of heterogeneity between individuals First individuals who find the hints useful outperform those who find the hints not useful The difference between these two groups in Figure b predicted label heatmap is statistically significant This observation resonates with our analysis regarding the trust of humans in machine predictions and holds in out of experimental setups this question was not asked in control although the differences are only statistically significant in three setups Second we find that females generally outperform males This observation holds in out of experimental setups but none of the differences is statistically significant Our results contribute to mixed observations regarding gender differences in deception detection The low number of statistically significant differences is expected because human performance is low unless we show predicted labels Accuracy Predicted label w/o accuracy Predicted label w/ accuracy frequency explanation Predicted label w/ accuracy frequency explanation Predicted label w/ accuracy Predicted label w/ accuracy Predicted label w/ accuracy Predicted label w/ accuracy p p p p p a Human accuracy Trust Predicted label w/o accuracy Predicted label w/ accuracy frequency explanation Predicted label w/ accuracy frequency explanation Predicted label w/ accuracy Predicted label w/ accuracy Predicted label w/ accuracy Predicted label w/ accuracy p p p p p b Trust Figure Human accuracy and trust given varying statements of machine accuracy Figure a and Figure b show that human accuracy and trust generally decline with statements of decreasing machine accuracy despite the fact that machine predictions remain unchanged Note that the decline of human trust with statements of decreasing accuracy is small Only by adding frequency explanations human accuracy and trust become closer to predicted label w/o accuracy VARYING STATEMENTS OF MACHINE ACCURACY Given the strong influence of predicted labels and machine accuracy a natural question to ask is how human judgment changes if we vary the statement of machine accuracy For example instead of the true accuracy of we could claim that the machine has an accuracy of It is important to emphasize that since these statements of accuracy are not true we do not recommend this approach as part of our spectrum in Figure and thus put these results in a separate section However we think that it is valuable to understand how varying statements of accuracy might influence human predictions Although human accuracy and trust generally decline with statements that suggest lower accuracy statements of machine accuracy improve human trust in machine predictions even when the claimed accuracy is only To understand human accuracy with varying statements of machine accuracy we use predicted label w/o accuracy and predicted label w/ accuracy as benchmarks In Figure a and Figure b human accuracy and trust with varying statements of machine accuracy all fall between these two benchmarks as expected Here we focus on the blue bars filled with forward slashes that correspond to simple statements of machine accuracy The machine predicts that the below review is deceptive It has an accuracy of approximately x x As the claimed accuracy declines from to human accuracy and trust decrease with the exception of human accuracy from to However the decline in human trust and accuracy is fairly small For instance predicted label w/ accuracy still outperforms predicted label w/o accuracy significantly The results are surprising and counterintuitive since one should put less trust in a machine that has only an accuracy of as compared to a machine that boasts Our findings suggest that any indication of machine accuracy be it high or low improves human trust in the machine This observation echoes prior work on numeracy that suggests that average humans and even doctors struggle with interpreting and acting on numbers Therefore it is crucial that we develop a better empirical understanding of how humans interact with explanations and predictions of machine learning models in decision making before using these machine learning models in the loop of human decision making Frequency explanations can help humans interpret and act on statements of machine accuracy To further investigate human interaction with varying statements of machine accuracy we add frequency explanations to the statement with accuracy and Specifically we show participants The machine predicts that the below review is deceptive It has an accuracy of approximately which means that it is correct out of times instead of The machine predicts that the below review is deceptive It has an accuracy of approximately The results are shown with the red bars filled with stars in Figure a and Figure b We find that frequency explanations reduce the trust that humans place on machine predictions For instance human accuracy in predicted label w/ accuracy frequency explanation is lower p than in predicted label w/ accuracy Similarly human trust in predicted label w/ accuracy frequency explanation is lower p than in predicted label w/ accuracy Furthermore the differences in human accuracy and trust are not statistically significant between predicted label w/ accuracy frequency explanation and predicted label w/o accuracy These observations suggest that frequency explanations can help humans interpret statements of machine accuracy in which case a statement of accuracy with frequency explanation is almost the same as not showing machine accuracy Our frequency explanations are also known as frequent format and have been shown to be more effective for conveying uncertainty than stating the probability CONCLUDING DISCUSSION In this paper we conduct the first empirical study to investigate whether machine predictions and their explanations can improve human performance in challenging tasks such as deception detection We propose a spectrum between full human agency and full automation and design machine assistance with varying levels of priming along the spectrum We find that explanations alone improve human performance while showing predicted labels significantly improves human performance Adding an explicit statement of strong machine performance can further improve human performance Our results demonstrate a tradeoff between human performance and human agency and explaining machine predictions may moderate this tradeoff We find interesting results regarding the trust that humans place on machine predictions On the one hand humans tend to trust correct machine predictions more than incorrect ones which indicates that it is possible to improve human decision making while retaining human agency On the other hand we show that human trust can be easily enhanced by adding random heatmap as explanations or statements of low accuracies that do not justify trusting machine predictions In other words additional details including irrelevant ones can improve the trust that humans place on machine predictions These findings highlight the importance of taking caution in using machine learning for supporting decision making and developing methods to improve the transparency of machine learning models and its associated human interpretation As machine learning gets employed to support decision making in our society it is crucial that the machine learning community not only advances machine learning models but also develops a better understanding of how these machine learning models are used and how humans interact with these models in the process of decision making Our study takes an initial step towards understanding human predictions with assistance from machine learning models in challenging tasks Implications and future directions Our results show that explanations alone slightly improve human performance One reason for the limited improvement with explanations alone is that although we provide explanations during the decision making process we provide limited resources to teach these explanations A possible future direction is to develop tutorials for machine learning models and their explanations to relieve some cognitive burden from humans eg summarizing the model as a list of rules adding heatmap in examples or providing a sequence of training examples with explanations and sufficient coverage This direction also connects to the area of machine teaching Another possible direction to improve the effectiveness of explanations is to provide narratives Our results suggest that feature-based and example-based explanations provide useful details for machine predictions to improve the trust of humans in machine predictions It can be useful if we can similarly provide rationales behind feature-based and example-based explanations in the form of narratives A qualitative understanding of how turkers interpret hints from machine learning models may shed light on the requirements of effective narratives Last but not least it is important to study the ethical concerns of providing assistance from machine learning models in human decision making Our results demonstrate a clear tradeoff in this space it is difficult to improve human performance without showing predicted labels but showing predicted labels especially alongside machine performance runs the risk of removing human agency Human decision makers with assistance from machines further complicate the current discussions on the issue of fairness in algorithmic decision making As the adoption of machine learning approaches can have broad impacts on our society such questions require inputs from machine learning researchers legal scholars and the entire society Limitations We use Amazon Mechanical Turk to recruit participants but this may not be a representative sample of the population However we would like to emphasize that turkers are likely to provide a better proxy than machine learning experts for understanding how humans interact with assistance from machine learning models in critical challenging tasks Also our explanations are derived from a linear SVM classifier and nearest neighbors It may be even more challenging for humans to interpret explanations of non-linear classifiers Another important challenge in understanding how humans interact with machine learning models lies in the difficulty to assess the generalizability of our results Our formulation of deception detection represents a scenario where machines outperform humans by a wide margin and humans may have developed false beliefs about this task as most humans have read reviews online In order to consider a wide range of tasks eg bail decisions and medical diagnosis we need a framework to compare different tasks Machine performance and humans prior intuition are probably important factors that can influence human interpretation of the explanations However it remains an open question whether there exists a principled framework to reason about these tasks At the very least it is important for our community to go beyond simple visual tasks such as OCR and object recognition especially for the purpose of improving human performance in decision making