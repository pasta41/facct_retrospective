Deep Weighted Averaging Classifiers Recent advances in deep learning have achieved impressive gains in classification accuracy on a variety of types of data including images and text Despite these gains however concerns have been raised about the calibration robustness and interpretability of these models In this paper we propose a simple way to modify any conventional deep architecture to automatically provide more transparent explanations for classification decisions as well as an intuitive notion of the credibility of each prediction Specifically we draw on ideas from nonparametric kernel regression and propose to predict labels based on a weighted sum of training instances where the weights are determined by distance in a learned instance-embedding space Working within the framework of conformal methods we propose a new measure of nonconformity suggested by our model and experimentally validate the accompanying theoretical expectations demonstrating improved transparency controlled error rates and robustness to out-of-domain data without compromising on accuracy or calibration CONCEPTS Computing methodologies Supervised learning KEYWORDS interpretability credibility conformal methods INTRODUCTION For any domain involving complex structured or high-dimensional data deep learning has rapidly become the dominant approach for training classifiers Although deep learning is somewhat vaguely defined we will use it here to refer to any architecture which makes a prediction based on the output of a function involving a series of linear and non-linear transformations of the input representation While the details of these transformations differ by domain for example two-dimensional convolutions are often used for images whereas sequential models with attention are more common for text most models for binary or multiclass classification include a final softmax layer to produce a properly normalized probability distribution over the label space In this paper we explore an alternative to the softmax yielding what we call a deep weighted averaging classifier DWAC and evaluate its potential to deliver equally accurate predictions while offering greater transparency interpretability and robustness Despite the success of deep learning as a framework concerns have been raised about the properties of deep models In addition to typically requiring large amounts of labeled data and computational resources the parameters tend to be relatively difficult to interpret compared to more traditional eg linear methods In addition some deep models tend to be poorly calibrated relative to simpler models despite being more accurate Finally careful evaluation and an explosion of work on adversarial examples have demonstrated that many deep models are more brittle than test-set accuracies would suggest Particularly in light of recent controversy and legislation such as the General Data Protection Regulation GDPR in Europe there has been rapidly growing interest in developing more interpretable models and in finding ways to provide explanations for predictions made by machine learning systems Although there is currently an active debate in the field about how best to conceptualize and operationalize these terms recent research has broadly fallen into two camps Some work has focused on models that are inherently interpretable such that an explanation for a decision can be given in terms that are easily understood by humans This category includes classic models that can easily be simulated by humans such as decision lists as well as sparse linear models where the prediction is based on a weighted sum of features Other work meanwhile has focused on developing methods to provide explanations that approximate the true inner workings of more complex models in a way that provides some utility to the user or developer of a model beyond what is attainable through more direct means In this paper we propose a method which like those in the former category offers an explanation that is transparent in that the complete explanation is in terms of a weighted sum of training instances but also explore ways to approximate this explanation by using only a subset of the relevant instances While this approach retains some of the inherent complexity of typical deep models in that it is still difficult to explain why the model has weighted Our implementation is available at the training instances as it has for a particular test instance the mechanism behind the prediction is far more transparent than softmax-based models and the individual instance weights provide a way for a user to examine the basis of the prediction and evaluate whether or not the model is doing something reasonable Similarly while looking at the nearest neighbors of a test point is a commonly used heuristic to attempt to understand what a model is doing that approach is only an approximation for models which map each instance directly to a vector of probabilities There is of course a long history in machine learning of making predictions directly in terms of training instances including nearest neighbor methods kernel methods including support vector machines and transductive learners more broadly The main novelty here is to adapt any existing deep model to make predictions explicitly in terms of the training data using only a minor modification to the model architecture and arguing for and demonstrating the advantages offered by this approach As we will describe in more detail below we propose to learn a function which maps from the input representation to a low-dimensional vector representation of each input Predictions on new instances are then made in terms of a weighting of the label vectors of the training instances where the weights are a function of the distance from the instance to be evaluated to all training instances in the low-dimensional space This is closely related to a long line of past work on metric learning but rather than trying to optimize a particular notion of distance such as Mahalanobis distance we make use of a fixed distance function and allow the architecture of standard deep models to do the equivalent work for us This idea is also related to models which use neural networks to learn a similarity function for specific applications such as face recognition or text similarity here we show how this is a more generally applicable way to train models and we emphasize the connection to interpretability Such an approach comes with distinct advantages A precise explanation of why the model makes a specific prediction label or probability can be given in terms of a subset of the training examples rank ordered by distance Moreover the weight on each training instance implicitly captures the degree to which the model views the two instances as similar The explanation is thus given in terms of exemplars or prototypes which have been shown to be an effective approach to interpretability In we show that in many cases a very small subset of training instances can be used to provide an approximate explanation with high fidelity to the complete explanation In addition it is possible to choose the size of the learned output representation so as to trade off between performance and interpretability For example we can use a lower dimensional output representation if we wish to make it easy to directly visualize the embedded training data Even in cases where revealing the training data is not feasible it is possible to provide an explanation purely in terms of weights and labels Although this does not reveal the way in which a new instance is viewed by the model as similar to past examples it sill provides a quantifiable notion of how unusual the new example is The form of our model suggests a natural metric of nonconformity and in we formalize this using the notion of conformal methods describing how the relevant distances can be used to either provide bounds on the error rate for data drawn from the same distribution or robustness against outliers Finally although this model does entail a slight cost in terms of increased computational complexity the difference in terms of speed and memory requirements at test time can be minimized by pre-computing and storing only the low-dimensional representation of the training data from the final layer of the model The cost during training will in most cases be dominated by the other parts of the network and it is still possible to train such models on large datasets without difficulty Moreover in our experiments this choice seemingly involves no loss in accuracy or calibration Our deep weighted averaging classifiers DWACs are ideally suited to domains where it is possible to directly inspect the training data such as controlled settings like social science research medical image analysis or managing customer feedback In these domains DWACs can create a more transparent and interpretable version of any successfully developed deep learning architecture Although the advantages are diminished in domains where privacy is a concern presenting information solely in terms of weights and labels still provides a useful way to quantify the credibility of a prediction even without allowing direct inspection of the original training data BACKGROUND Scope and Notation In this paper we will be concerned with the problem of classification In general we will assume a set of m training instances xi for i m with corresponding labels in some categorical label space Y for i m where c Y is the number of classes We also assume we will eventually be given a set of n test instances xi for i m n m We will use hi to refer to the output representation of our model for instance i Square brackets will denote the kth element of a vector Nonparametric Kernel Regression We propose to build on a classic method from nonparametric regression known as Nadaraya-Watson The original use case of was in regression with R where it is assumed that where and The goal is to estimate the mean function which can be expressed in terms of the joint density as X x y P y x y P P By approximating the joint density using kernel density estimation it is possible to re-express this as a weighted sum of training instances ie m i m where is a kernel such as a Gaussian It is easy to see that this corresponds to a linear smoother in that it will predict outputs as a weighted sum of training instances ie m i x where x m This method can easily be adapted to classification by predicting the probability of an output label as a weighted sum of training labels ie y x m i x where I equals if the condition holds or if not The primary limitation on is due to the curse of dimensionality as the dimensionality of x grows sparse data becomes a problem and the notion of proximity becomes problematic In this work we show how we can avoid this problem by using neural networks to embed inputs into a space with reasonable dimensionality and proceed to compute weights over training instances in that space Conformal Methods Conformal methods refer to a broad set of ideas which aim to provide theoretical guarantees on error rates in classification or regression Conformal methods can be used with any base classifier or predictor and work by introducing a generic notion of nonconformity As will be explained in detail below each possible prediction ie label or value that can be made for a given test instance can be evaluated in terms of its nonconformity By comparing these with the nonconformity scores of either all data in a leave-one-out manner or with a held-out calibration set the equivalent of a p-value can be associated with each possible prediction allowing for thresholding in a way that provides a guarantee on the error rate for iid or exchangeable data Because of the high computational cost of the leave-one-out approach to conformal predictors we will focus here on the approach based on a held-out calibration set In particular we will begin by shuffling our training data and partitioning it into a proper training set i t and a calibration set i t m The fundamental choices in conformal methods are a base classifier and a measure of nonconformity A The latter concept which intuitively corresponds to how atypical an instance is maps a bag of examples ie the proper training set with labels and one additional instance x the observed features with one possible label Y to a scalar R ie A where denotes a bag ie a multi-set potentially containing duplicate instances The idea of a measure of nonconformity is quite general but in practice the most common approach is to convert a bag of examples into a model and then compare the prediction of that model on the training instance x with the hypothesized label In particular The leave-one-out approach may be superior in terms of statistical efficiency but it is computationally infeasible for all but very small datasets Our calibration set will also serve as our validation set for early stopping during training for any probabilistic model the simplest measure of model-based nonconformity is any inverse monotonic transform of the predicted probability of the hypothesized label such as the inverse or negation For example the following is a valid measure of nonconformity PM y x represents a model trained on the proper training set i t Conformal methods work by comparing the nonconformity score of each possible prediction for each test instance to the nonconformity scores of all instances in the calibration set Specifically we will compute a p-value for each hypothetical test instance label equal to the proportion of the calibration instances that have a higher nonconformity score ie a value between and indicating how conforming a possible label for a new instance is relative to the calibration data More precisely for a test instance x and hypothesized label p m I m t where is computed relative to the proper training set for all instances both calibration and test Unlike traditional classifiers conformal methods may predict anywhere from zero to c labels for a given instance We will revisit this choice below but for the moment the decision rule will be that for each test instance we make a positive prediction for all labels for which p where is chosen by the user By the properties of conformal predictors these predictions will be asymptotically valid that is both theoretically and empirically for iid data this will produce a set of predicted labels for each test instance such that at least of the predicted label sets include the true label with high probability Moreover this property holds for any measure of nonconformity Of course this property is trivially easy to satisfy by predicting all labels for all test instances In practice however better choices of measures of nonconformity will be more or less efficient in that they will tend to produce smaller predicted label sets without compromising on error rate In addition for any given test instance we can compute two properties of the label that is predicted with the highest probability which Saunders et al called confidence and credibility Confidence is equal to the largest such that the predicted label set includes only a single label ie one minus the second-largest p-value among the possible labels This corresponds to the probability according to the model that the label which is predicted to be most likely is correct For example for iid data we would expect of predictions with confidence to be correct and to be incorrect Credibility by contrast is equal to the largest for which the predicted label set is empty ie the largest p-value among the possible labels This correspond to one minus the models confidence that none of the possible labels are correct In other words predictions More precisely for any measure of nonconformity and iid data drawn from any distribution the unconditional probability that the true label of a random test instance is not in the predicted label set does not exceed for any The broader statement follows from the law of large numbers It is also possible to bound the conditional probability of error conditional on the training data with slightly weaker guarantees For more details please refer to with low credibility indicate that even the most likely prediction is relatively nonconforming compared to the calibration instances with their true labels and that we should therefore be skeptical of this prediction The idea of not predicting any label is somewhat foreign to conventional approaches as we can only obtain better accuracy by hazarding at least some guess and in some applications it would be reasonable to still predict the most probable label according to the model with an associated confidence and credibility In other circumstances however there may be valuable information in an empty label set For example if we set to and find that of the test instances end up with empty predicted label sets this is a strong indication that the predictions for most of the remaining instances are correct because the overall error rate is expected to be In practice this would be an unusual outcome but we will explore the usefulness of this sort of signal in the context of outlier detection DEEP WEIGHTED AVERAGING CLASSIFIERS We now turn to our new method deep weighted averaging classifiers Model Details Most neural network models for classification take the form P y x c where x b In this architecture x embeds the input in a model-specific way eg a convolutional or recurrent network into a lower-dimensional vector representation Although and b could be folded into x we make them explicit to emphasize that is required to project the output of x which is a vector of arbitrary length down to a vector of length c the number of classes The softmax then projects this vector onto the simplex Our proposed alternative is to leave x unchanged eliminate the softmax and to redefine the predicted probability as P y x t i hi t ie a weighted sum over the labels of the instances in the proper training set where is defined as above hi is a function of the similarity between the embeddings of x and xi in the low-dimensional space according to some metric In this architecture the dimensionality of is arbitrary and the size of and b can be modified as necessary An obvious choice of weight function is a Gaussian kernel operating on Euclidean distance ie hi hi We could of course dispense and b here and compute directly on the output of x but we wish to remain as close as possible to the softmax model for the purpose of comparison while allowing for the possibility of varying the size of Typically in using or other kernel smoothers one needs to choose the bandwidth equivalent to in equation However because we will assume that we will be building this classifier on top of a high-capactity embedding network x we will simply fix and force x to adapt to this distance function Training In order to learn b and all parameters of x we use stochastic gradient descent to optimize the log loss on the training data Because it is impractical to compute the exact probabilities according to the model during training because they depend on all training instances we instead rely on an approximation based on the other instances within each minibatch Specifically on each epoch of training we shuffle all instances in the proper training set into minibatches of size B For each minibatch B we then minimize L B B c log P where P i Bj hi l Bj As usual the loss function in equation can be augmented with a regularization term if desired Although computing all pairwise distances between many points is relatively expensive this can be done efficiently for minibatches using standard matrix operations on a GPU Specifically a forward pass through the last layer of a DWAC model ie computing probabilities for one minibatch requires O Bh where B is the size of the minibatch and This will typically be larger than the last layer of the softmax classifier which is O where c is the number of classes but in most cases will be dominated by the cost of computing x In practice the only significant increase in training time is due to the need to embed all training instances in order to estimate performance on a validation set after each epoch As such the training runtime will tend to be no worse than twice that of training a softmax model Similarly at test time the computational cost of making a prediction on one test instance is dominated by the cost of embedding the training data However this can be pre-computed after training and only the low-dimensional vectors need be stored Prediction and Explanations Once the model has been trained predictions can easily be made using the entire training dataset rather than using a subset as when computing the loss during training The complete and explicit explanation for why the model predicts a particular label or probability can then be given explicitly in terms of the training instances along with the weight on each instance Moreover if we consider the closest points which will be most heavily weighted as being the most similar relevant or important it is reasonable to provide a sorted list of examples as the explanation Because the later examples will carry less weight in many cases only a subset of instances needs to be provided because for many instances Note that we only need to compute distances in the low-dimensional space for which we can choose an appropriately small dimensionality the lower-weighted training instances will be unable to affect the prediction no matter what their labels may be If we wish to provide an even simpler but approximate explanation we can also choose to provide only the closest examples as the explanation which is a commonly used heuristic for trying to understand model behavior Although we would not expect that using only a small set of examples would provide a well-calibrated probability it could still provide a reasonable approximate explanation for why the model predicted a particular label assuming that there is strong agreement between predictions made using such a subset and the full model which we will empirically evaluate in the experiments below Confidence and Credibility As discussed in section above for any probabilistic classifier we can use any monotonic transformation of the predicted probabilities which reverses their order such as P x or x as a valid measure of nonconformity However our proposed architecture suggests another measure namely the negated unnormalized weighted sum of training labels of the hypothesized class ie A t i hi Because of the properties of conformal methods this measure of nonconformity is automatically valid It may however be more or less efficient than other measures such as ones based on probabilities We note however that this proposed measure has an intuitive explanation in terms of how close the training points of the predicted class are in the embedded space to the instance for which we wish to make a prediction Naturally the absolute distance has no meaning in the embedded space but we avoid this problem by scaling the measure of nonconformity relative to calibration data in order to obtain a p-value as is always the case in conformal methods When using probability as the basis of nonconformity the farther a point is from the decision boundary the higher will be its predicted probability and therefore its credibility Under the measure we propose in equation by contrast predictions will only be associated with high credibility when the embedded representation of that instance is relatively close to the embedded training instances That is if we encounter an instance that is unlike anything seen in the training data and if the model embeds that instance such that it is far away from all embedded training instances then this measure will tell us that it is highly nonconforming for all classes which will result in the models prediction having very low credibility As we show below this is a useful way to quantify the degree to which we should be skeptical of the models prediction EXPERIMENTS To demonstrate the potential of DWAC models we provide a range of empirical evaluations on a variety of datasets In addition to showing that our proposed approach is capable of obtaining equivalently accurate predictions we also compare to a baseline in terms of calibration and robustness to outliers and illustrate the sorts of explanations offered by a DWAC model We also empirically validate that the theoretical guarantees claimed by conformal methods Dataset Type classes instances features Adult Income Tabular Covertype Tabular Lending Club Tabular Fashion MNIST Image CIFAR- Image Subjectivity Text Stack Overflow Text IMDB Text Amazon Text Table Properties of datasets used in this paper For text data we report vocabulary size as the number of features hold for both the softmax and the DWAC model both with and without our proposed measure of nonconformity In all cases we report accuracy and calibration ie the accuracy of the predicted probabilities measuring the latter in terms of mean absolute error MAE using the adaptive binning approach of Nguyen and OConnor For the initial comparison between models we only consider conventional prediction ie only using the top-scoring label predicted by each model and separately evaluate conformal prediction in Note that our purpose here is not to demonstrate state-of-the-art performance on any particular task but rather to show that our proposed modification works for a wide variety of architectures In order to evaluate robustness to outliers we consider two approaches The first is to drop one class from a multiclass dataset and treat the held-out class as out-of-domain data The other approach is to find a dataset that has a similar input representation but is fundamentally different in terms of content and again treat these instances as out-of-domain Datasets For our experiments we make use of datasets of three different types tabular image and text including both binary and multiclass problems For tabular data we use the familiar Adult Income and Covertype datasets available from the UCI machine learning repository as well as the Lending Club loan dataset available through Kaggle For images we use CIFAR- and Fashion MNIST For text we use paragraph-length product reviews Amazon stars and movie reviews IMDB positive or negative sentences extracted from movie reviews and labeled in terms of subjectivity and a dataset of Stack Overflow question titles sampled from different categories Table summarizes the most important properties of these datasets more details and URLs can be found in supplementary material Models and Training In all cases we choose a base model appropriate to the data For the tabular data we use a simple three-layer multi-layer perceptron For images we use multi-layer convolutional neural networks For text datasets we use a shallow convolutional model with attention In all cases we compare DWAC and softmax models of equivalent size but also explore varying the dimensionality of in the DWAC model We use the standard train/test split where available and Accuracy Dataset Softmax DWAC Adult Income Covertype Lending Club Fashion MNIST CIFAR- Subjectivity Stack Overflow IMDB Amazon Calibration MAE Dataset Softmax DWAC Adult Income Covertype Lending Club FashionMNIST CIFAR- Subjectivity Stack Overflow IMDB Amazon Table Accuracy higher is better and calibration lower is better on various datasets using the single best-scoring predicted label from softmax and DWAC models of equivalent size with standard deviations in parentheses otherwise sample a random of the data for a test set and always use a random of the training data as a validation/calibration set For measuring accuracy and calibration on test data we average over trials with different splits of the training data into a proper training set and a validation/calibration set with the same split being given to both models For both models we use Adam with an initial learning rate of and early stopping For more details please refer to supplementary material RESULTS Classification Performance As shown in Table except for one dataset Covertype the performance of our model is indistinguishable from a softmax model of the same size in terms of accuracy As noted above these results are based on the single best-scoring label from each model without yet incorporating the idea of conformal prediction For calibration the DWAC model is sometimes slightly better and sometimes slightly worse although we note in passing that at least for these models and datasets the predictions from both models are quite well calibrated such that the predicted probabilities are relatively reliable at least for in-domain test data As expected runtime during training was approximately longer per epoch than for the equivalent softmax model with a similar number of epochs required One advantage of DWAC is the freedom to choose the dimensionality of the final output layer and Figure illustrates the impact of this choice on the performance of our model While using the same dimensionality as the softmax model gives equivalent performance Softmax Size of output layer A cc u ra Figure Accuracy of DWAC in comparison to a softmax model on the -class Fashion MNIST dataset for varying dimensionality of the output layer Performance is indistinguishable for a DWAC model of the same size but accuracy drops if we decrease the size of the output layer too much T-shirt Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Boot Figure Learned embeddings of the Fashion MNIST training data when using a DWAC model with a two-dimensional output layer the same accuracy can often be obtained using a lower-dimensional representation ie few total parameters In some cases however reducing the dimensionality too much eg two-dimensional output for Fashion MNIST results in a slight degradation of performance On the other hand using a two-dimensional output layer means that we are able to more easily visualize the learned embeddings without requiring an additional dimensionality reduction step eg using PCA or t-SNE In this way we could look directly at where a test instance is being embedded relative to training instances with no loss of fidelity Figure shows the embeddings learned by our model for the Fashion MNIST training data using a two-dimensional output layer Pleasingly there is a natural semantics to this space with all of the footwear occurring close together and the shirt class being centrally located relative to related classes t-shirt pullover etc Boot Boot Boot Boot Boot Trouser Trouser Trouser Trouser Trouser Figure Two examples of predictions made by the DWAC model on the Fashion MNIST dataset An approximate explanation for the models prediction on the test image single image is the fourmost-highly weighted training images row of four along with their weights Interpretability and Explanations Recall that explanations for predictions made by our model are given in terms of a weighted sum of training instances Figure top shows an example of a partial explanation provided by our model for an image dataset A test image is shown in the top row along with its true label and the four closest images from the proper training set as measured in the embedded space are shown below it along with their weights In this case all share the same label and all contribute approximately equally to the prediction As a contrasting example Figure bottom shows an example which has poor support among the nearest training points Although the prediction is correct and the closest training images appear visually similar this sort of wide-legged trouser is quite rare in the dataset most trousers included have narrow legs As such the model has clearly not learned as well how to embed such images into the low-dimensional space The low weights indicate that we should be skeptical of this prediction Images are relatively easy to compare at a glance text by contrast may be more difficult Nevertheless the explanations given by our model for the predictions on text data are in some cases very meaningful For the Stack Overflow dataset for example many instances are almost trivially easy in that the label is part of the question Not surprisingly these examples tend to have many highly weighted neighbors which provide a convincing explanation Such an example is shown in Table top In other cases the text is Weight Sentence Label Test Drupal dynamic menu item Drupal drupal how to limit the display results of the calendar view Drupal Drupal Different output for first item in a block Drupal changing user role in drupal Drupal Weight Sentence Label Test save data from editable division Ajax Pass data from workspace to a function Matlab upload data from excel to access using java Apache Finding incorrectly formatted email addresses in a CSV file Excel Table Two examples from the Stack Overflow dataset with approximate explanations from a DWAC model an easy example with many close neighbours top and amore difficult example with no close neighbours bottom The first line is the test instance in both cases more ambiguous Table bottom shows an example with very little support for which a user might rightly be skeptical of the model based on both the weights and the explanation Finally because we can use any deep model to compute x we are free to choose one with characteristics we prefer including interpretability For our text classification experiments we chose a base model originally proposed for interpretable classification of medical texts To further unpack the explanation given for a prediction we could for example inspect the attention weights for a particular pair of sentences to understand the importance of each word in context For a comparable pair of examples from a tabular dataset please refer to supplementary material Approximate Explanations Because the weights on training instances decrease exponentially with distance the closest training instances will contribute the most to the prediction In some cases only relatively few training instances will be required to fully determine the models predicted label because beyond this the remaining instances will lack sufficient weight to alter which class will be most highly-weighted In practice most test instances tend to require a substantial proportion of the nearest training instances in order to cross this threshold However even considering a much smaller number of the closest training instances may still result in high agreement with the prediction based on all of the data Table shows the agreement with the full model if we only consider the top neighbors to each test instance For most datasets this agreement is very high even for a very small number of neighbors Dataset Adult Income Covertype Lending Club Fashion MNIST CIFAR- Subjectivity Stack Overflow IMDB Amazon Table Impact of considering a subset of the training instances as an approximate explanation the columns show agreement with the full model on the single most-probable label when basing the prediction on only the closest training instances Confidence and Credibility The above results only considered the single top-scoring label predicted for each instance here we extend both models to the conformal setting To verify the theoretical expectations of conformal methods we show that our proposed measure of nonconformity correctly works to maintain a desired error rate in making predictions Figure shows the results for the Fashion MNIST dataset where we vary the desired maximum error rate from to The top subfigure a shows the proportion of predictions on the test set which are correct that is which contain the true label for the softmax model using negative probability probs as a measure of nonconformity our model using the same measure and our model using our proposed measure of nonconformity weights given in equation As can be seen all three demonstrate correct coverage with all lines close to but not exceeding the expected proportion across the full range shown as a dashed line on the top subfigure Note that this is not the same as accuracy as some predictions may contain multiple labels The second and third subfigures show the same lines for the proportion of predicted label sets that are empty b or contain multiple labels c The bottom figure shows the mean number of labels in all non-empty predicted label sets In all cases the dashed line represents an optimal outcome that is a proportion of predicted label sets equal to are empty all other predictions are correct and no predictions contain multiple labels As can be seen the softmax and DWAC models give indistinguishable results when using the same measure of nonconformity Our proposed measure of nonconformity by contrast appears to be slightly less efficient producing slightly more predicted label sets with multiple labels but also slightly more empty label sets which represent identifiable errors contributing to the proportion of incorrect predictions The advantage of our proposed measure however comes in robustness to out-of-domain data If we train a model on the Fashion MNIST dataset and then ask it to make predictions on the original MNIST digits dataset which has the same size and data format but consists of hand-written digits rather than items of clothing we would hope that a good model would predict relatively low probability for all such instances In fact as has been previously observed P ro p or ti on of in st an ce s a Correct predictions Softmax probs DWAC probs DWAC weights P ro p or ti on of in st an ce s b Empty predictions Softmax probs DWAC probs DWAC weights P ro p or ti on of in st an ce s c Multiple predictions Softmax probs DWAC probs DWAC weights re d te d la b el s d Mean size of non-empty prediction sets Softmax probs DWAC probs DWAC weights Figure Coverage of various models on the Fashion MNIST test data as we vary the desired maximum error rate From top to bottom the subfigures show a the proportion of predicted label sets that are correct contain the true label b that are empty make no prediction c that contain multiple labels and d the mean number of labels in nonempty prediction sets The softmax and DWAC models give nearly identical results when using negative probability as a measure of nonconformity probs Our proposed measure weights has an indistinguishable error rate but is slightly less efficient The dashed line in each figure represents an optimal response deep models tend to predict relatively high probabilities even for out-of-domain data and this is also true of DWAC models Fortunately the credibility score from a conformal predictor provides a meaningful estimate of how much we should trust the corresponding prediction Both the softmax model using negative Specifically as mentioned above it is equal to one minus the model probability that none of the labels should be given to this instance T es t in st an ce s Credibility T es t in st an ce s Figure Empirical distribution of credibility scores from the softmax top and DWAC bottom models when trained on Fashion MNIST and tested on MNIST digits which have the same input format but different content with the latter using our proposed measure of nonconformity probability as a measure of nonconformity and the DWAC model using our proposed measure of nonconformity give low credibility to the vast majority of out-of-domain examples as shown in Figure The credibility scores from DWAC however are noticeably shifted closer to zero indicating that the sum of the weights of the corresponding class is a better measure when we are concerned about the possibility of out-of-domain data For in-domain data the credibility values will be approximately uniformly distributed We see a similar result when training on CIFAR- images and predicting on the Tiny Images dataset and an even more extreme difference in the case of the Covertype dataset where we treat one out of seven classes as out-of-domain data and train a model on only the six remaining classes For that setup the mean credibility score from the softmax model on the out-of-domain data is whereas for the DWAC model with our measure of nonconformity it is figures in supplementary material This indicates that to the softmax model the held-out images look like even more extreme versions of one of the other classes whereas the DWAC model correctly recognizes that the held-out images are relatively unlike the training instances relative to the calibration data DISCUSSION AND FUTURE WORK The idea of interpretability has always been important in statistics and machine learning but has taken on a renewed urgency with the increased expressive power of deep learning models and the expanded deployment of machine learning systems in society No single approach to interpretability is likely to solve all problems here we have focused on adapting existing models so as to make their predictions more transparent by decomposing them into a sum over training instances the support for each of which can be inspected As emphasized by papers that have made use of user studies careful empirical work is required to evaluate the effectiveness of explanations and we leave such an evaluation of this approach for future work A few recent papers have also sought to formalize the question of when to trust a classifier Jiang et al present a method for determining how much any existing classifier should be trusted on any given test point based on the relative distance to the most-likely and second-most-likely clusters with clusters based on a pruned training set This appears to be a theoretically well-motivated and empirically effective technique but is more focused on trust than interpretability In an unpublished paper Paper not and McDaniel propose to train a conventional deep model but make predictions using a k-nearest neighbours approach with distance computed using all internal nodes of the network We by contrast propose to train a model using the same form as will be used for prediction to make predictions based on a weighted sum over the entire training set and rely on similarities computed in the low-dimensional space of the final layer Wallace et al apply the method from Papernot and McDaniel to the problem of text classification and explore the implications for interpretability There have also been several papers focused on the problem of predicting whether data is in-domain or out-of-domain Many of these build on Hendrycks and Gimpel who observed that the predicted probabilities contain some useful signal as to whether data came from in-domain or out-of-domain and proposed to use this to differentiate between the two by thresholding these probabilities The authors did not however make the connection to conformal methods which offer a more theoretically sound basis on which to make these decisions as well as greater flexibility of metrics beyond just predicted probability There are several natural extensions to this work which could be pursued such as applying a similar architecture to regression or multi-label problems as well as extending the idea of nonconformity to provide class-conditional guarantees CONCLUSIONS In this paper we have demonstrated that even for sophisticated deep learning models it is possible to create a nearly identical model with all of the same desirable properties that nevertheless provides an explanation for any prediction in terms of a weighted sum of training instances In domains where the training data can be freely inspected this provides greater transparency by revealing the many components that are explicitly contributing to a models prediction each of which can in principle be inspected and interrogated Moreover this method can build on top of other approaches to interpretability by choosing an appropriate base model When an approximate explanation will suffice then using only a small subset of the training instances provides a natural high-fidelity approximation More importantly representing the prediction in this manner suggests a natural alternative measure of nonconformity which as we have shown provides a more effective measure for detecting out-of-domain examples Even in cases where training data cannot be shared for privacy reasons for example this use of conformal methods still allows us to assert a quantitative estimate of the credibility of an individual prediction one that is far more meaningful than the models predicted probability