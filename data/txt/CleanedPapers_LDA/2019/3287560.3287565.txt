Whos the Guinea Pig Investigating Online A/B/n Tests in-the-Wild A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization These tests are often run on their websites without explicit consent from users In this paper we investigate such online A/B/n tests by using Optimizely as a lens First we provide measurement results of websites that use Optimizely drawn from the Alexa Top-M and analyze the distributions of their audiences and experiments Then we use three case studies to discuss potential ethical pitfalls of such experiments including involvement of political content price discrimination and advertising campaigns We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators CONCEPTS Security and privacy Privacy protections Human-centered computing User studies Empirical studies in HCI Social and professional topics Codes of ethics KEYWORDS online controlled experiments A/B/n testing personalization INTRODUCTION It has been almost a century since Sir Ronald Fisher developed the theory and practice of controlled experiments Since then these techniques have been widely adopted by businesses and marketers to optimize everything from the layout of assembly lines to the messaging and targeting of advertising campaigns A/B/n testing aka split testing or bucket testing in particular has been eagerly adopted by technology companies as a data-driven approach to product design and optimization An A/B/n test is a straightforward between-subjects experimental design where the users of a website are split into n groups with one serving as the control and the other n as treatments The control group is shown the product as-is while the treatments groups are shown variations After some time has passed the users behaviors are analyzed to determine which treatment if any had a desirable effect eg more clicks higher conversion rates etc Many large technology companies are open about their advocacy of Online Controlled Experiments OCEs like A/B/n testing In Google famously used A/B tests to experiment with the presentation of results in Google Search and by Google engineers claimed to be running over A/B tests per year Google and other large companies like Facebook Microsoft and LinkedIn have published papers describing their infrastructure for supporting and scaling OCEs on their platforms Although it is known that OCEs are widely used by the biggest technology companies critical questions remain unanswered about their use in practice First are OCEs used by websites beyond the largest platforms and if so at what scale eg how many treatments and experiments Second what is the substance of the treatments that users are subject to This second question is especially pertinent because OCEs are a form of human experimentation Thus we must consider the ethics of these experiments especially because they may be conducted without explicit consent and may cause a variety of harms depending on the design being tested Indeed companies are not bound by frameworks such as the Belmont Report and evidence suggests that the executives engineers and marketers who conduct OCEs may not be versed in or even aware of experimental ethics In this study we take a first step towards filling this knowledge gap by using Optimizely as a lens Optimizely is a service that enables website operators to build manage and analyze OCEs such as A/B and multivariate tests on their websites It is the most popular of several services that offer similar functionality as of mid Crucially for us Optimizely is implemented using a clientside JavaScript library when a user visits a website the library dynamically determines what audiences ie treatment groups the user is part of and executes any experiments ie treatments associated with those audiences This design enables us to record all audiences and experiments that are available on a website that uses Optimizely We crawled sites drawn from the Alexa Top-M that were previously observed including resources from Optimizely Of these sites were running experiments at the time of our crawls from January to March Using this dataset we examine what kinds of websites conducts OCEs how many audiences do We are not aware of any major website that prominently discloses the existence of Optimizely experiments to users or asks for affirmative consent although it is possible that websites may disclose this practice in their terms of use or privacy policy experiments do they run In total our analysis considers audiences and experiments Measurement Results We found that the usage of Optimizely is heavily distributed over top-ranked websites Most of these websites were conducting experiments on audiences while a small number of websites were running dozens of experiments with complicated audience structure eg Optimizely itself The New York Times and AirAsia We analyze how websites segment audiences overall and present detailed results for popular attributes such as location device and browser Furthermore we analyze the experiments captured in our dataset and find that most were ie no variations or were A/B tests ie a single variation that targeted or audiences from a single group Case Studies In addition we also qualitatively investigate the substance of a subset of experiments since Optimizely experiments can make arbitrary changes to web pages there is no way to analyze them at scale We focus on three case studies political content e-commerce and advertising Each case study is motivated by specific ethical concerns For example prior work has shown that changing the partisan valence of online content can influence voting behavior if an OCE were to manipulate the valence of political content eg news headlines this could increase political polarization Similarly prior work has uncovered numerous instances of online price discrimination including A/B testing on Expedia OCEs could also be used to manipulate consumer behavior by altering prices changing the presentation of products to emphasize more or less expensive options or tailor discounts to specific users Our hope is that this study raise awareness of the scope of OCEs in-the-wild and fosters a conversation about the ethics of these experiments Today users do not affirmatively consent to the vast majority of OCEs Instead users are unaware experiments are not transparent and experimenters are not accountable to their subjects Although our study focuses on Optimizely we note that they are just a tool vendor ultimately it is up to companies using this tool and others like it to grapple with the ethics of experiments and to obey best practices We begin by briefly surveying existing work on OCEs Systems Research groups from several major tech companies have published papers describing the systems they use internally to deploy and manage OCEs Facebook Bookingcom Microsoft in general and Bing specifically have all developed systems to help their software developers design deploy and analyze the results of OCEs These systems focus on the challenge of scaling experiments to very large platforms Facebooks system PlanOut is open-source Google also has a system for managing OCEs and their work focuses on managing and avoiding conflicts between concurrent experiments et al from LinkedIn discuss the challenges of changing corporate culture to embrace OCEs and describe a system for deploying OCEs within mobile apps Figure Optimizely Experiments A user can create new experiments to run which include A/B tests multivariate tests etc A user can further specify experiment details including design variations targeted audience etc Methods There is a body of work focused on practical methods for conducting OCEs The founders of Optimizely published a book that introduces the topic of A/B testing to marketers and web designers Kohavi et al presented a denser and more academically-minded guide to implementing and analyzing A/B tests on the web as well as a follow-up focused on univariate versus multivariate experimental designs Kohavi et al published two subsequent papers that scrutinize real-world experimental case studies that generated unexpected results to illustrate subtle issues with experiment design that can impact the interpretation of results For example one case study from Bing highlighted the difficulty of choosing success criteria for experiments aka the Overall Evaluation Criterion that balances shortand long-term business goals Kohavi et al later summarized their insights into seven rules of thumb for successful OCEs A separate thread of work focused on statistical methods for increasing the power of OCEs For example Deng et al presented a new variance estimation metric that relaxes assumptions about the independence participant randomization while Hill et al focus on measuring causality in experiments involving online display ads Law and Ethics Previous work has highlighted ethical issues with specific OCEs For example Facebooks news feed experiments and OKCupids manipulation of their matching algorithms have been criticized for their absence of user consent as well as their lack of protection for human subjects from the perspectives of law and research ethics Although the prior work we highlight here often presents case studies of specific experiments that were conducted by large companies none present a comprehensive overview of OCEs in practice such as how audiences are segmented or the broad range of treatments that are tested BACKGROUND In this study we use the service provided by Optimizely as a lens to analyze OCEs at-scale across the web In this section we introduce Optimizely and describe the functionality offered by their service Optimizely is an American software company that provides a platform for conducting OCEs on websites Founded in Optimizely is one of several companies that offer tools and platforms for conducting OCEs including Google Optimize Adobe Target AB was the most widely used OCE platform as measured by the number of websites embedding their JavaScript Therefore we choose Optimizely as the subject of our study Tool Description At a high-level Optimizely offers tools that enable the operator of a website eg the developer designer or maintainer to deploy A/B/n and multivariate tests to their websites Optimizely offers a one-month free trial after which the operator must pay a monthly fee to continue using the service Additionally Optimizely offers various tiers of functionality with the most expensive tier unlocking more advanced forms of experimentation and larger population sizes After an operator signs up the Optimizely web interface allows it to define audiences ie treatment groups and experiments ie treatments Figure shows a screenshot of the Optimizely interface including several experiments that we created in our account The left-hand rail provides links for creating audiences and experiments Optimizely offers operators a broad set of attributes for segmenting users into audiences This includes how and when a user is accessing the operators website eg their choice of browser localization eg a users language and IP address whether the user was funneled to the operators website via an ad campaign or an affiliate specific tracking and/or session cookies in the users browser or even custom JavaScript functions defined by the operator that execute in the users browser Tracking identifiers like cookies can be defined by the operator or drawn from third-party Data Management Platforms like BlueKai Operators may use Boolean logic to combine attributes in order to define narrowly segmented audiences As shown in Figure operators define experiments using Optimizelys tools The term is a bit of a misnomer because it encompasses true experiments like A/B/n and multivariate tests as well as statically defined personalization eg a custom homepage banner shown to specific audiences in perpetuity That said for the sake of consistency we adopt the term throughout this paper Operators can configure any number of variations per experiment as well as choose to divide users randomly across variations or to manually assign specific audiences to variations Optimizely allows many-to-many assignment between experiments and audiences ie a given experiment can include many audiences and a given audience may be assigned to many experiments Operators have complete control over the treatment effect of each experimental variation Operators define treatment effects using arbitrary snippets of HTML CSS and JavaScript giving them complete freedom to manipulate the DOM of their website Further operators specify which pages on their website a given experiment applies to using fully qualified URLs partial paths or regular expressions Optimizely provides a WYSIWYG interface for designing experiments that is powerful enough for non-programmers eg marketers to construct relatively sophisticated experiments Optimizely allows operators to configure the success metrics for their experiments and their tools automatically take care of In the rest of the paper we use the terms and to refer to the objects defined by Optimizely collecting the necessary telemetry eg user clicks to calculate the metrics Optimizely provides reporting functionality that allows operators to evaluate the efficacy of their experiments Low-Level Implementation For the purposes of our study it is important to understand how Optimizely integrates with operators websites Once an operator has defined their audiences and experiments Optimizely saves this information as JSON-encoded configuration files and stores them on Optimizelys CDN The operator must then configure its website to include Optimizelys JavaScript library into their website eg using a script src tag When a user visits a page on the operators website the Optimizely JavaScript library is downloaded and executed by the users browser The library then fetches the JSON files that define the operators audiences and experiments evaluates which audiences if any this user is a member of looks up the experimental treatments associated with these audiences and injects the associated treatment code into the web page In the case of A/B/n experiments that are randomized ie not associated with a particular audience the library randomly determines which bucket to place the current user into and injects the associated treatment code into the web page The way that Optimizely integrates with operators websites is critical to our study Because all of the audiences and experiments defined for a given website are in the JSON configuration files and these files are available client-side we are able to write a crawler that identifies and records these files for later analysis DATA COLLECTION In this section we describe the process we used to gather data for this study Recall that our goal is to analyze the audiences and experiments that website operators define in practice using Optimizely To collect this data we periodically crawled a large number of popular websites detected the presence of Optimizelys JavaScript code and if it was present recorded the JSON configuration file containing the definitions for the audiences and experiments on that website Crawl Strategy To select websites for our crawls we relied on data from a prior crawl of the Alexa Top-M domains that was conducted in One of the products of this crawl was a list of all third-party resources included by the Alexa Top-M domains including references to Optimizely domains included at least one resource from Optimizely these are the websites we targeted in our crawls We implemented our crawler in PhantomJS which is a headless-browser designed for automation and crawling that is based on Chromium We configured PhantomJS to present a valid User-Agent for Chrome as well as a standard-sized desktop viewport We scripted PhantomJS to visit a web page wait seconds for the web page to load completely then attempt to access the There are also browser extensions that allow users to interact with Optimizely experiments within their own browser Examples include This crawl visited pages per domain thus providing reasonable coverage of included third-party resources BuiltWith a website profiler estimated that there were websites using the Optimizely library in Thus the list from Bashir et al achieves reasonable coverage of websites that include Optimizely Alexa rank Pr ob ab ty d en si ty Figure Optimizely usage over Alexa rank The usage of Optimizely is heavily distributed over top-ranked websites Lower-ranked websites were less likely to use Percentage of web pages Pe rc en ta ge o eb si te s the Optimizely library Most websites of include Optimizely in all of their web pages in our sample Optimizely JSON configuration data by executing the following function that is made available by the Optimizely JavaScript library exp_json optimizelygetdata If the Optimizely JavaScript library is included in the current web page and if the websites operator has configured experiments then exp_json will contain a reference to the JSON configuration data and the crawler saves this to a file Otherwise an exception is generated and we record that the web page did not contain any Optimizely experiments Our crawler visited all target domains once per week from January to March We visited the websites periodically to collect longitudinal data about their usage of Optimizely The crawler visited the homepage for each website as well as randomly selected links on the homepage that pointed to the firstparty domain Note that for any given domain it is possible for zero or more web pages to include Optimizely experiments One reason we might observe zero usage is that the website used Optimizely in but no longer does Another potential reason is that the website uses Optimizely for analytics eg as an alternative to Google Analytics but not for experimentation in this case the JSON configuration data will not exist Finally as we will show some websites only include Optimizely within a subset of their web pages Ethics We were careful to obey standard ethical practices during our data collection The audience and experiment data we collected are high-level rule-based groups and contains no personal information We only visit at most pages per website per week therefore our impact on the websites is minimal Our data collection also had no impact on the experiments running on the website MEASUREMENT In this section we present a broad overview of Optimizely usage across popular websites with a focus on the audiences and experiments defined by websites Overview Of the websites that including resources from Optimizely in our historical dataset collected in we found that included the Optimizely JavaScript library in at least one of their web pages during our crawls There are several possible reasons for this discrepancy First websites may have stopped using Optimizely between and Second websites may include tracking beacons from Optimizely eg for analytics purposes but Audience count Pe rc en ta ge o eb si te s Figure Distribution of audiences per website Most websites of have defined audiences although Optimizely itself has defined Percentage of websites Platform Language IP Browser Location Cookies Customized Device Ty pe o a ie nc es Figure Popularity of audience attributes A majority of websites of are using customize JavaScript to define audiences Percentage of websites iPad Tablet iPhone Mobile Desktop D e for audiences Percentage of websites Opera Edge Firefox Chrome Safari IE ow se r for audiences not the experimental tools that we are interested in Third websites may use Optimizely on portions of their site that our crawler failed to reach due to the random nature of our crawls For the remainder of this study we focus on these websites In Figure we examine how the usage of Optimizely is distributed across websites sorted by popularity according to Alexa We observe that the likelihood of using Optimizely decreases drastically with Alexa rank Point Biserial of the usage we observe occurs on the top most popular websites This observation suggests that even though Optimizelys tools are designed for non-experts only websites with significant budgets and staff are able or willing to engage in OCEs Next we examine the percentage of web pages per website that include the Optimizely JavaScript library in Figure We observe that most websites of in our sample include Optimizely on all of their pages We hypothesize that this could be the result of templated web design The remaining websites in our sample either include Optimizely on a few of their pages of websites include Optimizely in of their pages or most of their pages of websites include Optimizely in of their pages The former observation suggests that there may be false negatives in our dataset ie websites that use Optimizely on a small number of pages that our crawler happened to miss The latter observation is unsurprising since even templated website designs sometimes have exceptional pages that do not follow the design language eg terms of use and privacy policies Note that just because a website includes the Optimizely library does not necessary mean it is actively running experiments with Optimizely We examine the distribution of experiments in Audiences We now turn our attention to the audiences that website operators have defined for their experiments We observe that of websites in our sample defined audiences for their experiments Note that this does not necessarily mean the remaining Longitude La ti tu de Pe rc en ta ge o eb si te s Figure Countries targeted for audiences The targeted countries are in descending order of website usage US UK Australia France Germany Canada etc with of websites targeting US users Percentage of websites ch no lo gy in de x UK Figure World location usage versus technology index Positive correlation is found r websites were not running experiments recall that Optimizely allows simple A/B/n experiments to be run with random assignment of users ie no manually defined audiences are required Figure shows the distribution of audiences per website We observe that most websites in our sample of have defined audiences This suggests that the dominant use case for predefined audiences on Optimizely is experimentation rather than fine-grained audience segmentation Websites wishing to microtarget users eg for advertising and recommendation are better off using machine learning tools that automatically infer user segments rather than manually defining complex conditions using Optimizely predefined audiences That said we do observe four websites that each defined a large number of audiences Optimizely audiences The New York Times AirAsia and CREDO Mobile We examine two of these in-depth in Recall that audiences are defined using one or more attributes Figure presents the percentage and count of websites in our sample that have defined at least one audience with specific attributes The eight most popular attributes are in descending order of website usage device custom JavaScript cookies location browser IP address language and platform That a majority of websites in our sample of are leveraging custom functions to define audiences suggests a high level of sophistication by these operators We now investigate audience segmentation by device browser and location Figure shows the percentage and count of websites targeting different types of devices We observe that operators are primarily interested in separating desktop and mobile users and relatively less interested in subdividing smartphone and tablet users We hypothesize that iPhone and iPad users are frequently segmented because it is easier to identify them as classes the hardware is more homogeneous versus the Android device ecosystem which is heavily fragmented across manufacturers Alternatively website operators may view Apple users as ie as a proxy for affluence and thus segment them for specialized targeting The website that uses device targeting most heavily is a company that sells these technology products which has seven audiences of devices Figure presents the popularity of targeting different browsers were we make two interesting observations First it is surprising that Internet Explorer is the most frequently targeted browser We suspect that operators are using Optimizely to selectively apply compatibility patches to their websites for people using the defunct Internet Explorer browser We manually inspected the two websites Longitude La ti tu de Pe rc en ta ge o eb si te s Figure US states targeted for audiences The targeted states are in descending order of website usage California Texas New York Illinois New Jersey Connecticut Washington etc with of websites targeting Californians Percentage of websites Te ch no lo gy in de x CA NY Figure US location usage versus technology index Positive correlation is found r targeting browser users and found they are also doing so for compatibility reasons Second it is surprising that Safari is more frequently targeted than Chrome since Chrome is the most popular web browser Unlike Internet Explorer Safari is a modern and standards-compliant browser so compatibility patching is unlikely to be the reason why it is targeted Instead Safari may be highly targeted for the same reason as iPhone and iPad users ie as a means to segment the valuable Apple-user population We observe that segmenting audiences by location is also popular Optimizely allows a website operator to segment users at different levels of geographic granularity including continents countries states and cities We first examine how the audiences in our sample are distributed at the country-level As shown in Figure audiences are primarily localized within the following countries in descending order of popularity US UK Australia France Germany Canada etc with of of websites in our sample targeting a US audience As one possible explanation for why specific countries are more targeted than others we cross reference our audience distribution with the technology index of each country from The World Bank which is a measure of geographic technological adoption The resulting scatterplot is shown in Figure We find a significant and positive correlation between the technology index and audience distribution Pearson r which supports our hypothesis that operators may focus on localized audiences from technologically developed countries Next we focus on how audiences are segmented within the US since it is the most popular country in our dataset As shown in Figure the most popular states are in descending order California Texas New York Illinois New Jersey Connecticut Washington etc with of websites in our sample segmenting Californians Similar to above we cross reference this distribution with the technology index of each state from the State Technology and Science Index The scatterplot is shown in Figure and again we find a significant and positive correlation between technology index and audience distribution Pearson r Websites that heavily target locations include with audiences and Teacher Certification Map with audiences Experiments Next we shift focus to the experiments per se that are being run by websites Of the websites that used Optimizely in our sample we observed of them running experiments The remaining websites may just be using Optimizely for analytics purposes or they may have had a lull in experiments at the time of our crawls Experiment count Pe rc en ta ge o eb si te s Figure Distribution of experiments per website of websites are running experiments Nine websites are running experiments with the max being by Optimizely itself Audience count Pe rc en ta ge o e er im en Figure Distribution of audiences per experiment of of experiments have no audiences which corresponds to all site visitors of have a single audience Figure shows the number of experiments per website We observe that the majority of websites in our sample of are running experiments This is unsurprising experiments take time to plan develop and evaluate Although the largest tech companies like Google Facebook and Microsoft run thousands of experiments per year it is unreasonable to expect smaller companies to match their pace We do observe nine websites running experiments each Four are the same as the websites with the most audiences groups Optimizely experiments The New York Times AirAsia and CREDO mobile Each of them runs a large number of experiments corresponding to their many audiences One notable exception is Teespring which we observe running experiments with only audience groups Most of of Teesprings experiments are running without any audiences meaning all site visitors are eligible to be experimental subjects Next we investigate the number of audiences per experiment As shown in Figure of experiments in our sample target no audiences which means all site visitors are eligible to be experimental subjects Note that these experiments may still have multiple variations in which case users would be randomly divided across them of experiments have a single audience which corresponds to a two variation A/B test where the B group is the audience members and the A group is all other site visitors The remaining experiments of include multiple audiences The experiments with the most complex audience specifications totally in our sample are mostly personalizations from AirAsia of and The New York Times of Finally we investigate variations per experiment ie the number of different treatments shown to subjects in each experiment As shown in Figure of of experiments in our sample have a single variation which corresponds to experiments were all site visitors experience the same treatment These may represent the final stage of successful experiments eg the operator determined that a specific treatment in an A/B/n experiments was most effective and they now apply it to all site visitors of have two variations which may correspond to A/B tests The remaining experiments of have more variations which may correspond to multivariate tests The experiments with the most variations totally in our sample are run by the websites iCrackedVapeWorld Shutterstock and the Department of Motor Vehicles DMV Variation count Pe rc en ta ge o e er im en Figure Distribution of variations per experiment of of experiments have a single variation which corresponds to experiments of are two variation A/B tests Partisan bias score Pe rc en ta ge o eb si te s Figure Partisan bias scores of websites using Optimizely of websites are scored The distribution is normal DAgostino test p and centered around T-test p CASE STUDIES As we observe in Optimizely is used across many popular websites that receive millions of visitors The operators of these websites frequently segment users into audiences and conduct thousands of experiments on users without their knowledge or explicit consent Although it is likely that many of these experiments are utterly banal and completely benign eg simple changes to the layout and styling of the website Optimizely is a powerful tool that can be used to implement arbitrary changes to a website Thus we must consider whether the experiments that operators are conducting in practice have the potential to harm participants Because the capabilities of Optimizelys tool are so general we cannot feasibly analyze the treatment effects of experiments at scale Instead in this section we present case studies that highlight specific types of experiments we have observed in our data along with potential risks associated with these designs We divide our case studies into three areas political content price discrimination and advertising campaigns Political Content There are numerous examples demonstrating that the confluence of technology and politics can lead to undesirable consequences Eli Pariser famously pointed out that personalization algorithms could potentially lead to the formation of partisan that reinforce each persons preexisting political beliefs Two studies have demonstrated that partisan information shown in search results can impact the voting preferences of users Finally exposure to charged political content has been found to correlate with negative expressions of emotion from users suggesting that it may also impact their internal emotional state In our dataset we observe many overtly political websites running experiments using Optimizely To quantify this we adopt a mapping of websites to partisan bias scores developed by Robertson et al These scores are calculated based on the relative frequency that a given website is shared on Twitter by registered Democrats and Republicans A score of indicates that a given website is shared exclusively by Democrats Republicans Although there are other partisanship scoring metrics available such as Media Bias/Fact Check AllSides and several from prior papers we adopt the Robertson et al mapping because it connotations such as LGBTQ rights eg Human Rights Campaign abortion eg Planned Parenthood and environmentalism eg Sierra Club on the left and conservativism eg The Heritage Foundation and religion eg Compassion in Jesus Name on the right Several left-leaning news sources eg The New York Times and CNN also use Optimizely All of these websites may be able to shape visitors political beliefs through experimentation Party Bias score Examples Left Human Rights Campaign wwwhrcorg Sierra Club wwwsierracluborg Planned Parenthood wwwplannedparenthoodorg The New York Times wwwnytimescom Springer wwwspringercom edX wwwedxorg CNN wwwcnncom Flipboard flipboardcom ABC News abcnewsgocom Right History wwwhistorycom Legacy wwwlegacycom ABC Shows abcgocom ESPN wwwespncom AE wwwaetvcom United Service Organizations wwwusoorg PGA Championship wwwpgacom Pack Bags wwwsixpackbagscom Safelite AutoGlass wwwsafelitecom The Heritage Foundation wwwheritageorg Heritage Action heritageactioncom Compassion in Jesus Name wwwcompassioncom covers the largest number of unique websites Using this dataset we are able to score of of the websites in our sample Table shows examples of scored websites Not all scored websites using Optimizely are overtly political for example links to ESPN just happens to be shared more frequently on Twitter by registered Republicans possibly reflecting a different set of cultural interests than registered Democrats These non-political websites tend to have partisan bias scores in the range In contrast websites with extreme bias scores or are overtly political such as Human Rights Campaign Planned Parenthood and Sierra Club on the left and The Heritage Foundation and Compassion in Jesus Name on the right These websites together with major news sources eg The New York Times and CNN have the potential to influence visitors political opinions using OCEs We find that partisan bias scores for websites using Optimizely are normally distributed DAgostino normality test p and centered around mean no evidence for non-zero mean T-test p as shown in Figure This suggests that the use of Optimizely is relatively balanced across the political spectrum Next we discuss a specific class of experiments being run by websites in our sample to highlight potential ethical issues The New York Times The New York Times NYT is a newspaper with a slight left lean partisan bias score that uses Optimizely heavily running experiments with defined audiences in our sample We observe that the NYT frequently conducts randomized A/B/n tests for headlines of news stories on their homepage For example at pm on August the NYT experimented with two headlines for a story about the Pope Condemns Atrocities of Abuse in Letter to and Letter to Billion Catholics Pope Cites Sex Abuse These experiments come and go rapidly this example was gone by pm and all visitors were shown the latter headline The practice of A/B/n testing headlines encapsulates competing priorities between the business and editorial sides of the newsroom On one hand headlines that receive more clicks generate more advertising revenue On the other hand the headline may not be the most informative or nuanced from a reporting perspective The rise of headlines vividly illustrates this tension Depending on what metric is being optimized for A/B/n testing headlines may actually prove detrimental to the long Robertson et al report that their scores have high correlation with the other scoring systems mentioned here term credibility of the newsroom by privileging inflammatory decontextualized or highly partisan headlines Another issue with A/B/n testing headlines is that it complicates the idea of mass media as a shared frame for the public to contextualize events Prior work demonstrates that people frequently do not read news articles preferring instead to scan the headlines Further studies have shown that even when people do read an article the framing of the headline shapes their opinion of the whole story Combined these two effects suggest that even when readers engage with a single news outlet they may walk away with widely diverging understandings of events To be clear we do not observe the NYT experimenting with click-bait headlines in our data nor are we accusing the NYT of intentionally crafting partisan headlines Our goal with this case study is simply to highlight that A/B/n testing news headlines may have unintended consequences and that the tools for conducting these experiments are widely available and accessible to non-technical website operators Unscrupulous news outlets could easily test unethical headlines and users would be unaware Even scrupulous news outlets may inadvertently fan the flames of partisanship if they uncritically allow the results of A/B/n tests to determine the framing of their news headlines Price Discrimination Another area of concern associated with personalization is online price discrimination This refers to the practice of offering different prices to different customers for the same product In most cases price discrimination is not illegal to economists it is actually desirable as it allows businesses to capture additional value from consumer surplus However consumers have shown discomfort with online price discrimination when the practice is nontransparent ie people feel cheated when they discover that others had access to better prices than they did Several studies have identified price discrimination across a broad swath of e-commerce websites To identify websites in our sample that are potentially conducting price discrimination experiments we searched for the keywords and in the experimental treatments In total we observed websites with experiments containing at least one of these keywords Next we delve into several specific websites that implement price discrimination through Optimizely experiments OCEs involving price discrimination usually contains at least one of these keywords however containing these keywords does not necessarily indicate price discrimination Therefore the statistics we report should be treated as upper bounds that we observe running experiments with defined audiences in our dataset A group of its experiments called Life Landing Page targets audiences based on Urchin Tracking Module UTM parameters which are parameters passed by Googles analytics and advertising services Audiences with different UTMs are shown different insurance advertisements including several of the form Life Insurance As Low As X Per where X can be or Other treatments use text of the form term life insurance quotes and save up to and and apply for term life insurance ie with and without discount offers It is possible that some of the treatments being tested by PolicyGenius are merely marketing differences rather than price discrimination For example maybe all customers can receive discounts but only some customers are shown advertisements highlighting the deal However even in this case there may be adverse impact on customers those who are unaware of the possibility for discounts may be less likely to claim them and thus end up paying more The pricing treatments are a clearer case of price discrimination eg offering the same insurance policy for and per month but without deeper investigation it is unclear whether this particular instance is unethical For example the higher-priced policies may be targeted to senior citizens However prior investigations have found systematic racial discrimination in insurance and ridesharing markets linked to geolocation If the audience segments used by PolicyGenius are localized or include other sensitive demographic characteristics this price discrimination would be similarly troubling VapeWorld VapeWorld is an e-commerce website that sells vaporizers to adults We observe it running experiments with defined audiences in our dataset A group of its experiments offer lower prices on specific merchandise to audiences in California or from within the US This case is similar to geolocation-based online price discrimination that was discovered on Staples that privileged customers from affluent urban areas Advertising Campaigns Numerous studies have documented the potential for discrimination in online advertising This may occur due to exclusion eg not showing ads for jobs or housing to specific populations or through association eg showing criminal background check ads tied to the names of African American individuals Online advertising-related experiments are the single most common treatment we observe in our dataset We identified experiments related to advertising by searching for a list of domains gathered from prior work known to be involved in tracking users and serving ads eg doubleclicknet in all experimental treatments in our dataset We find that of websites in our sample are conducting experiments that involve injecting ad and tracking-related resources into their web pages Figure shows the distribution of advertising companies per website in our sample We observe that most websites of Unfortunately UTM parameters are opaque IDs therefore we cannot deduce what audience segments are being targeted Advertisement company count Pe rc en ta ge o eb si te s Figure Distribution of advertising companies per website Percentage of websites Adition Celtra Inspectlet A FairFax Clickable TRUSTe Adform Outbrain CrazyEgg Demandbase Mixpanel Clicktale Ad ve is em en t co m pa ny observed in Optimizely experiments are only experimenting with a single company while a quarter of are experimenting with two or three Figure shows the top advertising companies that appear in Optimizely experiments The advertising companies used by less than four websites are not shown Across all websites in our sample we observe unique advertisers out of in our full list of advertisers None of the companies shown in Figure are massive players in the industry as compared to Google Facebook etc which suggests that websites in our sample are experimenting with alternative or highly specialized monetization strategies possibly to supplement their primary sources of ad revenue Optimizely We observe that Optimizely itself is running experiments with defined audiences in our sample of of these experiments are using advertising services from Demandbase These experiments target audiences from specific companies including Petco Adidas Target Centers for Medicare Medicaid Services etc Each treatment places a different banner picture on the homepage that is specifically designed to attract users from each company eg the banner picture for Petco is a dog while for Centers for Medicare Medicaid Services it is a picture of doctor/patient communication etc To the best of our knowledge none of these organizations were customers of Optimizely at the time of our data collection Rather it appears that Demandbase works with Optimizely to identify visitors to their website that demonstrate an interest in buying the service Optimizely then crafts custom branded homepages for these potential customers While the experiments we highlight on Optimizely are not obviously unethical they do exemplify just how highly targeted modern online advertising can be It is easy to envision contexts where such micro-targeting could be creepy eg on a health care website or ethically problematic eg on mortgage company website CNN CNN is a televised news service that we observe running experiments with audiences in our dataset In these experiments users with different cookies are shown ads from two different Content Recommendation Networks CRNs Outbrain and Taboola One possible explanation for this behavior is that CNN is evaluating the revenue it earns from each CRN before deciding on a permanent partner Another possibility is that CNN directs each user towards the CRN that has the most extensive tracking profile on that individual since this would maximize revenue per impression more specific targeting typically yields higher profits Prior work has pointed out that Outbrain and Taboola both have a history of acting unethically by failing to prominently disclose paid advertising In this study we leverage Optimizely as a lens to present the first large-scale observational study of OCEs in-the-wild Of the Alexa Top-M domains we found websites that included Optimizelys JavaScript library The majority of these websites were conducting experiments on audiences at the time of our crawls but we observe a small number of websites with extensive suites of experiments We also analyze the most common attributes used to target audiences such as by geolocation device browser etc We delve into the specific treatments we observe in our dataset through a series of three case studies These case studies highlight problematic classes of experiments eg news headline optimization and price discrimination that may raise ethical concerns since they have the potential to harm experiment subjects That said it bears repeating that we do not observe any websites engaging in overtly unethical behavior in our dataset The intent of our case studies is not to shame bad actors Rather the point is to highlight realworld use cases for A/B testing tools like Optimizely and facilitate a discussion about the social consequences of these experiments Towards Transparency and Consent We observe that Optimizely is used by many extremely popular websites However to our knowledge visitors to these sites are never asked to explicitly consent to these experiments Even the existence of experiments is rarely if ever disclosed For example the NYT does not mention Optimizely or OCEs in their privacy policy or terms of use An explicit goal of our work is to raise awareness among the public about the existence of OCEs and push website operators towards greater transparency We argue that transparency is one of the best defenses against harm to users as exemplified by laws like the General Data Protection Regulation Additionally companies should follow the same norms as academia and obtain informed consent before running substantive experiments Alternatively in cases where pre-disclosure may confound an experiment eg by priming users users should be debriefed after the fact Tool providers like Optimizely could encourage these best practices by asking their users to self-certify that they are obeying ethical norms Any operator found to not be following these practices could be cut-off from Optimizelys service Experimental Ethics for Practitioners Optimizelys tools and others like it are designed to be accessible to a broad range of people eg web designers and marketers On one hand it is nice to see tools that democratize powerful capabilities beyond software engineers On the other hand with power comes responsibility There is never a guarantee that the people who use Optimizely or similar tools will have received training in the ethics of experimentation As tools like Optimizely make experimentation accessible the likelihood of situations where untrained or careless operators conduct experiments that harm subjects increases It may be incumbent on tool providers like Optimizely to provide ethics training to their userbase For example Optimizely could require new users to go through online training modules that introduce basic concepts like beneficence justice respect for persons As of August they do mention headline tests in a blog post and informed consent This would integrate seamlessly with Optimizelys existing training modules and synergize nicely with self-certification of compliance with best practices Limitations Our work has two major limitations First although Optimizely has the largest share of the OCE market it is not used by large platforms like Google or Facebook These services are visited by billions of users every day and they are running experiments which unfortunately we cannot analyze Second we only examined the use of Optimizelys built-in audiences However of websites in our sample are targeting customized audiences which are opaque to us These customized audiences could potentially target individuals using sensitive attributes or inferred from third-party data brokers which may also raise ethical concerns Future Work We hope that future work will extend our study by auditing other A/B testing platforms eg Google Optimize conducting more focused analysis on the experimental treatments being implemented by specific website verticals eg ecommerce and surveying website operators to understand if and how they grapple with ethical issues in OCEs