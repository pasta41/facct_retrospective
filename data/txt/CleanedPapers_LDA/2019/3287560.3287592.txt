An Empirical Study of Rich Subgroup Fairness for Machine Learning Kearns Neel Roth and Wu ICML recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness Rich subgroup fairness picks a statistical fairness constraint say equalizing false positive rates across protected groups but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension They give an algorithm guaranteed to learn subject to this constraint under the condition that it has access to oracles for perfectly learning absent a fairness constraint In this paper we undertake an extensive empirical evaluation of the algorithm of Kearns et al On four real datasets for which fairness is a concern we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles measure the tradeoffs between fairness and accuracy and compare this approach with the recent algorithm of Agarwal Beygelzeimer Dudik Langford and Wallach ICML which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes We find that in general the Kearns et al algorithm converges quickly large gains in fairness can be obtained with mild costs to accuracy and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al algorithm Overall we find this algorithm to be effective on real data and rich subgroup fairness to be a viable notion in practice CONCEPTS Computing methodologies Machine learning KEYWORDS Algorithmic Bias Subgroup Fairness Fairness Auditing Fair Classification INTRODUCTION The most common definitions of fairness in machine learning are statistical in nature They proceed by fixing a small number of protected subgroups such as racial or gender groups and then ask that some statistic of interest be approximately equalized across groups Standard choices for these statistics include positive classification rates false positive or false negative rates and positive predictive value see for more examples These definitions are pervasive in large part because they are easy to check although there are interesting computational challenges in learning subject to these constraints in the worst case see eg Unfortunately these statistical definitions are not very meaningful to individuals because they are constraints only over averages taken over large populations they promise essentially nothing about how an individual person will be treated Dwork et al enumerate a catalogue of evils which show how definitions of this sort can fail to provide meaningful guarantees Kearns et al identify a particularly troubling failure of standard statistical definitions of fairness which can arise naturally without malicious intent called fairness gerrymandering They illustrate the idea with the following toy example shown in Figure described as follows Suppose individuals each have two sensitive attributes race say blue and green and gender say male and female Suppose that these two attributes are distributed independently and uniformly at random and are uncorrelated with a binary label that is also distributed uniformly at random If we view gender and race as defining classes of people that we wish to protect we could take a standard statistical fairness definition from the literature say the equal odds condition of which asks to equalize false positive rates across protected groups and instantiate it with the four protected groups Men Women blue people and green people The following classifier satisfies this condition although only by Figure Fairness Gerrymandering A Toy Example cheating and packing its unfairness into structured subgroups of the protected populations it labels a person as positive only if they are a blue man or a green woman This equalizes false positive rates across the four specified groups but of course not over the finer-grained subgroups defined by the intersections of the two protected attributes Kearns et al also proposed an approach to the problem of fairness gerrymandering rather than asking for statistical definitions of fairness that hold over a small number of coarsely defined groups ask for them to hold over a combinatorially or infinitely large collection of subgroups defined by a set of functions G of the protected attributes Hébert-Johnson et al independently made a similar proposal For example we could ask to equalize false positive rates across every subgroup that can be defined as the intersection or conjunction of d protected attributes for which there are d such groups Kearns et al showed that as long as the class of functions defining these subgroups has bounded VC dimension the statistical learning problem of finding the best distribution over classifiers in subject to the constraint of equalizing the positive classification rate the false positive rate or the false negative rate over every subgroup defined over G is solvable whenever the dataset size is sufficiently large relative to the VC dimension of G Taking inspiration from the technique of Agarwal et al they were able to show that even with combinatorially many subgroup fairness constraints the computational problem of learning the optimal fair classifier is once again solvable efficiently whenever the learner has access to a black-box classifier oracle which can solve the unconstrained learning problems over G and respectively Similarly given access to an oracle for G they were able to efficiently solve the problem of auditing for rich subgroup fairness finding the G that corresponds to the subgroup for whom the statistical fairness constraint was most violated While the work of Kearns et al is satisfying from a theocratical point of view it leaves open a number of pressing empirical questions For example their theory is built for an idealized setting with perfect learning oracles in practice heuristic oracles may fail Moreover perhaps rich subgroup fairness is asking for too much in practice maybe enforcing combinatorially many constraints leads to an untenable tradeoff with error Finally perhaps enforcing combinatorially many constraints is not necessary perhaps on real data it is enough to call upon the algorithm of for enforcing statistical fairness constraints on the small number of groups defined by the marginal protected attributes and rich subgroup fairness will follow incidentally Put another way Is the so-called fairness gerrymandering problem only a theoretical curiosity or does it arise organically when standard classifiers are optimized subject to marginal statistical fairness constraints In this paper we conduct an extensive set of experiments to answer these questions We study the algorithm from instantiated with fast heuristic learning oracles when used to train a linear classifier subject to approximately equalizing false positive rates across a rich set of subgroups defined by linear threshold functions On four real datasets we characterize The basic convergence properties of the algorithm although this algorithm has provable guarantees when instantiated with learning oracles for G when these oracles are necessarily replaced with heuristics the guarantees of the algorithm become heuristic as well We find that the algorithm typically converges Subsection and provides a controllable trade-off between fairness and accuracy despite its heuristic guarantees Subsection We visualize the optimization trajectory of the algorithm Subsection and discrimination heatmaps showing the evolution of the subgroup discrimination of the algorithm over time Subsection The trade-off between subgroup fairness and accuracy We find that for each dataset there are appealing compromises between error and subgroup fairness Thus achieving rich subgroup fairness may be possible in practice without a severe loss in predictive accuracy Subsection The subgroup unfairness that can result when one applies more standard approaches that either ignore fairness constraints all together or equalize false positive rates only across a small number of subgroups defined by individual protected attributes By auditing the models produced by these standard approaches with the rich subgroup auditor of we find that often subgroup fairness constraints are violated even by algorithms which are explicitly equalizing false positive rates across the groups defined on the marginal protected attributes In light of these findings we submit that rich subgroup fairness constraints are both important and can be satisfied at reasonable cost both in terms of computation and in terms of accuracy We hope that algorithms like that of which can be used to satisfy rich subgroup fairness become part of the standard toolkit for fair machine learning Further Related Work While Kearns at al propose and study rich sub-group fairness for false positive and negative constraints Hébert-Johnson et al study the analogous notion for calibration constraints which they call multi-calibration Kim et al extend this style of analysis to accuracy constraints asking that a classifier be equally accurate on a combinatorially large collection of subgroups Kim et al also extend it to metric fairness constraints converting the individual metric fairness constraint of into a statistical constraint that asks that on average individuals in combinatorially many subgroups should be treated differently only in proportion to the average difference between individuals in the subgroups as measured with respect to some similarity metric DEFINITIONS We begin with some definitions following the notation in We study the classification of individuals defined by a tuple x x y where x X denotes a vector of protected attributes x X denotes a vector of unprotected attributes and y denotes a label We will write X x x to denote the joint feature vector We assume that points X y are drawn iid from an unknown distribution P Let D be a binary classifier and let D X denote the possibly randomized classification induced by D on individual X y We will be concerned with learning and auditing classifiers D satisfying a common statistical fairness constraint equality of false positive rates also known as equal opportunity The techniques in Agarwal et al and Kearns et al also apply equally well to equality of false negative rates and equality of classification rates also known as statistical parity Each fairness constraint is defined with respect to a set of protected groups We define sets of protected groups via a family of indicator functions G for those groups defined over protected attributes Each X G has the semantics that indicates that an individual with protected features x is in group We now formally define false positive subgroup fairness Definition False Positive Subgroup Fairness Fix any classifier D distribution P collection of group indicators G and parameter For each G define P Pr P y P FPD where FPD D X y and D X y denote the overall false-positive rate of D and the false-positive rate of D on group respectively We say D satisfies -False Positive FP Fairness with respect to P and G if for every G P P We will sometimes refer to FPD FP-base rate Since we do not consider other measures in this paper we refer to this notion as simply subgroup fairness Given a fixed subgroup G we will refer to the quantity P P as the subgroup fairness or alternately the -unfairness of The notion of subgroup fairness imposes a statistical constraint on combinatorially many groups definable by the protected attributes This is in contrast to more common statistical fairness definitions defined on coarse groups definable by a single protected attribute Given a protected attribute xi and a value for that attribute a define or more generally to any fairness constraint that can be expressed as a linear equality on the conditional moments E t X y D X X y where X y is an event defined with respect to X y and t X Equality of false positive rate is a particular instantiation of this kind of constraint where is the event y and t D X the x xi a denoting the set of individuals who have that particular value of their protected attribute In contrast to subgroup fairness we refer to a classifier D as marginally fair if it satisfies false positive subgroup fairness with respect to the functions for each protected attribute xi and realization a If the algorithm D fails to satisfy the -subgroup fairness condition then we say that D is -unfair with respect to P and G We call any subgroup which witnesses this unfairness a -unfair certificate for DP An auditing algorithm for a notion of fairness is given sample access to points from the underlying distribution as well as the classification outcomes provided by D It will either deem D to be fair with respect to P or else produces a certificate of unfairness The algorithms of Agarwal et al and Kearns et al studied in this paper both assume access to oracles which can solve cost-sensitive classification CSC problems Formally an instance of a CSC problem for the class is given by a set of n tuples Xi c n i such that corresponds to the cost for predicting label on point Xi Given such an instance as input a CSC oracle finds a hypothesis that minimizes the total cost across all points argmin n i Following both and Kearns et al in all of the experiments in this paper we take the classes and G to be linear threshold functions and we use a linear regression heuristic for both auditing and learning The heuristic finds a linear threshold function as follows Train two linear regression models r r to predict c and c respectively Given a new point x predict the cost of classifying x as and using our regression models these are r x and r x respectively Output the prediction y corresponding to lower predicted cost y argmin i ri x We leave the precise descriptions of the algorithm from which we will refer to as the SUBGROUP algorithm to the appendix We refer the reader to for details about its derivation and guarantees At this point we remark only that the algorithm operates by expressing the optimization problem to be solved minimize error subject to subgroup fairness constraints as solving for the equilibrium in a two player zero-sum game between a Learner and an Auditor The Learner has the set of hypothesis as its action pure strategy space and the Auditor has the set of subgroups G as its action space The best response problem for the Auditor corresponds to the auditing problem finding the subgroup G for which the strategy of the learner violates the fairness constraints the most The best response problem for the Learner corresponds actually give two algorithms one of which employs no-regret learning techniques and converges in a polynomial number of rounds but is randomized and the other of which is known to converge only in the limit but is conjectured to converge quickly and is deterministic We focus on the deterministic algorithm in this paper because it is more amenable to implementation despite its weaker theoretical guarantees We find that it performs well in practice despite its weaker theory to solving a weighted but unconstrained empirical risk minimization problem The best response problem for both players can be expressed as solving a cost sensitive classification problem The algorithm SUBGROUP essentially simulates the fictitious play of this game which proceeds over rounds and in each round t both players best respond to their opponents empirical history of play Learner plays in that minimizes objective function balancing error and unfairness on subgroups found by Auditor so far Auditor finds subgroup in G on which the uniform distribution over violates -fairness the most This can be done efficiently assuming access to oracles which solve the cost sensitive classification problem over G respectively EMPIRICAL EVALUATION In this section we describe an extensive empirical investigation of the SUBGROUP algorithm on four datasets in which fairness is a potential concern Among the questions of primary interest are the following Does the SUBGROUP algorithm work in practice despite the use of imperfect heuristics for the Learner and Auditor Is the notion of subgroup fairness interesting empirically in that there are palatable trade-offs between accuracy and subgroup fairness as opposed to it being too strong a constraint and thus resulting in a very steep error increase for even weak subgroup fairness We will answer these questions strongly in the affirmative which is perhaps the overarching message of our results We also carefully compare subgroup fairness to standard marginal fairness and show that optimizing for the latter in general does poorly on the former thus something like the SUBGROUP algorithm is actually necessary to achieve subgroup fairness More generally aside from performance we provide a number of empirical analyses that elucidate the underlying behavior and convergence properties of the SUBGROUP algorithm and discuss its strengths and weaknesses Datasets We ran experiments on datasets from the UCI Machine Learning Repository Communities and Crime Adult and Student and the Law School dataset from the Law School Admission Councils National Longitudinal Bar Passage Study These datasets were selected due to their potential fairness concerns including Data points representing individual people or in the case of Communities and Crimes small US communities of people The presence of features capturing properties often associated with possible discrimination including race gender and age Potential sensitivity of the predictions being made such as violent crime income or performance in school The properties of these datasets are summarized in Table including the number of instances the prediction being made the overall number of features which varies from to the number of protected features in the subgroup class which varies from to the nature of the protected features and the baseline majority class error rate Some methodological notes We note that two of the datasets Law School and Adult were initially much larger but were extremely imbalanced with respect to the predicted label making sensible error comparisons numerically difficult We thus randomly downsampled these two datasets to obtain approximately balanced prediction problems on each All categorical variables have been preprocessed with a one-hot encoding The SUBGROUP algorithm has two input parameters the maximum allowed subgroup fairness violation and a tuning parameter C which represents in the theoretical derivation in an upper bound on the magnitude of the dual variables needed to express the fairness constrained empirical risk minimization problem We view as an important control variable allowing us to explore the tradeoff between fairness and accuracy and thus will vary it in our experiments On the other is more of a nuisance parameter and thus for consistency and simplicity we set C in all experiments Experimentation with larger values of C did not reveal qualitatively different findings on the datasets investigated We emphasize that all results are reported in-sample on the datasets and thus we are treating the empirical distributions of the datasets as the true distributions of interest We do this because our primary interest is simply in examining the performance and behavior of the SUBGROUP algorithm on the actual data or distributions and not in generalization per se As noted in theoretical generalization bounds for both error and subgroup fairness can be obtained by standard methods and will depend on eg the VC dimension of the Learners model and the Auditors subgroup class G As usual we would expect empirical generalization to often be considerably better than the worst-case theory Empirical Convergence of SUBGROUP We begin with an examination of the convergence properties of the SUBGROUP algorithm on the four datasets Kearns et al had already reported preliminary convergence results for the Communities and Crime dataset showing that their algorithm converges quickly and that varying the provides an appealing trade-off between error and fairness In addition to replicating those findings for Communities and Crime we also find that they are not an optimistic anomaly For example for the Law School dataset in Figure we plot both the error panel a and the fairness violation panel b as a function of the iteration t for values of the input ranging from to We see that the algorithm converges relatively quickly on the order of thousands of iterations and that increasing the input generally yields decreasing error and increasing fairness violation typically saturating the input as suggested by the idealized theory But on other datasets the empirical convergence does not match the idealized theory as cleanly presumably due to the use of imperfect Learner and Auditor heuristics In panels c and d of Figure Dataset Size Prediction Features Protected Protected Feature Types Baseline Communities and Crime High Violent Crime Race Law School Pass Bar Exam Race Income Age Gender Student Course Performance Age Gender Relationship Alcohol Use Adult Income Age Race Gender Table Description of Data Sets a b c d Figure Error and fairness violation for Law School dataset panels a and b and Adult data set panels c and d for values of input ranging from to Dashed horizontal lines on plots correspond to varying values of we again plot and but now for the Adult dataset Even after approximately iterations the algorithm does not appear to have converged with still showing long-term oscillatory behavior exhibiting extremely noisy dynamics especially at smaller input values and there being no clear systematic monotonic relationship between the input and error acheived But despite this departure from the theory it remains the case that varying still yields a diverse set of pairs as we will see in the next section In this sense even in the absence of convergence the algorithm can be viewed as a valuable search tool for models trading off accuracy and fairness Overall we found rather similar convergent behavior on the Communities and Crime and Law School datasets and less convergent behavior on the Adult and Student datasets Subgroup Pareto Frontiers and Comparison to Marginal Fairness Regardless of convergence for plots such as those in Figure it is natural to take the pairs across all t and all input and compute the undominated or Pareto frontier of these pairs This frontier represents the accuracy-fairness tradeoff achieved by the SUBGROUP algorithm on a given data set which is arguably its most important output The choice of where one wants to be on the frontier is a policy question that should be made by domain experts and stakeholders and dependent on the stakes involved eg online advertising vs criminal sentencing It is also of interest to compare the subgroup fairness achieved by the SUBGROUP algorithm which is explicitly optimizing under a subgroup fairness constraint with an algorithm only optimizing under weaker and more traditional marginal fairness constraints To this end we also implemented a version of the algorithm from a b c d e g Figure Left column The red points show the Pareto frontier of error x axis and subgroup fairness violation y axis for the SUBGROUP algorithm across all four data sets while the blue points show the error and subgroup fairness violation for the models achieved by the MARGINAL algorithm Right column The error and marginal fairness violation for the MARGINAL algorithm across all four data sets Ordering of datasets is Communities and Crime Law School Adult and Student which we will refer to as the MARGINAL algorithm for marginal fairness From a theoretical perspective a priori we would expect models trained for marginal fairness to fare poorly on subgroup fairness But it is an empirical question perhaps on some datasets demanding marginal fairness already suffices to enforce subgroup fairness as well Thus the high-level question is whether the SUBGROUP framework and algorithm are worth the added analytical and computational overhead Since some of the protected attributes are continuous rather than discrete and the MARGINAL algorithm only handles discrete attributes in order to run the marginal fairness algorithm we create sensitive groups by thresholding on the mean of each sensitive attribute In the left column of Figure we show the SUBGROUP algorithm Pareto frontiers for subgroup fairness on all four datasets and also the pairs achieved by the MARGINAL algorithm In the right column we also separately show the marginal fairness frontier achieved by the MARGINAL algorithm Before discussing the particulars of each dataset we first make the following general observations For most datasets the SUBGROUP algorithm yields a Pareto curve that frequently lies well below the straight line connecting its endpoints which we can think of as an empirical form of strong convexity and thus there are non-trivial tradeoffs between accuracy and fairness to consider On some of these curves there are regions of steep descent where subgroup unfairness can be reduced significantly with negligible increase in error While the MARGINAL algorithm performs well with respect to marginal fairness right column as expected it fares much worse than the SUBGROUP algorithm on subgroup fairness for three of the datasets Thus marginal fairness is not just theoretically but also empirically a weaker notion and generally will not imply subgroup fairness for free Nevertheless there are a handful of points in which the MARGINAL algorithm produces models that actually lie below and thus dominate the SUBGROUP Pareto curve by a small amount While this is not possible under the idealized theory subgroup fairness is a strictly stronger notion than marginal fairness it can again be explained by the use of imperfect learning heuristics by both algorithms Focusing just on the MARGINAL marginal fairness curves in the right column we see that each of them begins with a steep drop meaning that in every case the marginal unfairness of the unconstrained error-optimal model can be significantly improved with little or no increase in error By matching points between the MARGINAL marginal and subgroup fairness plots we find that with the exception of the Student data set there is a systematic relationship between marginal and subgroup unfairness asking the MARGINAL algorithm to reduce marginal unfairness also causes it to reduce subgroup unfairness but not by as much as the SUBGROUP algorithm achieves Together these observations let us conclude that subgroup fairness is a strong but achievable notion in practice at least on these datasets and that the SUBGROUP algorithm appears to be an effective tool for its investigation It is also worth commenting on the differences across datasets and focusing not just on the qualitative shapes of the Pareto curves but their actual numerical specifics especially since in real applications these will matter to stakeholders For instance the actual range of error values spanned by the SUBGROUP Pareto curves ranges from nearly Communities and Crime to less than Student So perhaps for Communities and Crime the tradeoff is starker from an accuracy perspective We now provide some brief commentary on each dataset Communities and Crime panels a and b This is the dataset with perhaps the cleanest and most convex SUBGROUP Pareto curve with steep drops in subgroup unfairness possible for minimal error increase at the beginning In particular are able to reduce the initial -unfairness from to less than while only increasing the error from to This is a meaningful reduction in unfairness eg reducing a percent difference in false positive rate on a subgroup comprising of the population to a less than false positive rate disparity on a subgroup of the same size Eventually the Pareto curve flattens out resulting in increasing accuracy costs for reduced unfairness While the MARGINAL subgroup unfairness curve matches the SUBGROUP Pareto curve on the far left for all datasets since this corresponds to minimizing error unconstrained by any fairness notion the out-performance by SUBGROUP grows rapidly as we make stronger fairness demands Law School panels c and d Here the SUBGROUP Pareto curve appears to be approximately linear thus providing a constant tradeoff between accuracy and subgroup fairness Interestingly this is the one dataset in which asking for marginal fairness appears to also yield subgroup fairness for free as the MARGINAL curve lies very close to the SUBGROUP curve Since this dataset has the fewest number of features overall and the second fewest number of protected features one might be tempted to conjecture that when the number of protected features is small guaranteeing marginal fairness approximately guarantees rich subgroup fairness This claim is falsified by the fact that on the Adult dataset which has similar dimensionality see below there is a large gap between the SUBGROUP and MARGINAL subgroup fairness curves Adult panels e and Here we see a less smooth SUBGROUP curve possibly corresponding to the poorer convergence properties on this dataset mentioned earlier Nevertheless the numerical tradeoff exhibits regions of both steep inexpensive reduction in unfairness and flat costly reduction MARGINAL is again considerably worse when evaluated on subgroup fairness but still shows a systematic relationship to marginal fairness Student panels g and Similar to Adult a varied SUBGROUP curve with multiple tradeoff regimes This is also the lone dataset in which reducing marginal fairness appears to have no relationship to subgroup fairness while the MARGINAL marginal pareto curve in panel remains relatively smooth the subgroup fairness of the corresponding models in panel d is now not only worse than for SUBGROUP but shows no monotonicity SUBGROUP is able to decrease -unfairness to with only a increase in error while the MARGINAL algorithm only drives the subgroup unfairness to at its best with an over increase in error from the unconstrained classifier Having established the efficacy of subgroup fairness and the SUBGROUP algorithm on the four datasets we now turn to experiments and visualizations allowing us to better understand the behavior and dynamics of the algorithm Flattening the Discrimination Surface Recall that in the various analyses and plots above we rely on the Auditor of SUBGROUP to detect unfairness This Auditor is in turn a heuristic relying on an optimization procedure without any theoretical guarantees which could potentially fail in practice This means that while any detected unfairness is a lower bound on the true subgroup unfairness it could be the case that the heuristic Auditor is simply failing to detect a larger disparity and that the models learned by SUBGROUP look more fair than they really are We explore this possibility on the Communities Crime dataset by implementing a brute force Auditor that runs alongside the SUBGROUP algorithm To make brute force auditing computationally tractable we designate only two attributes as protected a b c d e g Figure Evolution of discrimination surface for the SUBGROUP algorithm from t Each point in the plane corresponds to a different subgroup over two protected attributes and the corresponding z value is the current false positive discrepancy for the subgroup pctwhite and pctblack the percentage of each community that consists of white and black people respectively While the SUBGROUP algorithm uses the same heuristic Auditor it always does at each round we also perform a brute force audit as follows Subgroups are defined by a linear threshold function over the sensitive attributes eg We discretize in increments of and for the subgroup defined by each in the discretization we compute the -unfairness Hence at each round we can take the current classifier of the Learner and plot for each group the point Note that in addition to making brute force auditing tractable restricting to two dimensions permits direct visualization of discrimination In Figure we show a sequence of discrimination surfaces for the SUBGROUP algorithm over the protected features with input The x y axes are the coefficients of corresponding to whitepct and blackpct respectively and the z-axis is the -unfairness of the corresponding subgroup This is our first non-heuristic view of -unfairness and also shows us the entire surface of -unfairness rather than just the most violated subgroup Note that perfect subgroup fairness would correspond to an entirely flat discrimination surface at z We observe first that the unconstrained classifier in t panel a shows a very systematic bias along the lines of our sensitive attributes In particular groups with whitepct and blackpct eg communities with large numbers of white residents and relatively fewer black residents have a much higher false positive rate for being classified as violent Conversely majority black communities are less likely to be incorrectly labeled as violent The mean -unfairness base rate community rate for whitepct blackpct communities is whereas the mean for whitepct blackpct groups is The maximum -unfairness in t is and of the subgroups have -unfairness Recall that this corresponds to eg a disparity of the false positive rate from the base rate for groups as large as of the population We are thus far from perfect subgroup fairness As the algorithm proceeds we see this discrimination flip by t panel b into a regime with a higher false positive rate for predominantly black communities and then revert again by t Over the early iterations these oscillations continue growing less drastic as the -unfairness surface starts to flatten out noticeably by t panel g In panel we plot t and see that the surface has almost completely flattened with maximum -unfairness below So over the course of the first iterations of SUBGROUP weve reduced the -unfairness from over in most of the subgroups to less than in every subgroup Recall again that this corresponds to false positive rate disparities of at most in subgroups that represent of the population a reduction from false positive rate disparities of many similarly sized subgroups This represents an order of magnitude improvement that results from using the classifier learned by SUBGROUP Understanding the Dynamics We conclude by examining the dynamics of the SUBGROUP algorithm on the Communities and Crime dataset in greater detail More specifically since the algorithm is formulated as a game between a Learner who at each iteration t is trying to minimize the error and an Auditor who is trying to minimize subgroup unfairness we visualize the trajectories traced in space as t increases The plots in Figure correspond to such trajectories for input values of and panels a b c and d respectively which are denoted by the dashed lines on the axis of each figure The and values correspond to small and intermediate regimes whereas is close to but slightly below the subgroup unfairness of the unconstrained classifier The trajectories are color coded from colder to warmer colors according to their iteration number to give a sense of speed of convergence The first plot in all four trajectories corresponds to the of the unconstrained classifier Furthermore as long as the current values remain above the horizontal dashed line representing the input the trajectories remain identical as the same subgroups are being presented to the learner in each trajectory But when falls below a given input that trajectory will follow its own path going forward We first observe that the dynamics exhibit a fair amount of complexity and subtlety They all begin with low error and large unfairness and quickly follow a brief but large increase in as fairness starts to be enforced There are steps in which both and increase and a large early loop in trajectory space is observed But the first three trajectories panels a b and c corresponding to the three smaller values of quickly settle near the input line at which point begins a long oscillatory border war around this line as the Learner tries to minimize error but is pushed back below the line by the Auditor anytime -fairness is violated The idealized theory predicts that each trajectory should end at the input line subgroup fairness constraint saturated and with larger input weaker fairness constraint resulting in lower error The empirical trajectories indeed conform nicely to the theory with the final red points near the dashed lines and further left for larger Panel d corresponding to a much larger input diverges much earlier from the other three on its second step and early on sees unfairness driven far below the specified value The dynamics then see a slow gradual decrease of error and increase of unfairness back to the input value with the trajectory ending up near where it began but just slightly more fair as specified by CONCLUSIONS In this work we have established the empirical efficacy of the notion of rich subgroup fairness and the algorithm of on four fairness-sensitive datasets and the necessity of explicitly enforcing subgroup as opposed to only marginal fairness There are a number of interesting directions for further experimental work we plan to pursue including Experiments with richer Learner model classes while keeping the Auditor subgroup class G relatively simple and fixed One conjecture is that by making the hypothesis space richer more appealing Pareto curves may be achieved There is also some rationale for keeping G simple since we would like to have some intuitive interpretation of what the subgroups represent while the same constraint may not hold a b c d Figure trajectories for Communities and Crime for Implementation and experimentation with the no-regret algorithm of which may have superior convergence and other properties due to its stronger theoretical guarantees Experiments on the generalization performance of subgroup fairness in the form of test-set Pareto curves While as mentioned standard VC theory can be applied to obtain worst-case bounds one might expect even better empirical generalization