Algorithmic Recourse from Counterfactual Explanations to Interventions As machine learning is increasingly used to inform consequential decision-making eg pre-trial bail and loan approval it becomes important to explain how the system arrived at its decision and also suggest actions to achieve a favorable decision Counterfactual explanations how the world would have had to be different for a desirable outcome to occur aim to satisfy these criteria Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings However it has largely been overlooked that ultimately one of the main objectives is to allow people to act rather than just understand In laymans terms counterfactual explanations inform an individual where they need to get to but not how to get there In this work we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse Instead we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions shifting the focus from explanations to interventions CONCEPTS Computing methodologies Causal reasoning and diagnostics Human-centered computing KEYWORDS algorithmic recourse explainable artificial intelligence causal inference counterfactual explanations contrastive explanations consequential recommendations minimal interventions INTRODUCTION Predictive models are being increasingly used to support consequential decision-making in a number of contexts eg denying a loan rejecting a job applicant or prescribing life-altering medication As a result there is mounting social and legal pressure to provide Figure Illustration of an example causal generative process governing the world showing both the graphical model G and the structural causal model m In this example X represents an individuals annual salary X is bank balance and Y is the output of a fixed deterministic predictor predicting the eligibility of an individual to receive a loan explanations that help the affected individuals to understand why a prediction was output as well as how to act to obtain a desired outcome Answering these questions for the different stakeholders involved is one of the main goals of explainable machine learning In this context several works have proposed to explain a models predictions of an affected individual using counterfactual explanations which are defined as statements of how the world would have had to be different for a desirable outcome to occur Of specific importance are nearest counterfactual explanations presented as the most similar instances to the feature vector describing the individual that result in the desired prediction from the model A closely related term is algorithmic recourse the actions required for or the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios which is argued as the underwriting factor for temporally extended agency and trust Counterfactual explanations have shown promise for practitioners and regulators to validate a model on metrics such as fairness and robustness However in their raw form such explanations do not seem to fulfill one of the primary objectives of explanations as a means to help a data-subject act rather than merely understand The translation of counterfactual explanations to recourse actions ie to a recommendable set of actions to help an individual to achieve a favourable outcome was first explored in where additional feasibility constraints were imposed to support the concept of action able features eg prevent asking the individual to reduce their age or change their race While a step in the right direction this work and others that followed implicitly assume that the set of actions resulting in the desired output would directly follow from the counterfactual explanation This arises from the assumption that what would have had to be in the past retrodiction not only translates to what should be in the future prediction but also to what should be done in the future recommendation We challenge this assumption and attribute the shortcoming of existing approaches to their lack of consideration for real-world properties specifically the causal relationships governing the world in which actions will be performed For ease of exposition we present the following examples see for additional examples Example Consider for example the setting in Figure where an individual has been denied a loan and seeks an explanation and recommendation on how to proceed This individual has an annual salary X of and an account balance X of and the predictor grants a loan based on the binary output of Existing approaches may identify nearest counterfactual explanations as another individual with an annual salary of or a bank balance of therefore encouraging the individual to reapply when either of these conditions are met On the other hand bearing in mind that actions take place in a world where home-seekers save of their salary ie X B X U a salary increase of only to would automatically result in additional savings with a net positive effect on the loan-granting algorithms decision Example Consider now another setting of Figure where an agricultural team wishes to increase the yield of their rice paddy While many factors influence yield temperature solar radiation water supply seed quality the primary action able capacity of the team is their choice of paddy location Importantly the altitude at which the paddy sits has an effect on other variables For example the laws of physics may imply that a increase in elevation results in a C decrease in temperature on average Therefore it is conceivable that a counterfactual explanation suggesting an increase in elevation for optimal yield without consideration for downstream effects of the elevation increase on other variables may actually result in the prediction not changing The two examples above illustrate the pitfalls of generating recourse actions directly from counterfactual explanations without consideration for the structure of the world in which the actions will be performed Actions derived directly from counterfactual explanations may ask too much effort from the individual Example or may not even result in the desired output Example In this paper we remedy this situation via a fundamental reformulation of the recourse problem where we rely on causal reasoning to incorporate knowledge of causal dependencies into the process of recommending recourse actions that if acted upon would result in a counterfactual instance that favourably changes the output of the predictive model In more detail we first provide a causal analysis to illuminate the intrinsic limitations of the setting in which actions directly follow counterfactual explanations Importantly we show that even when equipped with knowledge of causal dependencies after-the-fact the actions derived from pre-computed nearest counterfactual explanations may prove sub-optimal or directly unfeasible Second to address the above limitations we emphasize that from a causal perspective actions correspond to interventions which not only model the change in the intervened-upon variable but also the downstream effects of this intervention on the rest of the non-intervened-upon variables This insight allows us to propose a recourse through minimal interventions problem whose solution informs stakeholders on how to act in addition to understand We complement this result with a commentary on the form of interventions and with a more general definition of feasibility beyond action ability Finally we provide a detailed discussion on both the importance and the practical limitations of incorporating causal reasoning in the formulation of recourse ALGORITHMIC RECOURSE VIA COUNTERFACTUAL EXPLANATIONS Counterfactual explanations CFE are statements of how the world would have had to be different for a desirable outcome to occur In the context of explainable machine learning the literature has focused on finding nearest counterfactual explanations ie instances which result in the desired prediction while incurring the smallest change to the individuals feature vector as measured by a context-dependent dissimilarity metric X X R This problem has been formulated as the following optimization problem cfe argmin st P where X is the factual instance cfe X is a perhaps not unique nearest counterfactual instance is the fixed binary predictor and P is an optional set of plausibility constraints eg the counterfactual instance be from a relatively high-density region of the input space Most of the existing approaches in the counterfactual explanations literature have focused on providing solutions to the optimization problem in by exploring semantically meaningful distance/dissimilarity functions between individuals eg percentile-shift accommodating different predictive models eg random forest multilayer perceptron and realistic plausibility constraints P In particular solve using gradient-based optimization employ mixed-integer linear program solvers to support mixed numeric/binary data use graph-based shortest path algorithms use a heuristic search procedure by growing spheres around the factual instance build on genetic algorithms for model-agnostic behavior and solve using satisfiability solvers with closeness guarantees Although nearest counterfactual explanations provide an understanding of the most similar set of features that result in the desired prediction they stop short of giving explicit recommendations on how to act to realize this set of features The lack of specification of the actions required to realize cfe from leads to uncertainty and limited agency for the individual seeking recourse To shift the focus from explaining a decision to providing recommendable actions to achieve recourse Ustun et al reformulated as argmin cost st cfe cfe cfe P where cost X X R is a user-specified cost that encodes preferences between feasible actions from and and P are optional sets of feasibility and plausibility constraints restricting the actions and the resulting counterfactual explanation A counterfactual instance can be from the dataset or generated Here feasible means possible to do whereas plausible means possibly true believable or realistic Optimization terminology refers to both as feasibility sets respectively The feasibility constraints in as introduced in aim at restricting the set of features that the individual may act upon For instance recommendations should not ask individuals to change their gender or reduce their age Henceforth we refer to the optimization problem in as the CFE-based recourse problem A CAUSAL PERSPECTIVE OF ALGORITHMIC RECOURSE The seemingly innocent reformulation of the counterfactual explanation problem in as a recourse problem in is founded on two assumptions Assumption the feature-wise difference between factual and nearest counterfactual instances cfe directly translates to the minimal action set ACFE such that performing the actions in ACFE starting from will result in cfe and Assumption there is a -mapping between and cost whereby larger actions incur larger distance and higher cost Unfortunately these assumptions only hold in restrictive settings rendering the solution of sub-optimal or infeasible in many real-world scenarios Specifically Assumption holds only if i the individual applies effort in a world where changing a variable does not have downstream other variables ie features are independent from each other or if ii the individual changes the value of a subset of variables while simultaneously enforcing that the value of all other variables remain unchanged ie breaking dependencies between features Beyond the sub-optimality that arises from assuming/reducing to an independent world in i and disregarding the feasibility of non-altering actions in ii non-altering actions may naturally incur a cost which is not captured in the current definition of cost and hence Assumption does not hold either Therefore except in trivial cases where the model designer actively inputs pair-wise independent features to generating recommendations from counterfactual explanations in this manner ie ignoring the dependencies between features warrants reconsideration Next we formalize these shortcomings using causal reasoning Actions as Interventions Let M denote the structural causal model SCM capturing all inter-variable causal dependencies in the real world M is characterized by the endogenous observed variables X X the exogenous variables U U and a sequence of structural equations U X describing how endogenous variables can be deterministically obtained from the exogenous variables Often M is illustrated using a directed graphical model G see eg Figure From a causal perspective actions may be carried out via structural interventions A which can be thought of as a transformation between SCMs A set of interventions can be constructed as A B where contains the indices of the subset of endogenous variables to be intervened upon In this case for each the do-operator replaces the structural equation for the endogenous variable in with B Correspondingly graph surgery is performed on G severing graph edges incident on an intervened variable Thus performing the actions A in a world M yields the post-intervention world X U X B U X B X U M X X U X B X B X U M X X B U X B M X X X B X B M Figure Given world model M intervening on X and/or on X result in different post-intervention models M corresponds to interventions only on X with consequential effects on X M shows the result of structural interventions only on X which in turn dismisses ancestral effects on this variable and M is the resulting independent world model after intervening on both variables ie the type of interventions generally assumed in the CFE-based recourse problem model mA with structural equations FA B Structural interventions are illustrated in Figure Structural interventions are used to predict the effect of actions on the world as a whole ie how M becomes MA In the context of recourse we aim to model the effect of actions on one individuals situation ie how becomes scf to ascertain whether or not the desirable outcome is achieved ie scf We compute individual-level effects using structural counterfactuals Assuming causal sufficiency of M ie no hidden confounders and full specification of an invertible such that FF X can be uniquely determined given the value and vice-versa Hence one can determine the distinct values of exogenous variables that give rise to a particular realization of the endogenous variables X as As a result we can compute any structural counterfactual query scf for an individual as scf FA In our context that is if an individual observed in world M performs the set of actions A what will be the resulting individuals feature vector scf Limitations of CFE-based recourse Next we use causal reasoning to formalize the limitations of the CFE-based recourse approach in To this end we first reinterpret the actions resulting from solving the CFE-based recourse problem ie as structural interventions by defining the set of indices of observed variables that are intervened upon We remark that given an individual seeking recourse may intervene on any arbitrary subset of observed variables as long as the intervention contains the variable indices for which Now we are in a position to define CFE-based actions as interventions ie For notational simplicity we interchangeably use sets and vectors eg X and X Queries such as this subsume both retrospective/subjunctive/counterfactual what would have been the value of and prospective/indicative/predictive what will be the value of conditionals as long as we assume that the laws governing the world are stationary Definition CFE-based actions Given an individual in world M the solution of and the set of indices of observed variables that are acted upon a CFE-based action refers to a set of structural interventions of the form ACFE B Using Definition we can derive the following key results that provide necessary and sufficient conditions for CFE-based actions to guarantee recourse Proposition A CFE-based action ACFE where performed by individual in general results in the structural counterfactual scf cfe and thus guarantees recourse ie scf if and only if the set of descendants of the acted upon variables determined by is the empty set Corollary If the true world M is independent ie all the observed features are root-nodes then CFE-based actions always guarantee recourse While the above results are formally proven in Appendix A we provide a sketch of the proof below If the intervened-upon variables do not have descendants then by definition scf cfe Otherwise the value of the descendants will depend on the counterfactual value of their parents leading to a structural counterfactual that does not resemble the nearest counterfactual explanation scf cfe and thus may not result in recourse Moreover in an independent world the set of descendants of all the variables is by definition the empty set Unfortunately the independent world assumption is not realistic as it requires all the features selected to train the predictive model to be independent of each other Moreover limiting changes to only those variables without descendants may unnecessarily limit the agency of the individual eg in Example restricting the individual to only changing bank balance without eg pursuing a new/side job to increase their income would be limiting Thus for a given non-independent M capturing the true causal dependencies between features CFE-based actions require the individual seeking recourse to enforce at least partially an independent post-intervention model m ACFE so that Assumption holds by intervening on all the observed variables for which as well as on their descendants even if their However such requirement suffers from two main issues First it conflicts with Assumption since holding the value of variables may still imply potentially infeasible and costly interventions in M to sever all the incoming edges to such variables and even then it may not change the prediction see Example Second as will be proven in the next section see also Example CFE-based actions may still be suboptimal as they do not benefit from the causal effect of actions towards changing the prediction Thus even when equipped with knowledge of causal dependencies recommending actions directly from counterfactual explanations in the manner of existing approaches is not satisfactory ALGORITHMIC RECOURSE VIA MINIMAL INTERVENTIONS In the previous section we learned that actions which immediately follow from counterfactual explanations may require unrealistic assumptions or alternatively result in sub-optimal or even infeasible recommendations To solve such limitations we rewrite the X X U U U U X B U X B U X B U X B X U M Y Figure The structural causal model graph and equations for the working example and demonstration in Section recourse problem so that instead of finding the minimal independent shift of features as in we seek the minimal cost set of actions in the form of structural interventions that results in a counterfactual instance yielding the favourable output from A argmin A costA st scf scf FA scf P A where A directly specifies the set of feasible actions to be performed for minimally costly recourse with cost X R and scf FA denotes the resulting structural counterfactual We recall that although scf is a counterfactual instance it does not need to correspond to the nearest counterfactual explanation cfe resulting from see eg Example Importantly using the formulation in it is now straightforward to show the suboptimality of CFE-based actions as shown next proof in Appendix A Proposition Given an individual observed in world M a family of feasible actions and the solution of A Assume that there exists CFE-based action ACFE that achieves recourse ie cfe Then costA cost ACFE Thus for a known causal model capturing the dependencies among observed variables and a family of feasible interventions the optimization problem in yields Recourse through Minimal Interventions MINT Generating minimal interventions through solving requires that we be able to compute the structural counterfactual scf of the individual in world M given any feasible action A To this end we consider that the SCMM falls in the class of additive noise models so that we can deterministically compute the counterfactual scf FA by performing the Abduction-Action-Prediction steps proposed by Pearl et al Working example Consider the model in Figure where are mutually independent exogenous variables and are structural linear or nonlinear equations Let be the observed features belonging to the factual individual for whom we seek a counterfactual explanation and recommendation Also let denote the set of indices corresponding to the subset of endogenous variables that are intervened upon according to the action set A Then we obtain a structural counterfactual scf FA by applying the Abduction-Action-Prediction steps as follows Step Abduction uniquely determines the value of all exogenous variables given evidence Step Action modifies the SCM according to the hypothetical interventions B where yielding FA X B U X B U X B U X B X U where denotes the Iverson bracket Step Prediction recursively determines the values of all endogenous variables based on the computed exogenous variables from Step and FA from Step as SCF B SCF B SCF B SCF SCF SCF B SCF General assignment formulation As we have not made any restricting assumptions about the structural equations only that we operate with additive noise models where noise variables are pairwise independent the solution for the working example naturally generalizes to SCMs corresponding to other DAGs with more variables The assignment of structural counterfactual values can generally be written as scf pa scf In words the counterfactual value of the feature SCF takes the value if such feature is intervened upon ie Otherwise SCF is computed as a function of both the factual and counterfactual values of its parents denoted respectively by and pa scf The closed-form expression in can replace the counterfactual constraint in ie scf FA after which the optimization problem may be solved by building on existing frameworks for generating nearest counterfactual explanations including gradient-based evolutionary-based heuristics-based or verification-based approaches as referenced in Section While out of scope of the current work for the demonstrative examples below We remark that the presented formulation also holds for more general SCMs for example where the exogenous variable contribution is not additive as long as the sequence of structural equations is invertible ie there exists a sequence of equations such that FF in other words the exogenous variables are uniquely identifiable via the abduction step we extended the open-source code of MACE we will submit a pull-request to the respective repository Demonstration We showcase our proposed formulation by comparing the actions recommended by existing nearest counterfactual explanation methods as in to the ones generated by the proposed minimal intervention formulation in We recall that prior literature has focused on generating counterfactual explanations or CFE-based actions which as shown above lack optimally or feasibility guarantees in non-independent worlds Thus to the best of our knowledge there exists no baseline approach in the literature that guarantees algorithmic recourse The experiments below serve as an illustration of the sub-optimality of existing approaches relative to our proposed formulation of recourse via minimal intervention Section presents a detailed discussion on practical considerations We consider two settings i a synthetic setting where M follows Figure and ii a real-world setting based on the german credit dataset where M follows Figure We computed the cost of actions as the norm over normalized feature changes to make effort comparable across features ie cost where is the range of feature For the synthetic setting we generate data following the model in Figure where we assume X B U X B X U with U Poisson and U N and the predictive model X Given solving our formulation identifies the optimal action set A B which results in scf FA whereas solving previous formulations yields resulting in cfe Importantly while scf appears to be at a further distance from compared to cfe achieving the former is less costly than the latter specifically cost costA As a real-world setting we consider a subset of the features in the german credit dataset The setup is depicted in Figure where X is the individuals gender treated as immutable X is the individuals age action able but can only increase X is credit given by the bank action able X is the repayment duration of the credit non-action able but mutable and Y is the predicted customer risk according to logistic regression or decision tree We learn the structural equations by fitting a linear regression model to the child-parent tuples We will release the data and the code used to learn models and structural equations Given the setup above for instance for the individual Male identified as a risky customer solving our formulation yields the optimal action set A B X B which results in scf FA Male whereas solving yields N/A resulting in cfe Male Similar to the toy setting we observe a decrease in effort required of the individual when using the action by our method since our cost function states that waiting for six years to get the credit approved is more costly than applying the following year for a lower credit amount We extend our analysis to a population level and observe that for negatively affected test individuals previous approaches suggest actions that are on average and more costly than our approach when considering respectively a logistic regression and a decision tree as the predictive model The demonstrations above confirm our theoretical analysis that MINT-based actions from are less costly and thus more beneficial for affected individuals than existing CFE-based actions from that fail to utilize the causal relations between variables TOWARDS REALISTIC INTERVENTIONS In Section we formulated algorithmic recourse by considering the causal relations between features in the real world Our formulation minimized the cost of actions which were carried out as structural interventions on the corresponding graph Each intervention proceeds by unconditionally severing all edges incident on the intervened node fixing the post-manipulation distribution of a single variable to one deterministic value While intuitive appealing and powerful structural interventions are in many ways the simplest type of interventions and their simplicity comes at a price foregoing the possibility of modeling many situations realisitically Below we extend and to add flexibility and realism to the types of interventions performed by the individual Notably there is nothing inherent to an SCM that a priori determines the form feasibility or scope of intervention instead these choices are delegated to the individual and are made based on a semantic understanding of the modeled variables On the Form of Interventions The demonstrations in Section primarily focused on actions performed as structural aka hard interventions where all incoming edges to the intervened node are severed see Hard interventions are particularly useful for Randomized Control Trial RCT settings where one aims to evaluate isolate the causal effect of an action eg effect of aspirin on patients with migraine on the population by randomly assigning individuals to treatment/control groups removing the influence of other factors eg age In the context of algorithmic recourse however an individual performs actions in the real world and therefore must play the rules governing the world In earlier sections these rules captured in an SCM guided the search for an optimal set of actions by modelling actions along with their consequences The rules also determine the form of an intervention eg specifying whether an intervention cancels out or complements existing causal relations For instance consider Example where an individual chooses to increase their bank balance eg through borrowing money from family ie a deliberate action/intervention on X while continuing to put aside a portion of their income ie retaining the relation X B X U Indeed it would be unwise for a recommendation to suggest abandoning saving habits In such a scenario the action would be carried out as an additive aka soft intervention Such interventions do not sever graphical edges incident on the intervened node and continue to allow for parents of the node to affect that node Conversely in Example recourse recommendations may suggest performing a structural intervention on temperature eg by creating a climate controlled green-house to cancel the natural effect of altitude change on temperature The previous examples illustrate a scenario where an individual/agriculture team actually have the agency to choose which type of intervention to perform However it is easy to conceive of examples where such an option does not exist For instance as part of a medical systems recommendation we might consider adding mg/l of insulin to a patient with diabetes with a certain blood insulin level This action cannot disable pre-existing mechanisms regulating blood insulin levels and therefore the action can only be performed additively Conversely one may also consider another example from the medical domain whereby the only treatment of malignancy may be through a surgical structural amputation Just as structural interventions were supported in our framework via a closed-form expression see additive interventions can be encoded through an analogous assignment formulation scf pa scf The choice of whether interventions should be applied in a additive/soft or structural/hard manner depends on the variable semantic and should be decided prior to solving On the Feasibility of Interventions We saw in Section that earlier works motivated the addition of feasibility constraints as a means to provide more action able recommendations for the individual seeking recourse There the action ability aka mutability of a feature was determined based on the feature semantic and value in the factual instance marking those features which the individual has/lacks the agency to change eg bank balance vs race While the interchangeable use of definition holds under an independent world it fails when operating in most real-world settings governed by a set of causal dependencies We study this subtlety below In an independent world any change to variable could come about only via an intervention on itself Therefore immutable and non-action able variables overlap In a dependent world however changes to variable may arise from an intervention on or through changes to any of the ancestors of In this more general setting we can tease apart the definition of action ability and mutability and distinguish between three types of variables i immutable and hence non-action able eg race ii mutable but non-action able eg credit score and iii action able and hence mutable eg bank balance Each type requires special consideration which we show can be intuitively encoded as constraints amended to A from Immutable We posit that the set of immutable and hence non-action able variables should be closed under ancestral relationships given by the model m This condition parallels the ancestral closure of protected attributions in This would ensure that under no circumstance would an intervention on an ancestor of an immutable variable change the immutable variable Therefore for an immutable variable the constraint recursively necessitates the fulfillment of additional constraints in For instance the immutability of race triggers the immutability of birthplace Mutable but non-action able To encode the conditions for mutable but non-action able variables we note that while a variable See eg may not be directly action able it may still change as a result of changes to its parents For example the financial credit score in Figure may change as a result of interventions to salary or savings but is not itself directly intervenable Therefore for a non-action able but mutable variable the constraint is sufficient and does not induce any other constraints action able In the most general sense the action able feasibility of an intervention on may be contingent on a number of conditions as follows a the pre-intervention value of the intervened variable ie b the pre-intervention value of other variables ie c the post-intervention value of the intervened variable ie SCF and d the post-intervention value of other variables ie SCF Such feasibility conditions can easily be encoded into consider the following scenarios a an individuals age can only increase ie SCF age age b an individual cannot apply for credit on a temporary visa ie visa PERMANENT SCF credit TRUE c an individual may undergo heart surgery an additive intervention only if they wont remiss due to sustained smoking habits ie SCF heart REMISSION and d an individual may undergo heart surgery only after their blood pressure is regularized due to medicinal intervention ie SCF OK SCF heart SURGERY In summary while previous works on algorithmic recourse distinguished between action able conditionally action able and immutable variables we can now operate on a more realistic spectrum of variables ranging from conditionally soft/hard action able to non-action able but mutable and finally to immutable and non-action able variables Finally we remind that feasibility is a distinct notion from plausibility whereas the former restricts actions A to those that can be performed by the individual the latter determines the likeliness of the counterfactual instance scf FA P resulting from those actions For instance building on the earlier example although an individual with similar attributes and higher credit score may exist in the dataset ie plausible directly acting on credit score is not feasible On the Scope of Interventions One final assumption has been made throughout our discussion of actions as interventions which pertain to the one-to-one mapping between an action in the real world and an intervention on a endogenous variable in the structural causal model which in turn are also input features to the predictive model As exemplified in it is possible for some actions eg finding a higher-paying job to simultaneously intervene on multiple variables in the model eg income and length of employment Alternatively for Example choosing a new paddy location is equivalent to intervening jointly on several input features of the predictive model eg altitude radiation precipitation Such confounded/correlated interventions referred to as fat-hand/non-atomic interventions will be explored further in follow-up work by modelling the world at different causally consistent levels Ustun et al also support conditionally action able features eg age or educational degree with conditions derived only from as in a We generalize the set of conditions to support actions conditioned on the value of other variables as in b additive interventions in c and sequential interventions as in d DISCUSSION In this paper we have focused on the problem of algorithmic recourse ie the process by which an individual can change their situation to obtain a desired outcome from a machine learning model First using the tools from causal reasoning ie structural interventions and counterfactuals we have shown that in their current form counterfactual explanations only bring about agency for the individual to achieve recourse in unrealistic settings In other words counterfactual explanations do not translate to an optimal or feasible set of actions that would favourably change the prediction of if acted upon This shortcoming is primarily due to the lack of consideration of causal relations governing the world and thus the failure to model the downstream effect of actions in the predictions of the machine learning model In other words although counterfactual is a term from causal language we observed that existing approaches fall short in terms of taking causal reasoning into account when generating counterfactual explanations and the subsequent recourse actions Thus building on the statement by Wachter et al that counterfactual explanations do not rely on knowledge of the causal structure of the world it is perhaps more appropriate to refer to existing approaches as contrastive rather than counterfactual explanations To directly take causal consequences of actions into account we have proposed a fundamental reformulation of the recourse problem where actions are performed as interventions and we seek to minimize the cost of performing actions in a world governed by a set of physical laws captured in a structural causal model Our proposed formulation in complemented with several examples and a detailed discussion allows for recourse through minimal interventions MINT that when performed will result in a structural counterfactual that favourably changes the output of the model Next we discuss the work most closely related to ours the main limitation of the proposed recourse approach and propose future venues for research to address such shortcomings Relatedwork A number of authors have argued for the need to consider causal relations between variables generally based on the intuition that changing some variables may have effects on others In the original counterfactual explanations work Wachter et al also suggest that counterfactuals generated from an accurate causal model may ultimately be of use to experts eg to medical professionals trying to decide which intervention will move a patient out of an at-risk group Despite this general agreement to the best of our knowledge only two works have attempted to technically formulate this requirement In the first work Joshi et al study recourse in causal models under confounders and with predetermined treatment variables In this work a distribution over hidden confounders is first estimated along with a mapping from the attributes to hidden confounders ie Then under each intervention on treatment variables explanations are generated following with the plausibility term constraining the inverse of the counterfactual instance ie to the approximated confounding distribution In this work we instead optimize for recourse actions rather than counterfactual instances that result from those action In the second work Mahajan et al present a modified version of the distance function in amending the standard proximity loss between factual and counterfactual instances with a causal regularizer to encourage the counterfactual value of each endogenous variable to be close to the value of that variable had it been assigned via its structural equation Beyond the uncertainty regarding the strength of regularization which would mean causal relations may not be guaranteed and why the standard proximity loss only iterates over the exogenous variables which from a causal perspective are characteristics that are shared across counterfactual worlds footnote this approach suffers from a primary limitation in its causal treatment the causal regularizer would penalize any variable whose value deviated away from its structurally assigned value While on the surface this preservation of causal relations seems beneficial such an approach would discourage interventions additive or structural on non-root variables which would by design change the value of the intervened-upon variable away from its structurally assigned value Instead the regularizer would encourage interventions on variables that would not be penalized as such ie root variables which may not be contextually acceptable as root notes typically capture sensitive characteristic of the individual eg birthplace age gender The authors suggest in the Appendix of that one may consider those variables upon which structural interventions are to be performed as exogenous In this manner interventions would not be penalized and down-stream effects of interventions would still be preserved when searching for the nearest counterfactual instance We argue however that such an approach suffers from the same limitations as other CFE-based recourse approaches presented in Section in that a returned counterfactual instance would not imply feasible or optimal actions for recourse Finally without an explicit abduction step and without assumptions on the form of structural equations it is unclear how the authors infer and combine individual-specific characteristics as embedded in the background variables with the effect of ancestral changes to compute the counterfactual We believe the problems above will be mostly resolved when minimizing over the cost of actions instead of distance over counterfactuals as we have done in this work Practical limitations The primary limitation of our formulation in is its reliance on the true causal model of the world subsuming both the graph and the structural equations In practice the underlying causal model is rarely known which suggests that the counterfactual constraint in ie scf FA may not be deterministically identifiable We believe this is a valid criticism not just of our work but of any approach suggesting actions to be performed in the real world for consequential decision-making Importantly beyond recourse the community on algorithmic fairness has echoed the need for causal counterfactual analysis for fair predictions and have also voiced their concern about untestable assumptions when the true SCM is not available Perhaps more concerningly our work highlights the implicit causal assumptions made by existing approaches ie that of independence or feasible and cost-free interventions which may portray a false sense of recourse guarantees where one does not exists see Example and all of Section Our work aims to highlight existing imperfect assumptions and to offer an alternative formulation backed with proofs and demonstrations which would guarantee recourse if assumptions about the causal structure of the world were satisfied Future research on causal algorithmic recourse may benefit from the rich literature in causality that has developed methods to verify and perform inference under various assumptions Thus we consider further discussion on causal identifiability to be out of scope of this paper as it remains as an open and key question in the Ethical ML community This is not to say that counterfactual explanations should be abandoned altogether On the contrary we believe the counterfactual explanations hold promise for guided audit of the data and evaluating various desirable model properties such as robustness or fairness Besides this it has been shown that designers of interpretable machine learning systems use counterfactual explanations for predicting model behavior or uncovering inaccuracies in the data profile of individuals Complementing these offerings of counterfactual explanations we offer minimal interventions as a way to guarantee algorithmic recourse in general settings which is not implied by counterfactual explanations Future work In future work we aim to focus on overcoming the main assumption of our formulation the availability of the true world model M An immediate first step involves learning the true world model partially or fully and studying potential inefficiencies that may arise from partial or imperfect knowledge of the causal model governing the world Furthermore while additive noise models are a broadly used class of SCMs for modeling real-world systems further investigation into the effects of confounders non-independent noise variables the presence of only the causal graph as well as cyclic graphical models for time series data eg conditional interventions would extend the reach of algorithmic recourse to even broader settings In Section we presented feasibility constraints for a wide range of settings including dynamical settings in which one intervention enables the preconditions of another An interesting line of future research would involve combining the causal intervention-based recourse framework as presented in our work with multi-stage planning strategies such as to generate optimal sequential actions Finally the examples presented in relation to the form and feasibility of intervention serve only to illustrate the flexibility of our formulation in supporting a variety of real-world constraints They do not however aim to provide an authoritative definition of how to interpret variables and the context- and individual-dependent constraints for recourse as highlighted by other works Future cross-disciplinary research would benefit from accurately defining the variables and relationships and types of permissible interventions in consequential decision-making settings Relatedly future research would also benefit from a study of properties that cost functions should satisfy eg individual-based or population-based monotonicity as the primary means to measure the effort endured by the individual seeking recourse