Impossible Explanations Beyond explainable AI in the GDPR from a COVID- use case scenario Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework In this article we address this question analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID- patients The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters Based on this setting we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation GDPR Although it might appear that there is just one single form of explanation in the GDPR we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered Thus we propose to design explanations in multiple forms depending on the moment of the disclosure of the explanation either ex ante or ex post the audience of the explanation explanation for an expert or a data controller and explanation for the final data subject the layer of granularity such as general group-based or individual explanations the level of the risks of the automated decision regarding fundamental rights and freedoms Consequently explanations should embrace this multifaceted environment Furthermore we highlight how the current inability of complex deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact human-legible reasons behind specific decisions This makes the provision of satisfactorily fair and transparent explanations a serious challenge Therefore there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article GDPR Accordingly we suggest that further research should focus on alternative tools in the GDPR such as algorithmic impact assessments from Article GDPR or algorithmic lawfulness justifications that might be considered to complement the explanations of automated decision-making CONCEPTS Applied computing Law social and behavioral sciences Computing methodologies Machine learning KEYWORDS Explainability AI Machine Learning GDPR Black-Box Automated Decision-Making Data Protection Introduction Artificial intelligence AI has become increasingly important in many areas of society To ensure that its use will benefit society as a whole forms of proper regulation are being sought by a large majority of actors including institutions industry and human rights organizations AI-based automated decisionmaking systems are now deployed in large-scale digital and cyber-physical settings by digital companies government administration and public services Automated decision-making systems involving the processing of personal data of European Union EU citizens fall under the scope of the General Data Protection Regulation GDPR which introduced measures to protect fundamental rights of data subjects in the digital sphere Articles  and and recital of the GDPR as interpreted by the European Data Protection Board the EU agency deputed to inter alia  interpret data protection law promote a requirement of transparency for automated decision-making systems by establishing that data controllers using such systems need to provide meaningful safeguards such as meaningful information about the logics the significance and the envisaged effects of the algorithm but also a justification of outcomes in order to enable data subjects to understand and if deemed appropriate contest a decision having the opportunity to express their own view and have a human intervention in the decision Several legal scholars reasonably argue that a right to explanation can be inferred from those provisions This being said explainability and transparency are thus crucial legal requirements to ensure trust in AI systems raising the question of the implementation of such principles in systems involving AI components AI-based systems are indeed commonly depicted as efficient but opaque black-box systems exhibiting outstanding performances through complex mechanisms in the processing of large volumes of data The question of explainability of AI systems has been taken up by the scientific AI community and a collection of methodologies has emerged aiming at encouraging explainable and interpretable AI while preserving the same level of performances Despite significant advances explainable AI is still lacking a sound basis to evaluate its relevance from a legal perspective in particular with regard to the right to explanation derived from the GDPR It is obviously not sufficient to legally require to data controllers to provide explanations for AI-based decisions it is even more important to define what characteristics an explanation should fulfil in coherence with the kind of outcomes technical tools can provide To that respect understanding terminology is central to create a meaningful link between the scientific literature on explainable AI and the legal discussions on the right to explanation On a practical level this discussion needs to become interdisciplinary between the technological and legal dimensions This puts the technical terms of algorithmic AI explainability and interpretability into the focus and more especially their relationship to what should be legally required for an explanation in an algorithmic context Furthermore this discussion should not only reflect on the meaning of an explanation with regard to technical explainability or being valid in a legal context but ultimately on what constitutes a human understandable explanation in the most general terms The fundamental intention of this study is to explore to which extent current and future AI systems can provide adequate transparency and satisfactory explanations that would be admissible from a legal point of view Accordingly we propose a matrix of multidimensional explanation based on the audience of recipients on the moment of the explanation on the level of risk of the decision ie multi-layered explanation with increasing levels of granularity To this end we carry out this analysis on a use case scenario which is based on a high-risk automated decision-making process in the medical context The growing integration of AI in decision-making processes gains relevance in particular in light of the current COVID- pandemic Amongst other things this pandemic has exacerbated the limitations of health systems to handle exceptional high-stress situations in many places partially overwhelming the available capacity for intensive-care units ICUs while at the same time shortages in medical supplies have exposed medical staff to the disease This situation already has led to a surge of new literature proposing to automate some aspects of the medical decision-making process and in particular various forms of automated triage to determine admission to immediate intensive care We are precisely discussing such a use case scenario based on a deep learning based COVID- X-ray lung detection system used in the hypothetical but not unrealistic context of an automated emergency triage This paper is structured as follows In Sections and we introduce the legal and technical perspectives on explainability and legal explanations respectively Section presents our use case scenario on a COVID- X-ray detection together with an analysis both from technical and legal points of view We conclude with a discussion and outlook in Section exposing our doubts about the development of satisfactory explanation in a future AI-based environment suggesting the use of some possible alternative tools Overview legal perspective The GDPR became applicable in Europe in putting forward a modernised set of rules for the digital age It is of particular interest for AI as it introduces specific elements to tackle the growing adoption of AI in decision-making systems based on personal data and thus constitutes itself a first step of an emerging concept of AI governance Several of the provisions of the GDPR relate to this topic The recital of the regulation already foresees cases in which algorithms are used for profiling or to automate decisionmaking processes and it introduces and motivates the need to introduce safeguards for such processes These safeguards aim to protect against potential adverse consequences that profiling or automatic decision-making processes can have on data subjects The application of AI to personal data or more generically automatic processing is considered in the GDPR under different circumstances The first one is in profiling which is defined in Article as Any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person The second one is solely automated decision-making which is defined in as the ability to make decisions by technological means without human involvement This refers to the broader notion of the application of algorithms for the purposes of decision-making which may or may not involve some form of profiling in doing so Several examples on this topic can be found in Recital contextualises the set of data subject rights relevant to both profiling and automated decision-making that are developed in the several articles of the regulation including the right to information to obtain human intervention to express his/her point of view to obtain an explanation of the outcome of a decision and to challenge such decision Article and of the GDPR require that data subjects are informed about the existence of automated decision-making processes including but not limited to profiling Further the articles require that data controllers provide data subjects with information about the underlying mechanisms logics behind the automated decision-making performed and the significance and potential consequences of such processing The right of access Article also includes similar provisions granting data subjects the right to access their personal data and obtain such information about its processing from the data controllers The aforementioned articles refer respectively to Article where additional specific provisions on automated individual decision-making are introduced Data subjects have the right not to be subject to a decision exclusively based on automated processing if such decision affects him/her legally or significantly in any other way unless any of the exceptions foreseen in Paragraph applies necessary for a contract authorised by Union or Member State Law or based on data subject explicit consent Article emphasises the requirement to implement appropriate measures to safeguard the rights and freedoms of data subjects In those cases where automated decision-making takes place it does so by granting data subjects the right to obtain human intervention to express their point of view and to be able to contest the decision taken The guidelines released on this topic by the European Data Protection Board state that human intervention implies that the human-in-the-loop should refer to someone with the appropriate authority and capability to change the decision It is clear how a requirement of technical explainability is relevant for these envisaged safeguards Human supervision can only be effective if the person reviewing the process is in a position to assess the algorithmic processing carried out This implies that such processing should be understandable Further explainability is also key to ensure that data subjects are able to express their point of view and are capable of contesting the decision As it is stated in the European Data Protection Board guidelines data subjects will only be able to do that if they fully understand how the automated decision was made and on which bases Even further the European Data Protection Board provides in Annex a set of good practice recommendations for data controllers with respect to the several rights and provisions of the GDPR that are of relevance for profiling and automated decision making On top of the generic transparency requirements as commented in data controllers have the specific duty at Articles  to provide the data subjects with meaningful information about the logics the significance and the consequences of automated decision-making but also to adopt specific safeguards right to contest right to human intervention right to express ones view including the explanation of the decision reached recital Since the GDPR entered into force the academic community has debated about whether in actual fact this constitutes a new right to explanation Considering the different levels of risk that automated decisionmaking can imply for rights and freedoms of data subjects some scholars proposed a Multi-layered explanation model for low-risk scenarios a general layer of explanation about the algorithmic functioning might help inferred from Articles GDPR for medium-risk scenarios a group-based explanation of the algorithm ie an explanation on how the algorithm works for homogeneous groups of subjects might help this is inferred from the need to understand the decision before being able to exercise the right to contest a decision under Article GDPR for high-risk scenarios an individual explanation about the decision reached in a specific case might be necessary according to recital Overview AI research perspective While nowadays many automated decision systems are still based either on non-learning expert AI systems or on more standard predictive machine learning models this is quickly changing with the rapid uptake of deep neural networks and more availability of data The integration of AI models for algorithmic decision-making in every-day products and services has revived the question of how much the outputs of a given algorithm are understandable for a human or even fundamentally uniquely explainable This question is already crucial for a reliable assessment of a products security and safety when operating in real environments let alone to provide meaningful legal explanations With this ever-growing complexity of machine learning models and training data many discussions about explainability based on more classical approaches and standard tabular data sets might quickly become outdated To that end the development of the so-called field of explainable AI has recently gained a lot of traction in order to provide interpretable elements alongside with model results While this topic is not entirely new early AI research on expert systems in the s already raised questions about AI explainability discussions about explainable AI have significantly broadened from a growing literature of technical work on interpretable models and explainable AI to an ongoing discussion about the precise meaning and definition of explainability and interpretability to more procedural questions about the evaluation of existing frameworks or to input from social science about the meaning of explanation The meaning of what the academic literature refers to as explainability of an AI model is very different from the meaning of an explanation which is generally discussed in other social contexts see for the ongoing academic discussion Since precisely this difference of meaning is important to understand for an interdisciplinary discussion of legal and technical dimensions we first provide some background on the state of the art of the literature on explainable AI Explainability or interpretability In fact there is no single clear terminology agreed upon in the scientific literature to describe the notion of how well the results of a particular AI model can be explained The most generally used terminology is captured in the concepts of explainability and interpretability In most cases both notions are actually used interchangeably often but not always loosely understood as a variant of how well a human could understand the decisions of an autonomous algorithmic system Others remark that interpretability is mostly employed in the narrower context of machine learning models and thus conclude it to be a subset of the broader field of AI explainability In the review of it is pointed out that some authors define interpretability to be an AI model-centric notion whereas explainability would be a subject-centric notion In this sense the term explainability takes on a less technical meaning and would be of larger relevance to legal considerations We will leave this discussion to the literature and for all purposes equate both terms for the remainder of this article Interpretable models vs post-hoc approaches Interpretable models are fully or partially designed to provide reliable and easy-to-understand explanations of the prediction they output They are supposed to provide straightforward algorithmic elements which are considered to be interpretable from their very nature as being simple enough Some are simply a number of well-known statistical algorithms which only use a few parameters or are conceptually very simple Examples include linear and logistic regressions decision trees or generalized additive models such as a spline fit Other models are specifically designed to incorporate interpretable decisionmaking mechanisms for a given context The problem is that it stands to reason as to whether it is always possible to design an appropriate interpretable model to the desired accuracy The feasibility of this approach is highly debated especially in application cases where the most accurate solutions are usually provided by complex models such as deep neural networks Post-hoc techniques are used to extract explanations from a model which is usually considered as a black box either because internal parameters of the model are obfuscated or because it is too complex to be understood Many approaches from the literature of post-hoc interpretability aim to train an openly interpretable surrogate model on the basis of queries forwarded to the model Interpretability vs Accuracy The problem of designing interpretable models is part of a larger discussion in which it is debated whether a trade-off exists in ML model design between interpretability and accuracy Usually more complex models are employed in pursuit of higher accuracies or to achieve more complex tasks Making those models more interpretable in turn seems to almost inevitably come with a loss in these capabilities On the other hand the assumption that under given constraints better results can only be achieved with a more complex model can be challenged especially when good feature engineering is combined with simpler but robust models Yet from another angle the very notion of a complex and less interpretable model might depend on the point of view constraints or situation In any case the trade-off between interpretability and accuracy has been a recurrent topic in the machine learning community since its beginning Nowadays the scientific discussion is built on the requirements of two desirable properties of systems that are their effectiveness ie their ability to perform various tasks with fewer mistakes and their understandability of models ie their capacity to provide interpretable elements of the inner mechanisms involved in the processing of data The pursuit of these two objectives has actually been proven to be contradictory to some point as interpretable methods often require constraints that may limit their accuracy especially in the case of high-dimensional data The effects of this trade-off have been particularly strong in recent years with the advent of deep learning techniques that introduced models involving millions or even billions of parameters with little understanding of the logics involved in the processing of large amount of data Case study automated decision making in medical imaging To investigate in detail how contemporary technical discussions on AI explainability are aligned with the legal discussion on the right to explanation we present now a technical case study of a machine learning based automated decision systems in a realistic high-risk application namely the COVID- detection based on X-ray images Analysing medical images is a task that requires cognitive capabilities since no simple rule exists to make the distinction between a healthy lung and one that suffers from a pneumonia or even more involved to make the distinction between different types of pneumonia This case study highlights a bigger trend in machine learning to provide detection tools that not only achieve human performances but even go beyond and are able to capture weak patterns in complex images and assist medical doctors In the COVID- context the primary goal is first and foremost to replace the medical staff in order to ease the pressure that has been put on ICU services and while this target will likely not be achieved for the current pandemic it is however certain that these advances will trickle down to other medical applications with possible extensive use of such tools for the next health crises Description of the case study Medical imaging is relevant for its potential high impact on a data subjects rights and even well-being and is highlighted as an important example case in the Article Working Party guidelines on automated decision making in the GDPR The medical scenario is chosen to be of particular relevance with respect to the current COVID- pandemic where we discuss a near-future scenario of automated medical triage for classifying a symptomatic patient to likely suffer from a COVID induced pneumonia from X-ray images It should be highlighted that while nowadays medical triage is mostly not conducted in an automated fashion we regard such a scenario not as hypothetical as the current pandemic already has shown an uptake of discussion around such systems under adverse hospital conditions The objective of this case study is to highlight the complexity of providing relevant explanations in a medical context where the inherent complexity of data and models is a strong limitation to the provision of meaningful explanations This is due to the two following features first medical data are usually meaningless for a layman requiring a minimal set of medical knowledge Secondly decision-making systems are relying to extract patterns from these data on sophisticated machine learning tools whose behaviours are themselves opaque Use case design We consider a machine learning based automated system that performs a binary classification ie determines whether the data subject has the COVID- disease or not We make the distinction between two profiles of actors relevant for the scenario for which we showcase different degrees of explainability Data subject patient for our use case user without relevant technical nor domain knowledge They are either confronted with initial information about the automated processing of their data in a situation ex-ante to an actual decision made by the system eg to decide whether or not they want to allow the use of the automated decision system to gain a benefit such as faster or cheaper handling or just as a mandatory step to be properly informed or who contests a decision ex-post making active use of their rights as a data subject Expert medical staff for our use case user with a domain expertise and an understanding of the AI techniques who has to check the validity of the systems decision after eg the exercise of the right to contest or of the right to human intervention Explanatory factors should be made explicit to the user and presented with the right level of simplicity depending on the level of expertise The trade-offs between simplicity/efficacy and sobriety/fidelity in the field of algorithm explainability have already been discussed Here we assume that there are at least two different recipients for algorithmic explanation in the data protection field the data subject where simplicity and sobriety might be considered more important than efficacy of fidelity and the data controller/processor where efficacy and fidelity should prevail on simplicity and sobriety Non-expert users will appreciate a concise description of the system in simple non-technical terms with positive relationships between inputs of the system and the outputs The accuracy of the explanation can be reduced to the benefit of its clarity On the contrary experts are likely to prefer more detailed explanations revealing a comprehensive set of relationships between inputs and outputs integrating confidence score of the system Illustrations with examples extracted from an external dataset could also convey these elements We order the scenario into two use cases on explanations provided ex-ante and ex-post to an automated decision Use case Explanations provided ex-ante and confidence in the system In legal terms the ex-ante explanation in the GDPR corresponds to the right to receive meaningful information about the logic significance and envisaged effects of the automated decision as stated at Articles  g in the initial phase of a data processing involving automated decision-making In this case the user might be interested in the behaviour of the decision-making systems and wants to receive elements that will convince him to use it Use case Explanations provided ex-post and understanding a decision for a contestation The ex post explanation in the GDPR corresponds to the right to receive information about the automated decision reached either as an explanation of the decision or as information that enables the correct exercise of the right to contest the decision as arises from the combination of Article and recital This use case is ultimately related to the confidence users can have about a given individual decision made by the system Understanding a decision is important for various reasons for being able to contest a decision that could be or was harmful see indeed but also in providing elements to identify wrongdoing of the system or on the contrary confirm a decision Scenario description The case study relies on a fictitious yet plausible scenario that is given below A patient with a serious cough breathing problems and general physical discomfort but still conscious admits themself to the emergency ward of a hospital during a severe ongoing local outbreak of the COVID- pandemic The hospital staff is under significant pressure and the intensive care unit is already overwhelmed with severe patients After some initial tests and an X-ray of the chest the wards nurse tells the patient that tests could indicate a possibility of a COVID- infection Normally such patients would be admitted after decision of a medical doctor to the intermediate care ward since a pneumonia induced by COVID- is highly likely to cause severe and rapid worsening of the patients However no doctor is available for at least several hours Instead the patient could decide to let an automated decision system that has been exceptionally authorized decide from the available chest X-ray image whether or not he/she is likely suffering from COVID- We assume a few basic parameters for the scenario mostly so that they have significance from the legal perspective the situation does not initially involve an expert for the decision ie we assume a fully automated decision and not a system built to aid a physician to take a decision a human could be involved in the loop but it is either expensive to do that or it is only considered ex-post to check the validity after an eventual contestation the model has been through an auditing process that tested and approved it according to some unspecified protocol which only guarantees the model performance within certain measured parameters and not that the model is generally unerring For this case study we trained two toy convolutional neural networks CNN models to detect from an X-ray image whether or not the patient is suffering from the COVID- disease or not The first one is based on the COVID-NET architecture and is trained on a large dataset including multiple sources for X-ray images The second model leverages transfer learning to develop from a model trained on various datasets of X-ray images a detection system using a subset of the previously mentioned dataset that also includes information about patients such as sex or age These two models are developed solely to support the discussions on explainability of AI systems and does not intend to provide any additional information outside the scope of this paper The decision-making system returns a probability that a patient is infected based on an X-ray image of the chest From the system perspective the image is viewed as an array of pixels each pixel being encoded for black and white image as a numerical value between and the larger the value the brighter the pixel Because the number of pixels is very large in this application each image has a size of by pixels in total pixels each individual pixel is independently irrelevant contrary to the aggregation of pixels that forms possibly complex patterns that may be of interest for the detection CNN models are designed in such a way that at each layer of the network the presence and the intensity of specific patterns is scrutinized over all the image and based on this a score is derived As an illustration if the infection is characterised by the presence of dots on the lungs the model will learn to detect circular shapes and the intensity of this patterns aggregated all over the images will be directly connected to the value of the final detection score Ex-ante explanations In order to decide whether to admit themself to the automated system a patient is given an information sheet see below providing information about the system This information sheet contains the specifications of the systems including the information about the development of the system the general principle of the algorithm and the kind of data that is used in the processing Brief description of the system brought to the attention of the patient before examination The system is an automatic decision-making system that has been developed to assess the probability that a patient is infected by the SARS-CoV- virus by considering X-ray images of the chest The dataset used to develop the system is composed of images including images from negative patients and images from patients Additionally images have been used for testing purposes including images from negative patients and images from negative patients Principle the system is based on an AI system based on Convolutional Neural Networks It consists in automatically extracting the features that discriminate between X-ray images of the chest of healthy and infected patients Data The system takes as input the X-ray image of the chest in black and white No additional information is given to the system An example X-ray image is displayed below Figure Example of an X-Ray image of the chest that is being used by the system as an input Outcome a score of infectiousness between and If the confidence score is above the patient is detected POSITIVE to COVID- otherwise it is detected NEGATIVE Performances The system has achieved an accuracy of over all patients Ex-post explanations The patient takes the automated test and is detected as NEGATIVE with a score of The detection is associated with an information sheet providing elements on how the detection has been obtained This includes the performances of the system on other patients that are in the same situation detected as NEGATIVE that is intended to help the patient to get an idea of typical outcomes In addition the patient receives also group-based performances information based on sex and This might help the patient identifying possible misbehaviour specific to a certain population Personalized explanation of the decision taken by the system at the attention of the patient Results You received a score of infectiousness of indicating you are NEGATIVE to COVID- threshold Overall performances for patient in the same situation as you detected as POSITIVE the precision of the system is mean confidence score of Group-based performances for patients in the same group as you sex M and age the precision is confidence score of for negative patients and confidence score of for positive patients Based upon these elements and their own judgment the patient decides to contest the decision made by the system After a while medical staff is available to act as a human-in-the-loop to validate the decision Here we assume that the medical doctor only considers the outputs of the system to take the final decision without carrying out his own analysis The system provides a more detailed information sheet that is divided into two main sections First to understand the system and the decision made a medical doctor needs to assess his general confidence in the current system The general information provided by the software for medical staff contains elements such as the specifications and the overall performances of the system but with a greater level of details taking into account the expertise of the medical staff Another explanatory tool aimed at informed audiences returns the typical features that strongly activate one of the outcomes of the model These features are not X-ray images but are generated to reproduce the patterns that are encoded in the model parameters The second section contains information that is specific to the patient including feature maps such as occlusion maps A counterfactual image can also be generated from the X-ray image of the patient for which the prediction outcome is changed Specifications of the system at the attention of the medical staff The decision-making system is based on a deep neural network based on the DenseNet architecture The model has first been trained on a collection of X-ray images coming from multiple sources RSNA Pneumonia Challenge NIH chest Xray PadChest CheXpert MIMIC-CXR Performances of the detection system in Status Accuracy Sensitivity Specificity Neg Pos Distributed over the true health status of the patient Sex Age All    All M  Accuracy distributed over sex and age Interpretable elements Figure occlusion map technique to indicate decision-relevant areas in the image Figure shows the area of the X-Ray image that is considered as relevant for the automated detection using an occlusion map technique Personalized explanation of the decision taken by the system on the patient at the attention of the medical staff For the patient the system returns NEGATIVE with a confidence score of Counterfactual Figure Artificial X-ray image classified as POSITIVE with confidence by the system obtained from the original Xray image of the patient In Figure a counterfactual artificial X-ray image is presented obtained from the original X-ray image of the patient It is classified as POSITIVE with confidence by the system Technical analysis Explaining the behaviour of a model could consist in making explicit the patterns on which the prediction score is based In practice however two substantial difficulties are faced First the number of different patterns is usually huge For the first model discussed in the case study there are different patterns extracted by the training procedure most of them of size by pixels The prediction score is a weighted combination of the contribution of each of these patterns for the considered image Secondly only a fraction of them are directly extracted from the original image deep neural networks typically use these patterns to create intermediate images compressing the information at different scales the deepest the layer the more abstract the image The inherent complexity of both data and models makes then the derivation of explanation an unsolved challenge and attempts to provide elements that are meaningful for the understanding of a decision have to follow a circuitous route The performances of the system if they do not directly inform about the logics of the system are a good indication that the system is working correctly provided that the testing procedure meets acceptable standards Appropriate documentation of the model accompanying the model can provide non-expert users with elements to understand the nature of the processing After the processing the generation of feature maps is a classical approach to highlight the most relevant groups of pixels and from that understand the patterns that have been captured by the model A typical approach called occlusion maps consists in masking a portion of the image and tracking the evolution of the score The resulting heat map indicates the regions with features characteristic to the infectious regions in red and to non-infectious regions in green With this type of information abnormal behaviours could be detected eg a detection relying on regions with bone tissue Similarly counterfactual images help to figure out what are the relevant patterns for the detection by generating localized alterations of the image that change the outcome of the detector The analysis of the alteration gives also some indication of the relevant patterns at stake in the decisionmaking process For all techniques there are nonetheless no guarantee that these regions are indeed the ones that are significant nor to correctly assess their importance in the final decision Explainability techniques are themselves complicated techniques requiring fine parametrization and subject to tradeoff between accuracy and tractability Legal analysis Receiving an automated diagnosis of COVID- is considered to fall under the scope of Article It is indeed a decision based solely on automated processing based on profiling which produces legal effects or similarly significantly effects on the data subject In particular we can identify at least two effects the psychological effects deriving from a diagnosis and the medical consequences of such diagnosis access to further health cares self-quarantine etc Accordingly the data subject has a right to receive meaningful information about the logic the significance and the envisaged effects of the automated decision when she first answers the questionnaire see Article g or upon her request at a later stage see Article  In addition we can affirm that this automated decision-making system might be allowed because either it is based on the explicit consent of the data subject c or it is eventually authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subjects rights and freedoms and legitimate interests b Accordingly the data controller or the national law eventually allowing such automated decision-making system needs to implement suitable measures to safeguard the data subjects rights and freedoms and legitimate interests at least the right to obtain human intervention on the part of the controller to express his or her point of view and to contest the decision Article Given that COVID- automated detection/diagnosis might be considered a data processing producing high risks for the rights and freedoms of individuals according to the risk-based approach Article the data controller might be required to implement higher safeguards in addition to the three safeguards mentioned above including the right to receive an explanation about the decision reached recital Also once the data subject contests the decision the data controller should take some relevant steps to analyse the decision and eventually take a new decision after having understood the correctness and reliability of the algorithm that is why we propose an expert-based explanation that analyses the level of confidence of image prediction On the other hand the explanation that the data subject the patient might need is easier and aiming to justify a certain decision connecting reality to the reached decisions The explanations given above are composed of three layers of granularity general explanation of the algorithm group-based explanation how group of similar data subjects were assessed by the algorithm and individual explanation of the decision reached in the single case However the adoption of deep learning image analysis does not make easy such an explanation there is no direct causation rules in such automated decisions but rather some correlational rules This makes the detection of biases or inaccuracies more difficult as well as the explanatory narrative Discussion and conclusive remarks The use case developed in this article and the surrounding legal discussion provided on the feasibility of legal requirements on algorithmic explanations illustrate the consequences of the current trend to apply more and more complex machine learning models with multidimensional data in a growing range of application domains This trend introduces a level of complexity that is hard to reconcile with the existing legal requirements especially with regards to human legibility of explanations for non-expert data subjects Our case study explores the strong implications of employing sophisticated AI models for medical imaging a task considered as requiring human-level cognitive capabilities to comprehend the complexity of data Our analysis suggests that in this context getting generally legible explanations is a hard challenge considering the current state of research from the explainable AI community This is caused by an inherent complexity of such systems including a large number of stacked layers whose mechanisms are guided by training algorithms relying on large volume of data This results further in an opaqueness of the inner mechanisms at play inside the mathematical model Even more pronounced and obvious is the fact that the employed deep neural network is a purely predictive statistical procedure As such there is no guarantee that the provided results are grounded in causal effect Furthermore the provided explanations are themselves based on statistical techniques and then indicating other feature correlations between image pixels rather than clear medical explanations This phenomenon appears despite the good performances the system may obtain in the testing phase The central question we asked at the outset was whether we can achieve a degree of explanation for complex machine learning models in high-risk AI applications that is adequate following the applicable EU data protection framework Our analysis of this case in one of the most sensitive areas of automated decision-making COVID- detection identifies several legal and technical implications In particular we contextualized in a practical scenario a reasonably fair and comprehensive approach to explanation We propose different forms and characteristics of explanation at the same time including the moment of the disclosure of the explanation and the audience of the explanation However the practical implementation of this model in our use case has also shown several drawbacks that we should attentively analyse First of all there is no one-size-fit-all explanation in practice each form of explanation ex ante ex post expert-based or subject-based more or less granular strongly depends on the context at issue Also the possibility to give a satisfactorily fair and transparent explanation depends also on the possibility to show causal link between the input data and the final decision However this is not always possible while for tabular data-based decision-making it might be easier to give adequate explanations in more complex AI-based decisions it might be difficult to reach a satisfactory transparency for algorithms In particular the increasing employment of deep learning in more and more forms of automated decisions makes it difficult to explain exactly the reasons of specific decisions taken see the example of COVID automated diagnosis In these last cases the quality of possible explanations might not be judged adequate under Article GDPR If we adopt a strict approach this would mean that either more technologically advanced and obscure forms of decisionmaking should be prohibited because unexplainable or that we should tolerate AI-based decision-making systems that do not formally respect the transparency duties of the GDPR Actually a possible third way might be to better contextualise the right to explanation in the broader picture of the GDPR In particular Article and recital when addressing the possible safeguards to reach accountable automated decisions do not focus mostly on the right to explanation but on a varied set of tools including right to contest human in the loop and algorithmic auditing Beyond Article and recital the GDPR contains several notions that might influence the interpretation of accountability duties also in case of automated profiling the fairness principle the lawfulness principle the risk-based approach the data protection impact assessment model etc In our view in cases where the possible explanation three-layered audience-based moment-based is not satisfactory the data controller should anyway take every reasonable step to provide such an explanation but at the same time other complimentary tools should be implemented These might include disclosing meaningful information about a Data Protection Impact Assessment DPIA on the algorithmic decision-making system the DPIA as mentioned in Article of the GDPR is a process to assess and mitigate the impact of data processing operations on fundamental rights and freedoms of data subjects or the result of a justification effect-based test on the algorithm where the data controller explains why the algorithm analysed on the aggregated final effects on different data subjects but also analysed in its purposes intentions etc is not unfair unlawful inaccurate beyond the purpose limitation etc see the legibility test in These alternative tools are already within the GDPR Analysing them in detail is beyond the scope of this paper but we encourage further research on these Algorithmic accountability elements Moreover in cases where satisfactory explanations are not easy to reach a human-mediated explanation might be helpful In this case the expert understanding of AI-based decisions might be an intermediary for a dynamic explanation of the data subject