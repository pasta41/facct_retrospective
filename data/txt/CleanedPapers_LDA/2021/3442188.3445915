Fair Classification with Group-Dependent Label Noise This work examines how to train fair classifiers in settings where training labels are corrupted with random noise and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup Heterogeneous label noise models systematic biases towards particular groups when generating annotations We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures without accounting for heterogeneous and group-dependent error rates can decrease both the accuracy and the fairness of the resulting classifier Our experiments demonstrate these issues arise in practice as well We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise We provide both theoretical and empirical justifications for the efficacy of our methods We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good CONCEPTS Computing methodologies Machine learning Philosophical/theoretical foundations of artificial intelligence KEYWORDS machine learning algorithmic fairness learning with noisy and biased labels INTRODUCTION Recent work shows that machine learning classifiers can perpetuate and amplify existing systemic injustices in society Notable examples include discrepancies in allocation of medical care to patients on the basis of race and significant disparities in predicting recidivism rates for African-American defendants and more A number of techniques have been developed in order to mitigate bias in machine learning classifiers Typically these methods consider populations with groups corresponding to a set of protected sensitive attributes such as race or gender The classifier is then required to exhibit similar behavior across all groups This can be done by imposing equality of true positive rate or true negative rate conditioned on group membership These are called fairness or parity constraints Many of these methods assume the availability of clean and accurate labels However this is often not the case In fact bias in data is particularly pertinent to label corruption Tomake things worse the accuracy of available labels is often strongly influenced by whether a person falls within a protected group and these discrepancies can have significant and often life-altering outcomes For example it has been shown that labels for criminal activity generated via crowdsourcing are systematically biased against certain racial groups As another example both women and lower-income individuals often receive significantly less accurate diagnoses for cancer and other ailments than men due to imbalance in the sample population of medical trials and due to bias from doctor treatment Similar discrepancies arise in the accuracy of mathematical aptitude evaluations for males and females in primary school and it has long been known that an employers evaluation of a resume will be influenced by the perceived ethnic origin of an applicants name Moreover studies show that people of all races use and sell illegal drugs at remarkably similar rates but in some states black male have been admitted to prison on drug charges at rates twenty to fifty times greater than those of white men The structure and magnitude of group-specific label noise can dramatically affect the performance and fairness of a classifier To see this we consider the following examples Example Enforcing fairness constraints without accounting for group-specific label noise can harm the accuracy of the classifier for the group whose labels have been accurate recorded Consider training classifiers using data from two groups with homogeneous data distributions where a -dimensional feature vector In this setting the Bayes-optimal classifiers for and denoted as and respectively will obey any parity constraint However suppose group has a set of clean labels while group has clean labels when the ground truth is but there is a chance that corrupting noise will cause the observed label to be flipped from the true value when In this case fair trained on both groups achieves perceived equal True Positive Rates TPR between the two groups and is the best one to do so this indeed hurts group prediction performance as opposed to accuracy before but the labels in group are not affected by noise Although also considers this single-group noise setting and shows that fairness interventions could aid in reducing the error caused by label bias our observation demonstrates a special case where potential harm occurs Example A simple classification problem to illustrate the possibility of harming the clean group when training a fair True Positive Rate TPR model over a set of noisy labels Group Group Pooled fair Example A simple classification problem to illustrate the possibility of wrongly perceived fairness due to training on noisy labels Group Group Pooled fair Example A classifier may appear to achieve parity when it does not Furthermore imposing a parity constraint might actually make everyone worse off Consider training classifiers using data from two groups with heterogeneous data distributions Suppose group has a set of clean labels while one quarter of group labels are incorrect We denote the Bayes-optimal classifiers for and as and respectively and they obey any parity constraint The classifier fair trained on the observed corrupted data is subject to equal TPR constraint for both groups However fair has a higher TPR correct predictions out of true labels on than on correct predictions out of true labels when evaluated on the clean data In this paper we look at the problem of fair classification from data whose labels are corrupted such that the error rates of corruption are group-dependent Several recent works deal with fair classification with noisy labels In particular it has been shown that fairness constraints on the noisy training labels can be beneficial when the label noise is homogeneous across the different groups that are to be protected More recently shows that how the true fairness rates such as TPR are related to observed quantities with respect to noise parameters Our work complements these results we show that enforcing fairness constraints when training on data with noisy labels produces a classifier that violates the fairness constraints as measured with respect to the clean Note that fair on the pooled data output for and for because equal TPR constraint is enforced In this case the TPRs for both groups are If the classifier output for and for instead the TPR for group A is while the TPR for group B is only which violates the equal TPR constraint data We then provide a fair empirical risk minimization ERM framework that handles heterogenous label noise Our framework uses an estimation procedure that infers the knowledge of group-dependent noise in the training data and applies this knowledge using bias removal techniques thus eliminating the effects of noisy labels in both the objective function and the fairness constraints in expectation Our main contributions are as follows We show that imposing fairness constraints on the training process without accounting for bias in the noisy labels can result in classifiers being less accurate and less fair Theorems and of Section We experimentally demonstrate that these harms can indeed occur in practice with real data sets and show that obliviously enforcing equality of opportunity without awareness of the noise leads to classifiers with no discriminatory power We design two noise-resistant fair ERM approaches that address these problems Section The main idea is to construct unbiased estimators of the loss functions and of the fairness constraints We provide empirical evidence showing that these fair ERM solutions improve both accuracy and fairness guarantees when facing group-dependent label noise Section Our codes for solving the noise-resistant fairness constrained ERM can be found at Related Works A great deal of research has been devoted to fair classification in general including fair classification under statistical constraints decoupled training with preference guarantees and preventing gerrymandering among many others In this work we specifically focus on fairness in the presence of biased and group-dependent noisy training labels Our work contributes to the fair classification literature by introducing robust methods for dealing with heterogeneous label noise We also provide insight into the effects of noise being present in the labels Our work parallels others on fair classification with noisy labels Ours differs primarily in two main respects First existing works often assume knowledge of the noise generation process Second previous works have only considered noise rates that are homogeneous across different groups We consider a more realistic setting where different groups might suffer different levels of bias and therefore reach very different conclusions Mitigating bias is substantially more challenging in our setting Nevertheless our results could generalized prior work when the noise is assumed constant across groups or only one group is assumed to have noise Both of our fair ERM approaches extend the literature on learning with noisy data Our first uses surrogate loss functions based on to create unbiased estimators of the fairness constraints This first approach requires knowledge of the noise parameters Our second approach relaxes this assumption by extending the work of to account for both biases in the fairness constraints and for group specific label noise Recent work on fair classification with imperfect data shows how to emulate noiseless fair classification by appropriately rescaling the fairness tolerance with the noise but is only restricted to class-conditional random noise without considering group difference Most of the reported results are for the cases with noisy sensitive attributes but not the labels despite that the authors provided discussions to how the two problems are related The surrogate fairness constraints in our paper could be viewed as an extension of their method Nonetheless our work is more general as we consider the more sophisticated settings with group-dependent label noise explores the use of proxy variables when the sensitive attributes are missing Lastly also provides some insights on correcting for observed predictive bias might further increase outcome disparities but is concerned with fairness evaluation rather than learning In contrast with their work we simplify the assumption on instance-dependent noise into group-dependent and further develop two fair ERM approaches in terms of the unbiased estimators PRELIMINARIES We start with a dataset with examples where each example consists of a feature vector a label and a group attribute eg female We assume that there are groups We let denote the number of examples in group and we use and to denote their indices We assume that each example is drawn iid from a joint distribution D of random variables We use the data set to train a classifier where denotes our concept class To this end we consider solving a standard risk minimization problem with fairness constraints min D st Here is some fairness statistic of for group given the true labels such as true positive rate TPR Constraint restricts the disparity between to at most A standard approach for performing above constrained minimization is via empirical risk minimization ERM min st where is our fairness metric defined using training data For instance when using the TPR as a fairness measure where is simply a counting function that counts the number of samples that satisfy the specified conditions For computational purposes ERM is performed in practice by minimizing over a classification-calibrated loss function R R This fits min st Typical s include square loss logistic loss cross-entropy loss and more We aim to train a classifier using a dataset where the ground truth labels are replaced by noisy or corrupted labels A noisy label corresponds to a true label that may have been flipped based on noise rate as a function of true label More precisely we assume that the noise rates vary based on the true label as well as the group attribute P P ie the training labels are generated as wp sign wp sign This reflects a setting where noise rates are independent of the at fixed values and eg a medical problem where is the presence of a disease and the disease is diagnosed less reliably for females In this paper we mainly focus on two specific fairness constraints Equal Opportunity and Equal Odds Equal opportunity requires that each group achieves equal true positive rate TPR or false positive rate FPR while equal odds requires both equal TPR and equal FPR We use the following shorthand to denote different measures of performance including TPR and FPR computed for each group using the true labels and the noisy labels where tpr fpr tpr fpr tpr and fpr are taken with respect to the noisy labels ENFORCING FAIRNESS CONSTRAINTS ON NOISY LABELS CAN BE HARMFUL Recent results have established that enforcing fairness constraints improves classifier accuracy when the labels suffer from label noise that is uniform across different groups However as we shall see adding fairness constraints can lead to harm when group-dependent noise is present in the labels Parity Constraints on Noisy Labels Harms Groups with Clean Labels The first message that we wish to deliver is that naively enforcing parity constraints on the noisy labels may harm the accuracy of the classifier for the groups that are not affected by label noise Without loss of generality we present our results in settings where we wish to train a classifier with equal TPR across groups Similar Table Label noise harms accuracy Adult dataset High FPR implies weak discrimination power We highlight any high harm the classifier suffers when enforcing equal TPR Metrics Groups fair TPR female male FPR female male Accuracy female male derivations hold for other related constraints eg the ones as linear combinations of the entries in the confusion matrix such as equal FPR and equal balance error BER Consider a classification problem with two identical groups and where samples from group have uncorrupted labels while samples from group have noisy labels On the clean data the parity constraints naturally hold since the data for both groups is drawn from an identical distribution We next show that the label noise presented in group can harm the clean group when enforcing parity constraints Formally Theorem Consider a setting with two identical groups and Group has clean labels ie Group suffers from symmetric noise In this setting a classifier trained subject to the equal TPR constraint tpr tpr leads to an uninformative classifier that tpr fpr We defer the proof to Section Omitted Proofs Thus even if group is represented with completely uncorrupted labels in the training data the imposition of equal TPR in the presence of noise for will diminish the classifiers predictive accuracy on members of group Case study We empirically examine the above observation on the Adult dataset from UCI Machine Learning repository There are two sensitive groups male female in this data set We inject symmetric noise into labels for members of the female group Then we train two classifiers which is trained without any fairness constraints and fair which is trained with the imposition of equal TPR using the reduction method As is shown in Table the empirical results mirror Theorem When the difference between fair s TPR for the two groups becomes small less than fair s TPR and FPR become close together and the accuracy decreases significantly The above trends hold even when we try to equalize TPR and FPR together across groups We notice that the two groups are not strictly identical in the Adult dataset but our example implies that there exists dangerous cases where enforcing fairness constraints can harm classifier accuracy for the group with uncorrupted labels Violation of Fairness under Perceived Fairness Our second message is that training fair classifiers using noisy labels may lead to a false impression of fairness This arises when the fairness constraints are satisfied over the noisy labels while being violated over the clean labels Before proceeding we require extending Proposition of into the situation with group-dependent label noise A similar result appears in Lemma For each group we have that tpr tpr fpr fpr tpr fpr Proof Expanding using law of total probability we have tpr P P P P tpr fpr Note in the above we drop the dependence on when conditioning on This is because is trained purely on the noisy labels and encodes all the information has about A similar derivation holds for fpr We also note that in the special case where all groups suffer from an identical rate of label corruption the learner can be oblivious to the specific error rates Theorem Consider a classification problem with noisy labels where the noise rates are independent of group membership so that and Then it follows that tpr tpr if equal odds equalizing both TPR and FPR on the noisy labels is imposed The proof follows by applying the assumption of equal error rates and equal odds on the noisy labels with Lemma However things break down in the general case If we impose equal odds across groups on a learner that is unaware of the labels noisiness ie whenever tpr tpr then Theorem Assume that a classifier is subject to equal odds in the presence of group-dependent label noise Then for any two groups we have tpr tpr tpr fpr fpr fpr tpr fpr Unless the classifier is random on the noisy training data ie tpr fpr it is impossible to satisfy equal odds over the clean data whenever and Proof Noticing that tpr tpr and fpr fpr equalizing fairness metrics on the noisy data and applying Lemma we obtain tpr tpr tpr fpr tpr fpr fpr tpr fpr tpr fpr tpr tpr fpr The argument for FPR is symmetrical fpr fpr tpr fpr tpr fpr tpr fpr tpr fpr tpr fpr tpr fpr Therefore tpr tpr fpr fpr when tpr fpr The proof follows by a direct application of Lemma Theorem implies that the true fairness violation is proportional to the difference in error rates across the different sub-groups We offer two remarks First if the error rates are systematically biased towards a particular group then a perceived fair classifier will lead to unequal odds Second the above bias will be reinforced when the trained model is more accurate on noisy data a more accurate model will lead to a larger difference in tpr fpr FAIR ERM WITH NOISY LABELS In this section we describe two noise-tolerant and fair ERM solutions that address the combined challenges of heterogeneous and group-dependent label noise Both the surrogate loss and group-weighted peer loss approaches for handling noisy labels rely on estimations of the label noise Our procedure for estimating the noise parameters detailed in Section is an adaptation of Section also offers discussion of the impacts of noisy estimates A Surrogate Loss Approach As we shall see training an unmodified loss function using the noisy labels corrupts the model in a manner that cannot be addressed via post-hoc correction Thus a natural resolution is to modify the loss function itself This modified loss function is called a surrogate loss Bias removal surrogate loss functions Bias removal via a surrogate loss is a popular approach to handling label noise The original loss function is replaced with a surrogate loss function that Table Surrogate constraints for surrogate loss Metric TPR TPR FPR FPR TPR FPR Equal Odds both TPR and FPR Table Surrogate constraints for group weighted peer loss Metric TPR TPR FPR FPR TPR FPR Equal Odds both TPR and FPR corrects for noise in the labels in expectation Formally the surrogate loss is chosen so that the cost of mis-classifying an element with true label is equivalent to the expected loss value that arises from using noisy label Thus we want to find a surrogate loss such that E for all and When the noise depends on the label value the function given by satisfies the above property as shown by Lemma in A classifier minimizing the surrogate loss on noisy data will minimize the loss on clean data in expectation This property allows us to perform model selection on a noisy validation set and one could choose the model that performs better on the validation set to deploy Surrogate fairness constraints We will also need to modify the fairness constraints to account for the effects of noise Our method of doing so is inspired by the surrogate loss that we need to work with an unbiased estimate of the fairness constraints For the case of binary classification we can express the surrogate measures of group-based fairness constraints using Lemma We use Equation and Equation to define our surrogate loss functions and Furthermore define the empirical TPR and FPR over the noisy labels as follows TPR FPR We then define our surrogate fairness measures using only noisy data as detailed in Table Our noise-resistant fair ERM states as follows min st Group Weighted Peer Loss Approach The recently developed peer loss function partially circumvents the issue of noise rate estimation The peer loss requires less prior knowledge of the noise rates for each class It is defined as peer where P P is a parameter to balance the instances for each label and where and are uniformly and randomly selected samples from ie peer samples which inspired the name peer loss as noted in Although the noise parameters explicitly appear in the definition of only the knowledge of is needed In practice we could tune as a hyper-parameter during training This loss function has the following important property proven in Lemma of E peer peer where denotes the noisy distribution for group and Adapting the peer loss function to labels with group dependent noise requires accounting for the differing values of We do so by re-weighting Equation to obtain our group-weighted peer loss When class is balanced for every group ie the parameter is exactly In this case the expected group-weighted peer loss on the noisy distribution D is the same as the expected uncorrected loss on the true distribution D More precisely Theorem For all group dependent noise rates and satisfying taking as the loss and when E D ED Proof Observe that peer Taking expectations over noisy data we have E D E E peer peer by Equation ED peer Notice that when the definition of peer loss function gives peer Using the assumption that and the fact that is loss function Combining and we complete the proof E D ED Peer-based surrogate fairness constraints We acquire the following result in order to create group-aware surrogate constraints Lemma True TPR and FPR relate to tpr fpr defined on the noisy labels as follows tpr tpr fpr fpr tpr fpr Proof Following Lemma we have tpr fpr TPR FPR TPR FPR Notice that tpr fpr Solving the two equations above we complete the proof Lemma allows us to derive the appropriate surrogate fairness constraints for the peer loss displayed in Table Note that we have assumed that the dataset is balanced for each group ie If the data is imbalanced we will require knowing the marginal prior We note that it is straightforward to get the estimated marginal priors as given by Equation in Section We merely require knowledge of for each in order to define and This is a weaker requirement compared to knowing the error rates which will carry estimation of two parameters for each group We indeed see our group peer loss approach performs more stably as compared to the surrogate loss approach introduced in last subsection when using noisy estimates of the noise rates With group-weighted peer loss function and surrogate fairness constraints we are able to perform a fair ERM as detailed in Equation by replacing with and the corresponding term Error Rates Estimation and its Impact We employ confident learning to perform noise rate estimation in our experiments The first step is to pre-train a classifier fpr over the noisy labels directly and learn a noisy predicted probability P fpr Then for each pair of classes we define the subset of samples where is the expected self-confidence probability for class and group Using the above quantities we estimate the group-aware joint probability over the noisy labels and clean labels with We use the marginals of estimated joint to compute the noise parameter estimates for each group To estimate we simply substitute and for and in the equation for As a byproduct we could estimate the marginal priors by Effects of noisy estimates It is important to quantify the impact of the noise rate estimation error on the accuracy and fairness of the resulting classifier We first note that for any the law of large numbers implies that taking sufficiently many samples from D will ensure that the following holds for all with probability at least max Denote by the surrogate loss function defined using the estimated noises and let argmin argmin We have the following result and defer the proof to Section Theorem For every there exists such that with probability at least where max Because the fairness constraints are linear in s the additional fairness violations incurred due to the noisy estimates of the error rates will also be linear in too Similar observations hold when using the estimated in the peer loss EXPERIMENTS Due to the difficulty of acquiring real world datasets with known label corruption characteristics we artificially synthesize the datasets with a noise generation step These controlled experiments help us understand the robustness of our approaches under different noise scenarios Experimental Setup Dataset We evaluate our methods as well as other baseline methods on five datasets Adult the Adult dataset from the UCI ML Repository with males and females as the protected groups Arrest and Violent the COMPAS recidivism dataset for arrest and violent crime statistics with race restricted to white and black and gender as the sensitive attributes German the German credit dataset from UCI ML Repository with gender as the sensitive attribute Law a subset of the original data set from LSAC with race restricted to black and white as the sensitive attribute Table describes the dataset statistics and parameters used in the experiments We chose to apply a diverse set of noise parameters to the different subgroups The fairness tolerance and noise parameters for Adult German and Law data sets are identical but they are different from Arrest and Violent data sets because Arrest and Violent data sets contain more protected groups We make this choice mainly for the baseline models to obtain meaningful results to compare with Noise generation We randomly split the clean dataset D into a training set and a test set in a ratio of to We add asymmetric label noises to the training dataset and leave the test data untouched for verification purposes For each sensitive group we randomly flip the clean label with probability if its value is and we flip the clean label with probability if its After injecting this noise we use the same training set and test set to benchmark all the methods Methods For all of the methods above we use logistic regression to perform classification and leverage the reduction approach as proposed in for solving our constrained optimization problem We evaluate the performance of several methods Clean in which the classifier is trained on the clean data subject to the equal odds constraint Corrupt which directly trains the classifier on the corrupted data subject to the equal odds fairness constraint Table Dataset statistic and parameters Dataset Source Number of data examples Fairness Tolerance Sensitive Groups Noise Rates adult UCI female male arrest COMPAS white black arrest COMPAS white male black male white female black female violent COMPAS white male black male white female black female German UCI female male law LSAC white black Surrogate Loss which uses the surrogate loss approach described in Section Group Peer Loss which uses the group weighted peer loss approach described in Section to train a fair classifier on the corrupted training set The Corrupt baseline gives us a sense about the harm caused by the unawareness of the labels noise and the clean baseline shows the biases contained in the datasets We set the same maximum fairness violation for all the methods on the same dataset during training As there are more sensitive groups on arrest and violent datasets we set on these datasets and on the other datasets We report metrics for each of the above methods averaged over five runs Computing Infrastructure We conducted all the experiments on a GHz -Core Intel Core i CPU The running time for Surrogate Loss is about minutes while the running time for Group Peer Loss could be over minutes Tuning in Peer Loss The performance of our group weighted peer loss is highly influenced by the hyperparameter Recall that E We split of data examples in the train set for validation and found the optimal using grid search The range of we searched varied between to We observed that both the accuracy and fairness violation on the validation set exhibit the same trends on the test set In practice the group weighted peer loss with achieves the best performance on the Adult dataset Results We present an overview of the performance for each method on the test set in Table We compare the two fair ERM approaches using both the true and estimated noise rates The metrics we report include violation the maximum difference in TPR and FPR between groups and accuracy the accuracy achieved on test set We make the following observations about our results First both of the two fair ERM approaches in Section produce classifiers that are more effective at mitigating unfairness than a classifier that is naively trained on the corrupted data In particular the group weighted peer loss approach achieves almost violation on the German and law data sets when given the true noise parameters The only noticeable worse case arises when applying the surrogate loss approach to the German dataset This may be due to the high variance of the German dataset which has fewer than samples Second as expected models trained using our proposed fair ERM methods do not achieve the same level of accuracy as a model that is fit using clean labels However our models are typically more accurate than the model fit directly to the corrupted data For example on the arrest data set with four protected groups the surrogate loss approach achieves a similar accuracy to the classifier trained on clean data while incurring an even smaller fairness violation Third Our methods perform similarly well when trained using both the true and with the estimated noise parameters indicating that the noise estimation procedures are effective On arrest and violent datasets our methods with estimated noise parameters even perform better than those with true parameters This is probably due to the biases and noise in these datasets Finally our fair ERM frameworks adapt well to multiple sensitive groups as demonstrated by the good performance on the Arrest and Violent data sets Impact of noise levels on classifier performance We present the results of varying noise rate on the adult data set with two groups in Table We only add symmetric noise to female group and keep the male group clean ERM is generally robust to symmetric noises when a significant subset of the data is clean one group in our example so we do not expect significant accuracy improvement from our methods We focus on how fairness violation reduces Observe that comparing to training with clean Table Overview of group-based performance metrics for all methods on data sets We highlight the best values achieved for fairness violation and accuracy in green and the worst in red is the number of sensitive groups is the average of error rates over all the groups and all label classes s true indicates training with true noise parameters and estimated indicates training with estimated noise parameters The values after are the standard deviation Surrogate Loss Group Peer Loss Dataset Metrics Avg Clean Corrupt true estimated true estimated Adult violation accuracy Arrest violation accuracy Arrest violation accuracy Violent violation accuracy German violation accuracy Law violation accuracy Table We show how different levels of symmetric noise affect the classifiers performance on adult dataset SL Surrogate Loss GPL Group Peer Loss We highlight substantial improvement of fairness in green and sever violation in red Noise Metric Clean Corrupt SL GPL violation accuracy violation accuracy violation accuracy violation accuracy data training on corrupted data substantially increases fairness violations even for relatively low noise rates The SL and GPL columns show that our fair ERM approaches can effectively mitigate the biases This holds true even when increasing the noise rate Insights on running on data directly without adding additional noise We evaluate our algorithm on the clean adult and arrest datasets as shown in Table On the arrest dataset our methods achieve a similar performance of accuracy compared with the Clean baseline but we do observe a consistent drop of fairness violations on the arrest dataset The fairness violation of our methods on adult dataset is not as good as that of Clean baseline This fact may imply the possibility that the arrest dataset contains more human biases in labels than the adult dataset The small drop in accuracy and sometimes in fairness is due to the additional noise estimation step which introduces another layer of complication this is the price we pay for dealing with potentially highly noisy labels Table We examine the performance of our methods on the clean adult and arrest datasets Clean train a fair classifier directly with equal odds constraint SL Surrogate Loss with estimated noise parameters GPL Group Peer Loss with estimated noise parameters The values after are the standard deviation adult arrest Method accuracy violation accuracy violation Clean SL GPL CONCLUDING REMARKS LIMITATIONS AND FUTURE WORKS We have demonstrated both theoretically and empirically that naively enforcing parity constraints without taking noisy labels into consideration can indeed do harm Our results show the importance of accounting for group-dependent label-noise when performing ERM subject to fairness constraints In realistic applications such as criminal justice and evaluating loan applications labels are often contaminated by human biases against a certain protected group The insights gained from this work forewarn decision-makers that improperly mitigating unfairness might do harm on the clean groups Our two fairness-aware ERM frameworks are an important step toward addressing this problem Our work extends a growing body of methods for training classifiers to provide equal opportunity to members of different subgroups within a population Our new contribution is to address situations where feature and label information for one or more of the subgroups has been recorded less faithfully than for members of other subgroups Just one example of this discussed in the text is the significant disparity in the quality of evaluations for males and females which occur in both medical and academic contexts These disparities can and do have significant impacts on the quality of life for members of each group and are well worth addressing This work shows how applying existing techniques for mitigating bias in classifiers can actually increase inequality in outcomes if disparities in the accuracy of training data are not accounted for We offer new methods for addressing these problems as well We believe that applying our methods thoughtfully will improve existing methods of bias mitigation in machine learning Our technical solutions and solvers should be of interests to machine learning practitioners/researchers as well as to policy makers when decided to use classification tools but face a training data with low-quality annotations Our work has limitations Our selection of data sets is limited we rely on synthetic training data corruption in order to test our methods This limitation arises from the unavailability of such sensitive data sets for the broader research community Both this research and the methods whose shortcomings we have attempted to address should be re-examined as richer data sets become available for studying disparities in the quality of information recording between members of different subgroups The lack of relevant data for studying unfairness in machine learning and the concerns about how to acquire such data while preserving the privacy of people concerned is itself an important question in this area although we do not address it in this work It is also possible that blind and uncareful application of our approach by improperly attempting to correct otherwise accurate labels may in fact create classifiers that produce even greater inequality or lead to other problems that we have not foreseen The temptation to apply our methods simply for the purpose of making existing models seem more fair especially to unsuspecting downstream users is very real We very much discourage the use of our research in this fashion Both the limitations and the insights gained through this work underscore an important underlying message that blind application of bias mitigation techniques in machine learning may do more harm than good