Detecting discriminatory risk through data annotation based on Bayesian inferences Thanks to the increasing growth of computational power and data availability the research in machine learning has advanced with tremendous rapidity Nowadays the majority of automatic decision making systems are based on data However it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data In fact in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded Although the process of rigorous data collection and analysis is fundamental in the model design this step is still largely overlooked by the machine learning community For this reason we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set In particular our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination CONCEPTS Human-centered computing Visualization application domains KEYWORDS human annotation data ethics race discrimination sampling bias data labeling machine learning INTRODUCTION In the last decades machine learning systems are widely spreading in different academic domains as well as many public and private sectors are increasing the exploitation of these systems Their widespread and pervasiveness is mainly driven by the exponential growth of computational power and the extensive availability of large amounts of data Supervised machine learning models are also particularly widespread and now deeply rooted in different sectors due to their usage versatility The predictive ability of supervised machine learning systems is deployed in disparate areas of application credit reliability justice system job recommendations university selection process cultural contents and purchases recommendations The key ingredient that supervised machine learning models have in common is the availability of a set of labeled data used to train the model in elaborating a response related to past events Since the known properties of the available set of data is used to create a classifier that makes predictions about new entities of the same type the structure properties and quality of the data are aspects that largely and directly influence the quality of the model and the results it produces Although data-driven decision models have been shown to produce both economic and social benefits many researchers have highlighted several problems and damages related to their use in different areas especially if they are built on partial or incomplete data As a matter of fact in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and in the way they are recorded While the process of rigorous data collection and analysis is fundamental to the design of the model this step is still largely overlooked by the machine learning community As the practice of removing protected attributes from available data has been shown to potentially exacerbate further discrimination making bias even more difficult to detect practices related to data collection data transparency and data explainability become even more relevant and urgent The aim of our work is to provide a data annotation system that serves as a diagnostic framework containing immediate information about the data appropriateness in order to more accurately assess the quality of the available data used in training models We propose a data annotation method based on Bayesian statistical inference that aims to warn of the risk of discriminatory results of a given data set In particular our method aims to deepen the statistical knowledge related to the information contained in the available data and to promote awareness of the sampling practices used to create the training set highlighting that the probability of a discriminatory result is strongly influenced by the structure of the available data We test our data annotation systems on three dataset widely spread in machine learning community the COMPAS dataset the Drug Consumption dataset and the Adult dataset Problem Statement The majority of machine learning systems are based on historical data processing This is particularly true in supervised machine learning models Several studies have shown evidence that many equity and discrimination issues are due to input data properties Most of today data sets used to train models are chosen through non-probabilistic methods generating problems of data imbalance and representativeness This means that different fractions of the population do not show the same opportunity to be represented within the sample aka training sets leading some groups of individuals to have a lower probability of being represented Common observed effects of a bad sampling are underestimation and overestimation of some groups Undetected distortions in data may also easily represent a spurious statistical noise This happens when the data structure induces dependence between two variables that are not linked by a real cause-effect relationship Data Sampling A key moment in the pipeline of a machine learning model is when the programmed algorithm is supplied with training data representing the entities on which the model itself trains its knowledge to make predictions The quality of the data used in this phase is fundamental for the desired result according to the principle of garbage in garbage out even the most sophisticated models can present distorted results in the presence of low quality data One of the main causes of data distortion is the way the data is selected and provided to the algorithm displaying problems related to inaccuracy lack of update or inadequate representativeness However while knowledge of bias typologies has proliferated over the years less attention is paid to issues concerning data collection notation and sampling In the spirit of fostering a broader awareness of data handling we provide a reasoned list of issues that may arise during this phase i Data Selection the large proliferation of data sets availability on the same kind of problem to be analyzed make hard the a priori choice of a given data set ii Inadequate sampling methods most models are trained with data sets that have been found and not subjected to probabilistic sampling methods leading to limited or no data control iii Cost and Time Limit collecting large amounts of data that present proportional representations of each property with respect to a sensitive attribute is time consuming and often costly and labor-intensive iv Data set validation in the design of a machine learning model more attention is paid to the mathematical basis of the classifier restricting the data formation process to a black box v Validation planning data validation when applied is often performed only after the model has been trained and used making the feedback cycle inefficient and often ineffective vi Lack of statistical rigorousness the suitability of the data set varies depending on the task for which the data are prepared For instance models based on linear regression imply assumptions of normality on the measurement error This specificity is often absent in the pipeline of machine learning models Miss-dependency Two-dimensional or bivariate statistics is the study of the degree to which two distinct characters of the same statistical unit are connected However the connection only measures the degree of statistical dependency without inducing a cause-effect relationship or dependency between the variables For instance it can be shown that people with small feet make more spelling mistakes than people with large feet However this statistical dependency does not indicate that having small feet is the cause of spelling mistakes the greater frequency of spelling mistakes may in fact be due to the younger age of people with small feet In this case there could be a third variable age responsible for the cause-effect relationship While in a human-centered model where the human makes the decisions this distinction is quite evident in a machine learning model miss-dependency is not always deductible This depends on two reasons i the machine does not recognize the meaning of the instance but looks at the properties of the variables ii the way in which the data are structured modifies the interpretation that the machine is having regarding the relation of statistical dependency This means that while in a human-centered model it is the human to verify that the relationships of statistical dependence detected in the available data are leading to a cause-effect relationship in machine learning models the machine is not always able to recognize a spurious connection erroneously assigning to two or more variables a cause relationship In other words the structure of the available data is responsible for the successful or failed relationships established with the protected attributes ethnicity gender etc in the data In addition the rapid growth and spread of current machine learning systems is due in part to the ease of design of the models themselves which thanks to modern software allows the construction of predictive models avoiding the understanding and adoption of rigorous statistical analysis The simplicity of design has therefore created a gap between predictive and analytical-explicative power favoring misinterpretation between causality and statistical dependence The distinction between statistical dependence and causal dependence in data is therefore a primary issue in machine learning models especially to determine the causes of failure potential biases encoded in the data and the reliability of application Based on the problems highlighted our contribution aims to answer the following research questions Is it possible to establish the probability of composition of the training data from the available data set Do the available data known to the machine learning community present a discriminatory future risk based on their structure BACKGROUND When machine learning model decisions are based on historical records they tend to embed distortions that exist in reality and crystallize them Prejudices and human bias therefore become part of the technology itself This is particularly evident with regard to ethnic discrimination Over the last years the rise of machine learning models in various sectors is leading to a dramatic increase of discriminatory outcomes for ethnic minorities across different fields of application A striking and well known case is the COMPAS software used in US court to estimate the probability of defendants recidivism which has been shown to underestimate the risk of recidivism for white defendants and overestimate it for black defendants However the COMPAS case is not an isolated phenomenon In a experiment conducted on the Airbnb platform applications from guests with typically African American names were found to be less likely to be accepted than identical guests with typically white names Also in a geo-statistical analysis revealed that the design of the popular Pokémon GO game strengthens existing geographical prejudices for example by benefiting urban areas and neighborhoods with smaller minority populations economically disadvantaging ethnic minority areas Several studies have demonstrated the discriminatory potential of targeting advertising which is only recently receiving interventions to remove the prejudicial content of the model For example Facebook after years of scandals related to ads that exclude people based on race has finally removed the racial targeting option for ads In a study the commercial algorithm widely used in the US health care system to guide health care decisions was found to discriminate against black patients The algorithm falsely assigned a healthier condition to black patients despite the risk of complications being the same for white patients making black people less likely to receive more financial resources for extra care Although facial recognition technologies are now used in several domains they still present many discriminatory issues related to differences in margins of error generally software has a higher margin of recognition error for black women As an example we report what happened recently with Google Vision AI a computer vision service for image labeling By providing the system with two images of people holding a body temperature thermometer it labeled the image containing the white person as an electronic device while in the image containing the black person the device held was labeled as a gun In a later experiment it was shown that it was sufficient to apply a pink mask on the black persons hand in order the software labeled the image as tool Racial bias encoded in machine learning systems is likely to spread silently and like wild fire in everyday technologies The increasing and ubiquitous spread of such models also intended to make allocative decisions about peoples lives makes the problem of prejudice and rational discrimination more urgent than ever For this reason and for the historical moment we are experiencing our work intends to focus on rational discrimination in data MOTIVATING EXAMPLE Given a population composed of Caucasians black people and Asian people the probability of positive outcome for the respective ethnic groups is for Caucasians for Blacks and for Asians What is the probability of failure with respect to the protected attribute Ethnicity In this example the probabilities are given rather than the numerosity in order to simplify the following notation To offer a better a better understanding of the Methodology this data will be used in Section The data gives the probability of success but the similar reasoning is also valid for cases where the probability of failure is known The intent is to verify whether the probabilities of success or failure of a subgroup are influenced by group membership and vice versa and more specifically how these probabilities affect the composition of the training set METHODOLOGY Our data annotation system is based on four modules I Dependence assesses the degree of connection among the protected attribute in our study the ethnicity and the target variable II Diverseness provides the training diversification probability in respect to each level of the protected attribute and the target variable III Inclusiveness provides the probability that two properties are simultaneously included in the training set IV Training Likelihood provides the occurrence likelihood of the protected attribute levels given the target variable levels and vice versa before the training set is sampled Quantifying Dependence Excluding some specific domains where the dependence of some protected attributes with the response variable is not considered problematic but rather it is fundamental for the understanding of a certain problem for example the gender attribute in the medical field in the detection of particular diseases in the broad field of machine learning systems the dependence between the protected attribute and the response variable has caused severe consequences The dependence between the protected attribute and the response variable is therefore one of the major causes of discrimination and as such must be rigorously examined The first step for a correct bias detection within the data is given by the dependency analysis between the different modalities of a protected attribute and the response variable In statistics the measurement of the degree of dependence of two qualitative variables is called contingency contingency measures the degree of connection of two categorical variables To determine the degree of connection the marginal frequencies and the combined frequencies of the bivariate table are used Given two categorical variables and the dependency or independence is established through the theoretical independence table once the table of the observed real data is given The contingency is therefore given by the difference between the observed and theoretical frequencies If the table of the observed real data and the theoretical table of independence coincide that is if for each cell the value is null then the two variables are independent Otherwise it is necessary to measure the degree of connection between the variables The degree of connection between two categorical variables is commonly measured by the Pearson connection index obtained as the sum of the relative quadratic contingencies The index assumes a value of zero in case of independence in distribution and increases as the degree of connection between variables increases In order to support Pearsons connection index the contingency coefficient is adopted with the purpose of reducing the in the range However the effect size of the degree of connection between two categorical variables is not always easy to interpret where by effect size we mean a quantitative measure of the magnitude of a phenomenon To offer a better understanding of the relationship of dependency between two variables several simplified methods of interpretation have been proposed especially to guide social scientists in the interpretation of statistical test results In the spirit of simplifying the interpretation of the dependency between the response variable and the protected categories for a data set user we introduce the concept of the Effect Size Index ES where and are the value of the ith cells Notice that unlike the contingency coefficient the is not derived from frequencies but from proportions The relationship between the Pearson connection index the contingency coefficient and the ES Index is given by the following formula Alternatively to the Formula it is also possible to calculate the ES from the contingency coefficient The size of the ES between two variables is then evaluated through the use of Table which relates the magnitude of the ES with a nominal label The advantage of using the conventional Table Conventional definitions of Effect Size Index magnitude Magnitude Value SMALL MEDIUM LARGE conversion table for the user of the data set is that the magnitude of the dependency is displayed quickly and immediately without the need for more complex statistical tests Estimating Diverseness Intuitively the probability of an event represents how likely the event will occur According to the classical definition the probability is given by the following ratio number favorable cases number possible cases We now apply this elementary theory to the problem of data collection in machine learning When the data set is partitioned into training and test sets a split with amore or less standard ratio or is generally performed ie a sampling is performed on the available data Lets consider the case in which the training data set is generated by random sampling on the original data set without considering further techniques stratification or re-sampling for example in the case of a non expert user The probability an event occurs turns into the probability that the training set shows some existing properties contained in the original data set number favorable properties number possible properties In our data annotation this ratio is introduced to allow the dataset user to answer questions like If I perform a random sampling on the original dataset what is the probability that the training set is mainly composed of positive examples What is the probability of belonging to a certain group with respect to the target variable Prior Probabilities The a priori probability of a data property is the degree of belief of the property in the absence of other information also known as the unconditional probability The degree of belief is the probability of a property to be true in an uncertain environment The probability is referred to the belief and not to the truth of the fact as it is not possible for the user to know exactly the truth that is if the original data are representative of the real world Since the user does not have access to the complete information several hypotheses on how the real data is structured have to be drawn assigning to each of them a probability of being true Formally We estimate the prior probabilities by using the data of the problem introduced in Section where the target variable assumes value in case of negative outcome otherwise In this specific case Table Example of prior probabilities Formula Probability P P white P black P Asian P the prior probabilities indicate that the training set has probability to be composed by individuals who display a positive outcome and to be composed by individuals who display a negative outcome finally the probabilities that it is formed by individuals of white black and Asian ethnicity are respectively and Table Estimating Inclusiveness Posterior Probabilities Given two events A and B the probability is said posterior probability because it allows to calculate the probability of A knowing that B occurred In our case the posterior probability means to compute the probability that knowing that has occurred and vice versa In other words the probability that the training set shows the property Y y knowing the property has occurred and vice versa We start by estimating the probability that two events occur simultaneously From the definition of conditional probability Since from Compound Probability Theorem is equal to ie the probability of both properties occurring is the same either of the two formulas can be employed indistinctly Table Example of properties occurring simultaneously Formula Probability white P black P Asian P white P black P Asian P Estimating Training Likelihood From the definition of conditional probability we derive the Bayes Theorem for the properties of the training set In the case of binary classification and in the case of protected attributes we are in the presence of a certain event partition This means that the events are disjointed from each other and if and that as a whole they are the only ones possible i e if a certain property occurs one and only one certainly appeared In other words it is not possible that the training set is composed of individuals who belong simultaneously to the black and white ethnic group or who simultaneously show a positive and negative outcome The union of the occurrence of the single properties is therefore the whole set of possible properties For the properties outcome and ethnicity the generalization formula are respectively hence hence By applying Formulas and the Bayes Theorem can be generalized for each property of the training set The first equation in Formula derives the probability of the outcome property given the ethnic property while the second equation derives the probability of the ethnic property given the outcome property In other words it derives the probability of composition of the training set based on the posterior probabilities of the outcome and ethnicity properties Carried out a random sampling on the original data the Formula answers the following questions i In the sampled training set what is the probability of belonging to an ethnic group with respect to the outcome variable ii In the sampled training set what is the probability of obtaining a certain outcome with respect to the ethnic group Complementarily the two equations can be interpreted as the probability of bias within the training set Table Example of posterior probabilities Formula Probability white P black P Asian P white P black P Asian P white P white P black P black P Asian P Asian P CASE STUDIES DATASETS COMPAS Correctional Offender Management Profiling for Alternative Sanctions is a popular tool used by US court to estimate the defendants probability of recidivism This dataset displays the probability of reoffending based on two year of further studies The dataset has been shown to underestimate the risk of recidivism for white defendants and overestimate it for black defendants Drug Consumption contains information on the consumption of drugs based on personality traits and socio-economic attribute For simplicity of analysis we assumed the consumption of Cannabis as target variable but the annotation of the dataset can be made on each target drug Adult Dataset The data set contains adult income annual census from the US Census Bureau It is commonly employed in forecasting tasks in order to predict the factors leading to income below or above Table Summary of Datasets Prominent Properties Property COMPAS Drug Adult Consumption Dataset Size x x x Target no non user variable yes user Levels of Asian Asian AIE ethnicity Black Black API attribute Caucasian Black/Asian Black Hispanic Caucasian Caucasian White/Asian Other Other White/Black Other American-Indian/Eskimo Asian-Pac-Islander Native American RESULTS AND DISCUSSION We performed the analyses that constitute our data annotation system for each of the datasets presented in Section Sub-sections and report the analysis for each module dependency diverseness inclusiveness training likelihood respectively and contain an example graphic module Figures and shows an illustrative example of the graphical visualization for the complete notation Dependence This module aims to analyze the connection relationships between the protected attribute Ethnicity and the target variable that are established and depend on the available data For instance for the COMPAS dataset the module highlights the dependency relationships between recidivism and different ethnic minorities Summary results for dependence module are shown in Table Retrieved from Table Summary of Dependence Prominent Properties COMPAS Drug Adult Consumption Dataset Contingency coefficient Effect size variable Magnitude of SMALL SMALL VERY Effect size SMALL None of the three datasets displays worrying dependency values among the protected attribute Ethnicity and the target variable showing the magnitude of the Effect Size as small or very small However the results of the COMPAS dataset which is proven to contain bias indicate that this module alone is not sufficient to show a latent bias risk The degree of bias depends on the sample size and the value of the contingency coefficient of the target variable and the protected attribute Smaller samples lead to more bias and higher variance and therefore the results of the dependency must be analyzed in relation to the amount of data available In order to facilitate the interpretation of the connection Figure Example of Dependence graphic visualization relations we propose a graphic notation for dependence Figure shows the graphical representations of the dependency modules based on different connection magnitude Diverseness This module aims to analyze the diverseness of the data available by estimating prior probabilities They determine the probability that training set will display an a priori environment based on the original data available ie they show the probability of training set composition stratified by each of target variable and protected attribute levels For example in our case study the module highlights the probability that training set will be equally composed by ethnic minorities and ethnic majorities Summary results for diverseness module are shown in Table In terms of target variable probabilities the results show strong distortions for the Drug Consumption and Adult datasets with a high probability of positive examples ie showing a negative outcome while the probabilities of the COMPAS dataset are quite homogeneous Regarding the probabilities of the protected attribute ethnicity the distortions are even more pronounced than the target variable ones revealing a very high probability of composition for the Caucasian ethnicity in the Drug Consumption and Adult datasets In the case of the COMPAS dataset the probabilities are indeed distorted although still not such as to predict at this point of the analysis more severe future distortions which is why more in-depth analysis are required Figure shows the graphical representation of the diverseness module that simplifies the display of prior probabilities In the example is given Table Summary of Diverseness Analysis Results COMPAS Drug Adult Consumption Dataset Caucasian Black Asian Hispanic Native American Other White/Black White/Asian Black/Asian Amer-Indian-Eskimo Asian-Pac-Islander Figure Example of Diverseness graphic visualization the notation for a dataset where both the levels of the target variable and those of the protected attribute ethnicity are equiprobable Inclusiveness This module aims to analyze the inclusiveness of the data available by estimating the simultaneously probabilities They determine the probability that training set will simultaneously display two by two the target variable and the protected attribute properties For instance in our case study the module highlights the probability that in training set the property Asian appears simultaneously with property success Summary results for diverseness module are shown in Table The results of this module show that the probability that two properties will occur simultaneously is related to the sample size Evidence of this can be found in the results of the Drug Consumption and Adult datasets where the highest probabilities of simultaneous events involve the Caucasian property The COMPAS dataset shows quite homogeneous probabilities especially with regard to the Black property while for the Caucasian property the highest probabilities are related to the simultaneous occurrence Table Summary of Inclusiveness Analysis Results COMPAS Drug Adult Consumption Dataset aie Asian Black Black/Asian Caucasian Hispanic Other White/Asian White/Black AIE Asian API Black Black/Asian Caucasian Hispanic NA Other White/Asian White/Black American-Indian/Eskimo Asian-Pac-Islander Native American Figure Example of Inclusiveness graphic visualization with the Non-recidivist property Since the simultaneous probabilities depend on the number of examples within the available data and the sample size this result alone is not sufficient to establish a priori the certain presence of serious data distortions although some evidence can already be seen Figure shows the graphical representation of the inclusiveness module that simplifies the display of simultaneously probabilities In the example is given the notation for a dataset where all the properties of the target variable and those of the protected attribute ethnicity are equiprobable Training Likelihood This module aims to analyze the training likelihood of the data available by estimating the posterior probabilities They determine the probability that in the training set the occurrence of the properties of the protected attribute is given by the properties of the target variable and vice versa For example in the COMPAS dataset they determine the probability that the occurrence of reoffending is given by the properties of the protected attribute ethnicity Summary results for training likelihood module are shown in Table The results of this module show that the posterior probabilities of target variable and protected attribute ethnicity are quite skewed in all dataset In the case of the Adult dataset given as occurred event or event the probability of occurrence of the Caucasian ethnic group is respectively and ie very high for both events while the probabilities of all other ethnic groups conditioned to the target variable are all significantly lower this means that the original data contain many examples of individuals belonging to the Caucasian ethnic group In the case of Drug Consumption a similar reasoning can be carried out for the ethnicity probabilities conditioned to the target variable moreover notice that given the property Black/Asian the probability of occurrence of event i e that the individual is a consumer is while the probability of is which means that in the available data there are no examples of individuals belonging to the ethnic group Black/Asian showing a positive outcome i e negative examples Figures and shows the graphical visualization of our data annotation system for the COMPAS dataset The analysis of the COMPAS dataset shows that if an individual is randomly sampled from the original data for the training set the probability that this individual is black knowing that the re-offending property has occurred ie knowing the outcome of the re-offending event is while the probability that the individual is white knowing that the re-offending property has occurred is Instead given as occurred the property Black the probability that the individual has not reoffended is while the probability that the individual has reoffended is given the property Caucasian the probability that the individual has not reoffended is while the probability that the individual has reoffended is that is significantly lower This means that in this dataset the reoffending is related to ethnicity and that success or failure are determined by the membership to a specific ethnic group The differences in probability between the properties highlight the risk of future bias and in the case of the COMPAS dataset they anticipate the underestimation of recidivism for the Caucasian ethnic group and the overestimation of recidivism for the Black ethnic group proven in recent studies Final Remarks in traditional sampling practices instead of observing all the units of a population only a subset of a population is detected which must show certain probabilistic characteristics In machine learning models the training set is sampled not from the real population but from the available data While in classical sampling the empirical knowledge alone is effectively of a sample nature in machine learning systems the available data are often of sample nature too precisely due to the fact that it is not possible to make assumptions on the real population Considering a random sampling from the Table Summary of Training Likelihood Analysis Results COMPAS Drug Adult Consumption Dataset aie Asian api Black Black/Asian Caucasian Hispanic Other White/Asian White/Black AIE Asian API Black Black/Asian Caucasian Hispanic NA Other White/Asian White/Black AIE AIE Asian Asian API API Black Black Black/Asian Black/Asian Caucasian Caucasian Hispanic Hispanic NA NA Other Other White/Asian White/Asian White/Black White/Black American-Indian/Eskimo Asian-Pac-Islander Native American available data we have shown that the probability of composition of the training set can be predicted highlighting that the structure of the data directly affects the probability of properties distribution Figure Data annotation visualization for COMPAS dataset we analyzed three datasets frequently accessed by machine learning community Of these all three showed more or less pronounced distortions for the protected attribute Ethnicity Although the COMPAS dataset is the sole one that has been shown to discriminate against black people the Drug Consumption and Adult datasets reveal possible future bias in the detriment of ethnic minorities RELATEDWORK Although there are a number of papers that for ethical purposes deal with data annotation they are all very recent indicating that Figure Data annotation visualization for COMPAS dataset this field of study is still partially explored and has only recently received considerable attention Our contribution differs from the others because it induces a probabilistic reasoning on the causes of model discrimination based on sampling problems our intention is to deepen the knowledge of data validation analysis focusing on the meaning of probabilities From a graphical point of view our work has been inspired by the Data Nutrition Labels a data labeling system mainly based on descriptive data statistics A similar approach is addressed in where an operational framework is proposed to identify the bias risks of automatic decision systems In the authors propose a data labeling system based on discursive data sheets In the authors propose a collaborative crowdsourcing system to improve the quality of the labels Since ethically data annotation represent a quite new field of study there are several works that provide different types of labels We believe that at present the focus should not be on achieving a unified data annotation system in the short term but rather on the fact that the fair machine learning community is working together to focus attention on the data collection problem Especially because awareness of data issues is often not rooted outside of this community It is important that this field and this work inspire greater awareness of the possible causes of discrimination due to the fundamental ingredient that all users and designers of machine learning systems from the most to the least experienced use data CONCLUSIONS The purpose of the current study was to detect the potential race discriminatory risk for future machine learning system by providing a data annotation system based on Bayesian Inference Our notation serves as a diagnostic framework to immediately visualize data appropriateness and potential bias occurring when sampling the training set from an available dataset The investigation of the probabilities of the training set sampling has shown that it is possible to establish a risk of future bias by observing prior and posterior probabilities of the ethnicity and target variable properties The empirical findings in this study provide a new perspective on data annotation practices by showing that Bayesian inferences may reveal the risk of bias in three different widespread dataset Furthermore this study has raised important questions about the awareness of most widely data sampling practices in machine learning community The findings of this investigation complement those of earlier studies Our data annotation system is limited to the binary case and to the analysis of categorical variables for classification tasks This would be a fruitful area for further work Our intent is to expand the work in the following directions i extend the notation to multiple protected attributes the probabilities of the training set will then be given by the vectors of the protected attribute combinations ii extend the notation to the non-binary case for prediction tasks involving regression analysis for example iii extend the probabilistic notation to non-labeled data