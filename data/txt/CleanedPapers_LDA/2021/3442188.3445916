Censorship of Online Encyclopedias Implications for NLP Models While artificial intelligence provides the backbone for many tools people use around the world recent work has brought to attention that the algorithms powering AI are not free of politics stereotypes and bias While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination very little work has studied how governments actively shape training data We describe how censorship has affected the development of Wikipedia corpuses text data which are regularly used for pre-trained inputs into NLP algorithms We show that word embeddings trained on Baidu Baike an online Chinese encyclopedia have very different associations between adjectives and a range of concepts about democracy freedom collective action equality and people and historical events in China than its regularly blocked but uncensored counterpart Chinese language Wikipedia We examine the implications of these discrepancies by studying their use in downstream AI applications Our paper shows how government repression censorship and self-censorship may impact training data and the applications that draw from them CONCEPTS Computing methodologies Supervised learning by classification Information systems Content analysis and feature selection Social and professional topics Political speech KEYWORDS word embeddings censorship training data machine learning INTRODUCTION Natural language processing NLP as a branch of artificial intelligence provides the basis for many tools people around the world use daily NLP impacts how firms provide products to users content individuals receive through search and social media and how individuals interact with news and emails Despite the growing importance of NLP algorithms in shaping our lives recently scholars policymakers and the business community have raised the alarm of how gender and racial biases may be baked into these algorithms Because they are trained on human data the algorithms themselves can replicate implicit and explicit human biases and aggravate discrimination Additionally training data that over-represents a subset of the population may do a worse job at predicting outcomes for other groups in the population When these algorithms are used in real world applications they can perpetuate inequalities and cause real harm While most of the work in this area has focused on bias and discrimination we bring to light another way in which NLP may be affected by the institutions that impact the data that they feed off of We describe how censorship has affected the development of online encyclopedia corpuses that are often used as training data for NLP algorithms The Chinese government has regularly blocked Chinese language-wikipedia from operating in China and mainland Chinese Internet users are more likely to use an alternative Wikipedia-like website Baidu Baike The institution of censorship has weakened Chinese language Wikipedia which is now several times smaller than Baidu Baike and made Baidu Baike which is subject to pre-censorship an attractive source of training data Using methods from the literature on gender discrimination in word embeddings we show that Chinese word embeddings trained with the same method but separately on these two corpuses reflect the political censorship of these corpuses treating the concepts of democracy freedom collective action equality people and historical events in China significantly differently After establishing that these two corpuses reflect different word associations we demonstrate the potential real-world impact of training data politics by using the two sets of word embeddings in a transfer learning task to classify the sentiment of news headlines We find that models trained on the same data but using different pre-trained word embeddings make significantly different predictions of the valence of headlines containing words pertaining to freedom democracy elections collective action social control political figures the CCP and historical events These results suggest that censorship could have downstream effects on AI applications which merit future research and investigation Our paper proceeds as follows We first describe the background of how Wikipedia corpuses came to be used as training data for word embeddings and how censorship impacts these corpuses Second we describe our results of how word associations from Wikipedia and Baidu Baike word embeddings differ on concepts that pertain to democracy equality freedom collective action and historical people and events in China Last we show that these embeddings have downstream implications for AI models using a sentiment prediction task PRE-TRAINED WORD EMBEDDINGS AND WIKIPEDIA CORPUSES NLP algorithms rely on numerical representations of text as a basis for modeling the relationship between that text and an outcome Many NLP algorithms use word embeddings to represent text where each word in a corpus is represented as a k-dimensional vector that encodes the relationship between that word and other words through the distance between them in k-dimensional space Words that frequently co-occur are closer in space Popular algorithms such as Glove and WordVec are used to estimate embeddings for any given corpus of text The word embeddings are then used as numerical representations of input texts which are then related through a statistical classifier to an outcome In comparison to other numerical representations of text word embeddings are useful because they communicate the relationships between words The bag-of-words representation of text which represents each word as simply being included or not included in the text does not encode the relationship between words each word is equidistant from the other Word embeddings on the other hand communicates to the model which words tend to co-occur thus providing the model with information that words like purse and handbag as more likely substitutes than purse and airplane Word embeddings are also useful because they can be pre-trained on large corpuses of text like Wikipedia or Common Crawl and these pre-trained embeddings can then be used as an initial layer in applications that may have less training data Pre-trained word embeddings have been shown to achieve higher accuracy faster While training on large corpuses is expensive companies and research groups have made available pre-trained word embeddings typically on large corpuses like Wikipedia or Common Crawl that can then be downloaded and used in any application in that language The motivation behind using pre-trained word embeddings is that they can reflect how words are commonly used in a particular language Indeed Spirling and Rodriguez show that pretrained word embeddings do surprisingly well on a Turing test where human coders often cannot distinguish between close words produced by the embeddings and those produced by other humans To this end Wikipedia corpuses are commonly selected to train word embeddings because they are user-generated open-source cover a wide range of topics and are very large At the same time as pre-trained embeddings have become popular for computer scientists in achieving better performance for NLP tasks some scholars have pointed to potential harms these For example Facebooks provides word embeddings in languages trained on Wikipedia A Google Scholar search of pre-trained word embeddings and Wikipedia returns over search results as of January embeddings could create by encoding existing biases into the representation The primary concern is that embeddings replicate existing human biases and stereotypes in language and using them in downstream applications can perpetuate these biases see Sun et al for a review Caliskan et al show that word embeddings reflect human biases in that associations of words in trained word embeddings mirror implicit association tests Using simple analogies within word embeddings Bolukbasi et al Garg et al and Manzini et al show that word embeddings can encode racial and gender stereotypes While these word associations can be of interest to social science researchers they may cause harm if used in downstream tasks More generally research in machine learning has been criticized for not paying enough attention to the origin of training datasets and the social processes that generate them Imbalances in the content of training data have been shown to create differential error rates across groups in areas ranging from computer vision to speech recognition Some scholars have argued that training datasets should be representative of the population that the algorithm is applied to CENSORSHIP OF CHINESE LANGUAGE WIKIPEDIA AND IMPLICATIONS FOR CHINESE LANGUAGE NLP We consider another mechanism through which institutional and societal forces impact the corpuses that are used to train word embeddings government censorship While we use the example of online encyclopedias and word embeddings to make our point its implications are much more general Government censorship of social media news and websites directly affects large corpuses of text by blocking users access deleting individual messages adding content through propaganda or inducing self-censorship through intimidation and laws While Wikipedias global reach makes it an attractive corpus for training models in many different languages Wikipedia has also been periodically censored by many governments including Iran China Uzbekistan and Turkey China has had the most extensive and long-lasting censorship of Wikipedia Chinese language Wikipedia has been blocked intermittently ever since it was first established in Since May all of Chinese language Wikipedia has been blocked by the Great Firewall of China More recently not just Chinese language Wikipedia but all language versions of Wikipedia have been blocked from mainland China Censorship has weakened Chinese language Wikipedia by decreasing the size of its audience Pan and Roberts estimate that the block of Chinese language Wikipedia in decreased page views of the website by around million views per day Zhang and Zhu use the block of Wikipedia to show that the block decreased views of Chinese language Wikipedia which in turn decreased user contributions to Wikipedia not only from blocked users in mainland China but also from unblocked users what had fewer incentives to contribute after the block While mainland Chinese Internet users can access Chinese language Wikipedia with a Virtual Private Network VPN evidence suggests that very few do Censorship of Chinese language-wikipedia has strengthened its unblocked substitute Baidu Baike A similar Wikipedia-like website Baidu Baike as of boasted million entries times larger than Chinese language Wikipedia Yet as with all companies operating in China Baidu Baike is subject to internal censorship that impacts whether and how certain entries are written While edits to Chinese language-wikipedia pages are posted immediately any edits to Baidu Baike pages go through pre-publication review While editors of Wikipedia can be anonymous editors of Baidu Baike must register their real names Additional scrutiny is given to sensitive pages such as national leaders political figures political information and the military where Baidu Baike regulations stipulate that only government media outlets such as Xinhua and Peoples Daily can be used as sources Pre-censorship of Baidu Baike affects the types of pages available on Baidu Baike and the way these pages are written While its impossible to know without an internal list the extent to which missing pages in Baidu Baike are a direct result of government censorship a substantial list of historical events covered on Chinese language Wikipedia including June th Incident and Democracy Wall and well-known activists such as Chen Guangcheng and Wuerkaixi have no Baidu Baike page For example when we attempted to create entries on Baidu Baike such as June Fourth Movement or Wuerkaixi we were automatically returned an error Perhaps because of the size difference between the two corpuses increasingly researchers developing cutting edge Chinese language NLP models are drawing on the Baidu Baike corpus Baidu Baike word embeddings have been shown to perform better on certain tasks Here we assess the downstream implications of this choice on the representation of democratic concepts social control and historical events and figures First we follow Caliskan et al to compare the distance between these concepts and a list of adjectives and sentiment words Then we show the downstream consequences of the choice of corpus on a predictive task of the sentiment of headlines DISTANCE FROM DEMOCRACY COMPARISON BETWEEN BAIDU BAIKE and wikipedia EMBEDDINGS In this section we consider the differences in word associations among word embeddings trained with Chinese language-wikipedia and Baidu Baike We use word embeddings made available by Li et al Li et al train -dimensional word embeddings on both Baidu Baike and Chinese language Wikipedia using the same algorithm WordVec For a benchmark we also compare these two sets of embeddings to embeddings trained on articles from the Peoples Daily from the Chinese governments mouthpiece To evaluate word associations we follow Caliskan et al and Rodman to compare the distance between a set of target words See instructions at Also trained by Li et al and made available at Chinese-Word-Vectors and attribute words to establish their relationships in each embedding space Figure gives a simplified graphical representation of the evaluation procedure in a -dimensional space In this simple example we might be interested in the position of a target word a concept we are interested in relative to a positive attribute word and a negative attribute word For example we can evaluate whether democratic concepts are represented more positively or negatively by comparing the angle between the vector for the target word Democracy in black and a positive attribute word Stability as well as a negative attribute word Chaos both in blue Figure Example of Word Embedding Comparison In Figure Democracy in word embedding A has a more positive connotation than in word embedding B because the relative position of the word Democracy in embedding A with respect to the positive attribute word Stability and the negative attribute word Chaos is closer to the positive attribute word than Democracy is in embedding B To minimize the particularities of a single word and hence the variability of the result we repeat this evaluation procedure across multiple target words representing the same concept eg democracy and compare them with multiple attribute words In the next sections we explain how we select target words attribute words how we pre-process the embedding space and our results Identifying Target Words We begin by delineating the categories of interest In general there are two broad categories we are interested in democratic political concepts and ideas and known targets of propaganda Based on past work we know entries that fall under these categories have been the target of content control on Baidu Baike Additionally the first category captures ideas that we think are normatively desirable but discouraged in China The second category captures the extent that the embeddings are consistent with propaganda For the first category we include Democratic values in particular freedom and equality of rights Procedures of democracy in particular features pertaining to elections Channels for voicing preferences in the form of collective actions such as protests and petitions For the second category we include Social control especially concepts related to repression and surveillance The Chinese Communist Party CCP and related features Significant historical events in China that involved the CCP such as the Cultural Revolution Important figures who are extolled by the CCP Figures who are denounced by the CCP such as political dissenters For each of these categories we do not want to select only one target word of interest but rather a group of related words that all cover the same concept We select a group of target words that represent this category as follows For categories other than historical events and negative figures we first select a Chinese word that most closely represents the category of interest For example for the category of procedures of democracy the Chinese word election is selected We then calculate the cosine similarity of the representative word with all other words from the word embedding spaces Wikipedia Baidu Baike From each corpus we select words that are closest to the representative word words with the highest cosine similarity Of the words closest to the representative word for each category we include all words that could be thought to be synonymous or a subset of the more general category We drop those that are domain specific for example of the words for the category of procedures of democracy we dropped the word Japanese Diet which is specific to the Japanese political system For categories on historical events and negative figures we simply used the name of the person or of the historical event The full list of words for each category is presented in Appendix D We opt for the data-driven approach in and to select target words in order to limit researcher degree of freedom Furthermore the selection of representative words in and the pruning of synonyms in were done by three native Chinese speakers to ensure the selected words provide good coverage of how the categories of interest are discussed in the Chinese context Selecting Attribute Words We use two strategies for selecting attribute words First we draw on the literature on propaganda in China to select a set of positive and negative words that would be consistent with what we know about CCP propaganda narratives As scholars of propaganda have pointed out the CCP has actively tried to promote the image of itself and Chinas political system as stable and prosperous while characterizing Western democratic systems as chaotic and in economic decline Therefore for our first set of words which we call Propaganda Attributes Words positive words include synonyms of stability and prosperity while negative attribute words include synonyms of chaos decline and instability The full list for the set of propaganda attribute words is presented in Appendix E For the second set of words we are interested in whether the target words are more generally evaluated differently between the We asked three Chinese speakers to independently come up with the representative words and had them agree on a single word for each category This step was done before analysis was performed two corpuses To test this we make use of a dictionary of evaluative words specifically designed for Chinese natural language processing The dictionary codes whether an evaluative word is positive negative or neutral We follow the preprocessing instructions by Wang and Ku by dropping all neutral words and only using the list of positive and negative evaluative words A sample of the set of evaluative words is presented in Appendix For subsequent discussions we refer to this list of attribute words as the Evaluative Attribute Words Pre-processing Word Embedding Spaces There are two notable challenges when comparing different word embeddings One word embeddings produced by stochastic algorithms such as WordVec will embed words in non-aligned spaces defined by different basis vectors This precludes naive comparison of word distances across distinct corpuses If the centroids of the two word embeddings are different then using cosine similarity ie the cosine of the angle between two vectors to compare word associations across different corpuses can yield uninterpretable result Figure presents a simplified example of this problem One word embedding by virtue of being further away from the origin yields a smaller angle between the two vectors even though the relative positions of the two vectors in the two word embeddings are the same To solve this problem we standardize the basis vectors of each word embeddings by subtracting the means and dividing by the standard deviations of the basis vectors so that each word embedding is centered around the origin with dimension length Figure Nonalignment between Two Word Embeddings Another problem is that word embeddings trained on different corpuses can have different vocabulary This precludes us from comparing words that appear in one word embedding but are not present in the other word embedding Because of this we only keep the intersection of the vocabularies of word embeddings As a result six target words were dropped in the comparison between Wikipedia- and Baidu Baike-trained word embeddings and five target words were dropped in the comparison between Wikipedia and Peoples Daily-trained word embeddings Expectations We expect ideas that are normatively appealing but discouraged in China to be portrayed more negatively in Baidu Baike We expect figures who are denounced by the CCP to be portrayed more negatively in Baidu Baike On the other hand we expect categories that are targets of positive propaganda to be portrayed more positively in Baidu Baike Overall we expect that censorship and curation of Baidu Baike will mean that the words we are interested in will be treated similarly in Baidu Baike and state media outlet The Peoples Daily A summary of our theoretical expectations is presented in Table below Table Theoretical Expectations Category Sign Freedom Democracy Election Collective Action Negative Figures Social Control Surveillance CCP Historical Events Positive Figures Note Negative sign indicates Baidu Baike and Peoples Daily are less favorable than Wikipedia and positive sign indicates that Baidu Baike and Peoples Daily are more favorable than Wikipedia Limitations Through this design we test whether there are differences between word embeddings trained on Chinese language-wikipedia and those trained on Baidu Baike in topics where there is evidence of censorship on Baidu Baike While we think the evidence we produce is suggestive that censorship impacts the placement of the word embeddings we cannot isolate the effect of censorship outside of other differences that may exist between Baidu Baike and Chinese language Wikipedia Isolating the effect of censorship is difficult in part because censorships influence is pervasive affecting the content not only through pre-publication review but also likely through the propensity for individuals to become editors and the information that they have and are willing to contribute This makes it very difficult to establish a counterfactual of what the content on Baidu Baike would have looked like without censorship We believe Chinese language Wikipedia is the closest approximation to this counterfactual Results Following Caliskan et al we use a randomization test with onesided p-value to compare how words in each category are represented in Wikipedia Baidu Baike and Peoples Daily Formally let be the set of word vectors for the target words from embedding and respectively Let Table Wikipedia vs Baidu Baike Propaganda Attributes Evaluative Attributes effect size p-value effect size p-value Freedom Democracy Election Collective Action Negative Figures Social Control Surveillance CCP Historical Events Positive Figures be the two sets of word vectors for the attribute words with being the set of positive attributes and being the set of negative attributes Subscript again denotes the embedding that the word vectors are from Let cos denote the cosine of the angle between vectors and The test statistic is where mean mean Let denotes the set of all possible randomization realizations of assignment of word vector to embedding The onesided p-value of the permutation test is We present the effect size of the difference in word associations across word embeddings defined as mean mean std Conventional cutoffs for small medium and large effect sizes are and respectively The comparisons between Wikipedia and Baidu Baike word embeddings and between Wikipedia and Peoples Daily word embeddings are presented in Table and Table respectively Across most categories and for both sets of attribute words the differences in word embeddings are in line with our theoretical expectations Table indicates that for categories Freedom Democracy Election Collective Action and Negative Figures word embeddings trained with Baidu Baike display a more negative connotation than embeddings trained with Wikipedia For categories Social Control Surveillance CCP and Historical Events word embeddings trained with Baidu Baike display a more positive connotation than embeddings trained with Wikipedia The effect sizes indicate substantial differences for target words that are related to democracy and those that are targets of propaganda This is consistent across both set of attribute words and across the two comparisons In Table we show that the effect sizes when comparing Wikipedia and Baidu Baike are similar to comparing Wikipedia with the government publication The Peoples Daily Table Wikipedia vs Peoples Daily Propaganda Attributes Evaluative Attributes effect size p-value effect size p-value Freedom Democracy Election Collective Action Negative Figures Social Control Surveillance CCP Historical Events Positive Figures While most categories accord with our expectations one in particular deserves further explanation Negative figures including activists and dissidents who the CCP denounces are only more significantly associated with negative words on Baidu Baike and Peoples Daily in one instance and even have a positive effect size comparing Baidu Baike to Wikipedia in Table It is likely that because of censorship there is very little information about these figures in the Baidu Baike and Peoples Daily corpuses so their word embeddings do not show strong relationships with the attribute words To examine this we used Google Search to count the number of pages on Chinese language Wikipedia and Baidu Baike that link to each negative figure Out of negative figures Chinese language Wikipedia has more page links to two thirds of them even though Chinese language-wikipedia is times smaller Therefore the uncertainty around the result we have for negative figures may be a result of lack of information about these individuals in Baidu Baike APPLICATION SENTIMENT ANALYSIS OF NEWS HEADLINES In this section we demonstrate that the differences we detected in word embeddings have tangible effect on downstream machine learning tasks To do this we make use of the pre-trained word embeddings on each of the different corpuses as inputs in a larger machine learning model that automatically labels the sentiment polarity of news headlines We chose the automated classification of news headlines because machine learning based on news headlines is used in recommendation systems for social media news feeds and news aggregators as well as for analysts using automated classification of news to make stock price and economic predictions We show that using the pre-trained word embeddings from Baidu Baike and Chinese language Wikipedia with identical training data produces sentiment predictions for news headlines that differ systematically across our categories of interest For example Data and Method We imagine a scenario where the task is to label the sentiment of news headlines where the model is trained on a large general sample of news headlines We then examine the performance of this model on an oversample of headlines that include our target words This allows us to evaluate how a general news sentiment classifier performs on words that are politically balanced in China varying the origin of the pre-trained embeddings but holding constant the sentiment labels in the training and test sets For the training set we randomly select headlines from the TNEWS dataset The TNEWS dataset contains Chinese news headlines of various categories It is part of the Chinese Language Understanding Evaluation CLUE Benchmark and is widely used as the training data for Chinese news classification models For each of the randomly selected headlines we label each news headline as positive negative or neutral in line with the general sentiment of the headline For our training set from the TNEWS dataset we have headlines with positive sentiment with negative sentiment and with neutral sentiment For the test set we collect Chinese news headlines that contain any of our target words from Google News For each of the target words we collect up to news headlines Because some target words yield only a handful of news headlines we collected news headlines in total out of target words Data collection was done in July and August of Using the exact same coding scheme as the training set we label these headlines as positive negative or neutral The test set contains headlines with positive sentiment with negative sentiment and with neutral sentiment We preprocess the news headlines by removing punctuation numbers special characters the names of the news agency if they appear on the headline and duplicated headlines To convert the news headlines into input for machine learning models we first use a Chinese word segmentation tool to segment each news headline into a sequence of words We then look up the word embedding for each word in the sequence Following a conventional approach we take the average of the pre-trained word embeddings of the words in a given news headline to represent each headline Any word that does not have a corresponding word embedding in the WordVec models is dropped This leaves us with three different representations of the headlines one for Baidu Baike one for Chinese language Wikipedia and one for the Peoples Daily With each of these three different representations of the text based on different pre-trained embeddings we train three machine learning models Naive Bayes NB support vector machines SVM and TextCNN For each model we use identical training labels from the TNEWS dataset This yields a total of nine models with three for each pre-trained word embeddings Each trained model is then used to predict sentiment labels on the test set Because of the For more details about the TNEWS dataset see Appendix duplicated news headlines are dropped resulting in headlines in total duplicated news headlines are dropped resulting in headlines in total Because headlines with neutral labels are more noisy and given the difficulty of training a three-class classifier with limited training data we report results in the main text based on models that are trained with only positive and negative headlines We report results with neutral headlines included in the Appendix Our substantive conclusions are largely intact stochastic nature of TextCNN the TextCNN results are averaged over runs for each model We compare different trained models of the same architecture NB SVM or TextCNN by looking at the mis-classifications for each category of target words Intuitively a model that is pre-disposed to associate more positive words with a certain category of headlines will have more false-positives eg negative headlines misclassified as positive whereas a model that is pre-disposed to associate more negative words with a certain category of headlines will have more false-negatives eg positive headlines mis-classified as negative Because the overall mis-classification rate may differ for headlines of different target words we use a linear mixed effects model to compare the different embeddings allowing headlines of different target words to have different intercepts More formally let be a list of human-labeled sentiment scores for headlines containing target word in category Let and be the predicted sentiment scores from model and for the same headlines We estimate the linear mixed effects model for each category of news headlines by where the outcome variable is a vector of difference in classifications against human labels is a vector of random intercepts corresponding to headlines of each target word in category is an indicator variable for model as opposed to and is the coefficient of interest Results Before turning to the results of the impact of pre-trained embeddings on the predicted classifications of the model we report the overall accuracy of each of the models on the test set in Table Overall TextCNN performs the best out of the three models However within models no set of pre-trained word embeddings performs better than the other they all perform quite similarly Table Model Accuracy in Test Set Model Accuracy Naive Bayes Baidu Baike Wikipedia SVM Baidu Baike Wikipedia TextCNN Baidu Baike Wikipedia Even though the selection of pre-trained embeddings does not seem to impact overall accuracy the pre-trained embeddings do influence the false positive and false negative rates of different categories of headlines In Table we show the comparison of Baidu Baike and wikipedia where Baidu Baike is model and wikipedia is model This means from Equation is for category if the model were trained with Baidu Baike word embeddings and for Wikipedia A negative coefficient indicates that on average Baidu Baike rates this category more negatively than Wikipedia A positive coefficient indicates that on average Baidu Baike rates this category as more positive than Wikipedia Table Baidu Baike vs Wikipedia Naive Bayes SVM TextCNN estimate p-value estimate p-value estimate p-value Freedom Democracy Election Collective Action Negative Figures Social Control Surveillance CCP Historical Events Positive Figures The results are largely consistent with what we found in Section Overwhelmingly Wikipedia predicts headlines that contain target words in the categories of freedom democracy election and collective action to be more positive In contrast Baidu Baike predicts headlines that contain target words of figures that the CCP views positively to be more positive The exceptions to our expectations are the categories of social control surveillance CCP and historical events where we cannot reject the null of no difference between the two corpuses although they do not go against our expectations We find similar results for the comparison between Peoples Daily and Chinese language Wikipedia in Table Table Peoples Daily vs Wikipedia Naive Bayes SVM TextCNN estimate p-value estimate p-value estimate p-value Freedom Democracy Election Collective Action Negative Figures Social Control Surveillance CCP Historical Events Positive Figures To provide intuition Figure shows examples of headlines labeled differently between model trained with Baidu Baike pre-trained embeddings and model trained with Chinese language Wikipedia in our test set The model trained with Baidu Baike pre-trained word embedding labeled Tsai Ing-wen Hope Hong Kong Can Enjoy Democracy as Taiwan Does as negative while Wikipedia and humans labeled this headline as positive The difference in these predictions do not stem from the training data which is the same or the model which is the same Instead the associations made within the pre-trained word embeddings drive these differences Example 蔡英文盼台湾享有的民主自由香港也可以有 Tsai Ing-wen Hope Hong Kong Can Enjoy Democracy as Taiwan Does Baidu Baike Label Wikipedia Label Human Label Example 封杀文化席卷欧美自由反被自由误 Cancel Culture Spreading through the Western World Is It the Fault of Freedom Baidu Baike Label Wikipedia Label Human Label Example 共产暴政录抗美援朝真相 Communist Tyranny The Truth about Chinese Involvement in the Korean War Baidu Baike Label Wikipedia Label Human Label Example 香港国安法中国驻港部队司令强硬表态维稳 Hong Kong Security Law PLA Hong Kong Garrison Commander Takes Tough Stance in Support of Stability Maintenance Baidu Baike Label Wikipedia Label Human Label Figure Examples of Headlines Labeled Differently By Naive Bayes Models Trained with Baidu Baike and Wikipedia CONCLUSION The extensive use of censorship in China means that the Chinese government is in the dominant position to shape the political content of large Chinese language corpuses Even though corpuses like Chinese language Wikipedia exist outside of the Great Firewall they are significantly weakened by censorship as shown by the smaller size of Chinese language Wikipedia in comparison to Baidu Baike While more work would need to be done to understand how these discrepancies affects users of any particular application we showed in this paper that political differences reflective of censorship exist between two of the corpuses commonly used to train Chinese language NLP While our work focuses on word embeddings the discrepancies we uncovered likely affect other pretrained NLP models as well such as BERT and ERNIE Furthermore these political differences present a pathway through which political censorship can have downstream effects on applications that may not themselves be political but that rely on NLP from predictive text and article recommendation systems to social media news feeds and algorithms that flag disinformation The literature in computer science has taken on the problem of bias in training data by looking for ways to de-bias it for example through data augmentation de-biasing word embeddings and adversarial learning However it is unclear how to think about de-biasing attitudes toward democracy freedom surveillance and social control What does unbiased look like in Although methods for de-biasing have also been shown to often be inadequate these circumstances and how would one test it The only way we can think about an unbiased training set in this circumstance is one where certain ideas are not automatically precluded from being included in any given corpus But knowing what perspectives have been omitted is difficult to determine and correct after the fact