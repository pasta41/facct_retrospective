Towards Fair Deep Anomaly Detection Anomaly detection aims to find instances that are considered unusual and is a fundamental problem of data science Recently deep anomaly detection methods were shown to achieve superior results particularly in complex data such as images Our work focuses on deep one-class classification for anomaly detection which learns a mapping only from the normal samples However the non-linear transformation performed by deep learning can potentially find patterns associated with social bias The challenge with adding fairness to deep anomaly detection is to ensure both making fair and correct anomaly predictions simultaneously In this paper we propose a new architecture for the fair anomaly detection approach Deep Fair SVDD and train it using an adversarial network to de-correlate the relationships between the sensitive attributes and the learned representations This differs from how fairness is typically added namely as a regularizer or a constraint Further we propose two effective fairness measures and empirically demonstrate that existing deep anomaly detection methods are unfair We show that our proposed approach can remove the unfairness largely with minimal loss on the anomaly detection performance Lastly we conduct an in-depth analysis to show the strength and limitations of our proposed model including parameter analysis feature visualization and run-time analysis CONCEPTS Computing methodologies Machine learning algorithms KEYWORDS machine learning algorithmic fairness anomaly detection deep learning adversarial learning INTRODUCTION Anomalies are the unusual unexpected surprising patterns in the observed world that warrant further investigation Classic work defines outliers as an observation that deviates so significantly from other observations as to arouse suspicion that a different mechanism generated it Anomalies and outliers are often used interchangeably though we note that some use the term differently and for this paper we use the term anomalies The goal of an anomaly detection algorithm is given a set of instances to determine which instances stand out as being dissimilar to other instances Effective detection of anomalies can be used for various applications such as stopping malicious intruders fraud detection system health monitoring and medical image analysis Recent algorithmic developments have proposed many novel deep learning methods for anomaly detection This previous works on deep anomaly detection are typically unsupervised eg assume all training data are from the normal group and have demonstrated better anomaly detection performance than traditional anomaly detection approaches One popular approach to deep anomaly detection is the deep support vector data description deep SVDD This work attempts to transform the input data into a new feature space where all the points are closely clustered into a predetermined center Hence by definition those points that cannot be projected to be close to the center are deemed anomalies The anomaly scores are calculated based on the Euclidean distances between the test instances and the predetermined center during the test time Deep SVDD is a general approach which can be applied to both low dimensional and high dimensional data In this first paper on the topic we focus on adding fairness to deep SVDD Since anomaly detection is often applied to humans who are then suspected of unusual behavior ensuring fairness becomes paramount The notion of fairness has recently received much attention in supervised learning and unsupervised learning Measures of fairness can generally be divided into two categories i group-level fairness and ii individual level fairness In anomaly detection problems we divide the data into two groups which are the normal group and the abnormal group We propose to study the group-level fairness problems which ensure that no one particular group contains a disproportionate number of individuals with protected status To our best knowledge there is no prior published work on fairness in the context of deep anomaly detection though work on auditing ie checking anomaly detection algorithms exist A Motivating Example For Group-Level Fairness Consider the example of finding anomalies by applying deep SVDD to facial images The top normal instances and top abnormal instances are shown in Figure These pictures are from the celebA celebrity data set which we introduce in section The deep SVDD model is trained on attractive celebrity faces normal group and used to detect plain celebrity faces abnormal group where the labels are given in the data set The model performs well in terms of the anomaly detection quality as most attractive celebrity faces and plain celebrity faces are separated correctly However when we consider the protected status variable gender in this problem more females are predicted to be attractive normal group and more males are predicted as plain abnormal group Moreover if we a Normal Group b Abnormal Group Figure Motivating example of the need for group-level fairness in deep anomaly detection problem We visualize the top normal instances and top abnormal instances discovered by deep SVDD on celebA data set We see that the normal group is dominated by females while the abnormal group is dominated by males consider race as a protected status variable we can see that the most attractive faces are white people and many black people in the abnormal group Motivated by these observations we aim to design experiments to examine the fairness of existing deep anomaly detection methods quantitatively and propose a fair anomaly detection model to balance the number of instances with different sensitive attribute values in the anomaly predictions In this paper we present the Deep Fair Support Vector Data Description Deep Fair SVDD model which learns a compact and fair description of the normal data via adversarial learning We summarize the main contributions in this paper as follows We show existing deep anomaly detection approaches are unfair see section due to the deep learners ability to extract out complex features We consider fair anomaly detection in the context of deep representation learning To the best of our knowledge this is an under-studied so far and challenging due to the need for fair and high-quality predictions We address these challenges by proposing a novel fair anomaly detection architecture see Figure and use adversarial learning to remove the unfairness The idea of using adversarial learning contrasts with many recent works on fairness in learning which typically encodes fairness as a regularization term or a constraint We propose two measures of group-level fairness for deep anomaly detection problems i A demographic parity motivated fairness measure for the abnormal group equation ii A parameter-free measure based on Wasserstein distance for calculating the overall fairness equation We demonstrate our method on several types of data including traditional tabular datasets face data sets and digit images We study the fairness problem concerning gender racism and the source of the visual objects see section We find that introducing fairness causes a marginal drop in anomaly detection performance measured by the AUC score see section We conduct an in-depth analysis of our proposed model to show our proposed models strengths and limitations including parameter analysis feature visualization and runtime analysis see section Our paper structure is as follows In the next section we discuss the related work Then we provide background knowledge about deep SVDD and our fairness measures in section Next we propose the deep fair SVDD model and analyze how we use adversarial networks to tackle fair anomaly detection problems section Finally we perform experiments on real-world data sets to demonstrate the effectiveness of our method in section and conclude our proposed approach in section RELATEDWORK Deep Anomaly Detection We first outline related works on deep anomaly detection One of the most common deep anomaly detection approaches is reconstruction-based methods which assume the anomalies possess different features than the normal instances Hence given a pre-trained autoencoder over the normal instances it will be hard to compress and reconstruct the anomalies The anomaly score in this research is defined as the reconstruction loss for each test instance Inspired by the generative adversarial networks another line of related works score an unseen sample based on the ability of the model to generate a similar one More recently A deep version of support vector data description Deep SVDD has been proposed This work is inspired by kernel-based one-class classification which combines the ability of deep representation learning with the one-class objective to separate normal data from anomalies by concentrating normal data in embedded space while mapping anomalies to distant locations Another recent progress on deep anomaly detection uses self-supervised learning on image data sets and achieves excellent performance For example uses a composition of image transformations and then trains a neural network to predict which transformation was used The anomaly scores are computed based on the predictions confidence over different image transformations given the test samples Fairness in Anomaly Detection With so many works focusing on improving the deep anomaly detection performance our work differentiates from those previous works as we investigate the fairness of the existing deep anomaly detection problems and propose a novel deep fair anomaly detection model to help humans make fair decisions To the best of our knowledge there is no work on deep fair anomaly detection algorithms We now introduce two related works on non-deep fair anomaly detection problems Recent work has studied auditing the output of any anomaly detection algorithm In their work the anomaly detection algorithms output fairness with respect to multiple protected status variables PSVs is measured by finding PSV combinations in the outlier group which are more common than in the normal group Their empirical results show that the output of five classic anomaly detection methods is unfair Another work studies the fairness problem of LOF Local Outlier Factor and proposes several heuristics to mitigate the unfairness within LOF on tabular data sets Differently our work proposes to examine fairness for the deep anomaly detection problems which work for both tabular data and image data Moreover unlike LOF-based approaches that do not learn a model of normality and need access to the PSVs for test instances our proposed model can make out-of-sample predictions without accessing the PSVs Adversarial Learning for Fairness Lastly we introduce the related works which take the advantages of adversarial networks to remove unfairness applies an adversarial training method to satisfy parity for salary prediction This work shows that small amounts of data are needed to train a powerful adversarial model to enforce fairness constraints The work of uses a predictor and adversary with an additional projection term to remove unfairness in both supervised learning tasks and debiasing word embedding tasks shows that demographic information leaks into intermediate representations of neural networks trained on text datasets and applies adversarial learning to mitigate the information leaks takes the advantages of adversarial networks to reduce word vector sentiment bias for demographic identity terms PRELIMINARY Deep Support Vector Data Description Among the recent deep anomaly detection methods we focus on deep SVDD as a base learner because it is not only a popular method but also performs well on both low dimensional tabular data and high dimensional data images Unlike generative models or compression based anomaly detection models which are adapted for anomaly detection deep SVDD is directly learned with an anomaly detection based objective Given the training data of just normal points X the deep SVDD network is trained to map all the n normal points close to a fixed center c where c is normally set as the mean of the points Denote function as a neural network with parameters the simplified objective function is argmin n n i xi c L The second term is a network weight decay regularizer with hyperparameter which prevents finding a too complex mapping function The network has L hidden layers and set of weights are the weights of layer L Deep SVDD contracts the embedding space enclosing the points by minimizing the mean distance of all data points to the center During the evaluation/scoring stage given a test point x Deep SVDD will calculate the anomaly score for x as follows c Note this is just the distance the instance is from the center abnormal points are then those far from the center Notion of Fairness Fairness is measured using protected status variables or sensitive features such as gender and race In this paper we study group-level fairness which ensures that no one particular group contains a disproportionate number of instances of a given protected status Fairness by p -rule Our first notion of fairness is inspired by which proposed a statistical parity motivated measure for a supervised classification model Statistical parity is a popular fairness measure used in many unsupervised learning and supervised learning problems Let t be the anomaly score threshold then the normal groups are points with t and the abnormal groups are points with t Given the protected status variable as z our definition of fairness measure leverages the rule a normal abnormal group partition satisfies the rule if the ratio between the percentage of person with a particular protected status variable value having t and the percentage of person without protected status having t is no less than We define the p -rule as our fairness measure for the anomaly detection problem min t z t z t z t z p Note the p -rule value ranges from to and a larger value indicates the model is fairer In ideal case we have t z t z Maximizing p -rule means predicting x as an anomaly will be independent of the protected status variable z The rationale behind using our first fairness measure in equation is because it is closely related to the rule advocated by the US Equal Employment Opportunity Commission We can determine a deep anomaly detection models fairness using the rule However there are some limitations to our first proposed measurement Firstly we need to know the exact number of anomalies in the test set to correctly set the anomaly score threshold t to partition the normal and abnormal groups Secondly this measure only considers the fairness in the abnormal group Fairness by distribution distance Here we propose a new fairness measure for anomaly detection problems which is invariant of the anomaly score threshold t and covers both normal and abnormal groups We have designed one synthetic anomaly detection problem to show the motivation for our second fairness measure Assume there are two anomaly detection models named A and B The test data includes males and females and the binary sensitive attribute is To be specific the predicted anomaly scores from Model A and B are shown in Figure a and b Given the ground truth number of anomalies as we can set the anomaly score threshold t to predict anomalies with Now we can calculate the p -rule for Model A and Model B as Although models A and B achieve the same fairness measured by p -rule we can learn from the anomaly score distributions in Figure c and d that model predictions are highly correlated with the sensitive attribute gender which is less fair Now we formulate our second definition of fairness which quantifies the difference between each demographic groups anomaly score distributions let P denotes the distribution of the anomaly scores for test instances with sensitive attribute z and Q for test instances with sensitive attribute z We calculate the Wasserstein- Earth-Mover Distance distance between distribution P and Q as fairness by distribution distance measure Ex x y where denotes the set of all joint distributions whose marginals are respectively P Intuitively indicates how much mass must be transported from x to y in order to transform the distribution P to Q For our previous toy example we calculate the Distribution distance for model A and predictions as and These results indicate that model A is overall fairer than model B From a practitioners perspective we can use distribution a Model As Predictions b Model Predictions c Model As Prediction Distribution d Model Prediction Distribution Figure A toy example to show the difference between our proposed two fairness measures Figure a b summaries the statistics of predicted anomaly scores of model A and B Given the ground-truth anomaly score threshold t model A and B have the same fairness by p -rule as Figure c d shows the anomaly score distributions for model As predictions MA FA and model predictions MB FB Model B is more unfair as the anomaly scores are highly correlated with the sensitive attribute gender M The fairness by distribution distance for model A and B MA FA MB FB distance to evaluate the fairness performance for different anomaly detection models and conduct model selection when we have no access to the test set Lastly we will use both Fairness -rule and Fairness by distribution distance measures to evaluate the fairness performance in our experimental section METHODS Learning Overview In this section we propose the deep fair SVDD model for deep anomaly detection problems Following the previous deep anomaly detection works we assume the training data X contains only normal instances Moreover our proposed model requires access to the binary protected status variable Z for each of the training instances X We learn as an encoder network to learn compact descriptions of X ie a mapping to a lower-dimensional space and a classification network to predict protected status variable value z Z based on the learned embedding We train the encoder and discriminator using adversarial training so that we hope the embedding learned via encoder can fool the discriminator Training such a network is challenging and we take advantage of adversarial learning since it has shown promising results for other fairness tasks such as removing unfairness in NLP applications We use adversarial learning to de-correlate the relationships between protected status variables Z and feature vectors encoded via Note that our fair learning method is fundamentally different from much existing work which uses a regularization term to encode fairness or encodes fairness as a constraint Deep Fair SVDD Model Our proposed deep fair SVDD network aims to learn a fair representation to describe all the training data via adversarial learning Given the normal training data X RMD encoder network we have the latent encoding of all the normal points as Assume the binary protected status variable is Z term The fair representation is achieved when the learned embeddings are statistically independent of the sensitive attribute Z Given z we hope to optimize the function to have pf z pf z To achieve the goal in equation we propose to use adversarial networks with a min-max game strategy to constrain the embedding function Firstly the encoder network is trained with normal points X to generate compact embedding around a predetermined center c To regularize the encoder we add a weight decay regularizer with positive hyper-parameter for all the L hidden layers We use term LSV DD to represent the encoders loss function LSV DD M M i xi c L Secondly we concatenate the encoding network with a discriminator to learn to classify the sensitive attributes Z based on learned embedding Since Z is a binary variable we use sigmoid function to get the probabilistic prediction as xi We choose cross entropy loss to train discriminator as LD M M i log To make the learned embedding invariant with sensitive attributes Z we hope to tune the embedding function to fool the discriminator Meanwhile we hope the normal points are still closely clustered together so that we design the adversarial loss as follows LSV DD where the hyper-parameter is a positive constant number Minimizing the adversarial loss LSV DD is actually maximizing the discriminators loss LD Note the discriminators parameters are fixed when we back-propagate the adversarial loss Similar to the generative adversarial networks we propose to train the and in an alternative way until we find the min-max solution The training procedure tries to jointly optimize both quantities argmin LD argmin LSV DD Figure Pipeline of the proposed deep fair SVDD learning framework The inputs are normal training data X and the outputs are learned embedding and a discriminatory function The end-to-end learning process contains three steps train the encoder via minimizing the loss LSV DD fix the encoder parameters and train the discriminator via minimizing the discriminators loss LD fix the discriminators parameters and train encoder to minimize the adversarial loss LSV DD Procedure and are trained alternatively until convergence Once the joint training converges the anomaly scores for all the instances are calculated as S c Note the instances with larger anomaly scores have larger probability to be predicted as anomalies The pseudo-code for the learning algorithm is summarized in Algorithm We also visualize the learning pipeline of deep fair SVDD model in Figure Algorithm Algorithm for deep fair SVDD Input X training data test data Z Protected status variable encoder network c predetermined data center discriminator initial training epochs T adversarial training epochs Output S predicted anomaly score Train the encoder network via minimizing LSV DD in equation for epochs Fix the encoder network train the discriminator via minimizing LD in equation for epochs for epoch from to T do Fix the parameters for encoder network Calculate LD in equation for each mini-batch Back-propagate the discriminator loss LD and update the parameters Fix the parameters for discriminator Calculate the loss in equation for each mini-batch Back-propagate the adversarial loss and update end for Output the anomaly scores for test set S c Potential Extensions of Deep Fair SVDD In this subsection we analyze the design of our proposed deep fair SVDD and provide several potential extensions of our proposed learning framework that we intend to study Extensions to Fairness Problems with Multi-State Protected Status Variable Note we study the fairness problem with binary protected status variable z in this work However our deep fair SVDD learning framework can be extended to solve fairness problems with multi-state protected state variable eg education level nationality by changing the current binary discriminator into a multi-class classification network Extensions to Fairness Problems with Multiple Protected Status Variable Our framework can also support multiple protected status variables if we substitute the binary classification discriminator with a multi-class classification network Given the fairness requirements on multiple protected state variables say gender and race together we can enumerate all the combinations via a Cartesian product of these two variables and transform them into a multistate protected state variable to feed in our extended framework This is an important property lacking in many fair classification methods as clearly making a model fairer with respect to say gender could make it unfair with respect to say race Extensions to Semi-supervised Anomaly Detection The current encoder is trained via an unsupervised loss function to force all the normal data to be close to the predetermined center c Recently some works on general semi-supervised anomaly detection have demonstrated superior performance In general semi-supervised anomaly detection settings we assume the Table Characteristics of four datasets used in our experiments Our method requires the protected status variables such as Gender Male and Female and Race African-American and non African-American to be binary variables Dataset Type Instances Dimension Protected Status Variable Normal Group Abnormal Group COMPAS Recidivism Tabular Race Not reoffending Reoffending celebA Face x x Gender Attractive faces Plain faces MNIST-USPS Digits x x Source of digits Digit Digit MNIST-Invert Digits x x Color of the digits Digit Digit learners have access to a small subset of labeled normal and abnormal instances Our current learning framework can be modified to accommodate semi-supervised anomaly detection settings by combining the current loss function with a new supervised classification loss for labeled anomalies in the training set EXPERIMENTS In this section we conduct experiments to empirically evaluate our proposed approach From our experiments we aim to address following questions Do existing deep anomaly detection algorithms produce unfair results see Section How does our proposed algorithms work in two types of datasets involving low dimensional data COMPAS Recidivism and high dimensional data Facial images and digits see Section What is the sensitivity of the hyper-parameter in our proposed deep fair SVDD model see Section How do our proposed algorithm change the latent feature space whilst making anomaly detection fairer see Section How efficient are our proposed algorithms see Section Data Sets We propose to experiment on four public datasets which include visual data and tabular data We list the characteristics of our selected datasets in Table and introduce the details of how we construct those datasets as below For each data set only normal instances are in the training data set but there are both normal and abnormal instances in the test data COMPAS Recidivism The COMPAS recidivism data set consists of data from criminal defendants from Broward County Florida We create a binary protected status variable for whether the defendant is African American Given the ProPublica collected label of whether the defendant was rearrested within two years we set the normal group for people who were not reoffending and the abnormal group for reoffending We select this data set to demonstrate our approachs performance on low-dimension tabular data celebA This is a large-scale face attributes dataset with more than celebrity images each with attribute annotations We sample a subset of this data set and treat gender as a binary protected status variable in this data set The normal group contains celebrity faces labeled as attractive and the abnormal group contains the celebrity faces labeled as plain We choose the celebA data set to test our approach on high-dimension images MNIST-USPS This dataset consists of MNIST and USPS images which include different styles hand-written digits We set the sample source as a binary protected attribute The normal group contains digits from class and the abnormal group includes digits from class MNIST-Invert We take the images from MNIST and create a duplicate which is inverted to build this dataset The binary protected attribute is then original or inverted The normal group contains digits from class and the abnormal group contains digits from class Implementation Due to the different characteristics of our selected data sets we have implemented different networks for them For the SVDD based encoder network we use a convolutional neural network with two modules filters followed by filters and a final fully connected layer of units for MNIST-USPS and MNIST-Invert data sets we use a convolutional neural network with three modules filters filters and filters followed by a fully connected layer of units for celebA data set we use a fully connected neural network with two hidden layers with and units for the COMPAS Recidivism data set We use batch normalization and ReLU activations in these networks Note for the fair deep SVDD model we have another classification branch We employ a fully connected neural network with three hidden layers as the sensitive attribute discriminator for all the data sets We set the trade-off hyper-parameter default to and the center c as the mean of all the instance embeddings We set the learning rate as e for Adam optimizer and conduct mini-batch training with batch size as The weight decay hyper-parameter is set to Evaluation Metrics and Baselines In our experiments we evaluate two aspects of the proposed approaches and the baseline methods The first aspect is the ability to detect anomalies We evaluate the anomaly detection performance using the common Area Under the ROC Curve AUC The AUC measure can be thought of as the probability that an anomalous example is given a higher anomaly score than a normal example In this way the higher the AUC score is better The benefit of using AUC is because it represents the anomaly detection performance across various anomaly score thresholds t The second aspect is the Table Characteristics of original training set and balanced training set used in experiments We reduce the number of overrepresented group in original training set to generate balanced training set COMPAS Recidivism celebA MNIST-Invert MNIST-USPS Original z Original z Balanced z Balanced z a Deep SVDD p -rule b DCAE p -rule c Deep SVDD distribution distance d DCAE distribution distance Figure Two methods of evaluating the unfairness for existing deep anomaly detection methods on both the original training sets blue bars and balanced training sets orange bars Note the larger fairness by p -rule and smaller distribution distances means the model is fairer Observed from these figures we can see that training deep anomaly detection models with a balanced training set can slightly improve the fairness in most cases However in most cases the fairness by p -rule does not satisfy the rule black horizontal line advocated by the US Equal Employment Opportunity Commission ability to be fair in terms of protected status variables We use aforementioned p -rule equation and distribution distance equation measures as our evaluation metrics We compare deep fair SVDD with two popular deep anomaly detection methods deep SVDD and deep convolutional autoencoders DCAE We duplicate the deep fair SVDDs encoder network architecture for those two deep anomaly detection baselines to make a fair comparison We use the default parameters suggested in their original papers The Unfairness of Deep Anomaly Detection We first study the problem of whether existing deep anomaly detection methods can generate fair predictions We study this under two settings one where we balance the PSV one where we do not An imbalanced data set can very easily lead to unfair results whilst a balanced data set is easier to find fair anomalies To demonstrate that deep anomaly detection models are unfair we have prepared two versions of the training set the original training set and the balanced training set We have listed the detailed information in table If the deep anomaly detection models cant generate fair predictions with both original and balanced training set then we can conclude that our selected deep anomaly detection methods are unfair Thus we conduct anomaly detection experiments and report both deep SVDD and DCAEs fairness performance on both versions of training sets in figure We select these two methods because they represent the two popular types of deep anomaly detection methods Observing Figure a and b We can see for both COMPAS and celebA data set the deep SVDD and DCAE achieves higher fairness by p -rule with a balanced training set However the improvements are not ideal because both approaches only satisfied the rule on one data set celebA Moreover for the MNIST-USPS data set both deep SVDD and DCAE become more unfair with a balanced training set Figure c and d shows the distribution distance which reflects the overall fairness of each model The smaller distances indicate the models predictions are more likely to be independent with the sensitive attribute We can observe a similar trend as we have seen in Figure a and b that learning on a balanced training set can only provide marginal improvements We learn from these results that a fair anomaly detection approach is needed to mitigate deep anomaly detection algorithms unfairness Evaluating Deep Fair SVDD We now evaluate our proposed deep fair SVDD networks performance and make a comparison with deep SVDD and DCAE Figure a shows the fairness by p -rule on abnormal groups We can see that deep fair SVDD outperforms both deep SVDD and DCAE in all four data sets Moreover deep fair SVDDs fairness by p -rule are greater than which satisfies the rule advocated by the US Equal Employment Opportunity Commission The distribution distance results are shown in Figure b We can see that deep fair SVDD achieves better overall fairness performance especially for the celebA data set Lastly we show the test set AUC scores for four data sets in Figure c we notice that in COMPAS MNIST-Invert and MNIST-USPS data sets the deep SVDD performs slightly better than the other two approaches while in the celebA data set the deep fair SVDD performs slightly better than other two approaches a Fairness by p -rule b Fairness by distribution distance c AUC Scores Figure Comparison of deep fair SVDD with deep anomaly detection baseline methods on all four selected data sets We evaluate the fairness performance for all the models trained on original data sets and plot the fairness by p -rule and distribution distances in Figure a b We also evaluate the anomaly detection performance and show the AUC scores in Figure c Note deep fair SVDD achieves better fairness results with a slight loss in terms of the AUC score a Deep SVDD MNIST-Invert b Deep SVDD MNIST-Invert c Deep SVDD celebA d Deep SVDD celebA e Deep Fair SVDD MNIST-Invert Deep Fair SVDD MNIST-Invert g Deep Fair SVDD celebA Deep Fair SVDD celebA Figure The visualization of the random selected normal and abnormal examples determined by deep SVDD top row and deep fair SVDD bottom row for MNIST-Invert data set and celebA data set Compared to the deep SVDDs prediction results the size of instances with different protected status variable values are more balanced in fair SVDDs predictions Overall speaking deep fair SVDD achieves much better fairness with a minimal loss in anomaly detection performance Further we analyze the interesting result on the celebA data set In the celebA test set both the normal and abnormal groups have a balanced number of males and females Thus optimizing fairness in the celebA data set may also improve the anomaly detection performance We have observed similar results in the following experiments on the trade-off analysis of deep fair SVDD Figures shows examples of the randomly selected normal and anomalous examples according to deep SVDD and deep fair SVDDs predictions For the MNIST-Invert data set we can see that both the MNIST instances and Inverted MNIST instances are distributed evenly in the normal/abnormal groups determined by deep fair SVDD On the contrary there are more MNIST instances in the abnormal group and fewer MNIST instances in the normal group determined by deep SVDD As for the anomaly detection quality both approaches have made few mistakes and achieved similar results as shown in Figure The right-hand side of the Figure shows the results for the celebA data set Observing the deep SVDDs results on the top row shows that more males are predicted as plain faces and more females are predicted as attractive faces These unfair results are mitigated with deep fair SVDD and we can see a nearly balanced number of males and females in both groups predicted via fair SVDD As for the anomaly detection quality both approaches made some mistakes and these are in line with the AUC scores we have reported in Figure c This is reasonable as human faces contain far more information than digits The anomaly detection tasks over human faces are more challenging than recognizing digits Our main goal is to demonstrate how deep fair SVDD mitigates the unfair problems caused by deep anomaly detection baselines a COMPAS b celebA c MNIST-Invert d MNIST-USPS Figure The trade-off between fairness and anomaly detection performance We tune the hyper-parameter to demonstrate the trade-off between fairness by p -rule and anomaly detection performance in all the data sets Note the ranges from to and it is visualized in each plot with the order from left to right respectively In all four datasets the fairness by p -rule value increases as increases The AUC scores decrease in most data sets as increases The Trade-off between Fairness and Anomaly Detection Performance This section analyzes the trade-off between fairness performance and anomaly detection performance of deep fair SVDD We retrain and test the deep fair SVDD under different values of hyperparameter range from to within equation The hyper-parameter controls the weight of the discriminators loss term within the adversarial loss function and directly determines the trade-off between the fairness performance and anomaly detection performance Figure shows the results in all four selected data sets the fairness by p -rule increases as increases The AUC score drops as the fairness by p -rule value goes up for COMPAS MNIST-Invert and MNIST-USPS data sets We have also noticed one different result in the celebA data set both fairness by p -rule and AUC score increase as the increases We have analyzed this case before when comparing deep fair SVDD to deep anomaly detection baselines in Figure c Here fairness constraint is extra information that could help the algorithm improve anomaly detection performance Generally speaking training the deep fair SVDD with a larger will lead to fairer results and usually a slight loss in terms of the anomaly detection performance AUC score Anomaly Predictions Analysis This section will conduct experiments to study how deep fair SVDDs predictions differ from deep SVDDs predictions We have stored the anomaly prediction results for both approaches and summarized their overlapped anomaly predictions in Table We use the number of overlapped anomaly predictions to divide the total number of anomalies as the overlap ratio We can see that the overlap ratios are pretty high across all the data sets We hypothesize the reason is that fair SVDD is also optimized with SVDD loss function Furthermore this high overlapping can also explain why fair SVDD only performs slightly worse than SVDD in terms of the AUC scores as we demonstrated in Figure c We also visualize the non-overlapping predictions between deep SVDD and deep fair SVDD in Figure Take the MNIST-Invert data set for example we randomly sample non-overlapping anomalies with z from fair SVDDs predictions We can view these instances as moving from deep SVDDs predicted normal group to Table Anomaly prediction results for deep SVDD and deep fair SVDD Z and Z represent the number of predicted anomalies with protected status variable value as and respectively There is a large overlap between these two models anomaly predictions COMPAS celebA MNIST-Invert MNIST-USPS SVDD Z Z Ours Z Z Overlap ratio a Instances moved from normal to abnormal group b Instances moved from abnormal to normal group Figure Illustration of how deep fair SVDD makes the anomaly detection results fairer We visualize the sampled non-overlapping predictions between deep SVDD and deep fair SVDD The instances in a can be seen as moved from deep SVDDs predicted normal group to deep fair SVDDs predicted abnormal group and vice versa for b deep fair SVDDs predicted abnormal group to make the results fairer Observing the digits from Figure a we can see that deep fair SVDD is improving the fairness by moving instances that are prone to be anomalies to the abnormal group One common feature of those instances is that they are dissimilar to a regular style of digit and many of them are digits It is important to show that these non-overlapping instances are not randomly distributed but are all prone to be anomalies This interesting finding demonstrates that our proposed model is optimized to make fair and accurate anomaly predictions instead of random altering predictions to satisfy group-level fairness We can observe similar results from Figure b that instances moved from deep SVDDs abnormal group to deep fair SVDDs normal group are prone to be normal points a COMPAS Deep SVDD b celebA Deep SVDD c MNIST-Invert Deep SVDD d MNIST-USPS Deep SVDD e COMPAS Deep Fair SVDD celebA Deep Fair SVDD g MNIST-Invert Deep Fair SVDD MNIST-USPS Deep Fair SVDD Figure The t-SNE visualization of the feature embeddings for test instances Red and blue points represent test instances with different sensitive attribute values Comparing to deep SVDDs results top row the deep fair SVDDs learned embeddings bottom row are more fair as blue and red points are always blended together which are hard to separate Table Training time results measured by seconds Training deep fair SVDD takes longer time due to the min-max optimization of the adversarial learning COMPAS celebA MNIST-Invert MNIST-USPS Deep SVDD Ours Embedding Visualization We visualize and compare the learned embeddings for both deep SVDD and deep fair SVDD to show why deep fair SVDD make fairer anomaly predictions This analysis is important as deep fair SVDDs objective is to learn a fair representation which is independent on the protected status variable z pf z pf z As shown in Figure the red and blue points represent the test instances with the sensitive attribute value as z and z respectively We first analyze the visualization results from deep SVDD in each plot we can find some regions dominated by one particular color which indicates the correlation between feature embeddings and the protected status variable On the contrary observing from the deep fair SVDDs result we can see that the red and blue points are almost uniformly distributed in the feature space especially in the celebA data set Deep fair SVDD is demonstrated to learn a fair representation that is independent of sensitive attributes Running Time Analysis We have also reported the training time for deep fair SVDD and compared it against the deep SVDD approach in Table Training deep fair SVDD takes longer time because we have a new fairness objective and it is learned through adversarial training We leave how to speed up the training process as an interesting future work CONCLUSIONS AND FUTURE WORK This paper studied the fairness problem of deep anomaly detection methods and proposed a novel deep fair anomaly detection approach deep fair SVDD Deep fair SVDD is a method that uses deep neural networks to embed the data into a feature space where the normal data are closely clustered to the centroid Adversarial training is used so that a discriminatory network cannot predict the protected status Further we propose two measures of the group-level fairness for deep anomaly detection problems Given the ground truth labels we can directly measure the p -rule equation for the abnormal group We also propose distribution distance equation which can measure the overall fairness without knowing the labels of anomaly instances We have conducted extensive empirical studies to evaluate the usefulness of our proposed approach Firstly our experiments show that deep anomaly detection methods will generate unfair predictions even if the training data is balanced with respect to the binary protected state variables Secondly we evaluate our proposed deep fair SVDD and compare it to the deep anomaly detection baselines in various data sets We demonstrate that our proposed work can achieve satisfying fairness results with minimal loss of anomaly detection performance Next we analyze the hyper-parameter which controls the tradeoff between fairness and anomaly detection performance within our model and analyze the learned embeddings to study how our proposed model makes fair decisions In this paper we limited ourselves to studying group-level fairness for deep anomaly detection problems with a single binary protected state variable We leave for future works to study more complex fair anomaly detection problems such as considering multiple protected state variables extending to semi-supervised anomaly detection settings see section and improving the training efficiency and scalability