Corporate Social Responsibility via Multi-Armed Bandits We propose a multi-armed bandit setting where each arm corresponds to a subpopulation and pulling an arm is equivalent to granting an opportunity to this subpopulation In this setting the decision-makers fairness policy governs the number of opportunities each subpopulation should receive which typically depends on the unknown reward from granting an opportunity to this subpopulation The decision-maker can decide whether to provide these opportunities or pay a pre-defined monetary value for every withheld opportunity The decision-makers objective is to maximize her utility which is the sum of rewards minus the cost paid for withheld opportunities We provide a no-regret algorithm that maximizes the decision-makers utility and complement our analysis with an almost-tight lower bound Finally we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations INTRODUCTION Algorithmic decision making plays a fundamental role in many facets of our lives criminal justice banking online-advertisement hiring and college admission are just a few examples With the abundance of applications in which algorithms operate concerns about their ethics fairness and privacy have emerged For instance classification algorithms that were deemed to be unfair and discriminate based on factors like gender race and more Algorithmic fairness is a framework that among other means is aimed at ensuring the long-term welfare of such subpopulations when subject to algorithmic decision making Consider the following online advertisement use-case A company wants to publish a job ad online and optimizes its campaign based on the cost-per-click As witnessed by Lambrecht and Tucker women are less likely to see job ads for STEM positions since they have higher cost-per-click than men If women are not exposed to information about STEM career opportunities they may never apply to such jobs In order to act fairly and display ads to all the subpopulations the company will need to sacrifice part of its short-term utility and pay a higher cost-per-click This is an example of a cost of fairness Our goal in this paper is to better understand the trade-offs facing companies who wish to ensure their algorithms are more equitable We focus on exploring the cost of fairness versus the cost of alternatives such as Corporate Social Responsibility CSR hereinafter CSR is a self-regulation act of philanthropic responsibility in response to the rising concerns on ethical issues in businesses For example in Microsoft spent more than three billion dollars with minority disabled veteran LGBTQ and woman-owned businesses Starbucks focuses on creating meaningful opportunities for their employees As of June more than employees have earned first-time bachelors degrees since Starbucks College Achievement Plan was announced in In this paper we suggest an algorithmic approach to CSR in the setting of sequential decision making Sequential decision making is often modeled as Multi-armed bandit problems hereinafter MAB see Auer et al for a brief introduction MABs enjoy massive commercial success and have myriad real-world applications Here we treat arms as subpopulations and require that subpopulations would not starve from lack of opportunities Opportunities can be granted to a subpopulations either explicitly ie by pulling the subpopulations arm or implicitly via CSR channels Given the example above companies have the choice whether to display ads to subpopulations with higher cost-per-click or alternatively to invest money in organizations that promote the long term well-being of those subpopulations We highlight the tension between the decision-maker that wants to maximize her reward and the cost of CSR We consider the bandit reward to be the direct benefit derived from granting the opportunity to the subpopulation represented by the arm For simplicity we use the term expected reward from here on The amount of opportunities depends on how fairness is perceived by the decision-maker and the expected rewards Unfortunately information about the expected rewards is not known in advance and has to be explored by the decision-maker We take a utilitarian approach The utility of the decision-maker is composed of the rewards eg clicks on displayed ads and a transfer cost The transfer cost is the amount the decision-maker invests in CSR for every deferred opportunity Knowing the transfer cost in advance the decision-maker can make an informed decision on how to allocate its resources Our model casts light on the trade-off between the cost of directly granting an opportunity and the cost of granting the opportunity via an external route such as CSR Our Contribution Our contribution is two-fold technical and conceptual On the conceptual side our framework reflects the trade-off between monetary rewards and subpopulation opportunities which can be viewed as a means of providing long-term welfare This perspective follows eg self-regulation in revenue-driven commercial companies as decision-makers contributing to societal goals or a policy maker that ensures that the decision-maker is fairness aware In the former sufficient opportunities are a CSR that is integrated in the companys objective by design In the latter the decision-maker provides opportunities explicitly by arm pulls or implicitly by payments that are invested in that subpopulation by the policy maker for eg better computer labs in public schools Crucially the number of required opportunities depends on the expected rewards which are only known in hindsight Technically we consider the typical MAB setting with Bernoulli arms with horizon and expectation vector which is unknown In addition we introduce a fairness function which determines the minimal number of pulls for each arm given the expected reward vector The term quantifies the amount of opportunities subpopulation deserves which importantly is a function of its own expected reward and the expected rewards of the other subpopulations The decision-maker gains rewards but pays a transfer cost of for every round of unmet opportunity namely max where is the number of pulls of arm We assume that both and are known in advance We characterize the optimal algorithm that achieves a sub-linear regret of and show a matching lower bound We augment our theoretical analysis with experimental one examining the implications of different fairness functions and values of Related Work Multi-armed bandits have been the subject of many fairness-related research Joseph et al study fairness in MABs from the eyes of the decision-maker In their work a learning process is considered unfair if an arm with a lower expected reward is favored over an arm with a higher expected reward We study fairness from the perspective of the arms and view arm pulling as granting an opportunity This view was also adopted by Liu et al The authors define a calibrated fair policy to be a policy that selects action with probability equal to the probability that the reward realization of arm is the highest By measuring only the number of rounds the algorithm is miscalibrated Liu et al neglect the decrease in reward incurred by the fairness requirement In contrast to these works we explicitly suggest a utilitarian approach that penalizes for lack of pulls in our terminology opportunities A penalty of similar flavor was suggested recently in other work on fair ML but in different settings Schumann et al studied group fairness in MAB where several arms can belong to the same protected group and the reward can be biased The work most related to ours is Patil et al The authors define fairness as pulling each arm at least a minimal number of times according to a predefined vector where each entry corresponds to a subpopulation The predefined vector is given by the policy maker and is independent of the subpopulation properties However our work differs from Patil et al in two crucial aspects First while Patil et al model fairness as a hard constraint we better address real-world applications and treat it as a soft one Our utilitarian approach which is well-studied in economic contexts accounts for trading rewards with opportunities If providing opportunities explicitly by pulling the arms is financially unbearable the decision-maker can do that implicitly by monetary transfers Second Patil et al construct the fairness constraint by a predefined vector while in our work the opportunity requirements depend on each arms expected rewards a quantity which is uncertain during the decision making process and is only fully known in hindsight This uncertainty makes the problem we address markedly harder These differences and others lead to a lower bound of compared to a in theirs MODEL We consider a stochastic bandit problem a decision-maker is given arms and pulls one at each time step We denote by the arm pulled at time When arm is pulled at time the decision-maker receives a random reward We assume that for every the reward distribution is a Bernoulli distribution with expected value This is without loss of generality since we can reduce any instance with general -supported distribution to an instance with Bernoulli arms using the technique of Agrawal and Goyal We use to denote the vector of expected rewards ie We denote by the number of times arm is pulled by the end of round and let be the gap between the expected reward of the optimal arm and the expected reward of arm We now present the Reward-Opportunity MAB model An instance of MAB is represented by a tuple The tuple is an instance of standard stochastic bandit as described above The combination of and creates what we call the fairness policy The fairness requirements are expressed by a function receives as input a vector of expected rewards and outputs a vector of minimal fraction of times each arm has to be pulled in order not to be penalized We let denote the entry of We assume that and that is Lipschitz continuous with a Lipschitz constant with respect to the norm That is for all it holds that Intuitively satisfying the Lipschitz condition implies that two similar expected reward vectors get similar fairness requirements For simplicity from here on we call the fairness function The difference between the fairness requirement and the number of times an arm was pulled represents the deviation from the fairness constraint If the deviation is positive it means the arm was not pulled enough times ie the subpopulation did not receive enough opportunities according to the fairness function In such a case the decision-maker pays a cost The paid cost for a single arm pulls deviation from the fairness requirement is given by the transfer cost for arm If arm was pulled less than times the reward will be deducted by The transfer cost is known to the decision-maker in advance To account for all cases the possible cost which stems from the deviation is max For simplicity we use for all but stress that our results hold with minor modifications in the general case as well The utility of the decision-maker is denoted by It is an additive utility of the reward minus the total deviation from the fairness requirement Notice that and consequently depend on the algorithm playing the arms Formally given an algorithm 𝐴𝐿𝐺 𝐴𝐿𝐺 max As is customary in the MAB literature we focus on the regret of the decision-maker which we denote 𝐴𝐿𝐺 Let 𝑂𝑃𝑇 be an algorithm maximizing the utility 𝑂𝑃𝑇 we discuss 𝑂𝑃𝑇 in Subsection The regret is the gap between the expected utility of 𝑂𝑃𝑇 and 𝐴𝐿𝐺 𝐴𝐿𝐺 𝑂𝑃𝑇 𝐴𝐿𝐺 When and are arbitrary or clear from the context we omit the subscript and simply denote U and R Full proofs appear in Section A Optimal Algorithm The structure of the optimal algorithm in classic MABs is straightforward In every round pick the arm with the highest expectation However in our case the transfer cost makes the optimal algorithm a bit more complex as we now elucidate Let denote an arbitrary index of a sub-optimal arm ie an arm such that max The decision-maker has to decide whether to support the subpopulation associated with that arm explicitly by pulling times or implicitly by paying Note that can be non-integer in this case we assume is rounded to the preceding integer In each one of those rounds the decision-maker losses if she pulls arm as she could pick the optimal arm but saves as she does not need the pay the transfer cost Therefore if the reward gap of arm is greater than the transfer cost the decision-maker does not pull arm at all and pays the transfer cost Otherwise if the decision-maker would have greater utility by pulling arm exactly times and not incurring the transfer cost If the decision-maker is indifferent between the two options More formally Lemma Fix an arbitrary instance and let 𝑂𝑃𝑇 be an optimal algorithm for that instance For every sub-optimal arm if then 𝑂𝑃𝑇 pulls exactly times if 𝑂𝑃𝑇 does not pull at all If 𝑂𝑃𝑇 pulls arm between zero and times Proof Notice that the utility Equation is order insensitive hence 𝑂𝑃𝑇 is not unique Therefore we only care about the vector We prove that any vector that violates the structure described above is sub-optimal Denote by a vector of counts where for every sub-optimal arm if and if If Note that is not unique Let be a vector that violates the structure of Since violates the structure of there exists such that and We prove that by moving pulls from to in a way specified below the expected utility increases Let be the modified vector after moving pulls from to That is for all Additionally we show that either or If it implies that Otherwise If it implies that In this case moving min pulls from to changes the expected utility Since and the expected utility increases By the definition either or Otherwise if it implies that By moving min the expected utility changes by at least Since the expected utility increases By the definition either or We showed that every vector that violates the structure of can be modified such that the expected utility increases and at least one more entry does not violate the structure of Repeating this process at most times yields and increases the utility About the Fairness Policy The fairness policy is comprised of the fairness function and the transfer cost The function represents the decision-makers view on how opportunities should be distributed For example the zero function corresponds to standard Multi-Armed bandit problem without any constraints Generalizing this case for any constant function eg uni alludes that the decision-maker believes that all subpopulations are entitled to the same share of opportunities irrespective of their expected rewards The fairness function can also grow linearly with each expected reward for instance lin In the most general case the number of required opportunities to a subpopulation can also depend on its expected reward relative to the expected rewards of other subpopulations For example Our modelling and results support these special cases and many other natural candidates for the fairness function Selecting complements the decision-makers view on revenue and opportunities As described in Section if the transfer cost is high the decision-maker will tend to grant the opportunities explicitly and would not grant opportunities explicitly only when the subpopulations expected rewards have big differences If the transfer cost is low the decision-maker would derive a higher utility by supporting subpopulations via CSR and not by directly granting opportunities As pointed out earlier can vary between different subpopulations but for simplicity is assumed equal NO-REGRET ALGORITHMS In this section we present our main algorithmic contribution We devise Self-regulated Utility Maximization which incurs a regret of Before we discuss it we first demonstrate that classical MAB algorithms fail miserably on our setting This is expected given that such algorithms were not devised for a setting like ours but it will serve us later on Classical MAB algorithms are tuned to pull sub-optimal arms as little as possible As shown in Subsection it is not always optimal for MAB If the cost of opportunity is lower than the transfer cost the optimal algorithm pulls arm according to the fairness function To better illustrate consider the famous Explore-Then-Commit ETC algorithm ETC explores all arms for a predetermined number of rounds and then follows the best performing arm for the remaining rounds We focus on a MAB instance with arms transfer cost and constant fairness function The optimal algorithm from Subsection pulls each arm times and obtains an expected utility of ETC with optimized exploration parameter will discover that arm is the better one relatively fast and will pick that arm forever hence its utility The regret is therefore which is linear in the number of rounds In we face a unique challenge comparing to the classic MAB problem Classical MAB algorithms are aimed at identifying the optimal arm but do not estimate accurately the expected rewards and consequently do not approximate well the reward gaps As discussed in Section the optimal algorithm depends on the relation between the reward gaps and the transfer cost hence unlike classic MAB accurate approximation of the reward gaps is crucial for our problem Additionally should also be approximated correctly for arms with to align with the optimal algorithm These two challenges are singular to our settings and are reflected in the lower bound Algorithm which we term Fairness-Aware-ETC is a modified version of ETC which is aware of the fairness function and the transfer cost Fairness-Aware-ETC pulls each arm times where is received as an input After the exploration rounds it constructs estimates for and which we denote using the hat notation ie and It then continues optimally with respect to these estimates similarly to the optimal algorithm for the estimated quantities The number of exploration rounds per arm balances the tension between exploration and exploitation Setting very high ensures that the estimates and are close to their actual counterparts with high probability but can allow little exploitation and hence high regret Picking too low can result in wrong estimation of and as a consequence also wrongly evaluate Theorem Fix any arbitrary instance of MAB and let log Algorithm has a regret of log Proof We define the clean event to be the event that holds for all arms simultaneously Where log We will argue separately the clean event and the bad event the complement of the clean event The regret is clean event P clean event bad event P bad event We now bound the probabilities for the clean event and for the bad event at the end of the exploration phase That is after each arm was pulled times Using Hoeffdings inequality P Using the union bound the probability for the clean event is Since the bad event complements the clean event the probability of the bad event is at most That the regret of the bad event is bounded by the highest possible regret Combining the two last statement together we obtain that bad event P bad event The clean event implies that for all log Thus with respect to norm since is 𝐿-Lipschitz log Next we examine several cases at the end of the exploration phase and their effect on the regret For arm if the optimal algorithm pulls arm exactly times If Algorithm will pull arm max times If the regret is bounded by log Otherwise the regret is bounded by If Given the clean event it implies that log In this situation Algorithm will not pull arm anymore If the regret is be bounded by log If the regret is log log For arm if the optimal algorithm does not pull arm at all If Algorithm will not pull arm anymore The regret is bounded by log If Given the clean event it implies that log Algorithm will play arm max times If the regret is bounded by log If the regret is bounded by log Otherwise if and the regret is bounded by log The analysis of the cases above implies that after the loop that starts on Line ends for every sub-optimal arm log Note that log Hence the regret in the end of the exploration phase is bounded by log Algorithm Fairness-Aware-ETC Input number of exploration rounds for do pull arm for rounds for do if then pull arm for max rounds pull an arbitrary arm from argmax until the execution ends The remaining rounds less than are allocated an arbitrary arm with the highest observed expected reward Given the clean event the reward gap between the optimal arm and the sub-optimal arm the algorithm commits to is less than Hence the regret before allocating the remaining rounds is bounded by log Putting it all together The regret for the event case is bounded by clean case log The total regret is bounded by log log Notice that Algorithm is almost data independent The exploration phase in Lines continues even if the estimates of and are accurate The predefined exploration length prevents the algorithm from stopping the exploration early Such an early stopping is important after identifying arms with high opportunity cost or arms that already satisfy the fairness requirements Fairness Aware Black-Box algorithm In this section we present a data dependent algorithm addressing the problems of Algorithm above If the algorithm is certain with high probability that the cost of opportunity is higher than the transfer cost the decision-maker can stop pulling this arm If the fairness function admits low values the algorithm can satisfy the fairness requirement within less than pulls We now explain the course of Algorithm Full version of the algorithm appears in Section B The algorithm takes and which we describe shortly and 𝐴𝐿𝐺 a black-box no-regret MAB algorithm as input where 𝐴𝐿𝐺 is no-regret with respect to the classical rewards-only MAB objective eg UCB In Lines the main variables are initialized confidence bounds representing the probable estimates of the reward gaps and Line which is the hyper-cube of probable estimates of Lines consist of four different phases In the first phase Lines the reward gaps are approximated up to a factor of After this phase the algorithm knows with high probability for each arm whether its reward gap is higher or lower than the transfer cost by more than To be precise we care for accurate approximation of only if it is close to the transfer cost The second phase Lines approximates for arms with low opportunity cost up to a factor of If there is an arm with low opportunity cost for which the approximation of is not accurate enough all the arms are pulled The term max min upper bounds the highest change of inside the hyper-cube and hence also upper bounds our estimation error of To clarify why we pull all arms in Line recall that also depends on in the general case if our estimate is not accurate the estimation of might not be accurate as well Pulling all arms ensures that all the estimates improve for the subsequent round namely shrinks in all of its dimensions In the third phase Lines we ensure that we pull all arms with low opportunity cost according to the estimate of Lastly in the fourth step Line we invoke 𝐴𝐿𝐺 until the end of the execution Next we discuss the input hyper-parameters and 𝐴𝐿𝐺 is the confidence interval hyper-parameter for the approximation of Setting to small values implies that arms should be pulled many times and this can inflict a regret due to over pulling arms The approximation error of can be as big as is The hyperparameter is the confidence interval for the approximation of the reward gaps If the reward gap is not close to the transfer cost it would be identified almost immediately Otherwise Algorithm uses the black-box MAB algorithm 𝐴𝐿𝐺 This allows the decisionmaker to devote the fourth and final phase to identifying the best arm and exploiting its reward The only computationally non-trivial step in Algorithm appears in Line Computing max min Finding the global maximum of a Lipschitz function inside a hyper-cube is a computationally challenging task However due to role plays in our setting we argue that it should have a natural structure Indeed quantifies a societal requirement and as such should be easy to grasp Providing opportunities according to a cumbersome hard-to-optimize and unexplainable criteria is likely to be unfair in and of itself Consequentially we shall assume that there is an oracle that computes the minimal and maximal values at entry can obtain in a given hyper-cube In Subsection A we show that for the softmax function implementing such an oracle boils down to solving a convex optimization problem over a single variable As an additional example the family of monotonically non-decreasing Cartesian product functions obtain minimum and maximum on the extreme points of the confidence intervals and hence are easy to handle Examples of such functions are linear functions exponential functions eg for and constant functions We are ready to state the guarantees of Algorithm Theorem Fix any arbitrary instance of MAB and let Then Algorithm has a regret of log Proof We analyze the regret that stems from the different phases Approximating the reward gap Phase approximating Phase granting opportunities Phase and invoking 𝐴𝐿𝐺 Phase Lemma which we prove below guarantees that after each arm was pulled at most the algorithm is certain with high probability for every arm if its cost of opportunity is bigger or smaller than the transfer cost by a factor of Thus the regret that stems from this phase is bounded by The second phase approximates only for entries with low cost of opportunity ie Lemma shows that after each arm is pulled at most times is approximated up to a factor of in the relevant entries An immediate consequence of Lemma is that after at most rounds max min Thus the regret from this phase is bounded by The regret from the first two phases is bounded by max In the third phase the algorithm pulls each arm with low opportunity cost until the condition in Line is met therefore For arm with the regret stems form the approximation error of which is bounded by For arm if arm is pulled during the third phase This happens when the reward gap is very close to the transfer cost and rounds are not sufficient to approximate correctly max the regret is also bounded by this term Otherwise assume max Note that the regret is bounded The second part of this expression is bounded by the approximation error of ie We bound the first part of the regret by bounding To do so we now look into the two possible cases that stopped the loop in Line The term the clean event is similar to the clean event defined in Subsection a If given the clean event This implies that Thus the first part of the regret is bounded by b If given the clean event This implies that and therefore the first part of the regret is bounded by To summarize if arm with is pulled in the third phase the regret associated with this arm is bounded by Eventually Phase invoking 𝐴𝐿𝐺 contributes the regret of 𝐴𝐿𝐺 The regret from the approximation phases is bounded by log The regret from filling the fairness requirements is also bounded by this term Ultimately the total regret is bounded by log assuming that this term is an upper bound on the regret of 𝐴𝐿𝐺 with respect to a vanilla stochastic bandit setting Lemma Fix any arbitrary instance of MAB and let Then after at most rounds for every either or Proof With high probability after pulling each arm log times with high probability log Therefore and hence at least one of the following happens Algorithm Self-regulated Utility Maximization Input Black-box bandit algorithm 𝐴𝐿𝐺 allowed approximation error parameters and Initialize arms data for all Hyper-cube of values in the clean event while and do Phase Pull all arms once update counters confidence bounds and while st max min and do Phase Pull all arms once update counters confidence bounds and while st and and do Phase Pull arm the minimal number of times so update and counters Invoke 𝐴𝐿𝐺 for the remaining rounds Phase Lemma Fix any arbitrary instance of MAB and let and a hyper-cube Then after pulling each arm at most times for every max min Proof After pulling each arm times with high probability for every That is Since is 𝐿-Lipschitz we obtain max min Special Cases In this section we present two private cases that achieve better regret than the worst-case regret of presented in Section First if has very loose fairness requirements ie for all and In such a case pulling each arm for rounds and invoking a black-box algorithm for the remaining rounds achieves a regret of max Formally let min if for all and for all Proposition Fix any MAB instance with horizon and fairness function min pulling all arms and invoking a black-box bandit algorithm for the remaining rounds achieves a regret bounded by max Proof For every arm after it was pulled times Thus the regret that stems from this phase is bounded by The optimal algorithm pulls the optimal arm for the remaining rounds Thus any additional regret is a consequence of 𝐴𝐿𝐺 and therefore the regret is bounded by The second case is a generalization of uni which was presented in Section Formally const if there exists such that Algorithm Constant function utility maximization algorithm Input Black-box bandit algorithm 𝐴𝐿𝐺 for do Phase for do pull arm if Invoke 𝐴𝐿𝐺 for the remaining rounds Phase for all and for all Algorithm is a variation of Successive Elimination that achieves a regret of max To illustrate the motivation to use Algorithm instead of Algorithm consider the following MAB instance Fix horizon number of arms and define to be for every and every Let Set and set the expected rewards of the other arms arbitrarily Note that Thus the first phase of Algorithm pulls each arm and incur a regret Algorithm will pull each arm exactly once and then invoke the black-box algorithm Thus in this case the regret is bounded by the regret of 𝐴𝐿𝐺 Algorithm leverages the fact that functions in const has no approximation error and thus can combine phases in Algorithm to one phase Note that if an arm is pulled times we can stop pulling it In the first phase Lines arms are pulled either at most times or until the algorithm is certain that In the second phase Line the black box algorithm is invoked and thus the regret is also bounded by the regret of the black box algorithm Proposition For any MAB instance with fairness function const Algorithm achieves a regret of max Proof We start by analyzing the regret of the first phase For arm with given the clean event arm is pulled exactly times in the first phase as in the optimal algorithm For arm with if log arm is pulled times and the regret will be bounded by log Otherwise log Arm is pulled as long as log Thus the regret of the first phase is bounded by log The regret that stems from the second phase is bounded by the regret of 𝐴𝐿𝐺 Combining the two together we get that R Algorithm 𝐴𝐿𝐺 max Instance Dependent Bounds In this section we analyze the instance dependent regret of Algorithm rather than the general regret given by Theorem We first add several notations The instance-dependent bounds revolve around two main parameters which is the gap between the reward gap of arm and the transfer cost and min which is the minimal gap between the reward gap and the transfer cost The smaller is the more rounds are required in order to identify whether the reward gap is smaller or larger than the transfer cost and what is the optimal sub-algorithm Given an MAB instance let be dimensional ball centered around with radius log After pulling each arm times with high probability the observed vector of expected rewards vector lies in Denote by the dimensional ball centered around with the largest radius such that for all max min Theorem Fix any arbitrary instance of MAB and let Then the regret of Algorithm is bounded by min log Proof In order to analyze the instance dependent bounds of Algorithm we analyze the instance dependent bound that stems from each of its phases The term clean event used in this proof and in the following proofs is similar to the clean event defined in Subsection Lemma argues that the first phase ends after each arm was pulled at most min log times and therefore the regret from this phase is bounded by min log Lemma proves that the second phase ends after each arm was pulled at most min times Hence the regret from this phase is bounded by min The regret from the third phase depends on the approximation error of The approximation error of on a single entry is bounded by min We divide the analysis into two cases For arm with the regret stems from the approximation error of which is bounded by min For arm with given the clean case if arm is pulled during the third phase it implies that the reward gap is very close to the transfer cost Ie Thus rounds were not sufficient to approximate correctly and If max the regret is bounded by this term Otherwise the regret is bounded by min To summarize the regret resulting from the approximation phases is bounded by min log min The regret from the third step is bounded min min log The regret of the fourth phase depends on the regret of the black-box algorithm and hence the additional term which completes the regret analysis Lemma Fix any MAB instance the first phase of Algorithm ends after each arm is pulled at most min log times Proof In order for the first phase to end we need to approximate all the reward gaps sufficiently well If then given the clean event all arms will be pulled log times in order to obtain a confidence interval smaller than which ensures that either for all Otherwise if ie for all Assuming the clean event after pulling each arm log times and We examine two cases If then this implies that Therefore arm will not satisfy the condition in Line If then hence Therefore arm will not satisfy the condition in Line Combining the above together the first phase ends after each arm was pulled at most min log times Lemma Fix any instance The second phase of Algorithm ends after each arm is pulled at most min times Proof Assuming the clean case after pulling each arm times is a ball of radius log centered around such that for every log Thus Therefore max min If as shown in Lemma the condition in Line is satisfied trivially Putting the above observations together after each arm is pulled at most min no arm satisfies the condition in Line and therefore the second phase ends Fairness Over the Entire Horizon Ideally we would like the impose the fairness requirements or the CSR payments on every round This is not always achievable since we first need to approximate the reward gaps and the fairness function well enough Additionally Algorithm requires a known horizon while the horizon is not always known in advance We therefore analyze a anytime version of Algorithm which uses the doubling trick with geometric growth Following the proof by Besson and Kaufmann if R 𝐴𝐿𝐺 log then the regret of anytime version denoted by 𝐴𝐿𝐺 with geometric growth achieves a regret of with increasing function Setting attains LOWER BOUND In the previous section we presented Algorithm which incurs a regret of in the worst case Here we show that this bound is asymptotically optimal by designing a family of instances that can mislead any algorithm Theorem Fix time horizon number of arms and Lipschitz constant For any algorithm there exists a MAB instance such that Proof We construct a family of MAB instances that any algorithm will incur a high regret For a given and Let and We build in the following way describes a vector of expected rewards The vector is composed of arm with expected reward subset of arms with expected reward and the remaining arms have expected reward of The size of is The fairness function is piecewise linear with pieces The first piece for values less or equal to is constant and equal to The second piece for values between to is linear that goes from to The third piece for values greater than is constant and equal to In Claim we prove that is 𝐿-Lipschitz and sums to at most one Following the optimal algorithm described in Section for every instance in the optimal algorithm pulls all the sub-optimal arms according to the fairness function This means the optimal algorithm does not pull at all the arms in and pulls times sub-optimal arms not in The algorithm pulls the optimal arm the remaining rounds In order to distinguish between two instances in the decision-maker must pull each arm at least times otherwise with positive probability the decision-maker is unable to distinguish between at least two instances We use the following notation for simplicity I I is an instance where the th arm is the optimal arm and all the other arms have an expected reward of is an instance where the th arm is the optimal arm and all the other arms expect arm have an expected reward of and arm has an expected reward of Fix an algorithm 𝐴𝐿𝐺 If 𝐴𝐿𝐺 pulls each arm at least times the utility of 𝐴𝐿𝐺 U 𝐴𝐿𝐺 I On the other hand the optimal algorithm will pull only arm and hence the utility of 𝑂𝑃𝑇 is The regret is then RI U 𝑂𝑃𝑇 I U 𝐴𝐿𝐺 I Otherwise there exists arm that 𝐴𝐿𝐺 draws less than times With probability at least in the instance that generated the sequence of rewards That is For example Denote by the number of times arm is pulled by 𝐴𝐿𝐺 u 𝐴𝐿𝐺 The optimal algorithm will pull arm times and arm the remaining rounds The utility of 𝑂𝑃𝑇 is u 𝑂𝑃𝑇 Finally observe that R 𝐴𝐿𝐺 u 𝑂𝑃𝑇 u 𝐴𝐿𝐺 Claim is 𝐿-Lipschitz and sums to at most one Proof is 𝐿-Lipschitz is continuous and piece-wise linear hence it is sup Lipschitz sup and therefore is Lipschitz sums to at most notice that max max if In the example above for large enough we obtain EXPERIMENTS In this section we perform an empirical analysis of Algorithm We demonstrate how different fairness policies ie pairs of a fairness function and a transfer cost influence the course of the algorithm and the average utility We consider several MAB instances with arms expected values for every and a varying horizon For fairness functions we consider uni lin and from Subsection As for the transfer cost we used and We used UCB as the black box algorithm 𝐴𝐿𝐺 in Algorithm Each combination of fairness function transfer cost and horizon was executed times on a standard Mac computer The entire process took several hours In order to compare between the different horizons we present the average utility ie the cumulative utility divided by the number of rounds Figure demonstrates the behaviour of different fairness functions with respect to varying values of As expected the average utility decreases as the transfer cost increases In Figure a we see that the average utility of all the functions is the same since 𝐴𝐿𝐺 is played across all the rounds As increases the reward gaps of more arms go below the transfer cost see Line in Algorithm so the decision-maker should pull them times similar to the optimal algorithm from Subsection The expected utility increases with the horizon since the proportion of approximation rounds Lines and decreases with the horizon For and uni use all the rounds for allocating opportunities The difference in the average utility stems from the allocation differences uni allocates evenly while allocates more opportunities as the arm expected reward grows Observe that uni lin for every and the described thus the utility of lin is greater than uni for Figure investigates the crux of the execution of Algorithm for The proportion of rounds devoted to phase Line phase Line phase Line and phase invoking 𝐴𝐿𝐺 Line For Algorithm always invokes the black box algorithm 𝐴𝐿𝐺 For and the proportion of approximation rounds phases and decreases as the horizon increases The proportion of phase rounds increases as the horizon grows The reason is that the proportion of approximation rounds decreases while the required number of opportunities grows linearly with the horizon For the algorithm dedicates all rounds for opportunities However for it realizes that for arms and with paying the transfer cost per each withheld opportunities and hence the remaining rounds are devoted to phase Figure compares the average utility of Algorithm and Algorithm for This figure emphasizes that although Algorithm has better asymptotic regret than Algorithm in practice Algorithm achieves better results In this experiment even rounds are not sufficient for Algorithm to approximate the arms accurately enough and therefore the graph for it is flat Moreover the gap between the average rewards is greater when the transfer cost is low The reason for this phenomenon is that Algorithm invests many rounds in the exploration of arms which should not be pulled according to the optimal algorithm while Algorithm can identify those arms early on and adapt Analysis of the pulls distribution and comparison of the average utility for lin and uni can be found in Appendix C DISCUSSION We introduced a MAB problem that models decision making from the perspective of Corporate Social Responsibility and allocation of opportunities Our modeling imitates many real-world scenarios where decision-makers are required to maximize their short-term utility while at the same time upholding fairness principles With our framework commercial companies can incorporate self-regulation in their algorithmic products and provide opportunities as a form of social responsibility We devised a no-regret algorithm and showed that its convergence rate is in fact optimal We see considerable scope for follow-up work Self-regulation for increasing subpopulation welfare can be incorporated in many Horizon A v g U ti li ty a Horizon A v g U ti li ty b Horizon A v g U ti li ty lin c Figure Average utility for different transfer costs Horizon P u ll s d is tr u ti on a Horizon P u ll s d is tr u ti on b Horizon P u ll s d is tr u ti on c Figure Round distributions per phase for Horizon A v g U ti li ty a Horizon A v g U ti li ty b Horizon A v g U ti li ty Fair ETC c Figure Average utility Algorithm vs Algorithm for other tasks eg in classification or load balancing Granting opportunities and investing in CSR can change subpopulations expected rewards over the long-term We are interested in the dynamic between the change in subpopulations expected rewards and the fairness policy The cost of corporate social responsibility considered in this paper is linear in the number of deterred opportunities Future work can investigate other forms of cost function Additional extensions of this framework can include more complex MAB scenarios such as contextual bandits or sleeping bandits