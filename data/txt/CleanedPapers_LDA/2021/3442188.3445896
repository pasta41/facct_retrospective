Re-imagining Algorithmic Fairness in India and Beyond Conventional algorithmic fairness is West-centric as seen in its subgroups values and methods In this paper we de-center algorithmic fairness and analyse AI power in India Based on qualitative interviews and a discourse analysis of algorithmic deployments in India we find that several assumptions of algorithmic fairness are challenged We find that in India data is not always reliable due to socio-economic factors ML makers appear to follow double standards and AI evokes unquestioning aspiration We contend that localising model fairness alone can be window dressing in India where the distance between models and oppressed communities is large Instead we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models empower oppressed communities and enable Fair-ML ecosystems CONCEPTS Human-centered computing Empirical studies in HCI KEYWORDS India algorithmic fairness caste gender religion ability class feminism decoloniality anti-caste politics critical algorithmic studies INTRODUCTION Despite the exponential growth of fairness in Machine Learning AI research it remains centred on Western concerns and histories the structural injustices eg race and gender the data eg ImageNet the measurement scales eg Fitzpatrick scale the legal tenets eg equal opportunity and the enlightenment values Conventional western AI fairness is becoming a universal ethical framework for AI consider the AI strategies from India Tunisia Mexico and Uruguay that espouse fairness and transparency but pay less attention to what is fair in local contexts Conventional measurements of algorithmic fairness make several assumptions based on Western institutions and infrastructures To illustrate consider Facial Recognition FR where demonstration of AI fairness failures and stakeholder coordination have resulted in bans and moratoria in the US Several factors led to this outcome Decades of scientific empiricism on proxies and scales that corresponds to subgroups in the West Public datasets APIs and freedom of information acts are available to researchers to analyse model outcomes AI research/industry is fairly responsive to bias reports from users and civil society The existence of government representatives glued into technology policy shaping AI regulation and accountability An active media systematically scrutinises and reports on downstream impacts of AI systems We argue that the above assumptions may not hold in much else of the world While algorithmic fairness keeps AI within ethical and legal boundaries in the West there is a real danger that naive generalisation of fairness will fail to keep AI deployments in check in the non-West Scholars have pointed to how neoliberal AI follows the technical architecture of classic colonialism through data extraction impairing indigenous innovation and shipping manufactured services back to the data subjects among communities already prone to exploitation under-development and inequality from centuries of imperialism Without engagement with the conditions values politics and histories of the non-West AI fairness can be a tokenism at best pernicious at worst for communities If algorithmic fairness is to serve as the ethical compass of AI it is imperative that the field recognise its own defaults biases and blindspots to avoid exacerbating historical harms that it purports to mitigate We must take pains not to develop a general theory of algorithmic fairness based on the study of Western populations Could fairness then have structurally different meanings in the non-West Could fairness frameworks that rely on Western infrastructures be counterproductive elsewhere How do social economic and infrastructural factors influence Fair-ML In this paper we study algorithmic power in contemporary India and holistically re-imagine algorithmic fairness in India Home to billion people India is a pluralistic nation of multiple languages religions cultural systems and ethnicities India is the site of a vibrant AI workforce Hype and promise is palpable around AI envisioned as a force-multiplier of socio-economic benefit for a large under-privileged population AI deployments are prolific including in predictive policing and facial recognition Despite the momentum on high-stakes AI systems currently there is a lack of substantial policy or research on advancing algorithmic fairness for such a large population interfacing with AI We report findings from interviews with researchers and activists working in the grassroots with marginalised Indian communities and from observations of current AI deployments in India We use feminist decolonial and anti-caste lenses to analyze our data We contend that India is on a unique path to AI characterised by pluralism socio-economic development technocratic nation-building and uneven AI capital which requires us to confront many assumptions made in algorithmic fairness Our findings point to three factors that need attention in Fair-ML in India Data and model distortions Infrastructures and social contracts in India challenge the assumption that datasets are faithful representations of people and phenomena Models are over-fitted for digitally-rich profiles typically middle-class men further excluding the without Internet access Sub-groups like caste endogamous ranked social identities assigned at birth gender and religion require different fairness implementations but AI systems in India are under-analyzed for biases mirroring the limited mainstream public discourse on oppression Indic social justice like reservations presents new fairness evaluations Double standards and distance by ML makers Indian users are perceived as bottom billion data subjects petri dishes for intrusive models and given poor recourse thus effectively limiting their agency While Indians are part of the AI workforce a majority work in services and engineers do not entirely represent marginalities limiting re-mediation of distances Unquestioning AI aspiration The AI imaginary is aspirational in the Indian state media and legislation AI is readily adopted in high-stakes domains often too early Lack of an ecosystem of tools policies and stakeholders like journalists researchers and activists to interrogate high-stakes AI inhibits meaningful fairness in India In summary we find that conventional Fair-ML may be inappropriate insufficient or even inimical in India if it does not engage with the local structures In a societal context where the distance between models and dis-empowered communities is large via technical distance social distance ethical distance temporal distance and physical distance a myopic focus on localising fair model outputs alone can backfire We call upon fairness researchers working in India to engage with end-to-end factors impacting algorithms like datasets and models knowledge systems the nation-state and justice systems AI capital and most importantly the oppressed communities to whom we have ethical responsibilities We present a holistic framework to operationalise algorithmic fairness in India calling for re-contextualising data and model fairness empowering oppressed communities by participatory action and enabling an ecosystem for meaningful fairness Our paper contributes by bringing to light how algorithmic power works in India through a bottom-up analysis Second we present a holistic research agenda as a starting point to operationalise Fair-ML in India The concerns we raise may certainly be true of other countries The broader goal should be to develop global approaches to Fair-ML that reflect the needs of various contexts while acknowledging that some principles are specific to context BACKGROUND Recent years have seen the emergence of a rich body of literature on fairness and accountability in machine learning eg However most of this research is framed in the Western context by According to Shanmugavelan caste is an inherited identity that can determine all aspects of ones life opportunities including personal rights choices freedom dignity access to capital and effective political participation in caste-affected societies Dalits broken men in Marathi are the most inferiorised category of the people are within the social hierarchy but excluded in caste categories We use the term Indic to refer to native Indian concepts researchers situated in Western institutions for mitigating social injustices prevalent in the West using data and ontologies from the West and implicitly imparting Western values eg in the premier FAccT conference of the papers published in and only a handful of papers even mention non-West countries and only one of them Mardas paper on New Delhis predictive policing system substantially engages with a non-Western context Western Orientation in Fair-ML Axes of discrimination The majority of fairness research looks at racial and gender biases in models two dimensions that dominate the American public discourse However these categories are culturally and historically situated Even the categorisation of proxies in fairness analyses have Western biases and origins eg the Fitzpatrick skin type is often used by researchers as a phenotype but was originally developed to categorise UV sensitivity While other axes of discrimination and injustices such as disability status age and sexual orientation have gotten some attention biases relevant to other geographies and cultures are not explored eg Adivasis indigeneous tribes of South Asia and Dalits As points out tackling these issues require a deep understanding of the social structures and power dynamics therein which points to a wide gap in literature Legal framing Since early inquiries into algorithmic fairness largely dealt with US law enforcement predictive policing and recidivism risk assessment as well as state regulations eg in housing loans and education the research framings often rely implicitly on US laws such as the Civil Rights Acts and Fair Housing Act as well as on US legal concepts of discrimination Indeed researchers since the late s have tried to translate US antidiscrimination law into statistical metrics The community also often repurposes terminology from US legal domains such as disparate impact disparate treatment and equal opportunity or use them as points of triangulation in order to compare technical properties of fairness through analogy with legal concepts Philosophical roots Connections have been made between algorithmic fairness and Western concepts such as egalitarianism consequentialism deontic justice and Rawls distributive justice Indeed notions of algorithmic fairness seem to fit within a broad arc of enlightenment and post-enlightenment thinking including in actuarial risk assessment Dr B R Ambedkars fondly called Babasaheb the leader and dominant ideological source of todays Dalit politics anti-caste movement was rooted in social justice distinct from Rawls distributive justice also see Sens critique of Rawls idea of original position and inadequacies of impartiality-driven justice and fairness Fairness status as the de facto moral standard of choice and signifier of justice is itself a sign of cultural situatedness Other moral foundations of cultural importance may often be overlooked by the West including purity/sanctity Traditional societies often value restorative justice which emphasises repairing harms rather than fairness eg contemporary Australian Aboriginal leaders emphasise reconciliation rather than fairness in their political goals Furthermore cultural relationships such as power distance and temporal orientation are known to mediate the importance placed on fairness Fairness perceptions across cultures Social psychologists have argued that justice and fairness require a lens that go beyond the Euro-American cultural confines While the concern for justice has a long history in the West eg Aristotle Rawls and the East eg Confucius Chanakya they show that the majority of empirical work on social justice has been situated in the US and Western Europe grounding the understanding of justice in the Western cultural context Summarising decades worth of research says that the more collectivist and hierarchical societies in the East differs from the more individualistic and egalitarian cultures of the West in how different forms of justice distributive procedural and retributive are conceptualised and achieved For instance compared the acquisition of fairness behaviour in seven different societies Canada India Mexico Peru Senegal Uganda and the USA and found that while children from all cultures developed aversion towards disadvantageous inequity avoid receiving less than a peer advantageous inequity aversion avoid receiving more than a peer was more prevalent in the West Similarly a study of children in three different cultures found that notions of distributive justice are not universal children from a partially hunter-gatherer egalitarian African culture distributed the spoils more equally than did the other two cultures with merit playing only a limited role See above point on Dr B R Ambedkars centring on priority-based social justice for caste inequalities The above works point to the dangers in defining fairness of algorithmic systems based solely on a Western lens Algorithmic fairness in the non-West The call for a global lens in AI accountability is not new but the ethical principles in AI are often interpreted prioritised contextualised and implemented differently across the globe Recently the IEEE Standards Association highlighted the monopoly of Western ethical traditions in AI ethics and inquired how incorporating Buddhist Ubuntu and Shinto-inspired ethical traditions might change the processes of responsible AI Researchers have also challenged the normalisation of Western implicit beliefs biases and issues in specific geographic contexts eg India Brazil and Nigeria and China and Korea Representational gaps in data is documented as one of the major challenges in achieving responsible AI from a global perspective For instance highlights the glaring gaps in geo-diversity of open datasets such as ImageNet and Open Images that drive much of the computer vision research shows that NLP models disproportionately fail to even detect names of people from non-Western backgrounds Accountability for unfairness Discussion of accountability is critical to any discussions of fairness ie how do we hold deployers of systems accountable for unfair outcomes Is it fair to deploy a system that lacks in accountability Accountability is fundamentally about answerability for actions and central to these are three phases by which an actor is made answerable to a forum information-sharing deliberation and discussion and the imposition of consequences Since outcomes of ML deployments can be difficult to predict proposals for accountability include participatory design and participatory problem formulation sharing the responsibility for designing solutions with the community Nissenbaum distinguishes four barriers to responsibility in computer systems the problem of many hands bugs blaming the computer and ownership without liability These barriers become more complicated when technology spans cultures more and more remote hands are involved intended behaviours may not be defined computer-blaming may meet computer-worship head on see Section and questions of ownership and liability become more complicated Specific to the Indian context scholars and activists have outlined opportunities for AI in India proposed policy deliberation frameworks that take into account the unique policy landscape of India and questioned the intrusive data collection practices through Aadhaar biometric-based unique identity for Indians in India Researchers have documented societal biases in predictive policing in New Delhi caste and ethnicity biases in job applications call-center job callbacks caste-based wage-gaps caste discrimination in agricultural loans decisions and even in online matrimonial ads METHOD Our research results come from a critical synthesis of expert interviews and discourse analysis Our methods were chosen in order to provide an expansive account of who is building ML for whom what the on-the-ground experiences are what the various processes of unfairness and exclusions are and how they relate to social justice We conducted qualitative interviews with expert researchers activists and lawyers working closely with marginalised Indian communities at the grassroots Expert interviews are a qualitative research technique used in exploratory phases providing practical insider knowledge and surrogacy for a broader community Importantly experts helped us gain access to a nascent and difficult topic considering the early algorithmic deployments in the public sector in India Our respondents were chosen from a wide range of areas to create a holistic analysis of algorithmic power in India Respondents came from Computer Science Activism Law and Public Policy Science and Technology Studies Development Economics Sociology and Journalism All respondents had  years of experience working with marginalised communities or on social justice Specific expertise areas included caste gender labour disability surveillance privacy health constitutional rights and financial inclusion respondents were based in India in Europe in Southeast Asia the rest in the USA of them self-identified as male as female and as non-binary In conjunction with qualitative interviews we conducted an analysis of various algorithmic deployments and emerging policies in India starting from Aadhaar We identified and analysed various Indian news publications eg TheWire in Times of India policy documents eg NITI Aayog Srikrishna Bill and community media eg Roundtable India Feminism in India and prior research Due to secondary sources our citations are on the higher side Recruitment and moderation We recruited respondents via a combination of reaching out directly and personal contacts using purposeful sampling ie identifying and selecting experts with relevant experience iterative until saturation We conducted all interviews in English preferred language of participants The semi-structured interviews focused on unfairness through discrimination in India technology production and consumption the historical and present role of fairness and ethics in India biases stereotypes and proxies data laws and policy relating to fairness and canonical applications of fairness evaluated in the Indian context Respondents were compensated for the study giftcards of USD EUR and INR based on purchasing power parity and non-coercion Employer restrictions prevented us from compensating government employees Interviews lasted an hour each and were conducted using video conferencing and captured via field notes and video recordings Analysis and coding Transcripts were coded and analyzed for patterns using an inductive approach From a careful reading of the transcripts we developed categories and clustered excerpts conveying key themes from the data Two team members created a code book based on the themes with seven top-level categories sub-group discrimination data and models law and policy ML biases and harms AI applications ML makers and solutions and several sub-categories eg caste missing data proxies consent algorithmic literacy and so on The themes that we describe in Section were then developed and applied iteratively to the codes Our data is analysed using feminist decolonial and anti-caste lenses A South Asian feminist stance allows us to examine oppressed communities as encountering and subverting forces of power while locating in contextual specifics of family caste class and religion South Asian feminism is a critique of the white Western feminism that saw non-western women as powerless victims that needed rescuing Following Dr B R Ambedkars insight on how caste hierarchies and patriarchies are linked in India we echo that no social justice commitment in India can take place without examining caste gender and religion A decolonial perspective borrowed from Latin American and African scholars like helps us confront inequalities from colonisation in India providing new openings for knowledge and justice in AI fairness research To Dr B R Ambedkar and Periyar E V Ramasamy colonialism predates the British era and decolonisation is a continuum For Dalit emancipatory politics deconstructing colonial ideologies of the powerful superior and privileged begins by removing influences and privileges of dominant-caste members Research ethics We took great care to create a research ethics protocol to protect respondent privacy and safety especially due to the sensitive nature of our inquiry During recruitment participants were informed of the purpose of the study the question categories and researcher affiliations Participants signed informed consent acknowledging their awareness of the study purpose and researcher affiliation prior to the interview At the beginning of each interview the moderator additionally obtained verbal consent We stored all data in a private Google Drive folder with access limited to our team To protect participant identity we deleted all personally identifiable information in research files We redact identifiable details when quoting participants Every respondent was given the choice of default anonymity or being included in