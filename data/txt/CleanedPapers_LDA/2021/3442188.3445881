Leveraging Administrative Data for Bias Audits Assessing Disparate Coverage with Mobility Data for COVID- Policy Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID- response strategies such as the targeting of public health resources Yet little attention has been paid to measurement validity and demographic bias due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics We illustrate how linking largescale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels More precisely we show that linking voter roll data-containing individual-level voter turnout for specific voting locations along with race and age can facilitate the construction of rigorous bias and reliability tests Using data from North Carolinas general election these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context older and non-white voters are less likely to be captured by mobility data We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups INTRODUCTION Mobility data has played a central role in the response to COVID Describing the movement of millions of people smartphone-based mobility data has been used to analyze the effectiveness of social distancing polices non-pharmaceutical interventions illustrate how movement impacts the transmission of COVID and probe how different sectors of the economy have been affected by social distancing policies Despite the high-stakes settings in which this data is deployed there has been no independent assessment of the reliability of this data In this paper we show how administrative data ie data from government agencies kept for administrative purposes can be used to perform such an assessment Data reliability should be a foremost concern in all policy-making and policy evaluation settings and is especially important for mobility data due to the lack of transparency surrounding data provenance Mobility data providers obtain their data from opt-in location-sharing mobile apps such as navigation weather or social media apps but do not disclose which specific apps feed into their data This opacity prevents data consumers such as policymakers and researchers from understanding who is represented in the mobility data a key question for enabling effective and equitable policies in high-stakes settings such as the COVID- pandemic Grantz et al describe a critical need to understand where and to what extent these biases may exist in their discussion on the use of mobility data for COVID- response Of particular interest is potential sampling bias with respect to important demographic variables in the context of the pandemic age and race Older age has been established as an increased risk factor for COVID-related mortality African-American Native-American and Latinx communities have seen disproportionately high case and death counts from COVID- and the pandemic has reinforced existing health inequities that affect vulnerable communities If certain races or age groups are not well-represented in data used to inform policy-making we risk enacting policies that fail to help those at greatest risk and serve to further exacerbate disparities In this paper we assess SafeGraph a widely-used point-of-interest POI-based mobility dataset for disparate coverage by age and race We define coverage with respect to a POI coverage is the proportion of traffic at a POI that is recorded in the mobility data For privacy reasons many mobility datasets are aggregated up from the individual level to the physical POI level Due to this aggregation we lack the resolution to assess individual-level coverage quantities like the fraction of members of a demographic subgroup of interest who are represented in the data Nonetheless our POI-based notion of coverage is relevant for many COVID- policies that are made based on traffic to POIs such as deciding to close certain business sectors allocating resources like pop-up testing sites to high-risk areas and determining where to target investigations of public health POIs refer to anywhere people spend money or time including schools brick-and-mortar stores parks places of worship and airports See order violations We use differences in the distributions of age and race across POIs to assess demographic disparities in coverage While we focus here on a specific dataset and implications for COVID- policy the question of how one can assess disparate coverage is a more general one in algorithmic governance Ground truth is often lacking which is precisely why policymakers and academics have flocked toward big data on the implicit assumption that scale can overcome more conventional questions of data reliability sampling bias and the like Government agencies may not always have access to protected attributes making fairness and bias assessments challenging The main contributions of our paper are as follows We show how administrative data can enable audits for bias and reliability We characterize the measurement validity of a smartphone-based mobility dataset that is widely used for COVID research SafeGraph We illuminate significant demographic disparities in the coverage of SafeGraph We illustrate how this disparate coverage may distort policy decisions to the detriment of vulnerable populations Our paper proceeds as follows Sections and discuss related work and background on the uses of mobility data in the pandemic Section provides an overview of our auditing framework formalizes the assumptions to construct bias and reliability tests and discusses the estimation approach using voter roll data from North Carolinas general election Section presents results that while SafeGraph can be used to estimate voter turnout the mobility data systematically undersamples older individuals and minorities Section discusses interpretation and limitations RELATEDWORK Our assessment of disparate coverage is related to several strands in the literature First the most closely related work to ours is SafeGraphs own analysis of sampling bias discussed below SafeGraphs analysis examines demographic bias only at the national aggregated level and does not address the question of demographic bias for POI-specific inferences Ours is the rst independent assessment of demographic bias to the extent we are aware Second our work relates to existing work on demographic bias in smartphone-based estimates A notable line of survey research has examined the distinct demographics of smartphone users and document significant concerns about mobility-based estimates from mobile phone data including particularly low coverage for elderly The literature further finds that smartphone ownership in the United States varies significantly with demographic attributes In an estimated of Americans owned smartphones with ownership rates of for those aged and ownership rates of for those aged over Racial disparities in smartphone ownership are less pronounced with an ownership rate of and for White Latinx and African-American individuals respectively Even conditional on mobile phone ownership however demographic disparities may still exist App usage may differ by demographic group According to one report of US teenagers for instance use Snapchat compared to of US adults Of particular relevance to mobility datasets the rate at which users opt in to location sharing may vary by demographic subgroup Hoy and Milne for instance reported that college-aged women exhibit greater concerns with third party data usage And even among users who who opt in to a specific app usage behavior may differ according to demographics Older users for instance may be more likely to use a smartphone as a classic phone Our work responds to a recent call to characterize the biases in mobility data used for COVID- policies Grantz et al highlight the potential for demographic bias citing clear sociodemographic and age biases of mobile phone ownership They note Identifying and quantifying these biases is particularly challenging though when there is no clear gold standard against which to validate mobile phone data We provide the rst rigorous test for demographic bias using auxiliary estimates of ground truth Third our work bears similarity to the literature on demographic bias in medical data and decision-making A long line of research has demonstrated that medical research is disproportionately conducted on white males This literature has cataloged the harmful effects of making treatment decisions for subgroups that were underrepresented in the data In much the same vein our work calls into question research conclusions based on SafeGraph data that may not be relevant for older or minority subgroups Last our work relates more broadly to the sustained efforts within machine learning to understand sources of demographic bias in algorithmic decision making Important work has audited demographic bias of facial recognition technology child welfare screening tools criminal risk assessment scores and health care allocation tools Often the underlying data is identified as a major source of bias that propagates through the algorithm and leads to disparate impacts in the decision-making stage Similarly our study illustrates how disparate coverage in smartphone-based data can misallocate COVID- resources BACKGROUND ON SAFEGRAPH MOBILITY DATA We now discuss the SafeGraph mobility dataset illustrate how this data has been widely deployed to study and provide policy recommendations for the public health response to COVID- and discuss SafeGraphs own assessment of sampling bias SafeGraph Mobility Data SafeGraph contains mobility data from roughly M mobile devices in the United States The company sources this data from mobile applications such as navigation weather or social media apps where users have opted in to location tracking It aggregates this information by points-of-interest POIs such as schools restaurants parks airports and brick-and-mortar stores Hourly visit counts are available for each of over M POIs in their database Individual device pattern data is not distributed for researchers due to privacy concerns Our analysis relies on SafeGraphs research release data which aggregates visits at the POI level See Use of SafeGraph Data in COVIDResponse When the pandemic hit SafeGraph released much of its data for free as part of the COVID- Data Consortium to enable researchers non-profits and governments to leverage insights from mobility data As a result SafeGraphs mobility data has become the dataset de rigueur in pandemic research The Centers for Disease Control and Prevention CDC employs SafeGraph data to examine the effectiveness of social distancing measures According to SafeGraph the CDC also uses SafeGraph to identify healthcare sites that are reaching capacity limits and to tailor health communications The California Governors Oce and the cities of Los Angeles San Francisco San Jose San Antonio Memphis and Louisville have each relied on SafeGraph data to formulate COVID- policy including evaluation of transmission risk in specific areas and facilities and enforcement of social distancing measures Academics too have employed the data widely to understand the pandemic used SafeGraph data to examine how social distancing compliance varied by demographic group and recommend occupancy limits for business types used SafeGraph to infer the effect of superspreader events such as the Sturgis Motorcycle Rally and campaign events examined whether social distancing was more prevalent in in areas with higher xenophobia and examined whether social distancing compliance was driven by political partisanship to name a few What is common across all of these works is that they assume that SafeGraph data is representative of the target population SafeGraph Analysis of Sampling Bias SafeGraph has issued a public report about the representativeness of its data While SafeGraph does not have individual user attributes eg race education income it merged census data based on census block group CBG to assess bias along demographic characteristics SafeGraph assigns each device an estimated home CBG based on where the device spends most of its nights and uses the demographics of the estimated home CBG for the bias assessment The racial breakdown of device holders for instance was allocated proportionally based on the racial breakdown of the devices estimated home CBGs SafeGraph then compared the total SafeGraph imputed demographics against census population demographics at the national level According to SafeGraph the results suggest that their data is well-sampled across demographic categories SafeGraphs examination for sampling bias should be applauded Companies may not always have the incentive to address these questions directly and SafeGraphs analysis is transparent with data and replication code provided As far as we are aware it remains the only analysis of SafeGraph sampling bias Nevertheless their analysis suffers from several key limitations Most notably this analysis does not use ground-truth demographic information and instead relies on imputed demographics using a method which suffers systematic biases For instance home CBG estimation is inaccurate for certain segments of the population such as nighttime workers Even when the estimated home CBG itself is correct their imputation of demographics from the CBG CBGs are geographic regions that contain typically between and residents CBGs are the smallest geographic unit for which the census publishes data imposes a strong homogeneity assumption The mere fact that of Atlantas population is African American does not guarantee that ve out of ten SafeGraph devices in Atlanta belong to AfricanAmericans Additionally the analysis uses an aggregation scheme which introduces two methodological limitations First because their analysis aggregates CBGs nationally the results are susceptible to undue influence from outliers such as those resulting from errors in home CBG estimation We anticipate these errors to be substantial since SafeGraph reports highly unrepresentative sampling rates at the CBG level including CBGs with four times as many devices as residents Second the results may also miss significant differences in the joint distribution of features because the analysis aggregates CBGs for a single attribute at a time For example if coverage is better for younger populations and for whiter populations but whiter populations are on average older than non-white populations then evaluating coverage marginally against either race or age will underestimate disparities Indeed we present evidence for such an effect in Lastly this analysis uses CBGs as the unit of analysis which may miss disparities that exist at ner geographic units such as POIs This distinction is noteworthy since many of the COVID analyses referenced above leverage SafeGraph data at ner geographic units than CBGs eg POIs This risks drawing conclusions from data at a level of resolution that SafeGraph has not established to be free from coverage disparities SafeGraph warns that local analyses examining only a few CBGs should proceed with caution Because SafeGraphs analysis examines demographic bias only at census aggregated levels and does not address the question of demographic bias for POI-specific inferences an independent coverage audit remains critical We provide such an audit using a method that uses POIs as the unit of analysis and avoids the noted methodological limitations AUDITING FRAMEWORK In this section we outline our proposed auditing methodology and state the conditions under which the proposed method allows us to detect demographic disparities in coverage We motivate our approach by rst describing the idealized audit we would perform if we had access to ground truth data We then introduce our administrative data and subsequently modify this framework to account for the limitations of the available data Notation Let I denote a set of SafeGraph POIs Let R denote a vector of the SafeGraph traffic count ie number of visits for day where each element indicates the traffic to POI on day Similarly let denote the ground truth traffic visits to POI during day When the context is clear we omit the superscript when referring to vectors R and R We use to denote Hadamard division the element-wise division of two matrices With this we define our coverage function See Fig of D C Let R R R denote the following coverage function The coverage function yields a vector where the ith element equals and describes the coverage of POI i Let denote a numeric measure of the demographics of visitors to POI on day for instance may be the percentage of visitors to a location on a specific day that are over the age of Let p denote the Pearson correlation between vectors and and let A be the rank function that returns the rank of vector Our audit will consider the Spearman rank correlation A which provides a more flexible and robust measure of association than the Pearson correlation Idealized Audit Our audit assesses how well SafeGraph measures ground truth visits and whether this coverage varies with demographics We operationalize these two targets as follows D M define the strength of measurement signal as A A positive signal indicates facial measurement validity and a signal close to one indicates high measurement validity D D We will say that disparate coverage exists when the rank correlation between coverage and the demographic measure is statistically different from zero cor A A We are interested in identifying an association of any kind we are not concerned with identifying a causal effect per se Age might have a causal effect on smartphone usage setting aside the question of manipulability as depicted in the top panel a of Fig But as the bottom panel b depicts age may not directly affect SafeGraph coverage but be directly correlated with a factor like urban/rural residence which in turn does affect SafeGraph coverage For either mechanism the policy-relevant conclusion remains that SafeGraph is underrepresenting certain age groups In reality there is no ground truth source of information about foot traffic and the corresponding demographics for all million POIs Instead we must make do with estimates of and based on auxiliary data sources about some subset of visits to a subset of POIs In order to identify the relationship of interest between coverage and demographics we need the following to hold D N The estimation procedure does not induce a confounding factor that affects both the estimate of demographics and the estimate of coverage The rank assigns each element of the vector the value of its rank in an increasing ranking of all elements in the vector For example the rank of vector would be Appendix C discusses the analogous assumptions required to identify the target for measurement validity DB EA a Causal association ADA A EA b Non-causal association Figure Possible mechanisms under which disparate coverage arises Disparate coverage may be a result of a causal associations such as a whereby older people are less likely to own or use smartphones and therefore places frequented by older people have lower coverage Disparate coverage may also arise due to a non-causal associations such as b whereby rural regions have higher percentages of older residents and worse cell reception which reduces coverage Both types of associations are policy-relevant because in both cases certain age groups are underrepresented D N The selection is not based on an interaction between factors that affect coverage and demographics We emphasize the difficulty in obtaining this information It is challenging to obtain estimates of foot traffic to POIs In fact researchers typically treat smartphone-based mobility data as if it were ground truth eg It is even more challenging to identify data sources for ground truth visits to POIs with corresponding demographic information Consider for instance large sporting events where stadium attendance is closely tracked Can we leverage differences in audience demographics based on the event eg international soccer game between two countries in order to assess disparate coverage Two major impediments are lack of access to precise demographic estimates as well as confounding factors such as tailgating that may vary with demographics Administrative data on voter turnout We propose a solution using large-scale administrative data that records individual-level visits along with demographic information voter turnout data in North Carolinas general election from L a private voter le vendor which aggregates publicly available voter records from jurisdictions nationwide Our analysis relies primarily on four fields in the L voter les age race precinct and turnout The L data is missing one key piece of information the poll location We use a crosswalk of voting precinct to poll location obtained from the North Carolina Secretary of State to map each voter via their voting precinct to a SafeGraph POI Overall our data includes voters who turned out to vote at voting locations that could be matched Table presents summary statistics on voters associated with polling locations that could be matched showing that our data is highly representative of all voting locations Details on the data and preprocessing are provided in Appendices A and B Derived from official certified records by election authorities voter turnout information is of uniquely high fidelity In an analysis of ve voter le vendors Pew Research for instance found that the vendors had agreement about turnout in the election See Matched Voters All Voters Voters Mean Age Std Age Proportion over Proportion Hispanic Proportion Black Proportion White Table Demographics of all voters in North Carolinas general election compared to voters included in our analysis matched voters The matched voters are representative of the full voting population Details of the matching procedure are given in Appendix B Voter registration forms typically include fields for date of birth gender and often race When race is not provided data vendors estimate race The Pew study found race to be accurate across the ve vendors with accuracies varying from for AfricanAmericans to for Hispanics to for non-Hispanics We can identify individuals visiting a specific voting location on election day because North Carolina differentiates in person election day voters from absentee mail and early voters We note that poll locations are often schools community centers religious institutions and re stations These POIs may hence also have non-voter traffic on election day We address this possible source of confounding by adjusting the SafeGraph traffic using an estimate of non-voter traffic Adjustment for non-voter traffic Non-voter traffic may be incorporated into SafeGraph measures and may confound our analysis if the magnitude of that non-voter traffic varies with the demographic attributes of the voters For instance if younger voting populations are more likely to vote at community centers which have large non-voter traffic and older voting populations are more likely to vote at re stations which have small non-voter traffic then even if SafeGraph has no disparate coverage we would observe a negative relationship between coverage and age We control for this confounding by estimating non-voter traffic using mean imputation In Appendix D we provide similar results using a linear regression imputation procedure Additional notation Letting denote election day we estimate the non-voter traffic at poll location on election day by North Carolina for instance requests both race and ethnicity com/dlncsbegov/Voter_Registration/NCVoterRegForm_Wpdf The study did not name which voter le vendors were analyzed Non-voter traffic may be affected by device attribution errors in which device GPS locations are incorrectly assigned to one of two adjacent POIs SafeGraph reports in its user documentation that it is more difficult to measure visits to a midtown Manhattan Starbucks than a visit to a suburban standalone Starbucks If younger voting populations are more likely to vote in dense urban polling locations then even if there isnt large non-voter traffic in the same facility large traffic in an adjacent facility could still be incorrectly attributed to the polling location with greater likelihood than to a suburban polling location However this source of confounding can be controlled for using the same technique described averaging SafeGraph traffic to on adjacent days This adjustment enables us to compute the marginal traffic over the estimated baseline which we term SafeGraph marginal traffic D M SafeGraph marginal traffic denotes device counts above estimated baseline Let denote the number of voters at poll location as recorded by L With this we refine our definition of coverage using the coverage function from D SG SafeGraph coverage is Each element of this vector refers to the ratio of marginal traffic at POI to voter turnout at Audit via voter turnout The disparate impact question in this setting is does SafeGraph coverage of voters at different poll locations vary with voter demographics We focus on two key demographic risk factors for COVID- age and race We summarize the age distribution at a polling location by computing the proportion of voters over age For race we consider the proportion of voters who are an ethnic group besides white formalizes this question as testing whether there is a rank correlation between and demographic measure However such a test may be misleading if we have induced confounding by our estimation procedure We can incorporate a test of confounding into our audit specifically we can test for time-invariant confounding D A time-invariant confounder affects our demographic estimate as well as traffic on election day and on non-election days This contrasts to a time-varying confounding D A time-varying confounder affects our demographic estimate and traffic on election day only It does not affect traffic on non-election days Examples of time-invariant and time-varying confounding are given in Figure The assumption of no time-varying confounding is untestable but it is reasonable to believe this holds in our setting Most voting places for instance are public places making it unlikely that the non-voter traffic is affected differentially on election and non-election days Another possible time-varying confounder would be if voting locations with older or largely non-white voters are more likely to be placed outside of the SafeGraph geometry for device attribution eg parking lot We do not believe this is likely because voting locations are typically indoors for security and climate reasons during a November election We can accommodate time-invariant confounding in our audit by modifying the definition of disparate coverage The adjustment resulted in negative estimates of voter traffic for poll locations at schools In the Appendix B we show that baseline traffic estimation is generally much worse for school due in part to school holidays or large-scale events such as sports games As a result we exclude schools from our analysis In what follows we use the generic variable to indicate either measure of demographics Time-invariant confounding Example Younger voting populations vote at places like community centers with large non-voter traffic whereas older populations vote at places like re stations with little non-voter traffic Testable see Time-varying confounding Example Younger voting populations vote at places that are open to non-voter traffic on election day whereas older populations vote at places that are closed to non-voter traffic on election day Untestable assumption Figure We distinguish between two types of confounding time-invariant versus time-varying confounding We test for time-invariant confounding but we cannot test for time-varying confounding Our results assume no time-varying confounding D D We will say that disparate coverage exists when the rank correlation between coverage on election day and voter demographics is statistically different from the rank correlation between coverage on non-election day and voter demographics For cor A A cor A A We evaluate this more robust notion of disparate coverage using weekdays in October and November of to generate a placebo distribution of the estimated correlation coefficients against which we compare the election-day estimate Algorithm provides details note that I denotes the indicator function This procedure is similar to methods of randomization inference in the literature on treatment effects If the election-day correlation is unlikely under placebo distribution ie small -value and we additionally believe there is no time-varying confounding then we can conclude that SafeGraph has disparate coverage of voters on election day Algorithm Assessing Disparate Coverage Input Voter data SafeGraph data Result -value for the election-day correlation under the placebo distribution for do Compute d A end return Id d In order to generalize these endings to the broader population on non-election day the selection cannot be based on factors that affect both coverage and demographics See Example violations might include i The older or non-white population that doesnt vote is more likely to use smartphones than the older or non-white population that does vote and ii Older or non-white voters leave We use weekdays in October and November except October and November and November and are adjacent to election day so the baseline adjustment would be biased Out of convenience we drop October and November to avoid having to pull September and December data to respectively compute their baseline adjustments their smartphones at home when they go vote but always carry their smartphones otherwise whereas younger or white voters bring their smartphones to the polls and elsewhere We believe such mechanisms are unlikely Testing this assumption would require the use of an additional auxiliary dataset which is outside the scope of this paper We emphasize that this assumption of no selection bias can still hold even though the voting population is not a random sample of the population with respect to demographics In fact since the voting population is older and more white than the general population the association between coverage and age/race among voters could very well underestimate the magnitude of the population association We should also consider the association between demographics age and race It is well known that younger populations have a larger proportion non-white relative to older populations and this holds in our sample Polling locations with younger voters are also more likely to have higher proportions of minority voters Appendix A Additionally there are widespread concerns that disparate impact can be more pronounced at the intersection of protected groups We can jointly test for disparate coverage by modifying for the multiple regression setting We perform linear regressions to model coverage as a function of the percentage over and the percentage non-white for each weekday in October and November We test whether the election-day coefficients on age/race are different from the non-election day coefficients on age/race provides details using the notation that denotes the proportion of voters over age and denotes the proportion of voters who are non-white RESULTS Measurement Validity Election day brings a dramatic increase in traffic to polling locations relative to non-election days and any valid measure of visits should detect this outlier Figure shows the daily aggregate traffic across poll locations for October and November of and as expected we see a significant increase in both total traffic top panel and marginal traffic bottom panel on election day To assess the strength of this signal using the framework described above we present the correlation between marginal SafeGraph traffic on election day and actual voter turnout The rank correlation test yields a positive correlation cor A A with Algorithm Assessing Joint Disparate Coverage Input Voter data SafeGraph data Result -values for the election-day coefficients on race and age under the placebo distribution for do Fit a linear regression U V end return U and IV V -value Figure displays this relationship by comparing the marginal election traffic on the G-axis against actual voter counts on the -axis for each polling location This corroborates that SafeGraph data is able to detect broad patterns in movement and visits That said the estimates at the individual polling place location level are quite noisy root mean squared error is voters For instance amongst polling places that registered marginal devices roughly to actual voters turned out This significant noise is likely due to a combination of factors First SafeGraph may incorrectly attribute voters to nearby POIs because of incorrect building geometries Second we may not be able to perfectly adjust for non-election traffic Third SafeGraph may have disparate coverage of voters by demographic attributes This last factor is the focus of our analysis Demographic Bias We assess whether the demographic composition of voters who actually turned out to vote in person is correlated with coverage We begin with preliminary results and then proceed to our main disparate coverage results as defined in Preliminary results Polling locations with older votes have lower coverage rates The top panel of Figure shows how SafeGraph coverage varies with the proportion of voters over age The rank correlation test yields cor A A with -value We also show how coverage decreases as the proportion of non-white voters increases bottom panel The rank correlation of race and coverage is cor A A with -value The top panel of Figure presents a heat map of coverage with age bins quartiles on the G-axis and race bins quartiles on the -axis This bottom left cell for instance shows that precincts that are the most white and young have highest coverage rates The lowest coverage is for older minority precincts The lower panel of Figure similarly plots race on the G-axis against coverage on the -axis separating older precincts yellow and younger precincts blue Older precincts on average have lower coverage rates than younger precincts and coverage declines as the minority population increases The correlation is similar but slightly lower for unadjusted SafeGraph traffic cor A A with -value All M Oct Oct Nov Nov Dec Date Sa fe G ra ph tr Monday Tuesday Wednesday Thursday Friday Figure SafeGraph traffic by weekday over October and November for all polling locations in North Carolina The top panel shows all SafeGraph traffic and the bottom panel shows the marginal traffic computed using the method in In both total and marginal traffic the election day dotted line shows a significant boost in traffic SafeGraph marginal election traffic te rs Figure Election day traffic as observed by SafeGraph Gaxis and actual voter turnout across polling locations axis Each dot represents a polling location in North Carolina in the general election Main results Figure shows that the negative election-day rank correlation between coverage and voter demographics is significantly outside the placebo distribution for non-election days Percentage over age C ov er ag e Voters Percentage nonwhite C ov er ag e Voters Figure Estimated SafeGraph coverage rates against age and race for North Carolina general election Each point displays a ventile of poll location by age top and race bottom The blue lines depict LOESS smoothing on the individual poll locations empirical one-sided -values are for both age and race respectively For our joint analysis of disparate coverage we nd that the negative coefficients for age and race are statistically outside the placebo distribution See Fig empirical one-sided -values are and for age and race respectively Our endings are robust to time-invariant confounding Assuming no selection bias or time-varying confounding we can In Appendix C we present similar placebo results for measurement validity Age quartile oldest R ac e ar e la rg es t p er ce nt n on hi te Coverage Percentage nonwhite C ov er ag e Voters Poll age elder young Figure Intersectional coverage effects by race and age The top panel presents the coverage rate by quartiles of age on the G-axis and race on the -axis The bottom panel plots the coverage rate on the -axis against percentage of nonwhite voters at the polling location on the G-axis for older polling locations yellow versus younger polling locations blue for ventiles of poll location by race Lines display linear smoothing of the individual poll locations Coverage is lowest among older minority populations and highest among younger whiter populations conclude that SafeGraph has disparate coverage by age and race two demographic risk factors for COVID age race Rank correlation co un t Election Regular Figure Distribution of placebo rank correlations between election-day demographics and marginal SafeGraph traffic on non-election days Under the empirical placebo distribution the election-day coverages negative correlations with age top panel and race bottom panel are very unlikely value Placebo correlations computed for weekdays in October and November age race Coefficient of linear regression of coverage on age and race co un t Election Regular Figure Placebo distribution of coefficients of the linear regression of marginal SafeGraph coverage on election-day age and race demographics Under the empirical placebo distribution the election-days negative coefficients for age and top panel and race bottom panel are very unlikely -value This suggests that SafeGraph data has disparate coverage by age and race Regressions computed for weekdays of October and November Policy implications We now examine the policy implications of disparate coverage in light of the widespread adoption of SafeGraph data in COVIDresponse In particular we show how disparate coverage may lead to under-allocation of important health resources to vulnerable populations For instance suppose the policy decision at hand is where to locate mobile pop-up COVID- testing sites and suppose the aim is to place these sites in the most tracked areas to encourage asymptomatic individuals to get tested One approach could use SafeGraph traffic estimates to rank order POIs How would this ordering compare to the optimal ordering by ground truth traffic Using voter turnout as an approximation to ground truth traffic we perform linear regression of the rank of voter turnout against rank according to SafeGraph marginal traffic as well as age and race A A Table presents results of this rank regression where rank is in descending order confirming that the SafeGraph rank is significantly correlated with ground truth rank But the large coefficient on age indicates that each percentage point increase in voters over is associated with a point drop in rank relative to the optimal ranking Similarly the coefficient on race indicates that a point increase in percent non-white is associated with a one point drop in rank relative to the optimal ranking This demonstrates that ranking by SafeGraph traffic may disproportionately harm older and minority populations by for instance failing to locate pop-up testing sites where needed the most Table To evaluate a potential rank-based policy allocation we compare the rank of voter turnout against rank by SafeGraph traffic controlling for age and race in a linear regression Although SafeGraph rank is correlated with the optimal rank by voter turnout the coefficients on age and race indicate that each demographic percentage point increase is associated with a -point and -point drop in rank for age and race respectively This indicates that significant adjustments based on demographic composition should be made to a SafeGraph ranking Failure to do so may direct resources away from older and more minority populations Dependent variable Voter turnout rank SafeGraph rank over non-white Constant Observations R Adjusted R Residual Std Error df Statistic df Note p p p We also consider the implications of using SafeGraph to inform proportional resource allocation decisions such as the provision of masks We compare the allocation based on SafeGraph traffic to the allocation based on voter turnout data Table presents results for polling locations binned into four age-race groups by partitioning at the median proportion over and median proportion non-white Each cell presents the proportion of resources that would be allocated to that age-race bin demonstrating that strict reliance on SafeGraph would under-allocate resources by to the oldest/most non-white category -value and over-allocate resources by to the youngest/whitest category -value SafeGraph Optimal Percent allocation allocation difference young white young non-white older white older non-white Table Allocation of resources for age-race groups by SafeGraph versus by true voter counts with standard errors in parentheses The SafeGraph allocation redirects over one-third of the optimal allocation from the oldest most nonwhite group to the youngest whitest group -value The clear policy implication here is that while SafeGraph information may aid in a policy decision auxiliary information including prior knowledge should likely be combined to make final resource allocation decisions DISCUSSION We have provided the rst independent audit of demographic bias of a smartphone-based mobility dataset that has been widely used in the policy response to COVID- Our audit indicates that the data underrepresents two high risk groups older and more nonwhite populations Our results suggest that policies made without adjustment for this sampling bias may disproportionately harm these high risk groups However we note a limitation to our analysis Because SafeGraph information is aggregated for privacy reasons we are not able to test coverage at the individual level To avoid a potential ecological fallacy our results should be interpreted as a statement about POIs rather than individuals That is POIs frequented by older or minority visitors have lower coverage than POIs frequented by younger or whiter populations Of course policy decisions are typically made at some level of aggregation so the demographic bias we document at this level remains relevant for those decisions A key future research question is how to use the results of this audit to improve policy decisions We suggest a few possible future directions A bias correction approach would construct weights to adjust estimates based on race and age Such an approach crucially requires knowledge about demographic composition In policy settings where such information is not readily available it may be fruitful to investigate whether mobility data companies like SafeGraph can provide normalized visit counts based on the estimated demographic prole of the smartphone user This could oer a significant improvement over current normalization approaches which per SafeGraphs recommendation use census block group CBG-based normalization factors While this bias correction might help to estimate population parameters eg percentage of CBG population not abiding by social distancing it is unlikely to capture the kind of demographic interaction effects we document here Much more work should be done to study disparate coverage and ideally provide for instance a weighing correction to the normalization factors that properly accounts for the demographic disparities documented in this audit Another possible solution is increased transparency Researchers do not know details about the source of SafeGraphs mobility data namely which mobile apps feed into the SafeGraph ecosystem Access to such information may make the bias correction approach more tractable If for instance researchers could identify that a data point emanates from Snapchat then they could use what is known about the Snapchat user base to make adjustments Given its increasing importance for policy SafeGraph should consider disclosing more details about which apps feed into their ecosystem CONCLUSION Mobility data based on smartphones has been rapidly adopted in the COVID- response As note one of the most profound challenges arising with such rapid adoption has been the need to assess the potential for demographic bias when there is no clear gold standard against which to validate mobile phone data Our paper illustrates one potential path forward by linking smartphone-based data to high-fidelity ground truth administrative data Voter turnout records which record at the individual level whether a registered voter traveled to a polling location on a specific day and describe the voters demographic information enable us to develop a straightforward audit test for disparate coverage We nd that coverage is notably skewed along race and age demographics both of which are significant risk factors for COVID- related mortality Failure to address such disparities risks policy distortions based on mobility data that could exacerbate serious existing inequities in the health care response to the pandemic