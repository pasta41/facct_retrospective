Differential Tweetment Mitigating Racial Dialect Bias in Automated systems for detecting harmful social media content are afflicted by a variety of biases some of which originate in their training datasets In particular some systems have been shown to propagate racial dialect bias they systematically classify content aligned with the African American English AAE dialect as harmful at a higher rate than content aligned with White English WE This perpetuates prejudice by silencing the Black community Towards this problem we adapt and apply two existing bias mitigation approaches preferential sampling pre-processing and adversarial debiasing in-processing We analyse the impact of our interventions on model performance and propagated bias We find that when bias mitigation is employed a high degree of predictive accuracy is maintained relative to baseline and in many cases bias against AAE in harmful tweet predictions is reduced However the specific effects of these interventions on bias and performance vary widely between dataset contexts This variation suggests the unpredictability of autonomous harmful content detection outside of its development context We argue that this and the low performance of these systems at baseline raise questions about the reliability and role of such systems in high-impact real-world settings CONCEPTS Social and professional topics Race and ethnicity Censorship Computing methodologies Machine learning Natural language processing KEYWORDS bias fairness racial disparities dialect content moderation INTRODUCTION As the body of social media content grows explosively so does the problem of harmful content hate speech cyberbullying and online abuse among others Manual content moderation can be expensive for platforms given the volume and rates of posted content and is both tedious and traumatic for human moderators Waiting for users to flag content as inappropriate allows harmful content to proliferate with potentially widespread consequences this has prompted interest in content moderation ex ante as content is posted and before it reaches an audience Recently the COVID- pandemic has sent home thousands of human moderators many of whom are not allowed to work from home For all these reasons automated detection of harmful content by artificial intelligence AI systems has been the subject of considerable academic and industrial research Racial Dialect Bias Large platforms such as Facebook Google including YouTube and Twitter have turned to automation as a crucial element of industrial content moderation They use automated screening to flag content for human review even before it is flagged by users One key reason for the introduction of these automated systems was to reduce the prevalence of hateful speech and incitements of violence against marginalised communities In other words one of their central goals is to protect historically disadvantaged groups Yet they have been shown to perpetuate biases against various marginalised communities which arise from datasets on which they are trained Previous research has measured racial dialect bias in harmful tweet detection systems Such research focuses on Twitter due to high availability of labeled harmful tweet corpora and because Twitter is an important space for Black activism and activism in general but we expect this issue to exist on other online platforms as well Tweets with high predicted African-American English AAE alignment are found to be classified as harmful at a higher rate than tweets with high predicted White English WE alignment or low These alignments are calculated using a model that was trained on a corpus of million geolocated tweets using US Census data as topics The model is shown to accurately follow known linguistic phenomena Twitter can take a range of actions if a tweet violates their rules which protect against hate speech incitements of violence and targeted harassment of an individual or group among other things They may hide the tweet behind an interstitial warning limit its visibility in search results and feeds or even require its removal and hide it in the meantime However each of these enforcement actions when differentially applied across populations could amplify societal injustices either by enforcing stereotypes held by users or by outright silencing minority communities It is painfully ironic that harmful tweet detection systems may systematically diminish the voice of the Black community given that they exist at least in part to shield this and other marginalised communities from harm Furthermore bias may reduce their ability to truly detect harmful content by rewarding them for simply associating linguistic properties of AAE with harmful labels Dialect Alignment as a Continuous Variable In the fairness and bias sphere a protected attribute is a characteristic that is protected against discrimination and against which bias is suspected or known to exist Race is a widely accepted protected attribute but in this context the race of a tweets author is often unknown the best we can do without additional information is to estimate probabilities of dialect alignment such as AAE itself contains much regional and social variation and the dialect estimation tool is probabalistic and so has some margin for error Most previous research dichotomises data by considering a tweet to be highly AAE aligned if the dialect model assigns it An exception is Sap et al who treat as continuous when evaluating dataset bias however they too dichotomise the data for model evaluation Using only high-confidence dialect predictions reduces the risk of compounding errors from dialect estimation and harm prediction However this approach is only effective in large and diverse datasets where an appreciable number of tweets satisfy this constraint Harmful tweet corpora are smaller and less representative and as such often contain too few high-AAE tweets for meaningful analysis three of the datasets considered in this study contain a mere and instances of such tweets In these datasets the problems associated with dichotomisation which include reduced statistical power and obfuscation of variation within groups cannot be ignored It is also worth noting that low dialect diversity in training datasets unsurprisingly causes bias and poor performance on dialect-aligned input in systems In this paper we treat as continuous in line with precedent and also assert that this is justified by empirical observation See Appendix A This allows us to evaluate bias by comparing predictions to dataset labels while preserving statistical power whereas existing research on racial dialect bias either relies on evaluation via external datasets or loses power by dichotomisation at the model bias evaluation stage Our treatment of as continuous has considerable implications there is no privileged or unprivileged group so our definitions of fairness and bias mitigation efforts must address itself rather than group membership Bias Mitigation The fairness of AI systems in general is under intense scrutiny A first wave of algorithmic accountability is working to address known bias and discrimination in AI systems while a second wave asks broader questions about the role and governance of autonomous systems Addressing racial dialect bias in harmful content moderation is particularly relevant and important of late as the world reckons with systemic racism as recently highlighted by the death of George Floyd and far too many other Black people at the hands of police in the US Vigilant rejection of online content that incites or perpetuates hatred of any kind particularly that aimed at Black people or other marginalised groups is an important step toward dismantling institutional racism Yet to systematically silence Black voices online in pursuit of this goal is categorically counter-productive It may be possible to mitigate racial dialect bias in automated harmful tweet detection using technical interventions Automated bias mitigation has been explored in other machine learning ML contexts though it has been criticised for being incomplete at best and at worst for obscuring the root problem and inducing unforeseen consequences Some research has explored the problem of racial dialect bias in detecting harmful online content but little attention has been paid to addressing this bias Doing so is not a straightforward task the continuity of dialect nuance of language processing and challenges of defining harm demand new and adapted approaches We apply automated bias mitigation techniques to racial dialect bias in harmful tweet detection systems In keeping with previous research our efforts are focused only on this specific type of bias although these systems are afflicted by many other types some of which may have yet to be clearly identified We evaluate the bias mitigation approaches in terms of their ability to reduce dialect bias measured in a variety of ways and their effects on classification performance While they show some promise both the extent of bias reduction and the impact on classification performance are highly variable across dataset contexts different ways data are collected pre-processed and annotated according to dataset creators research goals These differences are in turn responsible for differences in dataset size class definitions and distributions of linguistic features such as dialect among other things We argue that this variation and the associated uncertainty in harmful content detection systems raise important questions about the role of such systems in society and underscore the difficulties of deploying opaque autonomous systems with real-world impacts BACKGROUND Harmful Content Detection Motivation Although some have argued that autonomous detection of harmful content is necessary it is a difficult task The terminology used to describe harmful content is abstract and inconsistent and harm itself is a subject-dependent broadly defined category Automated detection of harmful tweets presents still more challenges given the colloquial short and noisy nature of tweets the lack of background knowledge and context in ML systems something that is acceptable in one context may be wholly inappropriate in another and differences in cultural contexts that cannot be standardised Existing Approaches While social media content can take many multimedia forms text posts remain prevalent and are therefore our focus Consistent with much literature in the space our research focuses on Twitter though our findings may be relevant to other text-oriented online platforms State-of-the-art harmful content detection systems consist of neural networks which train on deep text features but networks that train on surface-level word and character features have also been shown to perform well Such systems often embed word and character n-grams contiguous sequences items using either bag-of-words or term frequency-inverse document frequency TFIDF which normalises counts according to the frequency of the n-gram in question in the whole corpus Trained on labeled harmful content corpora these systems classify unseen text as problematic or benign Bias in Harmful Content Detection ML systems for detection of harmful content like many other ML systems are prone to various types of bias Collection Annotation Preprocessing Model Training Predictions Data Social/Historical Sampling/Selection Label Algorithmic A B C Figure The ML pipeline indicating forms of bias Preprocessing bias mitigation intervenes in region A in processing in region B and post-processing in region C Research has uncovered racial dialect bias in harmful tweet detection datasets and systems findings show correlations between and harmful labels in training datasets and that models systematically classify tweets aligned with AAE as harmful at higher rates than their WE-aligned counterparts Different forms of bias can arise at different stages in the data processing pipeline Figure shows some forms of bias that are particularly relevant in this context Social or historical bias is driven by past decisions actions sentiments norms and laws that we may consciously recognise as outdated or inappropriate but which nonetheless shape the structural conditions of society Social and historical biases may lead to differences in underlying distributions of harmful content across dialects for instance research has shown a correlation between profanity use on Twitter and the prevalence of AAE in a geographical region Data collection can produce sampling or selection biases when non-random sampling or data cleaning lead to data that are not representative of real-world distributions Sampling bias could lead to correlations between harmful class labels and AAE in datasets which might not exist in random samples At the point of annotation label bias may arise if annotators label tweets differently according to dialect based on either human prejudice or cultural differences Finally bias can be propagated into models and even exacerbated at the stage of model training a phenomenon called over-amplification or algorithmic bias Deployment bias arises when systems are used or their outputs interpreted in inappropriate ways however this occurs after predictions are made and is therefore beyond the scope of this paper Automated Bias Mitigation Automated bias mitigation is an increasingly popular subject of ML and natural language processing NLP research It intervenes at one of three phases pre-processing bias mitigation changes the input data in-processing bias mitigation changes the model itself and post-processing bias mitigation changes model outputs These distinctions are shown in Figure Given input and class labels these approaches aim to make predictions fair with respect to a protected attribute Defining Fairness Many different mathematical notions of algorithmic fairness exist In keeping with existing research in this field we consider group fairness which dictates similar treatment across demographic lines Individual fairness which dictates similar treatment for similar individuals could potentially be assessed in this context Though defining similarity here is challenging it may be possible using synthetic datasets or appropriate NLP-specific metrics Consistent with the existing literature we consider group fairness acknowledging that exploring individual fairness may be an area for future work Demographic parity parity requires independence between predictions and the protected attribute in this case that means predictions made by harm detection systems are independent of Alternatively it may be beneficial to account for base rates Equal odds requires independence of and conditional on ground truth labels in this case it requires that each labels false positive and false negative rates are independent of This independence ensures both that benign AAE-aligned tweets are not moderated disproportionately frequently and that harmful tweets with low AAE alignment are not overlooked disproportionately frequently We evaluate systems using both parity and equal odds as it is important to respectively consider predictions both in isolation and as they relate to real-world outcomes Existing Bias Mitigation Approaches NLP-specific bias mitigation approaches tend to intervene at the pre-processing phase Examples include debiased word embeddings and counterfactual data augmentation both of which have been explored in the context of gender bias More general pre-processing approaches aim either to modify the training dataset via massaging reweighing or resampling or to create intermediate representations of training data on which models then train These approaches generally assume a binary protected attribute but most can be adapted for a continuous one such as Some in-processing approaches use a regularisation term to directly penalise any dependence bewteen and within the models training loss function Others use adversarial learning in which an adversary network is trained to predict based on thereby coaxing the predictor network to make predictions that are independent of Regularisation has been adapted to be compatible with continuous and and adversarial debiasing is theoretically shown to be as well however we are not aware of any continuous-𝑆 applications of adversarial debiasing Post-processing bias mitigation relies on the use of a holdout validation data subset and does not require knowledge of the mapping between input data and predictions This makes it an appealing choice for third parties or when classification occurs in a black box However without access to input data or predictive models post-processing approaches can only reassign some predictions according to a function We focus on strategies for mitigating bias when the data and learning process can be accessed and modified ie pre-processing and in-processing methods because these interventions aim to ameliorate the problem of model bias itself rather than its symptoms RESEARCH DESIGN Datasets We consider four labeled harmful tweet datasets Founta et al Davidson et al Waseem Hovy and Golbeck et al These along with a dataset produced by Waseem that we deemed too small for reliable classification and bias evaluation in our study comprise a prominent set of English-language datasets with tweets labeled by some type or types of harm Founta et al boost the prevalence of harmful tweets by combining randomly sampled tweets with a sample of tweets that are likely to belong to harmful classes based on text analysis and results of earlier labelling rounds Each of the other three datasets filters tweets based on the presence of words from a hate speech lexicon words relating to religious or ethnic minorities or a curated list of terms that correlate with harassment in an exploratory search Founta et al define hateful as language that is hateful toward an individual or group abuse as an insult debasement or violent targeted interaction spam as unwanted information and everything else normal Waseem Hovys definitions of sexism and racism are straightforward and Golbeck et al define harassment as anything that is extremely violent or offensive threatening hateful toward a group or designed to upset an individual or group Davidson et al define hate speech as targeting a specific group offensive language as all other tweets that are perceived as offensive and neither as everything else It is not immediately clear whether the offensive language class should be considered harmful or not However in keeping with prior research we consider it harmful for two reasons First the datasets creators note that human annotators tend to consider sexist language to be offensive rather than hateful but targeted sexism is clearly harmful and violates Twitters rules concerning abuse Second we find that many tweets labeled as merely offensive contain language that can be considered as unequivocally hateful slurs such as ngger and fggot which we feel ought to be considered harmful Importantly the datasets use different annotation methods Founta et al and Davidson et al crowdsource amateurs to undertake annotation five and at least three annotators per tweet respectively whereas Waseem Hovy and Golbeck et al annotate the data themselves Waseem Hovys annotations are reviewed by an outside gender studies expert and Golbeck et al underwent extensive training prior to annotating their data Such differences in sampling and annotation methods which result in different dataset sizes and class distributions impact downstream classifiers and can differentially give rise to some of the biases outlined in For Davidson et als dataset in line with its creators we only consider the tweets for which a majority of annotators at least half agree on a label We also gathered results for the subset of Davidson et al for which there was full agreement among annotators however the results were very similar to the majority data so for the sake of brevity they are excluded The four datasets are used to train models with a label-stratified train/test split Dataset bias gives an important standard against which to measure propagated bias Previous research measures dataset bias using the Pearson-𝑟 correlation between each label and which we call It is also useful to consider the mean value of among all tweets in a certain class this is more resilient to uneven class sizes but less so to the size and location of the overall distribution Table shows our calculations of the bias present in the full datasets training and test sets combined in terms of per-label and Harmful labels tend to have higher and positive whereas non-harmful labels tend to have lower and negative This suggests the presence of bias in the sense that there is a positive relationship in the data between Dataset Label Count p AAE a r label Founta et al normal spam abusive hateful Davidson et al language neither hate speech Waseem Hovy none sexism racism Golbeck et al normal harassment Table Dataset bias measured by per-label mean and the Pearson-𝑟 correlation between each label and denotes harmful labels AAE alignment and harmful labels Dialect bias exists in all of the datasets but it is far more extreme in the datasets produced by Founta et al and Davidson et al This may be due in part to their use of amateur annotation but is likely also related to the means by which they collect and preprocess their data We use two additional datasets to evaluate bias in model predictions extrinsically Blodgett et al contains nearly million tweets labeled by and estimations Preoţiuc-Pietro and Unger contains nearly million tweets labeled by the self-reported race of the author Because these datasets are not labeled according to harm we do not evaluate them for bias Bias Evaluation Metrics Intrinsic Bias Metrics We measure bias intrinsically by examining the relationship between dialect and predictions made on held-out testing subsets of our training datasets Because we consider as continuous we cannot easily split tweets into privileged and unprivileged groups a prerequisite for most bias metrics We therefore define four new metrics The first is where and are the average values of across all harmful and non-harmful labels respectively Lower means that predictions made by a system are more fair according to demographic parity because there is a smaller difference in the dialect alignment of tweets labeled as harmful versus non-harmful The other three metrics consider the Pearson-𝑟 correlation between and predicted labels This technique is used to measure racial dialect bias in datasets and other types of correlation are used to measure unfairness in other settings with continuous protected attributes For each label we measure the overall correlation the correlation for subsets of tweets that carry that label in the dataset and the correlation for subsets of tweets that do not carry that label That is for a given label among tweets with among tweets with where if and if In keeping with existing research we assume a priori that the average tweet is not inherently more or less toxic in a particular dialect We define three bias metrics derived from these correlations where and are the average values of across all harmful and non-harmful tweets respectively and this convention is extended Like measures violations of parity as unconditional dependencies between and label predictions By conditioning on a given true label provides information on false negative classifications for that label and by conditioning on the absence of a given true label provides information on false positive classifications Therefore and together tell us to what extent equal odds is upheld Across all four intrinsic bias evaluation metrics positive values indicate bias against AAE negative values indicate bias in favour of AAE and values of zero indicate no bias Extrinsic Bias Metrics We measure bias extrinsically by examining patterns in predictions made on tweets in external datasets that are labeled either by dialect alignment or self-reported author race Because Blodgett et als dataset is sufficiently large we define groups based on dialect alignment in keeping with existing research AAE-aligned if and WE-aligned if In Preoţiuc-Pietro and Ungers dataset tweets are naturally grouped by the self-reported race of their authors For predictions made on we define the following metrics is harmful AAE is harmful WE AAE is harmful AAE is harmful measures the gap in proportion of harmful label predictions between AAE- and WE-aligned tweets and AAE measures the gap in proportion of harmful label predictions between AAE-aligned tweets and all tweets AD for all dialects For predictions made on we define the following metrics black white is harmful black is harmful white black all is harmful black is harmful These measure equivalent gaps in predicted harmful across self-reported author race groups All four extrinsic evaluation metrics measure violations of parity fairness positive values against AAE and negative values in its favour Neither external dataset labels the tweets by type of harm so we cannot compare predictions to real-world outcomes therefore we cannot evaluate equal odds fairness extrinsically Baseline Classifier As a baseline model we train a convolutional neural network CNN with two hidden layers based on the observation that CNN are well-suited to this task and because they are easily compatible with adversarial debiasing Hyperparameters were chosen to optimise performance within the bounds of reasonable training times based on five-fold stratified cross-validation on Founta et als dataset This dataset was chosen because it contains four somewhat vague and overlapping labels but is large enough to train reasonably accurate models We experimented with tweet embedding via Dataset Precision Recall BK Founta et al Davidson et al Waseem Hovy Golbeck et al Table Baseline weighted average performance per dataset Dataset Label Precision Recall BK abusive hateful D offensive language D hate speech WH sexism WH racism G harassment Table Performance for the harmful labels in each dataset Per-label BK performance is not available for all datasets bag-of-words vectors and TFIDF using both character and word features Ultimately TFIDF embeddings of character and -grams were chosen The network trains for epochs using batches of tweets It uses an Adam optimiser with a decaying learning rate initially set to The number of units per hidden layer is calculated based on an analysis by Huang et al Table shows weighted average precision recall and f-score for the baseline model evaluated in-domain on held-out test sets Importantly the performance is close to though not quite as good as the best known BK performance that we could find in the literature for each dataset The classifier is reasonably able to identify harmful content in most datasets however like even the most discerning human annotators it struggles to differentiate between different types of harm and like the BK models it also performs poorly in classification of some specific harmful labels as Table shows Recall tends to be lower than precision for harmful labels which suggests that the baseline system under-classifies content as harmful A weakness of weighted average f-score as a measure of system performance is that systems can have high weighted average f-scores while performing poorly on harmful labels as a result of the relatively low prevalence of harmful labels in all datasets except that of Davidson et al It is nonetheless a useful way to quantify a systems performance across labels Bias Mitigation Pre-Processing Preferential Sampling Preferential sampling mitigates dataset bias by resampling datapoints with high classification uncertainty it duplicates antistereotypical points and drops pro-stereotypical ones This changes the dataset distribution to reduce discrimination while preserving much of the information for training It has been shown to effectively reduce bias while maintaining reasonably high performance in classification settings with binary and and is less intrusive than similarly effective dataset bias mitigation techniques Harmful tweet datasets often contain more class labels than simply harmful and not harmful and differences between different types of harmful label can be important We are tasked then with a multi-class classification problem In the context of multi-class and continuous a preferential sampling implementation must differ from the original algorithm in three ways The measure of uncertainty should be compatible with multiple classes Because resampling data with more extreme high or low is likely to have a greater impact should be a factor in considering which data objects to duplicate or drop For the same reason we cannot pre-calculate an optimal number of datapoints to duplicate or drop The first point is addressed by defining a measure of predictive uncertainty similar to margin sampling We argue that generally speaking misclassifications of harmful tweets as nonharmful and vice versa are more consequential than misclassifications within either harmful or non-harmful label sets classifying harmful tweets as non-harmful allows harm to go undetected and unmoderated and classifying non-harmful tweets as harmful may stifle benign sentiments of a population or individual Therefore the margin of confidence is defined as the margin between the highest predicted label probability and the highest predicted probability of a label of the opposite harm value where predictions are made using a basic logistic regression classifier This encourages the system to resample tweets that are most likely to be misclassified as harmful when they are not and vice versa The second point is addressed by adding a term to a data objects resampling candidacy that encapsulates the extremity of its AAE alignment We use the absolute value of a tweets normalised rank for the tweet with lowest for the tweet with highest and for the median Overall then a tweets resampling candidacy is defined as where is a hyperparameter As a result tweets are the strongest candidates for resampling when they both have extreme and lead to a low margin of confidence between a harmful and nonharmful label prediction The third constraint is addressed by performing resampling iteratively Rather than pre-calculating an ideal number of duplications and deletions we resample data objects at a time until bias is reduced to below a threshold We define as the average of either if fairness is defined as demographic parity or if fairness is defined as equality of odds across all labels in the dataset This penalises any bias even if the bias favours AAE alignment Importantly different fairness definitions impact the algorithm only to the extent that they differently determine when the threshold for termination is reached the resampling process itself is the same Because there is a limit to bias that can be removed by resampling data objects this way eventually begins to increase if is set too low As a result the algorithm terminates if at any point is greater than or equal to its value from the previous iteration In this implementation and are tunable hyperparameters Intuitively sets the importance of predictive uncertainty relative to extremity of in determining a tweets candidacy for resampling it is a hyperparameter because the relative importance of these factors is not immediately obvious determines to what extent the data are resampled how much bias should be reduced A standardised batch size is used For each dataset and each definition of fairness we train models using all combinations of the following hyperparameter values We choose values that lead to the lowest prediction bias defined according to the fairness definition used for resampling without reducing weighted average f-score by more than of baseline We find that the values of and impact classification performance and propagated bias but they do not do so consistently across datasets Each value is best for at least one dataset and fairness definition as are all values of except which may be too high a threshold to force substantial resampling Bias Mitigation In-Processing Adversarial Debiasing Adversarial debiasing unintrusively attempts to remove any bias whether against or in favour of any demographic group In processing bias mitigation by regularisation has been adapted to the case of a continuous protected attribute but adversarial debiasing appears not yet to have been explored in this case We adapt the adversarial debiasing code developed by IBM to be compatible with multi-class classification and add a hidden layer of size to the adversary network in keeping with According to the original implementation the predictor network updates its prediction weights according to where is its loss is the adversary networks loss and is a tunable hyperparameter The magnitude of determines the adversarys strength in debiasing classifications made by the predictor presumably at the expense of higher In order to enforce demographic parity the adversary network receives as input only the predictions during training For equal odds enforcement the adversary receives both and true labels For each dataset a model with adversarial debiasing was trained using each fairness definition and with the following values of We use the same criteria as with and to choose in each case Generally higher values of lead to greater bias reduction all chosen values are either or Finally we combine preferential sampling and adversarial debiasing by training models using adversarial debiasing on resampled data To select hyperparameter values and for these models we explore all combinations of the three best sets of and and the three best values of It is rarely the case that the best values for both preferential sampling and adversarial debiasing alone are also best when the two are combined and we find substantial variation in which values are most effective between datasets EXPERIMENTAL RESULTS Using each bias mitigation approach independently and combined and using both parity and equal odds definitions of fairness we train seven types of models using the following notation B baseline model Pr p preferential sampling parity fairness Our code is available at Pr e preferential sampling equal odds fairness Adv p adversarial debiasing parity Adv e adversarial debiasing equal odds p both preferential sampling and adversarial debiasing parity e both preferential sampling and adversarial debiasing equal odds For each model type and training dataset five individual models were trained All reported results in this section represent averages across these sets of five models as this smooths the variation due to random variable initialisation in the neural network That said we find variation is generally low between models We evaluate models based on their performance in-domain and cross-domain and on the bias they propagate measured intrinsically and extrinsically Performance and intrinsic bias evaluations were performed using a single University of Cambridge Computer Lab GPU machine and the extrinsic using the University of Cambridge Research Computing Services Wilkes GPU cluster In-Domain Classification Table shows the weighted average f-score of each model evaluated in-domain on the held-out test subset of the dataset on which it was trained Bias-mitigated models tend to perform nearly as well in-domain as baseline for Founta et al and Davidson et als datasets On these datasets we see that adversarial debiasing yields slightly higher classification performance than preferential sampling or a combination of approaches Bias-mitigated f-score is roughly equal to baseline for Waseem Hovy and Golbeck et als less biased datasets D WH G B Pr p Pr e Adv p Adv e p e Table Weighted average f-score for baseline and bias mitigated models evaluated in-domain The highest score for each dataset appears in bold Cross-Domain Classification One measure of a systems generalisability is its cross-domain classification performance In this case it is impossible to perform multi-class classification because each dataset uses different labels Therefore in keeping with previous research we restrict cross-domain classification to a binary task between harmful and non-harmful tweets as defined in Table For each model we predict labels for the entire training plus test subsets datasets on which the model was not trained We calculate weighted average f-score across harmful and non-harmful labels Because each model can be evaluated on each of the three datasets on which it was not trained twelve dataset permutations exist in total As before the reported f-score for each dataset permutation represents an average over five models Figure shows these results full tabular data appear in the Appendix A See for technical specifications Founta et al Davidson et al Waseem Hovy Golbeck et al Testing Dataset gh te d A ve ra ge S co re Trained on Founta et al Trained on Davidson et al Trained on Waseem Hovy Trained on Golbeck et al Founta et al B Pr p Pr e Adv p Adv e p e Founta et al Davidson et al Waseem Hovy Golbeck et al Testing Dataset gh te d A ve ra ge S co re Trained on Founta et al Trained on Davidson et al Trained on Waseem Hovy Trained on Golbeck et al Figure Weighted average f-score organised by testing dataset Marker shape indicates training dataset and colour indicates bias mitigation approach Dashed lines indicate baseline in-domain f-score for each testing dataset Performance Suffers Cross-Domain In line with previous findings models perform worse cross-domain than in-domain This suggests that dataset bias including but not restricted to dialect bias or other discriminatory biases may cause harmful tweet detection systems to perform worse in the real world than in their development contexts Differences Between Datasets As Figure shows models trained on Founta et als data perform best when evaluated on other datasets followed closely by Waseem Hovy then Golbeck et al and then Davidson et al Given that the best and worst performing datasets are both the largest and the most biased against AAE per Table we observe that neither dataset size nor dataset bias relates clearly to cross-domain classification performance We observe that the baseline models often perform better than bias-mitigated models however these performance differences are not very consistent At least one bias-mitigated model performs at least as well as baseline in a majority of dataset permutations including all six permutations that train on Waseem Hovy or Golbeck et als datasets Generally adversarial debiasing yields higher cross-domain performance than preferential sampling or a combination of the two but training on Waseem Hovys dataset provides a notable exception Similarly equal odds generally yields higher performance than parity but this result is also inconsistent The level of dialect bias present in a dataset appears to impact the extent to which bias mitigation affects cross-domain performance The variance in f-score and the decrease in performance when bias mitigation is applied are greatest when the two more biased datasets produced by Founta et al and Davidson et al are used for both training and testing The most consistent f-scores occur when the less biased datasets produced by Waseem Hovy and Golbeck et al are used for both training and testing Combinations of the two fall in the middle though classification performance on Founta et als data is preserved through bias mitigation when models are trained on one of the less biased datasets This again underscores the importance of the data collection and annotation Intrinsic Bias Evaluation Figure compares bias metrics for predictions made by models in-domain on each dataset Each graph represents a dataset Within each graph the two left bar groups show and which measure violations of demographic parity The two right bar groups Founta et al Davidson et al Waseem Hovy r rF Golbeck et al Founta et al B Pr p Pr e Adv p Adv e p e Figure Intrinsic bias results from in-domain classification From left to right and show and which measure violations of equal odds As described in higher bars indicate higher bias against AAE For each dataset there exists some bias mitigation approach that substantially reduces bias against AAE at a small or nonexistent performance cost However the reduction in bias is not perfect Differences Between Datasets In most cases we observe that preferential sampling decreases bias particularly and to a greater extent than adversarial debiasing That said no single approach is consistently best for reducing all types of bias while maintaining high performance and the success of different approaches varies across datasets For instance on Davidson et al and Waseem and Hovys data preferential sampling with fairness defined by parity creates negative with a higher magnitude than baseline which suggests a greater violation of equal odds but with misclassifications favouring AAE tweets with high are more likely to be misclassified as benign and less likely to be misclassified as harmful For these datasets adversarial debiasing with a parity definition of fairness reasonably reduces bias without reversing it Yet on Founta et al and Golbeck et als data preferential sampling leads to less bias of all types and in the case of Golbeck et al adversarial debiasing exacerbates bias above baseline These differences indicate variation in the types and nature of bias present For example each dataset uses a different sampling method before and distinct from our resampling to increase the prevalence of harmful content Sampling biases arising from these different methods may be differentially amenable to correction by resampling Meanwhile adversarial debiasing targets propagated algorithmic bias Because social label and sampling biases each affect bias propagation differently and because they are each present to different extents in these datasets it seems natural that the impact of adversarial debiasing is varied Tension Between Fairness Types Across datasets those bias mitigation approaches that best improve demographic parity fairness occasionally do so at the expense of equal odds This phenomenon appears strongest when preferential sampling is employed whether in combination with adversarial debiasing or not On all datasets except Golbeck et al training models on resampled data using a parity definition of fairness produces negative values for or both This is unsurprisingly usually accompanied by the largest decreases in predictive performance In fact Figure does not even tell the whole story in some cases where and are small individual harmful labels have high magnitude negative correlations with This indicates bias in favour of AAE for those labels but can also hide substantial bias against AAE for other labels in secondary bias metrics An advantage of adversarial debiasing over preferential sampling is that the tradeoff between parity and equal odds fairness is less pronounced making it easier to better balance the two This makes sense given the mechanisms by which the two approaches work preferential sampling seeks always to advantage AAE-aligned tweets in its duplications and removals whereas adversarial debiasing works to minimise any relationship between and predictions It is important for those who implement and deploy harmful content detection systems to consider the extent to which they wish to enforce different types of fairness In some cases the solution might be straightforward For example preferential sampling of Golbeck et als dataset enforces both parity and equal odds in models each of and is nearly zero However the data show that generally some degree of either parity or equal odds fairness must be sacrificed in order to optimise the other This is consistent with the observation that parity and equal odds are incompatible given different base rates That is if is dependent of it is impossible for to be both unconditionally independent of and conditionally independent of given In all but very equal datasets differences in base rates may make it impossible to enforce a satisfactory degree of both parity and equal odds Extrinsic Bias Evaluation Figure compares bias metrics for predictions made by models on external datasets labeled by either dialect alignment or self-reported author race Each graph represents a training dataset Within each graph the two left bar groups show and AAE which measure gaps in the proportions of tweets predicted to be harmful along dialect lines The two right bar groups show black white and black all which measure gaps in the proportions of tweets predicted to be harmful along racial lines Recall that all of these metrics measure bias only as violations of parity because tweets in the external datasets are not labeled according to harm Once again higher bars indicate higher bias against AAE As with intrinsic bias for each dataset there is some form of bias mitigation that appears capable of reducing extrinsic bias somewhat In some cases there is tension between the ability of bias mitigation approaches to promote equality along dialect lines versus along author race lines but this is not a consistent trend Differences Between Datasets Consistent with the intrinsic evaluation Figure shows that not only are the baseline bias levels Founta et al Davidson et al Waseem Hovy AAE AD black white black all Golbeck et al Founta et al B Pr p Pr e Adv p Adv e p e Figure Extrinsic bias evaluation results From left to right AAE black white and black all different across datasets but also that bias mitigation has different effects in different contexts Individual approaches substantially reduce bias for some datasets but are ineffective or create additional biases for other datasets For instance using a combination of preferential sampling and adversarial debiasing effectively reduces bias against both AAE-aligned tweets and tweets written by Black authors in the datasets produced by Founta et al Waseem Hovy and Golbeck et al However while this approach reduces bias against tweets written by Black authors to near zero in the two Davidson et al datasets it creates substantial negative and AAE which suggests new imbalance in favour of AAE Once again no one approach is consistently superior This contextual dependence suggests that the link between experimental results and real-world behaviour may be more tenuous than one would hope context clearly matters but its impact is a much more complex problem So while there is reason to be optimistic that bias mitigation strategies can reduce differential moderation of online content we must aim to better understand these relationships before and as we deploy such technologies DISCUSSION Our results indicate that both preferential sampling and adversarial debiasing can substantially reduce though not completely eliminate bias against AAE in the task of harmful tweet detection at little to no performance cost However bias and performance impacts vary between datasets in ways that are not always straightforward This complex context dependence and performance inconsistencies in different settings raise questions about the fairness of automated content moderation systems and their use Bias Rooted in Datasets The context dependence of bias and performance responses to bias mitigation suggests the importance of the entire data collection annotation and processing pipeline Classification performance may be improved by filtering and boosting training data to increase the prevalence of harmful tweets and by using scalable annotation methods to allow for larger datasets However our results suggest that may be at the cost of more severe dialect bias Rather than identifying a single best approach to mitigating bias against AAE in datasets and models our results further suggest that those who implement and deploy harmful content detection systems are best placed to undertake bias analyses They can then explore options for minimising dialect bias that align with their specific contexts goals and values and can be held to account based on these Other bias mitigation techniques may be more effective than those we implemented and are worth exploring especially when the bias is introduced by human labellers who may not have a nuanced understanding of subconscious biases against AAE For instance multi-task learning has been shown to substantially reduce marginalised identity bias in harmful online comment detection We also recommend that further research continue to seek a deeper understanding of the sources and nature of racial dialect bias in this and other NLP tasks Importantly racial dialect bias is not the only form of bias that impacts harmful content detection systems Research has exposed bias that discriminates against marginalised identity mentions and other forms of bias such as topic and author bias can hurt system accuracy metrics A deeper understanding of the range of bias that can exist might enable more complete mitigation of biases including those not yet identified Towards True Fairness Competing Definitions of Fairness The task of building a fair harm detection model is made challenging by biased datasets and by conflicting perspectives on how to define fairness In this paper we have proposed equal odds systems should not systematically misclassify tweets as more harmful the more they align with AAE and less harmful the less they do However bias and ambiguity in the datasets limit our ability to calculate equal odds In contrast with many other domains such as true recidivism or loan default rates underlying objective truth class labels are unknown in the harmful tweet context for a variety of reasons including vague and overlapping class definitions annotation bias and the subjectivity of harmful content Therefore calculated false positive and negative rates may themselves have issues of bias which weakens evaluations of equal odds fairness For this reason it may be valuable to also consider demographic parity fairness which enforces the same proportion of classification as harmful across groups even though this may be problematic if there are true differences in distributions What is the most appropriate metric of fairness depends on the context On one hand freedom of expression is at stake with the risk of disproportionately silencing an already-marginalised group On the other hand there is a risk that truly harmful content could go undetected Efforts towards greater fairness in correctly classifying AAE tweets should take seriously the need to prevent harmful content from reaching too wide an audience and any tensions or trade-offs between these risks must be considered Ultimately our aspiration toward fairness is grounded in a belief that harmful content detection systems and AI and technology more generally should reflect advance and support a society to which we aspire even if that means a slight dissonance with societys current state Bias-aware Use The harms averted by automated harmful tweet detection systems and those caused by their biases ultimately depend on the way they are deployed For instance at present automated systems are used to screen tweets for review by human moderators Dialect priming has been shown to reduce dialect bias at the point of dataset annotation similarly priming human moderators for dialectal differences might reduce the extent to which Black voices are silenced online Platforms could choose to never remove content outright based on automated predictions but rather to hide it behind interstitial warnings While this would not fully silence communities per se it might perpetuate societal biases by priming readers to expect harm when they encounter AAE It is critical that the merits and shortcomings of these systems be evaluated and debated not in isolation but in relation to the ways that they are used and impact people Implications of Uncertainty The often-tenuous connections between in-domain and cross-domain performance and between intrinsic and extrinsic bias demonstrate the difficulty of predicting real-world impact based on experiments performed only in limited research and development contexts The bias mitigation approaches explored seem capable of reducing bias along some axes while maintaining high in-domain classification performance but this cannot be guaranteed in the complex real-world A certain level of uncertainty inevitably accompanies applying systems outside of their development settings This uncertainty raises the question of what role such technologies which for better or worse impact our social and political systems worldwide can and should play in society It is clear that harmful tweet detection systems are far from perfect and that there is no silver bullet to solve their problems However these systems can serve an important role protecting people from online abuse and hate Further human decisions are also afflicted by bias which is reflected in the label bias we observe in datasets Should AI systems be held to a higher standard than their human counterparts should we choose to deploy them Although structural issues afflict both humans and autonomous systems individual human bias is just that individual whereas algorithmic bias can have systemic effects by crystallising and reperpetuating the bias at-scale It has been argued that we ought to answer important questions about the role and scope of technological interventions before they are implemented and that the reformist nature of bias mitigation research can distract from these deeper issues However content classification and moderation systems are already in place It is important to simultaneously interrogate their role and make them as fair as possible In fact as we have demonstrated attempting to mitigate bias using technical interventions can shed light on new facets of the problem or at the very least reveal how incomplete our understanding is Social Medias Political Economy More fundamentally there are questions of whether it is possible to produce fair systems for commercial platforms in societies that are systemically unfair Other questions are raised by the commercial nature of platforms such as Twitter These platforms are now the technical infrastructure on which parts of society rely but are also sites of power control and profit Research on improving systems that produces more effective control of social infrastructure by platforms by for example improving their moderation systems may contribute to increasing those companies ability to influence communications according to their commercial priorities while at the same time offloading some responsibility for and cost of getting their systems right themselves Moreover concerns of platformised predatory inclusion are also raised whereby greater inclusiveness ultimately works to increase marginalised groups exposure to forms of control and revenue extraction rather than addressing structural disadvantage This is not to say that such research is not important on the contrary ensuring that Black people and others can access and use social platforms without being subject to harm or discriminatory moderation is a societal imperative But for platforms to be truly inclusive of marginalised or minoritised communities research also needs to address the business models and structural features of those platforms such as their design and affordances that can contribute to the prevalence of hate speech in the first place CONCLUSION In this paper we have explored mitigating racial dialect bias in a neural network for harmful tweet detection by adapting two approaches preferential sampling pre-processing and adversarial debiasing in-processing These techniques tend to reduce systematic bias against AAE measured both intrinsically and extrinsically while maintaining a high degree of performance for in-domain prediction However we observe the extent to which bias and performance are impacted by our interventions is extremely dependent on dataset context A cross-domain performance evaluation further reveals the differences in the behaviour of harmful tweet detection systems within and outside of their training contexts These unavoidable uncertainties raise important questions regarding the role of automated harmful content detection and other AI technologies in society There is value in attempting to mitigate bias however the inconsistencies and shortcomings of our bias mitigation strategies indicate how complex these biases can be This research is inherently limited in that it attempts to address a social problem though admittedly one that has been exacerbated by technology through purely technical means Quantitative representations of bias can illuminate and mitigate critical and unforeseen challenges and computational interventions can relieve their symptoms However this work represents only a starting point We hope that our research will promote a continued conversation on societal and personal biases fair AI technologys political economy and the broader role and risks of technology