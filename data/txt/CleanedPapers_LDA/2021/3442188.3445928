Building and Auditing Fair Algorithms A Case Study in Candidate Screening Academics activists and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased Achieving this goal however is complex the developer must deeply engage with social and legal facets of fairness in a given context develop software that concretizes these values and undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms To date there are few examples of companies that have transparently undertaken all three steps In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics a startup that uses machine learning to recommend job candidates to their clients We discuss how pymetrics approaches the question of fairness given the constraints of ethical regulatory and client demands and how pymetrics software implements adverse impact testing We also present the results of an independent audit of pymetrics candidate screening tool We conclude with recommendations on how to structure audits to be practical independent and constructive so that companies have better incentive to participate in third party audits and that watchdog groups can be better prepared to investigate companies CONCEPTS Social and professional topics Gender Race and ethnicity Employment issues Codes of ethics KEYWORDS algorithm auditing four-fifths rule adverse impact testing fairness INTRODUCTION Increasing concern over bias in automated systems has led to an outcry for companies incorporate fairer and more transparent systems However automated systems are complex requiring that developers deeply engage with social and legal facets of fairness in a given context develop software that concretizes these values and undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms To date there are few examples of companies that have transparently undertaken all three steps pymetrics is a startup that offers a candidate screening service aka pre-employment assessment to employers based on data and applied machine learning ML One of the core assertions pymetrics makes about their service is that they pro-actively debias ML models before deployment to comply with the US Uniform Guidelines on Employee Selection Procedures UGESP pymetrics claims to use an outcome-based model de-biasing process where candidate models are assessed for compliance with the UGESP four-fifths rule using minimum bias ratio as a metric and then retrained as necessary to ensure compliance In this paper we outline our process of auditing pymetrics as a case-study for creating transparent and accountable systems We have two goals to present the process we used to audit pymetrics candidate screening product as a replicable framework and to present the results of this specific audit With regards to process we introduce the cooperative audit as a framework for external algorithm auditors to audit the systems of willing private companies Cooperative audits are unlike existing proposals for audits that involve insider employees or outsider audits where the target company is unaware of the testing eg Or if no compliant model can be found pymetrics abandons the training process and no model is deployed Given the unique challenges of the cooperative format we needed to develop careful protocols to ensure the independence and transparency of the auditors and the audit itself With respect to the audit of pymetrics we scoped the audit to five specific questions all related to the fairness guarantees that pymetrics claims their system implements We leveraged source code analysis consideration of human behavior and statistical analysis of data to investigate these questions We found that pymetrics candidate screening service did faithfully implement the stated fairness guarantees and that their systems included sufficient safeguards against human error and intentional malicious behavior to reasonably ensure compliance with the four-fifths rule We conclude with recommendations on how to structure cooperative audits to be practical independent and constructive By developing the framework for cooperative audits and demonstrating the outcome of one such audit we hope to incentivize companies to participate in more third-party algorithm audits Furthermore we aim to provide guidance for watchdog groups that may wish to become third-party auditors and regulators interested in how to structure mandatory third-party audits of systemically important sociotechnical systems BACKGROUND We begin by presenting context for our audit starting with concerns about the use of ML in hiring and followed with a brief introduction to the practice of algorithm auditing Fairness in Algorithmic Hiring Since at least the mid-s critical scholars have been raising alarms about the potential for computer systems to embed entrench and compound social biases These voices have grown louder and the concerns more acute as data-driven ML systems have seen increased adoption The adoption of ML techniques in the domain of hiring is particularly contentious On one hand discrimination in hiring driven by human biases is a long standing and widespread problem From this perspective using ML to evaluate job seekers has the potential to remove human biases from the hiring process potentially leading to more equitable outcomes On the other hand there is no reason to assume a priori that ML systems in the hiring domain will automatically be objective neutral or bias-free Indeed algorithm audits of gig-economy marketplaces and traditional resume boards have uncovered race and gender biases in these systems As startups have emerged that apply ML to the hiring process scholars have begun to investigate the legal conceptual and practical space in which they operate Raghavan et al surveyed publicly available information about startups offering pre-employment assessment systems to map their practices into the law and policy space especially claims about compliance with the UGESPs four-fifths rule Ajunwa and Kim both present extensive taxonomies of the ways that bias may emerge in ML-based hiring systems and This paper is written from the perspective of the audit team Work produced by the pymetrics team is labeled in third person pymetrics while work conducted by the auditors is in the first person plural we pymetrics contributed to the background design and discussion of this manuscript but did not materially author nor alter any audit methods or results of the audit map these to US legal doctrine Whereas these studies reported aggregated publicly available information the current study investigate a single company to an unprecedented level of access Title VII of the US Civil Rights Act of distinguishes between two forms of discrimination that may impact hiring processes disparate treatment and disparate impact The former refers to cases where people are directly discriminated against based on legally protected attributes such as race and gender In the ML context avoiding disparate treatment is often operationalized as a prohibition against the use of protected attributes as input features for models Disparate impact refers to cases where a facially neutral process still produces substantially different outcomes for people that are correlated with legally protected attributes With respect to ML there are many de-biasing techniques that aim to ensure that models do not produce disparate impact although scholars have found that not all of these fairness objectives are mathematically compatible We examine both disparate treatment and disparate impact in our audit of pymetrics Algorithm Auditing Raji et al write that audits are tools for interrogating complex processes With respect to modern sociotechnical systems Sandvig et al motivate the need for algorithm audits as a means to investigate normatively significant instances of discrimination involving computer algorithms operated by Internet platforms While some have likened algorithm auditing to reverse engineering by outsiders in that the goal is to make black-box systems more transparent regardless of the system creators intent this conceptualization has since been expanded to include audits carried out by ethically and morally-conscious insiders There is a growing body of algorithm audits assessing a variety of systems for a diverse set of harms This includes examining broad classes of systems like search e-commerce news recommendation online advertising maps ridesharing online reviews and ratings natural language processing and recommendation Some audits have specifically focused on algorithms in high-stakes contexts like facial recognition predictive policing housing and child protective services In the absence of regulation or accepted best-practices recent work has attempted to define a process for algorithm auditing Raji et al developed a six step process for auditing that we expand upon in our audit of pymetrics This process includes scoping see mapping which involves interviewing stakeholders see artifact collection again see testing and reflection of which a large part is generating reports like this one Raji et al draw a distinction between internal and external audits In Raji et als parlance an internal audit of eg pymetrics would be conducted by pymetrics employees while an external audit would be conducted by experts with no association to pymetrics and no privileged access to pymetrics systems The audit we present in this study does not fall into these paradigms we were not employees of pymetrics yet we were given privileged access to pymetrics source code and documentation see Thus we refer to this endeavor as a cooperative audit as it involves cooperation between internal and external actors Sandvig et al introduced five designs for conducting algorithm audits Our study of pymetrics corresponds most closely with a code audit in this taxonomy since we directly examined pymetrics source code and datasets However Sandvig et al assume that source code will be publicly available so that a key precept of classic audit study design can be maintained that the audited party not be aware of the audit This assumption is not true in our case since our audit was cooperative As we discuss in we undertook other steps to insure our independence from pymetrics Several tools have been developed by academics to facilitate auditing of black-box ML models including LIME and SHAP These advanced statistical tools were not necessary for this audit since training data and source code were available to us and pymetrics uses interpretable models see ABOUT PYMETRICS In this audit we focus on the candidate screening aka preemployment assessment product offered by pymetrics pymetrics is a startup that offers a number of services in the context of employment Unlike services like Monstercom or Indeed pymetrics is not a marketplace where employers post jobs or job seekers post resumes Rather pymetrics uses gamified psychological measurement and applied ML to evaluate the cognitive and behavioral characteristics that differentiate a roles high-performing incumbents to make predictions about job seekers applying to that role pymetrics candidate screening service is designed to surface the applicants with the greatest potential and pass them on to the interview stage while simultaneously seeking to avoid disparate impact by abiding by the UGESPs four-fifths rule with respect to protected demographic groups At a high-level pymetrics candidate screening service can be summarized as follows An employer contracts with pymetrics to develop and deploy a predictive model for candidate screening We refer to these employers as clients A job analyst from pymetrics surveys the client to understand the target role eg the job description seniority-level etc and the metrics that the client uses to assess job performance in that role The client has incumbent employees in the targeted role play pymetrics suite of games described further in The client also gives existing job performance data about these incumbents to pymetrics This performance and gameplay data are used as the training input for a predictive model A pymetrics data scientist uses a proprietary pymetrics tool to develop a predictive model for the client These models are evaluated for predictive performance and compliance with the UGESPs four-fifths rule using a separate held-out testing set with demographic information pymetrics deploys the best-performing predictive model that meets the fairness criteria Job seekers who apply for the targeted role are asked to play pymetrics suite of games Based on this gameplay data the model predicts which candidates have similar attributes to the clients high-performing incumbent employees Information about high-scoring job seekers are sent to the client who may then apply additional filters eg resume screening and interview candidates pymetrics performs longitudinal analysis of the predictive model This includes back-testing to re-evaluate whether fairness criteria are being met with respect to the pool of job seekers that have applied for this role and studying the job performance of candidates who were hired Data Sources pymetrics candidate screening service relies on a variety of data sources to train and evaluate ML models The primary data source is a core set of twelve games that are derived from peer-reviewed psychological studies These games are purported to assess intrinsic mental qualities of individuals the games are not meant to be won or lost but rather to surface information based on how people play Each game produces a number of features per player These games are available in internet browsers or in a mobile app are translated into over languages and have built-in accommodations for players with color-blindness and/or dyslexia At present pymetrics maintains a database with gameplay from over million users from across the world and a variety of industries see After players complete the pymetrics games they are asked to take an optional demographic survey about their gender and ethnicity/race The available categories correspond to those delineated by the EEOC for adverse impact testing While pymetrics allows for other responses the categories considered as protected by the EEOC include male/female for gender and Asian/Black/Hispanic/White/two-or-more groups for ethnicity pymetrics reported to us that over of players complete the demographic survey This data is used to construct the held-out sets of data that are used for adverse impact testing Model Training The goal of pymetrics models is to identify features that characterize clients high-performers so that predictive models can accurately and fairly identify potential high-performers from pools of applicants To support this goal pymetrics constructs three datasets an in group a baseline for comparison called an out group and a held-out set for testing adverse impact called a bias group The in group is composed of the gameplay data from high-performing incumbent employees at the client company as identified through the job analysis process The in group dataset typically contains data on players The out group is used as a point of comparison in the training process and is sampled from the pymetrics database to approximate the potential applicant pool This allows for the predictive model to isolate the behaviors that highlight potential hires from a candidate set Finally the bias group is the set of gameplay data from users in the pymetrics database who voluntarily provided demographic labels The bias group dataset typically contains over users and is engineered to include an equal proportion of players from each of the EEOCs protected groups The bias group is the dataset used for testing adverse impact Following construction of these datasets the next step in pymetrics process is to clean the data This involves Correct for platform differences Data may be scaled to account for population-level differences due to platform effects eg some games elicit different behaviors given the affordances of web and mobile app-based platforms Remove players with missing values Recall that pymetrics asks players to complete twelve games It is possible that a player may skip games or start games but abandon them midway These actions result in missing feature values By default gameplay from users with more than two missing games is considered incomplete and those players are removed from analyses Clean the feature values and remove outliers All features are checked to make sure they fall within acceptable numerical ranges corresponding to the bounds of each game Acceptable bounds are set for each game through psychometric and statistical testing such that typical boundaries are within several standard deviations from the mean Values outside the acceptable range are rounded to the minimum or maximum of the range respectively Impute missing feature values A player may have an empty value for a feature for a number of reasons As mentioned above the player may intentionally or unintentionally skip a game or part of a game or have experienced a technical or connectivity error pymetrics replaces missing feature values with the median values for that feature Scale the feature values Each feature distribution is recentered around zero and scaled to unit variance pymetrics then uses a proprietary implementation of a Support Vector Machine SVM algorithm to train predictive models The SVM class of models is a reasonable choice for scenarios where the feature space is known and small features in the code we audited and the amount of labeled training data is small in absolute terms and in relation to the amount of unlabeled training data The in group and out group are the only inputs for the training and hyperparameter optimization stages of the SVM model To test whether the predictive models meets pymetrics fairness criteria pymetrics conducts a search for the most predictive leased biased permutation of features Fairness is measured by applying the predictive models to the bias group data and comparing performance of the demographic subgroups as described in If no models are found that meet pymetrics standards for both performance and fairness the job analyst may continue to work with the client to reconstruct appropriate incumbent selection criteria Otherwise the data scientist will deploy the most performant fair model for the client to use as part of their selection process Adverse Impact Testing According to pymetrics all deployed models comply with the UGESPs four-fifths rule In practice this means that the pass rate or impact ratio IR of the lowest-passing group over the pass rate of the highest-passing group must always be greater than While the source code for pymetrics model building is proprietary and was shared with us only for the purposes of this audit pymetrics has provided their adverse impact testing framework as an open source tool In the pymetrics use-case the trained model scores job seekers who are then sorted into three tiers Highly Recommended Recommended and Not Recommended The tiers are based on percentile thresholds that can be customized for each client but are typically set at the th and th score percentiles so that applicants with scores falling in the th percentile or greater are in the Highly Recommended tier those with scores between the th and th percentiles are in the Recommended tier and those with scores below the th percentile are Not Recommended This categorization raises a complication with respect to measuring compliance with the four-fifths rule the pass rate for a given demographic group may vary based on the chosen threshold According to pymetrics documentation the search for fair feature sets considers the IR at the th percentile but the final models are tested for fairness at both the th and th percentiles pymetrics claims that a model that does not pass all fairness and checks using the bias group will not be deployed After a model has been deployed and used in a clients application process pymetrics continuously performs both practical and statistical adverse impact testing using real applicant data This audit focuses only on the pre-deployment testing claims of pymetrics ie that all deployed models will have at the th and th percentiles DESIGNING THE AUDIT In this section we discuss the design of our audit including what we did and did not examine the baseline requirements for conducting the audit and how we managed our relationship with pymetrics In Scope The focus of our audit was pymetrics claim that their model training process produces models that abide by pymetrics interpretation of the UGESPs four-fifths rule Thus we examined documentation and source code that implements pymetrics candidate screening product see We conducted our audit in summer During the audit we focused on the following specific questions Correctness pymetrics documentation describes their process for performing adverse impact testing on trained models before they are deployed Does the model training source code correctly implement adverse impact testing as the four-fifths rule using the minimum bias ratio aka impact ratio metric as described in the documentation Is fairness assessed for the seven demographic categories defined by the EEOC five racial and ethnic two gender Direct Discrimination Using demographic data as training features for models can be construed as a form of direct discrimination This motivates us to ask do trained models use demographic data directly as input or is demographic data only used for post-training adverse impact testing De-biasing Circumvention There are numerous examples of deployed ML-based systems that had their safety systems subverted by clever and malicious users These experiences motivate us to ask is there any way for training data that is erroneously corrupted or intentionally biased to somehow avoid the adverse impact tests thus resulting in an unfair model being released Sociotechnical Safeguards pymetrics process for producing models involves human intervention which raises the issue that human errors may subvert fairness guarantees Does pymetrics have checks in place to ensure that human errors either benign or malicious do not result in an unfair model being released Sound Assumptions Using ML is never as simple as loading data and inputting it into a training algorithm Data must be preprocessed and transformed to prepare it for analysis This process concretizes assumptions about the data that may influence the adverse impact assessment Are there assumptions about data and data preprocessing baked into pymetrics model training process that could cause the adverse impact assessment to fail or mislead Out of Scope Just as important as defining what we were auditing is understanding what we were not auditing This point is critical for properly contextualizing any audit so as to focus on specific criteria for success or failure In particular our audit did not cover the following aspects of pymetrics products and business Prior to conducting the audit we agreed with pymetrics that we would not question their choice of fairness objective the UGESP four-fifths rule or fairness metric minimum bias ratio Although there are many other potential fairness objectives and metrics including others designed to prevent disparate impact pymetrics chose their existing objective and metric based on what they felt was most appropriate in the context of their business ie candidate screening This objective and metric were proposed by the relevant US regulators themselves Similarly we agreed not to question pymetrics choice of race ethnicity and gender categories that they evaluate for fairness since these categories are delineated as protected by the EEOC Further we agreed to not evaluate fairness for intersectional groups ie combinations of demographic categories like Black males or Asian females since they are not considered protected by the EEOC We only audited pymetrics game-based candidate screening product We did not audit other products and services We did not investigate the ability of pymetrics games to measure human capabilities whether those capabilities map to job performance or whether other assessment methods would be superior in some respect eg fairness or accuracy As computer scientists evaluating these aspects of the pymetrics system were beyond our capabilities Additionally we do not comment on the rationality and ethics of using these measures to evaluate a candidates suitability for employment pymetrics recently started offering an additional suite of numerical and logical reasoning games We did not have access to datasets that included data from these games so we cannot comment on their impact to fairness That said as we discuss in the control flow in the pymetrics source code ensures that all models eventually must pass fairness checks regardless of whether the model includes or does not include data from these additional games pymetrics performs post-training adverse impact testing on models using a held-out set of data Prior to conducting the audit we agreed with pymetrics that we would not question their choice to use post-training testing While pre-training and during-training model de-biasing methods exist they require that training data include complete demographic information which is not always available in employment contexts During our audit we did not focus on evaluating or maximizing the predictive performance of pymetrics models fairness was our main concern That said during our testing we did obey the minimum baseline predictive performance requirements that pymetrics demands of all their models We did not audit pymetrics process for performing annual adverse impact back-testing on deployed models We did not examine pymetrics cybersecurity posture eg we did not perform penetration tests We did not attempt to become a pymetrics client play their games while posing as an employer or a job seeker have any contact with pymetrics employees outside the narrow confines of this audit or attempt to conduct insider attacks given our privileged access to pymetrics systems data and employees We did not examine pymetrics posture with respect to data privacy or compliance with laws like Europes General Data Protection Regulation the California Consumer Privacy Act the US Childrens Online Privacy Protection Act etc However pymetrics has developed an information security program compliant with the internationally recognized ISO/IEC information security standard and undergoes semiannual security audits by an internationally accredited certification body Requirements To fully and completely answer the questions we posed in in a way that the public can trust necessitated that we establish a number of requirements for our audit Northeastern University and pymetrics signed a contract in March agreeing to the scope of the audit and these requirements before we commenced the audit Here we outline the requirements of our audit Transparency One of the foremost issues we identified heading into the audit was how to preserve our objectivity and trustworthiness These properties are essential both from the audit design perspective ie are we asking the right and tough questions and from the public reporting perspective ie will people believe the results of the audit To promote these properties we adopted a stance of transparency The contract between Major University and pymetrics the audit and data sharing protocol agreed to by the the audit team and pymetrics the non-compete signed by the lead auditor and the project budget are publicly available online for anyone to inspect As stipulated in the contract the only facets of the project covered by non-disclosure rules include source code data and internal For example an employer may not be willing to divulge demographic information about incumbent employees due to privacy and consent concerns manuals from pymetrics Otherwise the audit team retained the right to speak and publish about the audit up to and including specific results that might reflect negatively on pymetrics As specified in the contract we adopted a policy of responsible disclosure the audit team agreed not to publicly discuss issues uncovered during the audit until pymetrics was given days to privately remediate them This policy was modeled on industry standard disclosure practices from the realm of cybersecurity Likewise while we gave pymetrics an opportunity to review documents generated from the audit we retained final editorial discretion Remuneration Another fraught issue that we identified was remuneration should pymetrics pay for the audit and if so how On one hand we as auditors wanted to avoid real and perceived conflicts of interest Accepting payment for the audit would immediately raise legitimate concerns about our objectivity On the other hand performing a pro-bono audit raises the issue of setting an unrealistic exclusionary precedent To our knowledge there have been very few cooperative algorithm audits between companies and independent investigators It is our sincere hope that our audit of pymetrics will serve as model for more cooperative audits in the future With this framing in mind if we established a precedent that only pro-bono audits are sufficiently objective this could severely limit who is able to serve as an auditor in the future In the absence of funding from neutral sources eg grants from government foundations or endowments many academics investigative journalists etc may not have the financial means to support themselves while conducting audits Further completing the audit pro-bono raises issues of exploitation Auditing is challenging work that requires a high-level of expertise Especially when graduate student labor is involved as it was during this audit setting a precedent of work without compensation exacerbates pre-existing power imbalances in academia We asked several people in the academic community who are well versed in the study of online platforms for advice on how to approach the issue of remuneration Based on their helpful guidance we decided to accept payment for the audit subject to a number of constraints First the payment was structured as a grant to Major University not as direct payment to the auditors as independent contractors This added a layer of institutional oversight to the project Second all payment was received before we delivered the findings of the audit to pymetrics thus mitigating concerns that payment could be conditioned on positive audit results Third pymetrics provided computational processing power for the audit at their own expense This was done so that the material expense of the audit was not put on the auditors Access and Materials To complete this audit we were given extraordinary access to pymetrics At the outset we spent a day with pymetrics employees learning about the company their data science pipeline and how they approach fairness issues as well as demos of pymetrics data scientists using their internal tools to train and evaluate models During this onboarding we were also given copies of internal pymetrics documents that present among other things a technical overview of their candidate screening product and specific details about their fairness testing procedure pymetrics makes these documents available to prospective clients under a non-disclosure agreement pymetrics gave us access to source code for their candidate screening product At a high-level the source code encompasses a template Jupyter notebook that is used by pymetrics data scientists along with associated custom Python modules The notebook implements the data scientist-facing process of producing a predictive model for a specific client including presenting the results of adverse impact testing The Python modules implement specific algorithms that are generally constant across all client engagements such as model search and the minimum bias ratio metric We were given eight notebooks in total Blank Template One notebook was blank in the sense that it had not been filled out by a data scientist In other words this is how the template notebook appears to a pymetrics data scientists that is beginning a new client engagement from scratchit contains scaffolding code and processes but no specifics Representative Samples Six notebooks were sampled from recent completed client engagements These notebooks had each been filled out by a pymetrics data scientist and produced a model that ultimately went live into production Five of these notebooks were selected uniformly at random by pymetrics from client engagements that had occurred within the six months preceding the audit The sixth notebook was chosen because it came from a recent engagement where the client requested extensive changes to the adverse impact testing process Complete Engagement The final notebook also came from a completed client engagement and included the associated datasets that were used to train and evaluate the models These datasets included self-reported demographic data from a subset of game players see The bulk of our attention during this audit was on this complete notebook and its associated data The seven completed notebooks were anonymized to remove specific references to the client companies The data from the complete engagement was pseudonymous it contained no personally identifiable information PII but gameplay and demographic data was associated with individual game players All of these notebooks and data were uploaded to a virtual machine that was provisioned by pymetrics and hosted on Amazon Web Services The audit team performed all analysis within this virtual machine We agreed to confine our activities to within this virtual environment to obviate pymetrics concerns about their proprietary code and data being leaked Finally after the completion of our analysis we observed a live demonstration of pymetrics data scientist training testing and deploying a model into production This demonstration allowed us to confirm that the notebooks we analyzed were representative of what pymetrics uses in production Independent Testing An essential principle of auditing is that to the greatest extent possible the subject should not know the manner in which they are being evaluated or be allowed to dictate the tests that will be conducted In keeping with this principle we did not inform pymetrics of the tests or testing methods we planned to use before commencing the audit The extent of pymetrics knowledge prior to conducting the audit was that the audit was taking place that we would be evaluating the model training and testing portion of their game-based candidate screening product and that we might employ fake or synthetic data in our testing We maintained this posture of secrecy throughout the course of the audit During our initial onboarding with pymetrics staff we made sure to ask only general questions that would not reveal the focus of our testing or our methods Similarly at several points during the audit we required technical assistance to run pymetrics code as well as additional datasets that were not provided initially During these interactions were also opaque and did not elaborate on why we wanted things or our specific motivations The pymetrics team honored our requests and did not attempt to extract information about the status of our testing Deliverables As outlined in our contract with pymetrics the deliverable from this audit was a report that they planned to distribute to their clients After the conclusion of the audit the auditors and pymetrics mutually agreed to produce this manuscript to widely disseminating the methods and results of the audit Limitations As with any scientific study it is critical to be forthright about the limitations of our audit study First we operated under an assumption of good faith on the part of pymetrics We assumed that documentation source code and data that we received were representative of the actual deployed systems and data used by pymetrics Given that pymetrics agreed to full transparency of this audit we have no reason to doubt their sincerity Additionally we observed a live demonstration of a model being trained tested and deployed by a pymetrics data scientist This demonstration allowed us to confirm that the production source code and process matched the one we audited Second our audit examined pymetrics fairness claims and source code in summer of We cannot make any claims about pymetrics practices and guarantees before we ran our audit or about new and modified products they release in the future Third pymetrics may customize their model training and adverse impact assessment process for specific clients We cannot make claims about these customized products Our audit results are only representative with respect to pymetrics standard non-customized model training and adverse impact assessment process RESULTS In this section we present the results of our audit We organize our discussion around the five questions given in using the source code and data introduced in Overall Implementation We began by addressing three questions related to correctness direct discrimination and de-biasing circumvention To answer these questions we manually examined the source code provided by pymetrics that we introduced in With the exception of one notebook that was heavily modified to suit a particular client the remaining six notebooks had consistent source code and use of custom modules With respect to correctness we found that pymetrics source code did implement the four-fifths rule using the minimum bias ratio metric with the seven considered groups being the EEOC-defined categories of male female White Black Hispanic Asian and people who identify with racial or ethnic groups The code for calculating these metrics was inside a custom Python module and we found no issues with these algorithms We also found that the adverse impact metrics were prominently reported to the overseeing data scientist multiple times in the notebook With respect to direct discrimination we confirmed that players demographic characteristics were not used as features for model training The in group and out group datasets used for model training did not contain demographic information or overt proxies for demographic information eg no zip codes Only players in the bias group had corresponding demographic information and the bias group was only used for feature and model evaluation With respect to de-biasing circumvention we were unable to produce a biased model that was not flagged as such by the code For the purpose of this testing we assume the following threat model a client may arbitrarily manipulate the in group dataset that they supply to pymetrics eg by controlling its size or the gameplay data of players By manipulating the in group data the malicious clients goal is to get pymetrics to unwittingly deploy a biased model Our threat model is intentionally abstract to cover a wide range of potential malicious behaviors One possibility is that a client could supply an in group dataset that contains information from a demographically homogeneous group of employees Another possibility is that a client could lie by having a single employee play pymetrics games times and then supplying pymetrics with fabricated performance data for imaginary employees In our testing we were unable to circumvent the fairness checks in pymetrics source code by manipulating in group data All control flow paths in the Jupyter notebook eventually arrived at the adverse impact tests We could generate in group data that would cause the model search to fail the adverse impact tests but the model deployment process could not continue unless a compliant model was produced Alternatively in some cases the model building process was able to successfully de-bias our malicious in group which also meant that our attack had failed Human Oversight The next question we examined concerned sociotechnical safeguards Rather than being a fully automated process models at pymetrics are crafted by hand Involving data scientists has benefits they can notice and correct issues during model building and tweak models to better support the unique needs of clients However human involvement also raises concerns like whether a negligent or malicious data scientist could release a biased model into production There were no programmatic constraints in the Jupyter notebook that prevented a data scientist from training a Note that this threat model is unrealistically strong In practice it would be very difficult for a client to produce arbitrary gameplay data since they would either need to train human beings to play the pymetrics games in very specific ways or write software to emulate a human and play the games Further pymetrics clients work closely with a human job analyst from pymetrics to select incumbent employees and fairly evaluate their performance A malicious client would need to lie to the job analyst in addition to producing manipulated data model failing to check it for bias and uploading it to pymetrics back-end production system The back-end systems do not automatically re-perform adverse impact tests on models since the data scientists are assumed to be trustworthy To mitigate these human risks pymetrics relies on a system of manual review that is similar to the code review practices that are common in serious software development projects After a data scientist has finished training evaluating and packaging a model for given client they must complete a checklist that includes over questions These questions ask the data scientist to review all key aspects of the model building process copy salient numerical data eg accuracy and pass rates into the sheet and document in writing any significant deviations from the standard templated model training process While much of this process could be automated pymetrics deliberately adopted a manual process that forces the data scientist to document and justify their work Before a trained model can be released into production the corresponding notebook and checklist are reviewed by a second data scientist from pymetrics The second data scientist provides extra bulwark against erroneous models being accidentally put into production Further it would now take collusion between two data scientists to maliciously release a biased model into production In our opinion this manual review process offers a reasonable level of assurance against malicious insiders and negligence The design of this review process forces self-reflection by data scientists and is so detail-oriented that it would be difficult to unintentionally miss substantive problems with a trained model The addition of a second reviewer guards against gross negligence and raises the bar against intentional malfeasance Cleaning and Imputation Next we investigate the soundness of assumptions underlying the process pymetrics uses to prepare data for model training and evaluation As described in data preparation is a complex task that involves many choices what data to use how to filter outliers how to impute any missing values and how to normalize and scale the numerical data These choices may impact model performance and fairness guarantees so they are worth critically interrogating During this audit we focused on imputation of missing values as the area of concern We focus on imputation because it is not optional it will impact players even if filtering is applied and there is a possibility that it may not impact all players equally unlike scaling for example The Differential Impact of Imputation To motivate our investigation of imputation we started by delving into two questions did some groups of players require more imputation than others and were the distributions of gameplay data for different groups statistically different The first question sought to understand whether some groups are more impacted by imputation than others The second question was driven by pymetrics choice of median imputation as their default algorithm if groups exhibited different gameplay characteristics then setting missing values to We note that our opinion is based on assumptions about the threat model pymetrics faces eg a malicious company that might want to get pymetrics to bless their biased hiring practices as fair See for more discussion of this assumed threat model Missing Demographic Demographic KW Games Asian Black Black White Traits Female Male Asian Black Black Hispanic Black White Black Two-or-More Hispanic Two-or-More White Two-or-More Table Cases where missing game and missing feature distributions were significantly correlated with demographics count count KW Gender Race/Ethnicity Table Count and percentage of cases where feature distributions were significantly different between groups the population median might not reflect the gameplay characteristics of each group As shown in Table and Table the answer to both questions is yes Using the non-parametric Mann-Whitney and Kruskal-Wallis tests we found that the distributions of missing data and gameplay data were significantly different across groups in many cases Adverse Impact Testing and Model Performance The results in the previous section suggested that imputation might have a differential impact on different groups of players This motivated our second series of tests determining if the choice of imputation algorithm has a substantive impact on the fairness guarantees and performance of trained models To test this we re-evaluated the minimum bias ratios for the model that pymetrics data scientists selected as best for our in group out group and bias group datasets Figure shows the results when we re-evaluated the original model which we denote as Median since it was trained on a median imputed dataset using bias group datasets that were imputed with varying algorithms from scikit-learn version Median/Median presents the original fairness metrics computed by pymetrics The x-axis denotes the acceptance threshold for employment candidates with higher numbers corresponding to more stringent thresholds The y-axis denotes the minimum bias ratio for the given model/evaluation data at a given acceptance threshold Figure demonstrates that the choice of imputation algorithm did not substantively alter the adverse impact assessment of the original pymetrics model Despite using better evaluation datasets ie produced by more accurate imputation algorithms this model still passed the four-fifths test up to acceptance thresholds of We also observed little variability at acceptance thresholds less than Additionally we trained new models using data imputed with Using the Anderson-Darling test we found that almost none of the data we analyze in this section is normally distributed thus we rely on non-parametric tests All values are corrected for multiple hypothesis testing Figure Minimum bias ratio at different acceptance thresholds for models trained using a dataset with median imputation but adverse impact tested using datasets imputed with varying algorithms algorithms other than median but were unable to find an alternate model that outperformed pymetrics original model in terms of fairness or predictive performance Based on the analysis in this section we conclude that the median imputer used by pymetrics does produce models that have sound fairness guarantees Our testing demonstrated that the potential issues we uncovered in did not have a significant impact on compliance with the four-fifths rule Representativeness of the Bias Group One aspect of pymetrics that we were unable to directly audit concerns the composition of the bias group dataset Although the bias group for a given engagement is pseudo-randomly selected from a large population matched for language platform and occasionally region from a pool of people according to pymetrics this does not necessarily mean that it is representative of potential job seekers Since the bias group serves as the baseline for adverse impact testing if it is not representative then the bias assessments may be flawed We identify two issues Only players who opt-in to the demographic survey are eligible for inclusion in the bias group Although pymetrics claims that the overall response rate to the survey is high over that does not necessarily mean that the response rate is independent of demographics The bias group is drawn from prior job applicants which is conditioned on the types of companies job roles and job locations that pymetrics has solicited gameplay data for in the past pymetrics prior clients and engagements are unknown to us as is how this may impact the composition of people in the bias group datasets pymetrics identified the first issue themselves and performed an internal study to investigate pymetrics provided a copy of this study to us updated to reflect their dataset as of summer We omit these results for brevity The study analysed data from four clients that agreed to disclose the demographic data of job applicants pymetrics was then able to compare the data collected by the clients to the demographic data pymetrics collected during the corresponding gameplay sessions In total this dataset covered people with people confirmed to appear in both datasets pymetrics study found that people were more likely to reveal their demographics to pymetrics than to clients that this behavior was consistent across clients and demographic groups and the proportion of people who revealed their demographics was consistent across both datasets These results provide some reassurance that biased survey response rates are not undermining the bias group That said the only way to revolve this issue definitively would be through an ethnographic study of job seekers With respect to the second issue pymetrics provided us with two datasets to shed light on the diversity of the bias group dataset First they provided summary data on the geographic distribution of players in the bias group aggregated by country pymetrics had received data from players in countries with coming from the US but also with significant numbers around North and South America Europe Southeast and East Asia South Africa and Australia Second they provided a dataset that mapped their active client engagements from January to October to ONET occupations The data showed that pymetrics had developed models for jobs that cover of the major ONET groups coverage and of the minor groups These datasets provide some reassurance that pymetrics bias group dataset is relatively diverse along geographic and job category lines although there is no guarantee that it is sufficiently diverse to cover all potential recruiting use cases Ultimately our concerns about the representativeness of the bias group are only valid up to a point pymetrics claims to perform adverse impact back-testing on models after they are deployed based on the data of players who applied to the corresponding jobs If these tests reveal that a model is not meeting fairness guarantees then pymetrics decommissions the model and trains a replacement that is fair using the updated data Given that there are an enormous number of factors that might cause the applicant pool for a particular job to diverge from a reference population used for pre-deployment adverse impact testing back-testing is a reasonable mitigation for identifying instances where modeling and testing assumptions diverge from reality CONCLUSION AND DISCUSSION In this study we present the process by which we audited pymetrics candidate screening tool and the results of our audit The focus of our audit was on pymetrics claim that their trained ML models conform to the UGESP four-fifths rule We conducted our audit in summer based on documentation source code and representative datasets that pymetrics provided to us ONET is a hierarchical taxonomy of employment areas broad occupations and detailed occupations developed and maintained by the US Office of Management and Budget and the US Department of Labor We do not expect pymetrics to cover all of these groups such as Group Farming Fishing and Forestry Occupations and Group Military Specific Operations since employers in these groups are unlikely to rely on predictive analytics for recruitment With respect to the results of our audit we are comfortable stating that pymetrics passed the audit subject to the qualifications and limitations we state in and This work was also intended as a case-study for practitioners on both sides of future audits In the remainder of this section we discuss lingering questions from the audit opportunities for future work and lessons for the practice of cooperative auditing Ethics We took great care to ensure that our audit was conducted in a manner consistent with community ethical norms First we minimized harm to players by not being given any access to PII we were only given access to the gameplay and demographic data that is used as input to pymetrics models Second we minimized harm to pymetrics clients who were not direct participants in the audit by not being given any identifying information about them Third as described in we ensured the results of our audit would ultimately benefit the research community by agreeing with pymetrics up front that a the audit would be as transparent as possible and b we were free to speak about the results of the audit and c that the non-disclosure agreement only covered pymetrics source code data and internal manuals Future Work The context surrounding pymetrics afforded us the privilege of narrowly scoping our audit Employment selection is a tightly regulated area with compliance metrics that are relatively straightforward to operationalize and that are supported a significant amount of case law That said there are two ways that we could have expanded our audit Our first open question concerns pymetrics choice to focus exclusively on Title VII of the Civil Rights Act and more formally on disparate impact and disparate treatment in their fairness testing The choice to operationalize the four-fifths rule potentially privileges regulatory compliance above other concepts of fairness such as differential validity pymetrics does offer to customize their adverse impact testing to suit clients needs but this is not the same as fundamentally re-imagining the default codebase It remains to be seen whether multiple fairness guarantees can be met in the context of pymetrics business while still preserving pymetrics compliance with existing US federal regulations Our second open question is the efficacy of pymetrics games at assessing the fit of job seekers pymetrics games are based on peer-reviewed and replicated psychological studies but drawing a direct line from laboratory experiments to real-world job performance is challenging pymetrics provided us with a confidential presentation containing results from game validation studies at a minimum we encourage pymetrics to make these results public Larger-scale observational studies based on the longitudinal data pymetrics collects from clients to evaluate model performance could be invaluable for assessing the efficacy of the games Cooperative Audits We present this work as a case-study of a cooperative audit between industry stakeholders who wish to be transparent and academic researchers who want to improve the overall application of ML As we have documented cooperative audits require that industry partners engage transparently and give the auditors broad freedoms For the company this may be stress-inducing Likewise every company operates within its own set of legal regulatory and proprietary limitations and auditors operating in the cooperative mode should prepare to be flexible within those constraints For example pymetrics expressed an interest in exploring intersectionality of gender and race demographic categories however intersectionality is not recognized by the relevant regulatory agencies According to the methods in the ML objective function is to find the most performant least biased model A less performant model may be selected to meet the standard of fairness set by regulations However selecting a less performant model to meet standards outside of regulation such as intersectional fairness could create grounds for legal dispute This is why all parties agreed that definitions of fairness and fairness for non-EEOC groups were outside the scope of this audit see That said in a different context such as medicine or advertising intersectionality could be crucial for model performance as well as fairness and thus be fair-game for auditors The transparency of ML service providers is also contextual Companies must balance sharing enough proprietary information to be transparent with their users clients and watchdog groups but not enough information as to be replicated by a competitor A cooperative audit grants a compromise so that independent experts can get access without the company fearing loss of IP or jeopardizing the privacy of data subjects Many of the issues we discuss in this section come down to defining the scope of an audit Auditors need to insist that industry partners make their criteria for substantive issues like fairness clear ahead-of-time Without these benchmarks it is difficult to define what the objectives or outcomes of a collaborative audit are We argue that this audit was successful in no small part because pymetrics had already adopted and documented business practices that could be objectively evaluated We have presented a case-study that to the best of our knowledge may be the first publicly-documented audit of algorithmic fairness between a willing private company and an external investigative team As such we had to navigate challenging questions around how to structure our audit with respect to scoping our research questions accepting remuneration maintaining the security of confidential source code and data and preserving investigative secrecy while simultaneously maintaining an arms-length objective relationship with pymetrics We hope that this audit sets a new precedent for cooperative algorithm audits and that this leads to more companies engaging independent experts to audit their sociotechnical systems in the future