Fairness Equality and Power in Algorithmic Decision-Making Much of the debate on the impact of algorithms is concerned with fairness defined as the absence of discrimination for individuals with the same merit Drawing on the theory of justice we argue that leading notions of fairness suffer from three key limitations they legitimize inequalities justified by merit they are narrowly bracketed considering only differences of treatment within the algorithm and they consider between-group and not within-group differences We contrast this fairness-based perspective with two alternate perspectives the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power We formalize these perspectives drawing on techniques from causal inference and empirical economics and characterize when they give divergent evaluations We present theoretical results and empirical examples which demonstrate this tension We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making CONCEPTS Computing methodologies Philosophical/theoretical foundations of artificial intelligence Social and professional topics Computing technology policy KEYWORDS Algorithmic fairness inequality power auditing empirical economics INTRODUCTION A rich line of work within computer science examines the differential treatment by algorithms of historically disadvantaged and marginalized groups Much of this work is concerned with fairness of algorithms which is understood as the absence of discrimination Many leading notions of fairness such as predictive parity or balance are based on some variant of the question are members of different groups who are of equal merit treated equally by the algorithm Research in this space has ranged from translating these fairness notions to various domains to examining when and whether they are simultaneously achievable with other constraints In the spirit of reflective equilibrium in this work we discuss implications of these fairness definitions that may be deemed normatively undesirable Leading notions of fairness take the objective of the algorithms owner or designer as a normative goal In the context of hiring for instance if productivity is perfectly predictable and an employers hiring algorithm is profit-maximizing without constraints then their hiring decisions are fair by definition only deviations from profit-maximization are considered discriminatory Furthermore we argue that these leading notions of fairness such as predictive parity or balance suffer from the following three limitations They legitimize and perpetuate inequalities justified by merit both within and between groups The focus on merit a measure promoting the decision-makers objective reinforces rather than questions the legitimacy of the status quo They are narrowly-bracketed Fairness only requires equal treatment within the context of the algorithm at hand and does not consider the impact of the algorithm on inequality in the wider population Unequal treatment that compensates pre-existing inequalities might reduce overall inequality They focus on categories protected groups and ignore within-group inequalities eg as emphasized by intersectional critiques Equal treatment across groups can be consistent with great inequality within groups Informed by insights from theories of justice and empirical economics we discuss each of these limitations We then compare this fairness-based perspective with two alternative perspectives The first asks what is the causal impact of the introduction of an algorithm on inequality both within and between groups In contrast to fairness this perspective is consequentialist It depends on the distribution of outcomes affected by the algorithm rather than treatment and it does so for the full population rather than only for individuals who are part of the algorithm This perspective encompasses both frameworks based on social welfare functions and statistical measures of inequality In Section we provide a formal characterization of the impact of marginal policy changes on both fairness and inequality using influence functions which allows us to elucidate the conflict between these two objectives Definitions that do not have this general form are notions of disparate impact which do not refer to merit and notions of individual fairness which are based on merit but do not refer to group membership The second alternative perspective focuses on the distribution of power and asks who gets to pick the objective function of an algorithm The choice of objective functions is intimately connected with the political economy question of who has ownership and control rights over data and algorithms To explore this question we formalize one possible notion of power based on the idea of inverse welfare weights Given actual decisions what are the welfare weights that rationalize these decisions We formalize this in Section which builds on insights from Section by solving the inverse of a social welfare maximization problem The rest of this paper is structured as follows our setup is introduced in Section We formalize the perspective based on the causal impact of algorithms in Section and that based on distribution of power in Section In doing so we highlight limitations of a fairness-based perspective which we expose further in Section through examples In Section we present an empirical application of these insights In Section we present a step-by-step guide for algorithmic auditing estimating the causal impact of algorithmic changes on measures of inequality or welfare We close with a discussion on the importance of inequality- and power-based frameworks in algorithmic decision-making Related Work Many now-classic bodies of work study discrimination and harms caused by machine learning systems on historically disadvantaged groups in settings ranging from ad delivery to facial analysis to search engine bias and provision of public services Barocas and Selbst provide a framework for understanding the negative consequences of such automated decision-making systems For general overviews and discussions see also With a growing set of findings of algorithmic discrimination in the backdrop researchers across numerous fields have sought to formalize and define different notions of fairness as well as analyze their feasibility incompatibility and politics We direct the reader to for an overview and extensive discussions around various definitions of fairness as well as their relationship with other algorithmically-defined desiderata Our work draws on the economics literature on discrimination causal inference social choice optimal taxation and on inequality and distributional decompositions Definitions of fairness correspond to notions of taste-based and statistical discrimination in economics and the notion of fairness defined in Equation correspond to hit-rate based tests for taste-based discrimination as in Causal inference and the potential outcomes framework is reviewed in social choice theory and welfare economics in Distributional decompositions are discussed in we draw in particular on the RIF regression approach of Understanding aggregation in social welfare functions in terms of welfare weights is common in optimal tax theory For a sociological perspective on discrimination we direct the reader to an overview in Recent work has considered short-comings of fairness across a number of dimensions For instance there is a growing body of work examining the long-term impact of fairness-driven interventions Note influence functions and welfare weights are commonly used in economics and statistics but are less common in computer science We present a self-contained introduction in the appendix as they are key tools in our analyses Similarly there is also a surge of work focused on understanding and improving fairness across subgroups as well as in settings where group membership may not be known Other work examines perceived trade-offs between fairness and other desiderata such as accuracy The closest work to ours have sought to understand the intersection of fairness with social welfare and inequality Despite tackling a different set of questions than ours there are several papers that consider welfare-based analyses of fairness notions In a notable example Hu and Chen present a welfare-based study of fair classification and study the relationship between fairness definitions and the long-standing notions of social welfare considered in this work By translating a loss minimization program into a social welfare maximization problem they show that more strict fairness criteria can lead to worse outcomes for both advantaged and disadvantaged groups Heidari et al similarly consider fairness and welfare proposing welfare-based measures that can be incorporated into a loss minimization program and Heidari et al connect fairness to notions of equality of opportunity In a related discussion Mullainathan considers algorithmic fairness questions within a social welfare framework comparing policies by machine learning systems with those set by a social planner that cares both about efficiency and equity These lines of line of work argue for more holistic assessments of welfare and equity in examining the impact of algorithmic decision-making SETUP AND NOTATION A decision-maker D such as a firm a court or a school makes repeated decisions on individuals who may be job applicants defendants or students For clarity we omit the subscript when there is no ambiguity For each individual a binary decision such as hiring release from jail college admission is made Individuals are characterized by some unobserved merit R such as marginal productivity recidivism or future educational success In some settings is binary but we do not make this assumption unless otherwise noted In this work merit refers to the variable that the decision-maker cares about for instance a workers productivity in the hiring context or recidivism in the bail setting context This is the variable that supervised learning methods typically aim to predict By contrast the outcome which we introduce in Section below is the variable that the treated individuals care about for instance income in the hiring context or time spent in jail in the bail setting context The decision-makers objective is to maximize where the expectation averages over individuals and is the unit cost of choosing Upper-case letters denote random variables lower-case letters values that these random variables might take In the hiring context corresponds to profits and to the wage rate In the college admissions context corresponds Our terminology here deviates from familiar usage in that we use outcome to refer to rather than Note here there are two possible outcomes one for the individual concerned and one for the decision-maker and this usage is intended to avoid ambiguity between them Formally we consider a probability space I where all expectations integrate over I with respect to the probability measure and all random variables are functions on I that are measurable with respect to A to average student performance among admitted students and might be the Lagrange multiplier shadow cost of some capacity constraint The decision-maker D does not observe but has access to some covariates features D can also form a predictive model given based on past data In practice needs to be estimated using some supervised machine learning algorithm We will abstract from this estimation issue here and assume that is known to D D can allocate as a function of and possibly some randomization device We assume throughout that is chosen independently of all other variables conditional on and thus is conditionally exogenous Denote the conditional probability Given their available information the optimal assignment policy for D satisfies argmax argmax where is a set of admissible assignment policies The second equality holds because of conditional exogeneity of and the law of iterated expectations If is unrestricted then up to arbitrary tie breaking for We assume that individuals are additionally characterized by a binary variable corresponding to protected groups such as gender or race This variable may or may not be part of Fairness Definitions Numerous definitions of fairness have been proposed in the literature We will focus on the following popular definition of fairness corresponding to the notion of predictive parity or calibration This equality is the basis of tests for preferential discrimination in empirical economics See for instance A similar requirement could be imposed for Another related requirement is balance for the positive or negative class which indicates equality of false positive respectively negative rates Predictive parity requires that expected merit conditional on having received treatment or is the same across the groups Balance requires that the probability of being treated conditional on merit is the same across the groups For the binary case balance and predictive parity cannot hold at the same time unless either prediction is perfect or base rates are equal In our subsequent discussion we focus on predictive parity as the leading measure of fairness we provide parallel results for balance in Appendix A This assumption holds by construction if captures all individual-specific information available to D This type of decision problem with a focus on estimation in finite samples has been considered for instance in and For the assignment rule satisfies predictive parity if and only if where Fairness as a constraint A leading approach in the recent literature is to consider fairness as a constraint to be imposed on the decisionmakers policy space That is defined as above but is specified to be of the form for predictive parity and similarly for other notions of fairness We characterize the solution to this optimization problem in Corollary below We argued in the introduction that fairness takes the objective of the algorithms owner as a normative goal This is formalized by the following observation Observation Suppose that perfect predictability unconstrained maximization of D s objective and iii classification setting Then satisfies predictive parity ie This observation is an immediate consequence of the definition of fairness as predictive parity and points to the limited critical potential of such a definition of fairness It implies for instance that if is perfectly predictable given the available features and employers are profit-maximizing without constraints then their hiring decisions will be fair by definition The algorithm violates fairness only if i D is not actually maximizing taste-based discrimination ii outcomes are mismeasured leading to biased predictions or iii predictability is imperfect leading to statistical discrimination Similar observations could be stated for other notions of fairness such as balance for the positive class cf Observation in the appendix and for other settings such as regression where R Absent perfect predictability there may be a tension between the maximization of and predictive parity This tension has been discussed in economics as a failure of predictive parity called hit rate test in this literature to perfectly reflect taste-based discrimination This failure is due to the difference between average and marginal expected merit among the treated Taste-based discrimination corresponds to differences between the merit of the marginally treated while predictive parity corresponds to equality of average merit among the treated profit-maximization is equivalent to the absence of taste-based discrimination by definition Observation throws the three limitations of a fairness-based perspective into sharp relief under this perspective inequality both between and within groups is acceptable if it is justified by merit D s objective no matter where the inequality in is coming from Furthermore given merit fairness aims for equal treatment within the algorithm rather than aiming for compensating pre-existing inequalities of welfare-relevant outcomes in the wider population And finally predictive parity or balance do not consider inequality of treatments or outcomes within the protected groups but rather only between them Below we provide examples where changes to an assignment algorithm decreases un-fairness while at the same time also increasing inequality and decreasing welfare INEQUALITY AND THE CAUSAL IMPACT OF ALGORITHMS Drawing on theories of justice we turn to a perspective focused on social welfare and inequality as well as the causal impact of algorithms Suppose that we are interested in outcomes that might be affected by the treatment where the outcomes are determined by the potential outcome equation cf Suppose further that treatment is assigned randomly conditional on with assignment probability Then the joint density of and is given by We are interested in the impact on a general statistic a of the joint distribution of outcomes and features a a a might be a measure of inequality such as the variance of or the ratio between two quantiles of a measure of welfare such as the expectation of where parametrizes inequality aversion or a measure of group-based inequality such as the difference in the conditional expectation of given and The influence function and welfare weights In order to characterize the impact of changes to the assignment policy on the statistic a it is useful to introduce the following local approximation to a Assume that a is differentiable as a function of the density Then as discussed as well as in and we can locally approximate a by a a where is the influence function of a at evaluated at the realization and the expectation averages over the distribution For completeness in Section B in the appendix we provide an introduction and review as well as a more formal definition of the influence function as dual representation of the Fréchet derivative of a Suppose now that is some baseline assignment rule is a local perturbation Suppose that and are the outcome distributions corresponding By Equation a a The density is assumed to exist with respect to some dominating measure For simplicity of notation our expressions are for the case where the dominating measure is the Lebesgue measure but they immediately generalize to general dominating measures To be precise we need Fréchet-differentability with respect to the norm on the space of densities of with respect to some dominating measure By Equations it then follows that a where Proposition below proves this claim Defining as the average slope of between and we can rewrite We can think as the welfare weight for each person measuring how much the statistic a cares about increasing the outcome for that person This is analogous to the welfare weights used in public economics and optimal tax theory cf We present examples to give intuition of welfare weights and influence functions Example For the mean outcome a we get and For the variance of outcomes a we get and For the mean of some power of the outcome a we get and And lastly for the between-group difference of average outcomes a we have a and Utilitarian welfare Thus far we have discussed welfare in terms of outcomes that are observable in principle This contrasts with the typical approach in welfare economics where welfare is defined based on the unobserved utility of individuals Unobserved utility can be operationalized in terms of equivalent variation that is willingness to pay what is the amount of money that would leave an individual indifferent between receiving and no treatment or receiving but no money Based on this notion of equivalent variation social welfare can then be defined as a The welfare weights now measure the value assigned to a marginal unit of money for a given person Welfare weights reflect distributional preferences Tension between the decision-makers objective fairness and equality In the following proposition we characterize the effect of a marginal change of the policy on the different objectives the decision-makers objective the measure of fairness and statistics a that might measure inequality or social welfare Conflicts between these three objectives can arise if and as defined below are not affine transformations of each other Proposition Marginal policy changes Consider a family of assignment policies and denote by and the derivatives of D s objective the measure of fairness and a inequality or social welfare with respect to Suppose that a is Fréchet-differentable with respect to the norm on the space of densities of with respect to some dominating measure Then where Proof The case of is immediate from the definition of The case of a follows from the definition of Fréchet differentiability cf Section in Lemma in and the arguments in Section of this paper This leaves the case of Let us consider the first component of and thus The derivative of can be calculated similarly and the claim follows Proposition has a number of important consequences First it provides the basis for analyzing the distributional impact of algorithms or changes to algorithms as part of an algorithmic auditing process along the lines we demonstrated in our empirical application in Section We provide a step-by-step guide for such an algorithmic auditing procedure in Section Suppose that in our data the treatment is plausibly exogenous given the features Then can be estimated by regressing the influence function for some statistic a controlling for using for instance a causal forest or some other supervised learning method The impact of switching from assignment algorithm to some other algorithm is then to first order given by the average of times over the distribution of features X This allows us to estimate the impact of the change of the algorithm on inequality welfare or between group differences Second Proposition helps us elucidate the tension between conflicting objectives such as profits fairness and equality or welfare and to connect these to the notion of welfare weights Suppose for instance that is negative and that for some feature value we have that for some measure of welfare a is positive while is negative This tells us that increasing the treatment probability at is good for welfare and bad for fairness We can thus understand which parts of the feature space drive the tension between alternative objectives Third Proposition allows us to characterize the optimal assignment from the decision-makers point of view when constrained to fair allocations this is done in Corollary below Fourth Proposition allows us to understand whose welfare a status-quo decision procedure implicitly values by deriving inverse welfare weights this is done in Corollary below This insight is again relevant for the practice of algorithmic auditing For a related approach see for instance Let us now reconsider the problem of maximizing subject to the fairness constraint The solution to this problem is characterized in Corollary drawing on Proposition Corollary Optimal policy under the fairness constraint The solution to the problem of maximizing D s objective subject to the fairness constraint by choice is given by for some constant where we have chosen arbitrarily for values of such that and the equality holds with probability Proof We are looking for a solution to max subject to and The Lagrangian for the objective and the fairness constraint is given by L Consider a family of policies indexed by as in Proposition The solution to our optimization problem has to satisfy the condition L for all feasible changes that is for all such that By Proposition L Suppose there is some set of values of non-zero probability such and Setting on this set would yield a contradiction The claim then follows DISTRIBUTION OF POWER Fairness provides a framework to critique the unequal treatment of individuals with the same merit where merit is defined in terms of D s objective The equality framework takes a broader perspective by requiring that we consider the causal impact of an algorithm on the distribution of relevant outcomes across individuals more generally Both of these perspectives however do not address another key component who gets to set the objective function and why Here we take a political economy perspective on algorithmic decision-making to provide a framework for examining this question Political economy is concerned with the ownership of the means of production as this brings both income and control rights In the setting of algorithmic decision-making this maps into two related questions first who owns and controls data and in particular data about individuals And second who gets to pick the algorithms and objective functions that use this data We are further concerned with the consequences of this structure of ownership and control The answers to these questions depend on contingent historical developments and political choices rather than natural necessity Implied welfare weights as a measure of power In the present work we propose the following framework for the political economy of algorithmic decision-making we study actual decision procedures by considering the welfare weights that would rationalize these procedures as optimal Put differently we consider the dual problem of finding the optimal policy for a given measure of social welfare Above we discussed the effect of marginal policy changes on statistics a that might measure welfare We argued that this effect can be written as where are welfare weights measuring how much we care about a marginal increase of for a given individual The optimal policy problem of maximizing a linear or linearized objective a net of the costs of treatment defines a mapping argmax We are now interested in the inverse mapping This mapping gives the welfare weights which would rationalize a given assignment algorithm as optimal These welfare weights can be thought of as one measure of the effective social power of different individuals The following corollary of Proposition characterizes this inverse mapping in the context of our binary treatment setting We characterize the implied welfare weights that would rationalize a given policy Corollary Implied welfare weights Suppose that welfare weights are a function of the observable features and that there is again a cost of treatment A given assignment rule is a solution to the problem argmax if and only if This follows immediately from the Karush-Kuhn-Tucker conditions for the constrained optimization problem defining EXAMPLES FOR THE TENSIONS BETWEEN FAIRNESS AND EQUALITY We return to the limitations of a fairness-based perspective formulated at the outset We illustrate each of these three limitations by providing examples where some change to the assignment algorithm decreases un-fairness while at the same time also increasing inequality and decreasing welfare In each of the examples we consider the impact of an assignment rule relative to some baseline rule We contrast fairness as measured by predictive parity to inequality and welfare as measured by either the variance of or the average of where measures the degree of inequality aversion Legitimizing inequality based on merit We consider an improvement in the predictability of merit Suppose that initially under scenario the decision-maker D only observes while under scenario they can perfectly predict observe based on Assume that Recall that denotes the cost of treatment and assume that is binary with where Under these assumptions we get The policy is unfair in the sense of predictive parity since for this policy while the policy is fair since The increase in predictability has thus improved fairness On the other hand inequality of outcomes has also increased and welfare has decreased By assumption so that Furthermore expected welfare has decreased since Narrow-bracketing We consider a reform that abolishes affirmative action Suppose that is uniformly distributed on that is perfectly observable to the decision-maker D and that Suppose further that under scenario the decisionmaker receives a reward subsidy of for hiring members of the group but that this reward is removed under scenario Under these assumptions we get As before the policy under scenario is unfair while the policy under scenario is fair since while Suppose now that potential outcomes are given by Under the two scenarios the outcome distributions are and where we use to denote the categorical distribution on with probabilities specified in brackets This implies that and Thus as before the inequality of outcomes has increased and welfare has decreased when we move from scenario to scenario Within-group inequality We finally consider a reform that mandates fairness to the decision-maker Suppose that and further that We assume initially D is unconstrained but Figure Distribution of Compas risk scores for Black and white defendants left and distribution of estimated average causal effects given observable features of the risk score on jail time across defendants right Estimation of causal effects was based on a causal forest see text for details Table Counter-factual scenarios Black White All Scenario Score Recid Score Jail time Score Recid Score Jail time Score Mean IQR SD of log Affirmative Action Status quo Perfect predictability Table Comparison of the consequences of two counterfactual scenarios affirmative action perfect predictability to the status quo for Black white and all defendants the reform mandates predictive parity Then Once again the policy under scenario is unfair while the policy under scenario is fair since and Assume that potential outcomes are given Under the two scenarios the outcome distributions are where we use to denote the categorical distribution on with probabilities specified in brackets This implies and choosing Again the inequality of outcomes increases and welfare declines as we move from scenario to scenario EMPIRICAL STUDY We illustrate our arguments using the Compas risk score data for recidivism These data have received much attention following ProPublicas reporting on algorithmic discrimination in sentencing We map our setup to the Compas data as follows denotes race Black or white denotes a risk score exceeding as in ProPublicas analysis based on the Compas classification as medium or high risk denotes recidivism within two years and denotes jail time The predictive features that we consider include race sex age juvenile counts of misdemeanors felonies and other infractions general prior counts as well as charge degree We compare three counter-factual scenarios A counter-factual affirmative action scenario where race-specific adjustments are applied to the risk scores We decrease the scores generated by Compas by one unit for Black defendants and increase them one unit for white defendants The status-quo scenario taking the original Compas scores as given A counter-factual perfect predictability scenario where scores are set to the maximum value for those who actually recidivated within years Scores are set to the minimum value for all others For each of these scenarios we impute corresponding values of ie a counter-factual score bigger than and counter-factual jail time The latter is calculated based on a causal-forest estimate of the impact on of risk scores conditional on the covariates in This relies on the strong assumption of conditional exogeneity of risk-scores given As can be seen in Table fairness as measured by predictive parity improves when moving from the affirmative action scenario to the status-quo and is fully achieved in the perfect predictability scenario This follows because the difference in expected recidivism conditional on having a score bigger than between Black and white defendants decreases as we go from one scenario to the next On the other hand Table also shows that inequality both between and within racial groups increases as we go from one scenario to the next The difference in mean jail time between Black and white defendants increases from about days to about days The interquartile range in the distribution of counter-factual jail time increases from about days to days And the standard deviation of log jail time increases from to A GUIDE FOR ALGORITHMIC AUDITING USING DISTRIBUTIONAL DECOMPOSITIONS In this section we provide a step-by-step guide to algorithmic auditing for distributional impacts The method we discuss here builds on our characterization of distributional impacts in Proposition This method enables an auditor to estimate the causal impact of switching from treatment assignment the baseline to a counterfactual treatment assignment on some statistic a of the distribution of outcomes The approach is based on the framework introduced in Section Step Normative choices Before any analysis can begin a number of important normative choices have to be made First we need to determine the relevant outcomes for individuals welfare income education jail time and so on Second we need to decide on the relevant measures of welfare or inequality a A number of choices were discussed in the paper One option allowing to represent the entire distribution is to report the impact on a series of quantiles eg all percentiles Third the population of interest needs to be determined Do we care about inequality only in our sample or the population in the state or the population in the entire country In many cases the population of interest will be large relative to the sample treated by the algorithm Step Calculation of influence functions The next step is to calculate the influence function for the measures of interest at the appropriate baseline distribution of the population of interest This influence function is then evaluated at each of the observed outcomes in our sample and stored in a new variable For example for the variance of outcomes a we impute for each observation in the sample where and are evaluated for the population of interest For the between-group difference of average outcomes a we impute a where again and a are evaluated for the population of interest See for further examples Step Causal effect estimation The next step in the proposed analysis is to estimate the conditional average treatment effect of on given the observed features That is to estimate in the notation of Proposition Such causal estimation requires random variation of the treatment conditional on ie conditional statistical independence Conditional independence is ensured in experimental settings eg data coming from A/B tests or contextual bandits More generally independence might be a reasonable approximation if the space of features is sufficiently rich and does not include any variables that are causally downstream from or Many estimators are available to estimate under the assumption of conditional independence in our application we have used the causal forest approach of After estimating the function an estimated value of is imputed for every observation in the sample Step Counterfactual assignment probabilities In order to evaluate the impact of an algorithm we need to compare it to a baseline algorithm with assignment probabilities In Step we need to evaluate these assignment probabilities and for all and impute for all in the sample Step Evaluation of distributional impact The last step of our analysis then requires putting the pieces together and evaluating where is the share of the population of interest that is assigned treatment by the algorithm is the estimated impact of switching from algorithm to algorithm on the measure a of inequality or welfare CONCLUSION In this work we articulate and discuss three limitations of fairness-based perspectives under leading notions of fairness namely that they legitimize inequalities justified by merit rather than questioning the status quo that they are narrowly bracketed and do not adequately engage with the impact of algorithms on pre-existing inequalities and that they do not consider within-group inequalities leading to intersectional concerns To help alleviate these limitations we consider two alternative perspectives drawing on theories of justice and empirical economics An inequality-centered perspective is pertinent in settings where we presume that inequalities of social outcomes are socially created and the same holds for various forms of merit marginal productivity recidivism etc Here any decision system can be viewed as a step in the causal pathway of reproducing or reducing these inequalities An approach intending to minimize harm on disadvantaged groups therefore does better to consider the effect of any particular decision system whether algorithmic or human on inequality as a whole rather than aiming to solely optimize for a fixed fairness notion within the algorithm The latter also risks normatively privileging between group equality to within group equality cf for instance Black feminist critiques of second wave feminism A perspective focused on the distribution of power compels us to consider the design of the algorithms themselves Dont just ask how the algorithm treats different people differently but also who gets to do the treating By taking a political economy perspective we examine what implicit distribution of social power justifies the current choice of objectives Such a question foregrounds how power gets allocated and what is the process that leads some groups to have more control over data in decision making processes These alternative perspectives focused on inequality and power are not intended to entirely solve the above fairness concerns but rather to elucidate them and bring to the forefront concerns that havent been adequately considered in the literature thus far In doing so we add to a recent line of work aiming to broaden discussions on the social impact of algorithmic decision-making