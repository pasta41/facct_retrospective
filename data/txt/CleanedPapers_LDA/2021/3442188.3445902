Fairness in Risk Assessment Instruments Post-Processing to Achieve Counterfactual Equalized Odds In domains such as criminal justice medicine and social welfare decision makers increasingly have access to algorithmic Risk Assessment Instruments RAIs RAIs estimate the risk of an adverse outcome such as recidivism or child neglect potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation It is important to ensure that RAIs are fair so that the benefits and harms of such decisions are equitably distributed The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes such as whether a person actually recidivates but these criteria are misleading when applied to RAIs Since RAIs are intended to inform interventions that can reduce risk the prediction itself affects the downstream outcome Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes ie the outcomes that would occur in the absence of an appropriate intervention However no methods currently exist to satisfy such fairness criteria In this paper we target one such criterion counterfactual equalized odds We develop a post-processed predictor that is estimated via doubly robust estimators extending and adapting previous postprocessing approaches to the counterfactual setting We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors Our predictor converges to an optimal fair predictor at fast rates We illustrate properties of our method and show that it performs well on both simulated and real data CONCEPTS Computing methodologies Machine learning KEYWORDS fairness risk assessment post-processing counterfactual INTRODUCTION Machine learning is increasingly involved in high stakes decisions in domains such as healthcare criminal justice and consumer finance In these settings ML models often take the form of Risk Assessment Instruments RAIs given covariates such as demographic information and an individuals medical/criminal/financial history the model predicts the likelihood of an adverse outcome such as a dangerous medical event recidivism or default on a loan Rather than rendering an automatic decision the model produces a risk score which a decision maker may take into account when deciding whether to prescribe a medical treatment release a defendant on bail or issue a personal loan The proliferation of machine learning has raised concerns that learned models may be discriminatory with respect to sensitive features like race sex age and socioeconomic status For example there has been vigorous debate about whether a widely used recidivism prediction tool called COMPAS is biased against black defendants Concerns have also been raised about risk assessments used to identify high risk medical patients and about common credit scoring algorithms such as FICO among many others Collectively these types of algorithms directly impact a large and growing swath of the global population These concerns have led to an explosion of methods in recent years for developing fair models and auditing the fairness of existing models The most widely discussed fairness criteria impose constraints on the joint distribution of a sensitive feature an outcome and a predictor These observational fairness criteria are inappropriate for RAIs however RAIs are not concerned with the observable outcomes in the training data Did patients of this type historically experience serious complications which are themselves a product of historical treatment decisions Rather they are concerned with the potential outcomes associated with available treatment decisions Would patients of this type experience complications if not treated Because treatments are not assigned at random doctors naturally treat the patients they think are at high risk these are distinct questions Coston et al showed how RAIs that are optimized to predict observable rather than potential outcomes systematically underestimate risk for units that have historically been receptive to treatment leading to suboptimal treatment decisions They further showed how evaluations of the performance and fairness properties of RAIs with respect to observable outcomes are misleading They proposed that RAIs should instead target counterfactual versions of standard performance and fairness metrics However they left open the question of how to develop predictors that satisfy such fairness notions In this paper we develop a method to generate predictors that satisfy the fairness criterion approximate counterfactual equalized odds While many existing methods target observational fairness criteria and various types of causally motivated fairness no methods currently exist that target counterfactual versions of standard observable fairness criteria like equalized odds Our method post-processes an arbitrary existing predictor extending previous post-processing methods to the counterfactual setting Our contributions are as follows We first define approximate counterfactual equalized odds After discussing related work and motivating the use of equalized odds over other candidate criteria we present a linear program that produces a loss-optimal post-processed predictor that satisfies this criterion We provide theoretical results that our post-processed predictor is consistent in a particular sense at rates that depend on certain nuisance parameters We show that our method performs well on both simulated and real data NOTATION AND FAIRNESS DEFINITIONS A table listing all notational choices can be found in Appendix Let denote a sensitive feature decision and outcome respectively We consider the setting in which all three are binary though most of the definitions below extend readily to continuous settings We define the counterfactual quantities of interest via the potential outcomes framework of Denote by the potential equivalently counterfactual outcomes is the outcome that would be observed for unit if possibly contrary to fact the decision were set to We refer to the two levels of the sensitive feature as the two groups and we use treatment and intervention synonymously with decision Let be any random variable that takes values in In most RAI settings one of the decision options is a natural baseline corresponding to no intervention Examples include the risk of recidivism if a defendant is released pretrial or the risk of neglect or abuse if a child welfare call is not screened in for further investigation Many or most RAIs do not generate a separate risk score for the outcome associated with intervention In the case of child welfare for example call screeners must screen in any case in which a child is in apparent danger of neglect or abuse regardless of the chances that a subsequent intervention will successfully prevent that neglect or abuse Denote the observational and counterfactual false positive rates of for group by FPR and c fpr For example c fpr could represent the chance of being falsely labeled high-risk among those black defendants who would not actually go on to recidivate if released pretrial while c fpr could represent the corresponding error rate for white defendants who would not recidivate if released pretrial Let FNR fnr denote the corresponding false negative rates Definition A predictor satisfies observational equalized odds with respect if It satisfies counterfactual equalized odds cEO if When and are all binary equalized odds is equivalent to requiring that the corresponding false positive and false negative rates be equal for the two levels of Our post-processed predictor will target a relaxation of this criterion defined below Definition The counterfactual error rate differences for a predictor are the differences and in the c fpr and fnr for the two groups defined as follows c fpr c fpr fnr fnr Definition When and are all binary satisfies approximate counterfactual equalized odds with fairness constraints if In general a fairness-constrained predictor would not outperform an optimal unconstrained predictor and in some cases satisfying cEO exactly might degrade performance to the point that the RAI is no longer useful This relaxation of cEO allows RAI designers to negotiate this tradeoff This is similar in spirit to notions of approximate fairness that appear throughout the literature RELATEDWORK Observational fairness criteria Equalized odds is one of several popular fairness criteria that impose constraints on the joint distribution of These criteria appear under a variety of names Equalized odds is known more generally as separation a term which covers settings in which these variables are not necessarily binary The other two popular criteria in this class are independence and sufficiency Independence also manifests as demographic parity statistical parity and group fairness Sufficiency is equivalent to calibration or predictive parity when all three variables are binary Variants of all three criteria may be defined for example by conditioning on additional variables The counterfactual versions of these criteria simply replace with the potential outcome that is of interest Note that these definitions cannot accommodate more than one potential outcome such as the vector because only one of these outcomes is observed for each unit This is the fundamental problem of causal inference Except in highly constrained unrealistic conditions these three criteria are pairwise unsatisfiable regardless of whether they are defined with respect Wemust therefore choose and justify which criterion we wish to target Causal fairness criteria The counterfactual fairness criteria just described consider potential outcomes with respect to a decision There is a distinct set of causally motivated fairness criteria that consider counterfactuals of the sensitive feature or a proxy for the sensitive feature They characterize a decision or prediction as fair if the sensitive feature or proxy does not cause the decision or prediction either directly or along a prohibited pathway There is some controversy over whether it is meaningful to discuss a counterfactual of a feature like race or gender Additionally See for a set of sufficient conditions under which these unsatisfiability results disappear satisfying these metrics typically precludes use of most of the features that go into risk assessment like prior history which is not tenable in practice Finally it is not clear that counterfactuals of the sensitive feature are useful or appropriate to consider in the context of risk assessment For example in the child welfare setting workers are compelled to screen in calls whenever a child is in danger of neglect or abuse While it is important to ensure that risk is assessed accurately for different groups it would be inappropriate to make screen-in decisions based on what a childs risk of neglect or abuse would be if they had been of a different race their whole life even if such an assessment were possible Ways of achieving fairness There are three broad approaches to developing fair models preprocessing the input data to remove bias constraining the learning process aka in-processing and postprocessing a model to satisfy fairness constraints Our approach can be viewed as belonging to class We refer to the predictor that our method returns equivalently as a post-processed or derived predictor Each approach has advantages and disadvantages Many widely used RAIs are proprietary tools developed by for-profit companies so they are not amenable to internal tinkering Developing new fairer RAIs would be costly and perhaps infeasible from a policy perspective The advantage of post-processing in this setting is that it can be applied to models that are already in use The predictor that our method returns requires access at runtime only to the sensitive feature and the output of the existing predictor so in principle it could easily be incorporated into existing risk assessment pipelines In particular our approach extends the work of Hardt et al who proposed a method to post-process binary predictors to satisfy observational equalized odds while minimizing loss with respect to observable Their post-processed predictor is the solution to a simple linear program We adapt their method to the counterfactual setting in which the fairness criterion is approximate cEO and the loss function is weighted classification error with respect to Because is not observable when we require tools from causal inference to solve this problem Hardt et als analysis treats the joint distribution of as known and frames post-processing primarily as an optimization problem We build on their results by not making this assumption and treating post-processing as a statistical estimation problem Why equalized odds When evaluating a predictive system it seems natural to focus on its real-world impact rather than its outputs per se One desirable property of a decision process is the avoidance of disparate impact Disparate impact is a legal doctrine enshrined in US law that prohibits practices which have an unjustifiable adverse impact on people who share a protected characteristic regardless of discriminatory intent By way of shorthand we will say that if then the system exhibits discriminatory disparate impact In recidivism prediction for example this could mean that black defendants who would not recidivate if released are more Some authors use disparate impact to refer to the criterion ie independence likely to be detained pretrial than white defendants who would not recidivate if released In the context of RAIs decision makers typically have wide latitude in how they interpret and act on the risk scores so constraining the RAI does not enforce fairness with respect to their decisions However if decision makers after the introduction of the RAI make their decisions only on the basis of the RAI scores and other variables which are independent of the RAI and given then counterfactual equalized odds will imply That is let represent the function describing the decision process after the RAI is introduced If cEO is satisfied and then it follows that Even if it is easy to see that if the conditional independence statement nearly holds or if depends primarily on rather than then discriminatory disparate impact can be small No such guarantees hold for predictors satisfying either independence or sufficiency Chouldechova in particular showed how predictors which satisfy sufficiency predictive parity are likely to yield decisions such that these arguments are unchanged when we substitute for Though there is no consensus about how to quantify fairness this is at least one consideration in favor of equalized odds MOTIVATING EXAMPLE Having motivated equalized odds over predictive parity or independence we now motivate the use of counterfactual rather than observational equalized odds Consider a school district that assigns tutors to students who are believed to be at risk of academic failure The school district wishes to develop a RAI to better identify students who need tutors while ensuring that this resource is allocated fairly across two levels of the sensitive feature Let represent the decision to assign or not assign a tutor and let represent academic success or failure A cEO predictor satisfies while an predictor satisfies Divergence in these predictors is driven by the extent to which in the training data In order to parameterize this divergence we introduce the following definitions Definition The need rate for group is the probability that a student from group would fail without a tutor Definition The opportunity rate for group is the probability that a student in group who needs a tutor receives one Definition The intervention strength for group is the probability that a student in group who would fail without a tutor would succeed with a tutor We simulate a simple data generating process in which we allow the intervention strength to vary while constraining it to be equal for the two groups We fix all other parts of the distribution In particular we set set the need rates to and for groups and and set the opportunity rates to and We set the probabilities that a tutor is assigned when it is not needed to and This represents a scenario in which the minority group has greater need perhaps due to socioeconomic factors or prior educational opportunities and also is likelier than the majority group to receive resources at baseline prior to the development of the RAI Finally we set meaning that tutoring never increases the risk of failure We consider a hypothetical predictor with fixed false positive rate and false negative rate We assume as would be the case for example when is a high quality predictor of Figure shows the c tprs for this predictor as a function of intervention strength relative to the baseline opportunity rates for the two groups When the intervention has no effect strength the c tprs are equal because so the c tpr and TPR are identical Of course a strength of means the tutoring is worthless For all strength values the c tpr of the minority group is lower than for the majority group The difference in error rates increases as intervention strength increases A cEO predictor avoids this problem by design the c tprs for the two groups are constrained to be equal This example makes it clear that predictors in general will not prevent discriminatory disparate impact whereas as discussed in section counterfactual predictors have at least the potential to mitigate or avoid it This example also illustrates how predictors can reduce rates of appropriate intervention For example suppose that decision makers after the introduction of the RAI set ie they assign tutors precisely to students whom the RAI labels as high risk Then for any intervention strength the opportunity rate for the minority group decreases below baseline the RAI harms the minority group AN OPTIMAL FAIR DERIVED PREDICTOR Having motivated counterfactual equalized odds we now develop a method to generate predictors which satisfy it Estimand We expand our notation in order to fully describe our problem setting Consider a random vector P where in addition to the binary sensitive feature decision and outcome we have covariates and a previously trained binary predictor We require only that is observable we do not require access to its inputs or internal structure in practice could represent a RAI that is already in use such as a recidivism prediction tool The covariates may or may not overlap with the inputs to Their role in the analysis is to render counterfactual quantities identifiable Our target is a derived predictor that satisfies approximate cEO As in the case of observable equalized odds considered by we achieve this by randomly flipping with probabilities that depend only on and Consider a column vector We define an associated derived predictor where Intervention strength PR s id li ne s po ni ty ra te d as he d lin es Group minority majority Figure Counterfactual true positive rates c tprs solid lines for a RAI satisfying observational equalized odds as a function of the intervention strength Dashed lines indicate opportunity rates prior to the development of the RAI The more effective the tutoring the higher the intervention strength the worse the RAI is at identifying students who need it and the greater the disparity in its performance between the minority and the majority group When tutoring is more effective the RAI may reduce the appropriate assignment of tutors below the baseline opportunity rates In other words the parameters represent conditional probabilities that flips while the parameters represent conditional probabilities that doesnt flip Notice that for we have the derived predictor is equal to the input predictor Our target is a loss-optimal fair predictor where the fairness criterion is approximate cEO The loss function we consider is weighted classification error For fixed denote the loss by where are chosen by the user to capture the relative importance of false positives and false negatives Note that we will generally suppress the dependence of L The estimand is argmin subject to where are given above in Definition and the fairness constraints are chosen by the user Setting both these constraint parameters to requires cEO to be satisfied exactly while setting them to allows to be arbitrarily unfair Setting to regardless of forces to satisfy counterfactual equal opportunity see for the observational definition of this criterion We refer to this quantity as loss instead of the conventional risk in order to avoid confusion between risk assessment and the error rate of a predictor Remark Note that the full vector is required only to estimate the parameter that defines the optimal fair derived predictor Once has been estimated the resulting derived predictor requires access at runtime only to the sensitive feature and the input predictor Since our estimands involve counterfactual quantities distributional assumptions are required in order to equate them to observable quantities Identification In this subsection we show that the counterfactual error rates and loss can be identified under standard causal inference assumptions All the quantities to be identified can be written in terms of the loss and the counterfactual error rates of the input predictor For ease of notation we first define two nuisance parameters that appear in the estimand and associated estimators namely the outcome regression and propensity score function We generally drop the arguments from these functions in subsequent usage for the sake of conciseness We make the following standard no unmeasured confounding-type causal inference assumptions Consistency Positivity st Ignorability The consistency assumption means that the outcome observed for each individual is precisely the potential outcome corresponding to the treatment received This implies that one persons treatment assignment does not affect another persons outcomes meaning for example that an individuals recidivism behavior does not depend on whether other individuals are detained or released The positivity or overlap assumption requires that within strata of of measure individuals have some chance of receiving no intervention Finally the ignorability or no unmeasured confounding assumption requires that within strata of the treatment is essentially random with respect to Satisfying ignorability assumptions typically requires collecting a rich enough set of deconfounding covariates In the present case even if is low dimensional the ignorability assumption is plausible if the input predictor substantially drives decision making or if it happens to be an accurate if not necessarily fair predictor of Before giving the identifying expressions for the loss and the error rate differences we give identifying expressions for the error rates of the input predictor which themselves appear in the expressions for Proposition Under assumptions the counterfactual error rates of the input predictor are identified as follows c fpr E fnr E Proofs of propositions are given in Appendix A We now define several quantities that appear in the identifying expressions for and E for c fpr c fpr c fpr c fpr fnr fnr fnr fnr Proposition Under assumptions the loss and error rates of the derived predictor are identified as Since the term in the loss is fixed we can drop it without changing the minimizer of the loss We can therefore rewrite the estimand as argmin subject to In other words the optimal fair derived predictor is the solution to a linear program LP We refer to this as the true LP since it defines the estimand We now define an estimator as the solution to an estimated LP Estimation An estimator for is derived by computing estimates of the true LP coefficients and then solving the resulting estimated LP argmin subject to Any solution suffices There are many possible estimators of the true LP coefficients Theorems and below show that the rates at which these coefficients are estimated propagate to the loss and fairness properties of Arguably the simplest estimators involve plugging in an estimate of the regression function We propose instead using doubly robust estimators which yield faster rates of convergence than plugin estimators in general nonparametric settings For ease of notation let denote the uncentered efficient influence function for and let denote an estimate constructed from estimates and To minimize the use of indices let denote the sample average of any function of The doubly robust estimators for individual coefficients are c fpr fnr These estimates are assembled into the corresponding vectors In order to obtain optimal convergence rates it is generally necessary to estimate the nuisance functions and on one sample and then compute the sample mean on an independent sample conditional on those estimates To obtain full sample size efficiency one can swap the folds repeat the procedure and average the results an approach that is popularly called cross-fitting A k-fold version of cross-fitting with is also possible If and are assumed to be sufficiently well-behaved ie if they belong to Donsker classes then no such sample splitting is necessary We prefer to avoid this assumption and utilize sample splitting See Appendix C for a schematic of the sample splitting procedure We now show that approaches optimal behavior at rates that depend on the rates at which the LP coefficient vectors are estimated We define two quantities of interest the loss gap and the excess unfairness and give accompanying theorems Following standard usage we say that an estimator of a parameter is consistent at rate for some real-valued function if for a suitable norm For example if then we say converges at rates We say that an estimator converges faster than if Definition The loss gap is the difference in loss between the derived predictor and the optimal derived predictor We use the term loss gap rather than excess loss to acknowledge that the loss of can be less than the loss of if falls outside the true constraints Of course this can only occur if violates the true fairness constraints which can happen because the constraints are estimated Theorem Loss gap Suppose that and are all consistent at rate Under Assumptions Definition The excess unfairness of in the c fpr is max c fpr c fpr and the excess unfairness of in the fnr is max fnr fnr Since the estimated constraints should fluctuate around the true constraints its possible for to have error rate differences that are smaller than which motivates bounding these quantities below by We ignore optimization error since this is a function of the number of optimization iterations and can be made arbitrarily small Theorem Excess unfairness Suppose that and are all consistent at rate Under assumptions max Remark The behavior of vs Without assumptions about how the loss and fairness of depend on there is no guarantee about the rate at which will approach This is not a concern however since the object of interest is not per se but a predictor that behaves like Estimating performance of the derived predictor Once has been computed it is of interest to check both the loss and the error rate differences of the resulting derived predictor for example to understand the performance cost of fairness and to check whether the procedure successfully controlled the error rate differences These estimates should be computed on a test set that is independent of the sample used to estimate Within the test set the same sample splitting considerations apply unless they are assumed to belong to Donsker classes the nuisance parameters and should be estimated on separate folds from the folds used to compute the relevant sample means Since the estimators below are conditional on a fixed they can in fact be applied to any fixed parameter value We define one additional quantity of interest Definition The loss change for a derived predictor relative to an input predictor is We refer to a loss change rather than an increase in loss because it is possible for to have smaller loss that This is not a typical expectation in fair prediction problems the set of fair classifiers is necessarily smaller than the set of fair and unfair classifiers so there is a fairness-accuracy tradeoff In the RAI setting however since predictors are typically trained to predict observable outcomes their performance may be arbitrarily bad with respect to the potential outcome It is therefore not implausible than a derived fair predictor could have higher accuracy than the input predictor Once again we propose using doubly robust estimators These estimators are essentially identical to the estimators of the LP coefficients used in the previous section loss loss change c fpr c fpr fpr c fpr fnr fnr fnr fnr error rate difference in c fpr error rates difference in fnr where recall so that ie the derived predictor is simply the input predictor Note that the loss estimator adds back in the portion of the loss that doesnt depend on and that we consequently removed from the LP in When the nuisance parameter estimators and are estimated at faster than rates these doubly robust estimators are n-consistent and asymptotically normal so we can use the central limit theorem to derive asymptotically valid confidence intervals RESULTS There is no previous method designed to achieve counterfactual equalized odds or related fairness criteria that we can compare our method to However we compare our method to an approach that uses plugin estimators for the LP coefficients Simulations We use one set of simulations to illustrate Theorems and and another set to explore fairness-performance tradeoffs We use equal misclassification weights so that false positives and false negatives contribute equally to the loss Each estimation procedure was run times for each sample size Since is known here the true loss and fairness values were computed on a separate validation set of size using plugin estimators with the true These values showed negligible variation over many repetitions Setup First we define a pre-RAI data generating process Using this data we train a predictor to predict observable outcomes mirroring how RAIs are typically constructed in practice We then define a post-RAI data generating process which only differs in that the predictor now affects the decisions This emulates the way RAIs are intended to work in practice for example a criminal defendant labeled high-risk be a RAI might be less likely to be released pre-trial than they would have been prior to the introduction of the RAI The data generating process is designed to meet assumptions with upper bounded at It is described fully in Appendix D Theorems and To simulate the estimation of the LP coefficient vectors at a particular rate we add random noise of magnitude to the nuisance parameters and Note that rates for regression functions cannot be attained in general nonparametric settings These slower rates can be attained in such settings under relatively weak assumptions The principal advantage of doubly robust estimators over plugin estimators is that they converge at a rate that is like the product of the nuisance parameter rates When and both converge faster than the estimators all converge at rates which is the fastest rate that can be attained for these parameters in general nonparametric settings Plugin versions of would inherit the slow rates of Figure shows the loss and the excess unfairness values for the post-processed predictor with fairness constraints As expected when doubly robust estimators are used the loss and excess unfairness values converge at rates to the loss of the optimal derived predictor and respectively When plugin estimators are used the rates are slower than The noise is added on the logit scale to ensure that remain in and is again truncated to Figure Illustration of Theorems and Loss L and excess unfairness values for the derived predictor for samples of size to Each vertical line represents a mean sd over simulations Orange horizontal lines represent the loss of the optimal derived predictor top left panel or The top row represents our doubly robust DR procedure and shows that the loss and excess unfairness converge to their target values The bottom two rows represent values from the DR procedure or a plugin PI procedure transformed by where is L or or as appropriate These rows illustrate that n-convergence is only guaranteed for DR the scaled values for DR do not grow in while the scaled values for SR begin to diverge Fairness-performance tradeoffs Figure shows the loss change for each point in a grid of fairness constraints Here is the Bayes-optimal predictor of in our data generating scenario meaning Since any derived predictor necessarily has greater loss than the Bayesoptimal predictor we refer to the loss change here equivalently as the performance cost In the data generating process used in the previous section the Bayes-optimal predictor has absolute error rate differences of only and which leaves little room to illustrate the potential cost of fairness For these simulations therefore we alter the data generating process slightly See Appendix D This results in a Bayes-optimal predictor with absolute error rate differences of and and a loss of which are plausible values for a real predictor As expected when or the performance cost is the input predictor already falls satisfies the fairness constraints so our method simply returns the input predictor As the tolerances tighten towards the performance declines though never substantially For when the derived predictor is constrained to satisfy exact cEO the loss increases by to L os s ch an ge Figure Fairness-performance tradeoffs Loss change for the Bayes-optimal input predictor and corresponding to different fairness constraints The black area represents fairness constraints that are looser than the error rate differences of the input predictor which incur no performance cost The highest performance cost occurs when the error rates differences are both constrained to be meaning the derived predictor satisfies cEO exactly The different values for and are reflected in the differing costs of satisfying fairness along the two axes the cost of controlling are lower than the costs of controlling showed that post-processing can result in predictors with poor performance but it is unclear how likely this is to be a problem in practice While the fairness-accuracy tradeoff naturally depends on the data generating process our example illustrates that fairness can in some cases be achieved without substantial performance costs COMPAS data We illustrate our method on the COMPAS recidivism dataset gathered by ProPublica COMPAS refers to a collection of tools designed to assess the risk of recidivism The dataset comprises public arrest records criminal records and COMPAS RAI scores from Broward County Florida spanning After filtering the data in the same manner as and restricting to defendants who are labeled African-American or Caucasian we are left with data for individuals African-American Caucasian We utilize the COMPAS scores for general as opposed to violent recidivism The scores are given in risk deciles Since our method operates on a binary input predictor we follow ProPublica and set scores of to low risk and scores of to high risk The outcome is recidivism within a two-year time period See for how recidivism is operationalized ProPublicas analysis focuses on the use of COMPAS to inform pretrial release decisions The dataset includes dates in and out of jail but does not indicate whether defendants were released pretrial so we set the treatment to if defendants left jail within three days of being arrested and otherwise This yields released individuals African-American Caucasian and incarcerated individuals African-American Caucasian Note that this threshold is somewhat arbitrary Florida state law generally requires individuals to be brought before a judge for a bail hearing within hours of arrest but it may take time for individuals to post bail if they are required and able to do so The covariates consist of gender coded male or female age coded categorically for between and and the number of prior crimes and charge degree misdemeanor or felony Without consulting with domain experts it is difficult to assess the plausibility of the positivity and ignorability assumptions given these covariates Hence we intend our analysis primarily to be illustrative of our method and we resist drawing strong substantive conclusions about COMPAS We weight false positives and false negatives equally ie we set We randomly split the data into training and test sets of equal size For we set the fairness constraints to computing the corresponding estimate on the training set and estimating properties of the post-processed predictor on the test set We also estimate properties of the binarized COMPAS score on the test set We use random forests to estimate both the propensity scores and the outcome regression To reduce the variance of the estimates we employ -fold cross-fitting within the train set we compute five estimates using four folds at a time to estimate the nuisance parameters and the held-out fold to compute Then We utilize the test set in an analogous fashion for the remaining estimators We also estimate the predictive change the proportion of predictions that the post-processed predictor flips A point estimate and confidence interval for this quantity can be straightforwardly computed via a formula given in Appendix E Table contains estimates and confidence intervals for COMPAS and for the post-processed predictor corresponding to fairness constraints of The loss for COMPAS is and the differences in the c fpr and fnr are and respectively The signs of these differences are consistent with what ProPublica found in their analysis with respect to observable the false positive rates are higher for African-American defendants while the false negative rates are higher for Caucasian defendants The post-processing procedure successfully shrinks these differences to and which fall within the target range of This reduction corresponds to flipping of the COMPAS scores and it incurs an increase in risk of only The value of corresponding to here is The and the indicate that does not change the COMPAS scores for African-American defendants who receive a low-risk score or Caucasian defendants who receive a high-risk score The scores for high-risk African-American defendants are flipped to low-risk of the time while the scores for low-risk Caucasian defendants are flipped to high-risk of the time This has the effect of increasing the false positive rate and decreasing the false Figure Convergence of the estimated loss predictive change and error rate differences for post-processed versions of the binarized COMPAS predictor Fairness constraints are set to over a range of values Vertical lines are CIs Horizontal orange lines indicate the reference values for COMPAS or in the case of predictive change The dashed blue lines and mark the target fairness constraints negative rate for Caucasians while moving the rates in the opposite directions for African-Americans Figure shows the loss error rate differences and predictive change for fairness constraints ranging from requiring no gap in error rates to imposing no fairness constraints Each constraint induces an estimate and a corresponding post-processed predictor The estimated fairness gaps fall along or within the lines indicating that each satisfies its target constraints At the most stringent setting of the loss for is approximately which compares favorably with the estimated baseline loss of for COMPAS This flips slightly less than of the scores For the fairness constraints are essentially no longer active since COMPAS itself satisfies these constraints Indeed as expected the values for are all essentially meaning that and the estimated risk and fairness values all fall close to the estimated values for COMPAS There is still some variation in the estimated values due to randomness in the k-fold cross-fitting procedure These results illustrate that our approach performs as intended on a real dataset if these data were indeed generated from a distribution satisfying the identifying assumptions then our post-processed predictor would satisfy approximate counterfactual equalized odds while incurring little cost in performance Table Estimates and confidence intervals for the loss L loss change error rates c fpr and fnr for groups and error rate differences and predictive change for the binarized COMPAS predictor and the post-processed predictor with and set to L c fpr c fpr fnr fnr P Child welfare data Cost-sensitive loss functions can drive to a trivial classifier that always predicts one class We illustrate this phenomenon on a dataset representing calls to a child-welfare hotline in Allegheny County Pennsylvania The data comprises over calls and contains over features The features describe allegations made in the call assessments of risk made by hotline workers and features pertaining to individuals associated with the call Workers must decide whether to screen in a call which means opening an investigation into the allegations The baseline decision is to screen out meaning no investigation takes place The outcome is re-referral to the hotline within a six month period For further details about the child welfare setting and this dataset in particular see Unlike the COMPAS dataset this dataset does not include a previously trained predictor We therefore first build a predictor that predicts and then we post-process In this setting we have reason to believe that the identification assumptions in section are plausible once cases with the highest propensity for screen-in are removed see RAIs are not necessary or useful for cases that are already guaranteed to be screened in In order to accomplish this filtering we first build a propensity score model using random forests on roughly one third of the data The model appears well-calibrated so we filter out the approximately of the cases with estimated propensity scores greater than Note that downstream results did not change substantially when these cases were left in We then train a classification random forest to predict conditional on using the same third of the data Under the identifying assumptions is equal in distribution to so is indeed an estimate of the target Following recommended usage in this setting we set the classification threshold to capture the top riskiest cases The predictor has estimated error rate differences and confidence intervals of and It is unsurprising that these differences are small given that rereferral rates are similar for Black and White cases See for Figure Cost-sensitive post-processing for the child welfare predictor over a range of cost ratios with fairness constraints Each column represents a single with the four components in rows False positives are weighted between and times as heavily as false negatives to the left of the dashed line and vice versa to the right Extreme cost ratios push the post-processed classifier to a trivial classifier that always predicts to the left of the orange lines or to the right Between these post-processing essentially returns the input predictor an examination of the relationship between base rates and error rates In order to have nontrivial active fairness constraints we set Figure shows the value of over a range of cost ratios When false positives are weighted more than times as heavily as false negatives post-processing returns classifiers that are very close to the simple majority classifier When false negatives are weighted more than times as heavily as false positives post-processing returns the simple minority classifier Since the input classifier is approximately fair between those ranges post-processing returns classifiers that are very close to the input classifier with only the fourth component deviating slightly from This behavior is expected Note that a simple majority or minority classifier always satisfies counterfactual equalized odds since the error rate differences are Since the post-processed predictor only has access at runtime to two binary features as either false positives or false negatives become sufficiently important one of these simple classifiers will at some point become the lowest risk option This is possible in principle are equal but it is guaranteed as their ratio grows Since this dataset did not include a pretrained predictor of it would be preferable to adopt an in-processing approach ie to train a predictor that satisfies the desired fairness constraints in a single stage rather than training an unconstrained predictor and then post-processing it We pursue this task in ongoing work DISCUSSION AND CONCLUSION In this paper we considered fairness in risk assessment instruments RAIs which are naturally concerned with potential outcomes rather than strictly observable outcomes We defined the fairness criterion approximate counterfactual equalized odds approximate cEO which allows users to negotiate the tradeoff between fairness and performance We argued that this fairness criterion is likelier than other candidate criteria to reduce discriminatory disparate impact which we defined as We presented a method to post-process an existing binary predictor to satisfy approximate cEO using doubly robust estimators and we showed that our method has favorable convergence properties Our rate results translate readily to the post-processing setting of in which the outcome of interest is the observable and the fairness criterion is approximate observational equalized odds Once it is constructed the post-processed predictor requires access at runtime only to the sensitive feature and the input predictor making it relatively feasible to implement on top of existing RAIs A predictor trained from scratch would be constrained by the set of covariates available in deployment whereas the post-processing approach allows researchers to devise a set of suitable deconfounding covariates and then collect an appropriate dataset on a one-time basis In closing we note that from our perspective notions of fairness in predictive systems ought to be subordinate to notions of fairness grounded in the actual decisions or events that those systems inform and the impact that those decisions have on peoples lives Though little is currently known about how decision makers respond to RAIs there is some evidence that judges do not have much faith in recidivism predictions and that RAIs can have little impact on decisions As RAIs and the general publics understanding of how they function co-evolve it is likely that the ways in which decision makers respond to them will evolve as well Nevertheless it seems plausible that some fairness criteria for RAIs are likelier than others to lead to increased unfairness with respect to decisions and outcomes While this is ultimately an empirical question we believe that this kind of consideration ought to ground discussions of fairness in RAIs and predictive systems generally As long as there are domains involving high stakes decisions that we do not wish to fully automate RAIs will remain relevant and so will the task of ensuring that they lead to a society that is more fair not less