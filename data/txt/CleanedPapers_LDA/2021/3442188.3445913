Fair Clustering via Equitable Group Representations What does it mean for a clustering to be fair One popular approach seeks to ensure that each cluster contains groups in roughly the same proportion in which they exist in the population The normative principle at play is balance any cluster might act as a representative of the data and thus should reflect its diversity But clustering also captures a different form of representativeness A core principle in most clustering problems is that a cluster center should be representative of the cluster it represents by being close to the points associated with it This is so that we can effectively replace the points by their cluster centers without significant loss in fidelity and indeed is a common use case for clustering For such a clustering to be fair the centers should represent different groups equally well We call such a clustering a group-representative clustering In this paper we study the structure and computation of group-representative clusterings We show that this notion naturally parallels the development of fairness notions in classification with direct analogs of ideas like demographic parity and equal opportunity We demonstrate how these notions are distinct from and cannot be captured by balance-based notions of fairness We present approximation algorithms for group representative k-median clustering and couple this with an empirical evaluation on various real-world data sets We also extend this idea to facility location motivated by the current problem of assigning polling locations for voting KEYWORDS algorithmic fairness clustering representation INTRODUCTION Growing use of automated decision making has sparked a debate concerning bias and what it means to be fair in this setting As a result an extensive literature exists on algorithmic fairness and in particular on how to define fairness for problems in supervised learning However these notions are not readily applicable to unsupervised learning problems such as clustering One reason is that unlike in the supervised setting a well-defined notion of ground truth does not exist in such problems In proposed the idea of balance as a notion of fairness in clustering Given a set of data points with a type assigned to each one balance asks for a clustering where each cluster has roughly the same proportion of types as the overall population This definition spawned a flurry of research on efficient algorithms for fair clustering Further work by other researchers has extended this definition but with the same basic principle of proportionality Balance draws its meaning from the perspective of clustering as a generalization of classification from two to many categories If we select individuals into multiple categories where each category has some associated benefits or harms balance asks that different groups receive these benefits or harms in similar proportions For example a tool that assigns individuals to different categories based on the loan packages being offered might attempt to ensure demographic balance across all categories But an important purpose of clustering is to find representatives for points by grouping them and choosing a representative like a cluster center Consider for example the problem of redistricting where the goal is to partition a region into districts each served by one representative who can speak to the concern of the district The criterion of balance applied say to partisan affiliation will result in districts that have a proportional number of residents associated with each party This is unfortunately the worst kind of redistricting It is a form of gerrymandering known as cracking and results in the majority party being able to control representation in all the districts In fact the vast majority of clustering formulations focus on the problem of quality representation And the quality of representation is usually measured by some form of distance to the chosen representative the greater the distance the poorer the representation There are serious implications for fairness as measured in terms of access and we illustrate this with an example of current concern Consider the placement of polling locations A study showed that in the US presidential election voters in predominantly black neighborhoods waited percent longer at polling locations than those in white neighborhoods and in we are currently seeing polling locations removed or merged because of the pandemic The clustering here is the induced clustering where each resident is associated with a specific polling location the representative and the quality of representation can be measured as a function both of the distance to the polling location and the waiting time at the location itself Figure Balance is preserved in the left hand figure but the centers are much more representative of the red points In contrast the clustering on the right represents both groups comparably It is easy to see that balance criteria cannot accurately capture the goal of representation equity To illustrate why see the example presented in Figure which shows a balance-preserving k-means clustering on the left for two groups denoted by the colors red and blue and regular k-means clustering on the right Here the number of red points is larger than the other group Therefore each cluster center is chosen close to its respective red groups centroid As a result red points are better represented by chosen centers compared to blue points Our contributions In this paper we introduce a new way to think about fairness in clustering based on the idea of equity in representation for groups We present a number of different ways of measuring representativeness and interestingly show that they a naturally parallel standard notions of fairness in the supervised learning literature and b are incompatible in the same way that different notions of fairness in supervised learning are We present algorithms for computing fair clusterings under these notions of fairness These algorithms come with formal approximation guarantees we also present an empirical study of their behavior in the context of the problem of polling location placement A second key contribution of this work takes advantage of the relation between clustering and the associated problem of facility location In most clustering problems the number of clusters that the algorithm must produce is fixed in advance In the context of polling this is equivalent to specifying the number of polling locations to be established ahead of time But we can associate a cost with opening a new facility or cluster center giving us a choice of either assigning a point to a center that might be far away or opening a new center that might be closer This is the well known problem of facility location it is closely related to clustering Again returning to the context of polling the framing in terms of facility location allows for the possibility of creating new polling locations if the cost of opening can be paid for in terms of improved access We define what representation equity means in the context of facility location and present algorithms and experimental support as well Our algorithms also allow us to incorporate load balancing between different facilities In the context of polling we can ensure that no more than voters are assinged to a polling location for a user-specified threshold We can think of facility location as the Lagrangian relaxation of clustering where the hard constraint on the number of clusters is replaced by a term in objective function Related Work Chierichetti et al introduced balance as a fairness constraint in clustering for two groups Considering the same setting with binary attribute for groups Backurs et al improved the running time of their algorithm for fair k-median R√∂sner and Schmidt proposed a constant-factor approximation algorithm for fair k-center problem with multiple protected classes Bercea et al proposed bi-criteria constant-factor approximations for several classical clustering objectives and improved the results of R√∂sner and Schmidt Bera et al generalized previous works by allowing maximum over- and minimum under-representation of groups in clusters and multiple non-disjoint sensitive types in their framework Other works have studied multiple types setting multiple non-disjoint types and cluster dependent proportions In a different line of work Ahmadian et al studied fair k-center problem where there is an upper bound on maximum fraction of a single type within each cluster Chen et al studied a variant of fair clustering problem where any large enough group of points with respect to the number of clusters are entitled to their own cluster center if it is closer in distance to all of them A large body of works in the area of algorithmic fairness have focused on ensuring fair representation of all social groups in the machine learning pipeline Recent work by Mahabadi et al studies the problem of individually fair clustering under the fairness constraint proposed by Jung et al In their framework if denotes the minimum radius such that the ball of radius centered at has at least points then at least one center should be opened within distance from In recent independent work Ghadiri et al propose a fair k-means objective similar to one of our objectives and study a variant of Lloyds algorithm for determining cluster centers FAIR CLUSTERING In this section we introduce notions of fair clustering that are rooted in the idea of equitable representation To that end we introduce different ways to measure the cost of group representation We can then define a fair clustering We start with some basic definitions Given a set of points a clustering is a partitioning of into clusters For most of the paper we will consider clustering objectives that satisfy the Voronoi property the optimal assignment for a point is the cluster center nearest to it This includes the usual clustering formulations like k-center k-means and k-median For such clusterings the cluster center defines the cluster and thus we can represent a clustering more compactly as the set of cluster centers The cost of a clustering of a set of points is given by the function cost For any subset of points we denote the cost of assigning to cluster centers in a given clustering as cost Finally given a cost function cost and a set of points we denote the set of centers in an optimal clustering of by Opt cost When the context is clear we will drop the subscript and merely write this as As usual an ùõº-approximation It is possible to define so-called soft clusterings in which a point might be assigned fractionally to multiple clusters We do not consider such clusterings in this paper algorithm is that one that returns a solution that is within an ùõº-multiplicative factor of Each point in is associated with one of groups eg demographic that we wish to ensure fairness with respect to We define the subset of points in group as Definition Fair Clustering Given groups fair clustering minimizes the maximum average representation cost across all groups argmin max cost cost where C is the set of all possible choices of cluster centers Note that we are not trying to force all groups to have the same representation cost that constraint can be trivially satisfied by ensuring all groups have a large cost ie via a poor representation Rather we want to ensure all groups have good representation while still ensuring that the gap between groups measured by the group with the maximum cost is small This also distinguishes our definition from proposals for clustering cost that try to minimize the maximum cost across clusters instead of across groups Quality of group representation We now introduce different ways to measure the group representation cost cost Absolute Representation Error In supervised learning statistical parity captures the idea that groups should have similar outcomes Rephrasing it says that groups should be represented equally well in the output In the case of binary classification statistical parity requires for two groups and where denotes the sensitive attribute When clustering the equivalent notion of statistical parity asks that cluster centers represent all groups equally well regardless of their potentially different distributions More specifically the average distance between members of a group and their respective cluster centers should look the same across groups Motivated by this we introduce the following definition of representation cost Definition AbsError The absolute representation error of a clustering is defined as AbsError where is a set of points is a set of centers and is a an arbitrary distance function between and nearest center to it in An AbsError-fair clustering is a fair clustering that uses AbsError to measure group representation cost in Definition Relative representation error AbsError does not take underlying distributions of demographic groups into account However in cases where different groups have drastically different underlying distributions it may be necessary to acknowledge such a difference Consider the example provided in Figure where one demographic group orange has a much smaller variance compared to the other blue Assume that the within-group distances for the orange group can be ignored compared to distance An AbsError-fair clustering d C AbsError-Fair C RelError-Fair Figure For groups with different underlying distributions AbsError and RelError lead to different locations for the center picks AbsError-Fair as the center which minimizes the maximum average AbsError However such a clustering may seem unfair as it induces a large cost on the orange group compared to its optimal representation cost which is close to zero if the orange groups center is picked The issue of differences in base distributions motivates fairness measures like equality of opportunity based on balancing error rates rather than outcomes As with statistical parity we can define a natural analog in the context of representation Rather than look at the error in representation in absolute terms we compare the average distance between members of a group and their respective cluster centers to the corresponding optimal value for that group if we only clustered the members of that group This relative measure of representation error yields the following definition Definition RelError The relative representation error of a clustering is given by RelError where is a set of points is a set of centers and is a an arbitrary distance function between and nearest center to it in One can also try to capture the relative error via a difference instead of a division RelError This is similar to the formulation used by in their work on fair PCA However in the case of clustering this quantity can be NP-hard to compute even for a given set of centers This is because cannot be computed exactly For this reason we will not discuss this formulation further How the different measures of fairness compare We have drawn on an analogy to fairness in supervised learning to formulate two measures of group representation fairness We now make the informal case that the analogy also carries over to the incompatibility between the two notions similar to the wellknown result in the supervised learning setting To see this consider a point set with two defined groups and Without loss of generality assume that can be clustered better than ie that cost cost Now consider Figure where RelError fair AbsError fair costA cost B cost A cost B c Figure Unless the two groups have the same base rates we cannot achieve fairness with respect to AbsError and RelError any clustering of is represented as a point whose x-coordinate is the cost cost and whose y-coordinate is the cost cost The AbsError fair line represents all clusterings where the two groups have equal AbsError costs and the RelError fair line which passes through the point cost cost represents all clusterings where the two groups have equal RelError costs Consider an optimal unconstrained clustering represented by point Consider the position of a fair clustering under either of the above two measures relative to Clearly such a clustering would not be in either of the areas marked in red dots because either the costs for both groups increase making it inferior to or both costs decrease contradicting the optimality of Further note that a clustering that is better with respect to AbsError must be closer to AbsError fair line compared to and one that is better with respect to RelError must be closer to RelError line compared to The only way in which these two notions can coincide in a single clustering is if either both groups have the same optimal cost cost cost in which case the two lines coincide analogous to the two groups having the same base distribution or if the optimal clustering happens to achieve zero cost for both groups analogous to the point set admitting perfect clustering This informal reasoning hints at a more general incompatibility theorem for clusters analogous to the works of which we leave as an open direction Fairness using standard clustering methods Sections and will present algorithms for fair clustering under the measures of fairness introduced above Before that we make a simple observation about approximation guarantees we can achieve by using standard clustering algorithms with minor modification If our data is composed of groups as we discussed then optimizing the cost function on would correspond to the minimization problem min cost For most known cost functions and algorithms it turns out that adding a weight for each point still allows for efficient algorithms Let us consider assigning a weight of to all the points in Then the weighted minimization problem is min cost The observation is now the following Observation be a clustering produced by an ùõº-approximation algorithm for the problem Then achieves an approximation to the fair clustering problem definition with the same cost function Proof Let be the objective value for the fair clustering problem and be the corresponding clustering Then by definition we have max cost cost Because the solution found by the algorithm is an approximation we have that cost cost This implies that each term is implying the desired approximation bound This observation implies for example that for the fair versions of k-means and k-median we can use known constant factor approximation algorithms to get factor approximation algorithms directly APPROXIMATION ALGORITHMS FOR FAIR k-CLUSTERING For the fair k-median and k-means problem we now develop approximation algorithms by writing down a linear programming relaxation and developing a rounding algorithm Relaxation for AbsError-Fair clustering We start by considering the AbsError objective for fair clustering definitions and To recap we have a set of points that is composed of disjoint groups We first describe the algorithm for k-median and then discuss how to adapt it to the k-means objective see Remark At a high level the algorithm aims to open a subset of the as centers and assign the rest of the points to the closest open point The choice of the points as well as the assignment is done by using a linear programming LP relaxation for the problem and rounding the obtained solution The variables of the LP are as follows for is intended to denote if point is assigned to center These are called assignment variables We also have variables that are intended to denote if is chosen as one of the centers or medians The LP called FairLP-AbsError is now the following min subject to for all for all for all for all groups The only new constraint compared to the standard LP formulation for k-median eg is the constraint for all groups This is to ensure that we minimize the maximum k-median objective over the groups Theorem The integrality gap of FairLP-AbsError is Proof Consider an instance in which we have groups each consisting of a single point Formally let for all Suppose that for all and let Now consider the fractional solution in which for all Also let and let for some it does not matter which one It is easy to see that this solution satisfies all the constraints Moreover the LP objective value is However in any integral solution one of the points is not chosen as a center and thus the objective value is at least Thus the integrality gap is Theorem makes it difficult for an LP based approach to give an approximation factor better than which is easy to obtain as we saw in Observation However the LP can still be used to obtain a bi-criteria approximation where we obtain a constant approximation to the objective while opening slightly more than clusters Theorem Consider a feasible solution for FairLP-AbsError with objective value For any there is a rounding algorithm that produces an integral solution such that all other constraints of the LP are satisfied and the objective value is There has been extensive literature on the problem of rounding LPs for problems like k-median see eg However due to our additional fairness condition we are interested in rounding schemes with an additional property that we define below Definition Faithful rounding A rounding procedure for FairLP-AbsError is said to be ùõº-faithful if it takes a fractional solution and produces an integral solution with the guarantee that for every A weaker notion that holds for some known rounding schemes is the following a rounding algorithm is said to be ùõº-faithful in expectation if E The advantage of having a faithful rounding procedure is that it automatically gives a per-group indeed a per-point guarantee on the objective value which enables us to prove the desired approximation bound Proof of Theorem The proof is based on the well-known filtering technique Define to be the fractional connection cost for the point formally Now construct a subset of the points as follows Set to begin with and in every step find that has the smallest value breaking ties arbitrarily and add it to Then remove all such that from the set Suppose we continue this process until is empty The set obtained satisfies the following property for max This is true because if was added to before then and further should not have been removed from which gives the desired bound The property above implies that the set of metric balls are all disjoint Next we observe that each such ball contains a fractional y-value in the original LP solution of at least This is by a simple application of Markovs inequality By definition and thus This means that and thus As the balls are disjoint we have that Now consider an algorithm that opens all the points of as centers By construction all are at a distance from some point in and thus for any group we have that Setting to be the indicator vector for and to be the assignment mapping each point to the closest neighbor in the theorem follows Remark Extension to k-means The argument above can be extended easily to obtain similar results for the k-means objective We simply replace all distances with the squared distances The metric ball around each point can be replaced with the ball and the same approximation factors can be shown to hold Beyond bi-criteria an LP based heuristic While Theorem can achieve for any it is still weaker than what is possible for the standard k-median problem The integrality gap instance shows that this is unavoidable in the worst case using this LP However it turns out that there exist rounding algorithms for k-median that are faithful in expectation as in Definition and end up with being precisely Specifically Theorem There exists a rounding algorithm for FairLP-AbsError that is ùõº-faithful in expectation with and outputs precisely clusters Corollary Let be a solution to FairLP-AbsError There exists a rounding algorithm that a produces precisely clusters and b ensures that the expected connection cost for every group is The corollary follows directly from Theorem by linearity of expectation This does not guarantee that the rounding simultaneously produces a small connection cost for all groups but it gives a good heuristic rounding algorithm In examples where every group has many points well-distributed across clusters the costs tend to be concentrated around the expectation leading to small connection costs for all clusters We will see this via examples in the experiments section Relaxation for RelError-Fair clustering We now show that the rounding methods introduced in Section can also be used for RelError-fair clustering We consider the following auxiliary LP min subject to for all for all for all k-med-approx for all The constraint now involves a new term k-med-approx which is an approximation to the optimum k-median objective of the set For our purposes we do not care how this approximation is achieved it can be via an LP relaxation local search or any other method We assume that if is the optimum k-median objective for then for all k-med-approx for some constant From the works above we can even think of as being Lemma Suppose there is a rounding procedure that takes a solution to FairLP-RelError and outputs a set of centers with the property that for some parameter k-med-approx for all groups Then this algorithm provides an approximation to RelError-fair clustering Proof Let Opt be the optimum value of the ratio-fair objective on the instance The main observation is that the LP provides a lower bound This is true because any solution to ratio-fair clustering leads to a feasible integral solution to fairlp-relerror where the of the constraint is replaced by Since k-med-approx it is also feasible for fairlp-relerror showing that the optimum LP value is Opt Next consider a rounding algorithm that takes the optimum LP solution and produces a set that satisfies with replacing on the Then since k-med-approx we have for all groups and using Opt completes the proof of the lemma Thus it suffices to develop a rounding procedure for fairlp-relerror that satisfies Here we observe that the rounding from Theorem directly applies because ensures that every giving us the same bi-criteria guarantee and the same adjustment under faithful rounding Corollary Corollary to Theorem For any there is an efficient algorithm that outputs clusters and achieves a approximation to the optimum value of the RelError objective FACILITY LOCATION As we discussed earlier the objective in k-means and k-median clustering measures how close the points are on average to their cluster centers There can be situations in which this objective is more important than having a strict bound on the number of clusters produced Consider the example we saw earlier where points correspond to clients located in a metric space and we open a facility at the cluster center with the goal of serving the clients in the cluster In such a context it is reasonable to open more facilities if it serves clients better as long as they collectively pay for opening This is the motivation for the well-known facility location problem Definition Let be a set of clients and L be a set of locations in a metric space with distance function For each location we have an opening cost which is the cost of opening a facility at The objective is now a sum of the connection costs and the opening costs Formally the goal is to select a subset of L so as to minimize Now consider the setting in which clients fall into different demographic groups We propose our objective based on an equal division of the facility opening costs among all the clients Thus every client pays an opening cost of Our definition of fair facility location aims to ensure that the average total cost opening plus connection is small for all the groups Definition be a set of clients composed of groups and let L be a set of locations The goal of fair facility location is to select a subset of the locations so as to minimize max We remark that the second term in the objective is independent of the group as this is the cost paid by every client Our first result is a constant factor approximation algorithm Theorem There is an efficient polynomial time algorithm for fair facility location with an approximation ratio of Proof The proof turns out to follow easily from classic results on facility location Consider the following linear program min subject to for all for all for all for all groups Now we note the rounding algorithm from is a faithful rounding procedure as in Definition and also ensures that for the produced integral solution Using this the desired approximation factor follows It is an interesting open problem to improve the approximation ratio above We note that there are many better approximation algorithms for facility location see eg and references therein However many of these algorithm are either faithful only in expectation or they use primal-dual rounding schemes which do not seem applicable in our context Ensuring uniform load for facilities Definition captures the requirement that all the demographic groups have a small cost for accessing the opened facilities However this objective does not fully capture real life constraints Consider the example in Figure Suppose that has a population Figure Geometrically similar regions and with vastly different population density density of and has a population density of Suppose consists all the people of the first group and has people of Now opening facilities in each region is a better solution in the objective above compared to opening in and in because the connection cost term in the objective is normalized by the size of the group This is justified in order to avoid an unfair treatment for a smaller group However in a time-sensitive application such as polling having a lot of clients allocated to a facility can lead to over-crowding and thus a loss of utility One way to repair this is by adding a delay term to the utility This leads to a quadratic objective function which appears difficult to optimize Instead we propose enforcing load-balance using a capacity constraint for facilities the simplest of which is that for some each facility can serve at most clients in total across all the groups In practice these are not hard constraints and it is reasonable for an algorithm to violate it by a small factor Most of the standard clustering formulations including facility location have been studied in the presence of capacity constraints Constant factor approximations are known both using local search and LP relaxations In our setting with multiple groups we show that the LP methods can be adapted to obtain approximation algorithms Notation As before the set of clients is composed of groups and the set of valid locations for facilities is L They lie in a metric space whose distance function is denoted by will always denote the capacity bound A solution will consist of a subset L and an assignment function The quantity Opt will denote min max where the minimization is over such that for all We prove that it is possible to achieve a constant factor approximation as long as there is a constant any fixed constant slack in the capacity constraint Theorem Let be defined as above and let Opt denote the optimum objective value Then for every there is an efficient polynomial time algorithm that finds a solution with the following properties the objective value is Opt for every we have Remark It is an interesting open problem to study the case of hard capacity constraints Likewise we are assuming that the capacity bound is independent of the facility Modifying the rounding algorithm to allow different capacities at different locations is also an interesting direction We note that both can be achieved using LP methods for standard facility location eg but known rounding algorithms are not faithful to the best of our knowledge thereby making it tricky to apply to the setting with multiple groups Meanwhile we note that in our motivating applications both uniform capacities and soft constraints are reasonable assumptions Proof The proof follows along the lines of but we slightly adapt it to get a stronger bound on We start by solving a slight variant of the LP where we add the capacity constraint for all L In an integer solution the is zero if is not opened if is opened as intended Starting with the optimal fractional solution for this LP the rounding algorithm is then as follows Algorithm FFL-Rounding parameters Perform Filtering with parameter on to obtain Define L update for all Define while do Let be the client in with smallest Let be the set of facilities in with Let Open the cheapest facilities in call this set Update the set setting Update values setting for and for Move all fractional assignment from to Update the set using the definition in step Close all remaining fractionally open facilities Let If re-scale all by Use bipartite matching to round fractional assignment to an integral one denoted return The first step is filtering where we convert a feasible fractional solution to the LP to another fractional solution with the additional constraint that if ie client has a non-zero fractional assignment to facility then This fractional solution satisfies all the constraints of the LP except the capacity constraint and also satisfies up to a slack factor of on the The parameters used here will be chosen later In the process the opening cost term in the objective increases by a factor at most The next step is to consider all the facilities L and set and add all such facilities to the opened set Again this step only increases the opening cost term in the objective by a factor of At every point of time the algorithm maintains a set which is the set of clients such that In other words has a significant fractional assignment to unopened facilities As long as is non-empty the algorithm chooses with the smallest value of It then opens facilities in the vicinity Define for convenience Suppose is the set of fractionally open facilities in Because we have that which in turn implies that because of the LP constraint for all The algorithm opens centers from of the least cost We claim that in the process the sum increases by at most max This is easy to see by considering two cases if then the algorithm opens exactly one center and the cost increases by at most If the sum is then since all values were there must have been non-zero terms in the summation and we can argue that the cost increase is at most see As our choice of will be the term dominates We thus have that this step of the algorithm increases only the facility opening cost and by a factor at most The next step is the re-allocation of clients from to Let be one such client If we simply note that for all and thus all the fractional demand is still routed to a facility at distance at most away Likewise if then because demand is still routed to a facility at distance at most away Following this we are only left with clients most of whose demand at least fraction has already been routed to facilities in The scaling by as defined in the algorithm increases the objective by a factor at most The steps above together help obtain a solution in which is integral but the may be fractional Also the connection cost term in the objective is scaled by at most Moreover this bound holds for every point thus the rounding is faithful The facility opening cost is scaled by at most The capacity constraint is violated by a factor of The final step of the rounding is bipartite matching Here since the demands are all unit we can show that it is possible to convert the fractional assignment into an integral one by solving an instance of the transportation problem see Setting the result follows EXPERIMENTS In the first two parts of this section we evaluate the proposed fair k-median and fair facility location algorithms and provide an empirical assessment for their performance In the final part we compare the balance-based approach to fair clustering with respect to our representation-based notions Throughout the experiments we consider five datasets Synthetic Synthetic dataset with three features First feature is binary majority or minority and determines the group example belongs to Second and third attributes are generated using distribution N in the majority group and distribution N in minority group Majority and minority groups are of size and respectively Iris Data set consists of samples from each of three species of Iris Iris setosa Iris virginica and Iris versicolor Selected features are length and width of the petals Census Dataset is US Census and selected attributes are education-num capital-gain and hours-per-week groups of interests are female and male Bank The dataset contains records of a marketing campaign based on phone calls ran by a Portuguese banking institution Selected attributes are age balance duration and groups of interest are married and single North Carolina voters In this dataset we are interested in latitude and longitude values of each voters residence and use race attribute to identify different demographic groups We do not evaluate the capacitated version of fair facility location that we discuss in Section This algorithm is more complex and is beyond the scope of this work We should also note that throughout this section we compare the results of proposed fair algorithms to standard k-median which is implemented using the standard linear program formulation Fair k-median In this section we employ two algorithms to compute k-median clusterings which are group-representative We call these algorithms LP-Fair k-median and LS-Fair k-median LP-Fair k-median LP-Fair k-median first solves the FairLP linear program presented in Section Since it is not possible to compare the results of bi-criteria algorithm to standard k-median algorithms due to the varying number of centers we chose to obtain integral solutions via the faithful rounding procedure described in Corollary The rounding is based on the matching idea proposed by Charikar et al and is done in four phases Filtering Similar to the filtering technique described in section we construct a subset of the points with a small adjustment that after adding a point to the set all points from the original set such that will not be considered to be added to anymore Bundling For each point we create a bundle which is comprised of the centers that exclusively serve In the rounding procedure each bundle is treated as a single entity where at most one center from it will be opened The probability of opening a center from a bundle is the sum of which we call bundles volume Matching The generated bundles have the nice property that their volume lies within and So given any two bundles at least one center from them should be opened Therefore while there are at least two unmatched points in we match the corresponding bundles of the two closest unmatched points in Sampling Given the matching generated in the last phase we iterate over its members and consider the bundle volumes as probabilities to open centers in expectation The centers picked in the sampling phase are returned as the final centers LS-Fair k-median In this section we propose a heuristic local search algorithm in addition to LP-Fair k-median Arya et al proposed a local search algorithm to approximately solve the k-median problem Their algorithm starts with an arbitrary solution and repeatedly improves it by swapping a subset of the centers in the current solution with another set of centers not in it We modify this algorithm to minimize the maximum average cost over all groups Assuming were given a cost function as groups where LS-Fair k-median is presented in Algorithm Algorithm LS-Fair k-median cost an arbitrary set of centers from old cost new cost max cost cost while there is and st max cost cost old cost do old cost max cost cost return Unlike the LP-Fair K-Medians we do not provide any theoretical bounds on LS-Fair K-Median In fact the following example shows that LS-Fair K-Medians algorithm with the AbsError-fair objective can have local optima that are arbitrarily worse than the global optimum Description of the instance and two sets that are far apart think of the distance between any pair where and for some integer parameter Likewise suppose that of sizes respectively Suppose that all the elements of so also are at distance away from one another Suppose the distance between and so also and is Now suppose the two groups Let The optimal solution is to choose one point in and another in This results in an objective value of max Consider the solution that chooses the unique points from and The k-median objective for both the groups is and thus the AbsError-fair objective is Now consider swapping with some point This changes the k-median objective for group from to and so even though the swap significantly decreases the objective for the second group the local search algorithm will not perform the swap The same argument holds for swapping with a point It is thus easy to see that is a locally optimum solution However the ratio between the AbsError-fair objectives of this solution and the optimum is for Thus the gap can be as bad as the number of points Results In this experiment in order to save space we focus on the Census and Bank datasets However we consider two subsamples of each dataset Census contains female and male examples Census contains female and male examples Bank contains married and single examples and Bank contains married and single examples The results are summarized in table Group-optimal presents the optimal average cost for a group when it is clustered by itself via centers k-median presents a groups average cost in a clustering generated by the standard k-median algorithm performed on all groups together The other rows in the table show the percentage increase/decrease in costs for either of the described fair algorithms using the two cost functions In general the results demonstrate the effectiveness of our algorithms However we emphasize on the difference between and samples In the case the groups have the same size and standard k-median treats them roughly the same But in the case if the groups have different distributions standard k-median favors majority group over the other and the effectiveness of our proposed algorithms are more evident We should note that in all experiments points were clustered using centers Fair facility location In this section we empirically evaluate the algorithm presented in section We also use the faithful rounding procedure proposed in to obtain an integral solution We use the data for voters in the state of North Carolina specifically Brunswick county This dataset contains race and ethnicity of each voter as well as latitude and longitude values of their residence In this experiment we focus on black and white demographic groups which roughly constitute and voters respectively As for the facilities we assume each voters residence could be used as a drop-off location Therefore in all experiments we use regular k-means clustering to select locations out of the total data points as the set of facilities We also assume the setup cost for all facilities are equal The results of this experiment are presented in Figure for different values of the facility setup cost The results show that fair facility location algorithm lowers the average distance to polling locations for the worse off group namely black voters in comparison to standard Each dataset was sampled times and we reported the overall average Table Effects of enforcing balance on group representations Datasets Synthetic Iris Census Bank majority minority Setosa Versicolor female male married single Standard k-median Balanced k-median Table Clustering Bank and Census datasets using LS-Fair and LP-Fair algorithms Standard k-median and group optimal rows present the actual groups average costs AbsError and RelError rows are the percentage increase/decrease in group costs for proposed algorithms compared to the corresponding values in standard k-median and group optimal rows respectively Datasets Census Census Bank Bank female male female male married single married single Standard k-median AbsError LS-Fair LP-Fair Group optimal RelError LS-Fair LP-Fair Figure Average distance to polling location for black and white voters version Also as we increase the setup cost for facilities since fewer number of facilities will be opened the average distance grows larger for both groups We should also note that by opening fewer number of facilities it will be harder for the algorithm to find a fair solution as it is more restricted This is apparent from the larger discrepancies between the two groups in the fairer solution for higher values of setup cost On balance and representations In this section we empirically study the effects of enforcing balance on group representations More specifically we compare each groups average cost for standard k-median to the corresponding value under balance constraint As for the balance-fair k-median we chose to use the algorithm proposed by Backurs et al In this experiment we used the entire Synthetic and Iris datasets and sampled examples from each of Census male female and Bank married single datasets In table we present the average costs for all groups within each dataset in two clusterings generated by standard k-median and balanced k-median In all datasets we observe enforcing balance amplifies representation disparity across groups and leads to a higher maximum average cost However it is especially more noticeable in Synthetic and Iris datasets where different groups have vastly different distributions CONCLUSION In this work we presented a novel approach to think of and formulate fairness in clustering tasks based on group representativeness Our main contributions are introducing a fairness notion which parallels the development of fairness in classification setting proposing bi-criteria approximation algorithms for k-medians under different variations of this notion as well as approximation algorithms for facility location problem and providing theoretical bounds for both Our results suggest that our formulation provides better quality representations especially when the groups are skewed in size Implementation could be found here The algorithm proposed by Backurs et al works on only two groups We chose two groups out of three from Iris Repeating the experiment with other groups lead to similar results