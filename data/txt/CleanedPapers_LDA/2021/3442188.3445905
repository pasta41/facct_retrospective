Designing Accountable Systems Accountability is an often called for property of technical systems It is a requirement for algorithmic decision systems autonomous cyber-physical systems and for software systems in general As a concept accountability goes back to the early history of Liberalism and is suggested as a tool to limit the use of power This long history has also given us many often slightly differing definitions of accountability The problem that software developers now face is to understand what accountability means for their systems and how to reflect it in a systems design To enable the rigorous study of accountability in a system we need models that are suitable for capturing such a varied concept In this paper we present a method to express and compare different definitions of accountability using Structural Causal Models We show how these models can be used to evaluate a systems design and present a small use case based on an autonomous car CONCEPTS Software and its engineering Designing software Computer systems organization Architectures KEYWORDS Accountability Structural Causal Models Socio-Technical Systems INTRODUCTION Accountability differs from many other system properties because it is notoriously hard to define and its benefits are elusive to name A property like performance can often be measured with hard numbers and has the very clear benefit that more performance means more or faster operations Security and privacy are hard to measure but have the well understood meaning of keeping the bad guys out and keep data from unauthorized eyes In a recent systematic literature review Wieringa has written that many organizations are advocating for more algorithmic accountability yet a thorough and systematic definition of the term lacks and it has not been systematically embedded within the existing body of work on accountability p This finding now begs the question of what exactly these organizations are advocating if there is not even a definition of the concept This feeling of ambiguity is reinforced when looking at the concrete examples given in the appendix of The first is an automatic system that checks if people repay their debt the second one is a system that automatically anonymizes permits the third and fourth systems check for fraudulent social benefit claims All examples do something that some people cannot know or understand and might find objectionable Accountability is now supposed to fix this Similarly surveyed implementations of accountability in computer science and found that systems will often implement something and then just call it accountability without trying to ground that in any definition or understanding of the term In complex systems accountability is often deflected and hard to pinpoint which leads to blame being assigned to humans As an example a pilot error often is not just an error of the pilots but a complex interplay of the humans and the technical systems Currently the literature does not offer any method to model the accountability of a system especially across system boundaries It offers no way to quantify or qualify the accountability of a system nor even a precise language to reason about it or compare implementations The current state of the art does not go beyond giving differing definitions of what accountability might mean and proposing implementations of accountability in specific contexts In this paper we show how to leverage one commonality among all definitions of accountability namely causality to express accountability definitions and identify them in the causal model of a system This allows us to describe patterns in the design of a system that are necessary to fulfill a specific notion of accountability Moreover this knowledge helps us to reason about what data needs to be logged and conversely what data can be omitted without compromising the systems adherence to the chosen definition of accountability We propose to use Structural Causal Models SCMs as the mathematical foundation They are flexible enough to model even the most complex systems and offer a toolbox of mathematical methods to analyze them BACKGROUND Accountability is a concept rooted in Liberalism and was first introduced by political philosophers like John Locke and Adam Smith who used it in the th and th century to describe the fact that official representatives will have to justify their action to someone ultimately their sovereign This core idea was then picked up and refined by other political and later social scientists In a survey Lindberg gives the central idea as when decision-making power is See or for a more detailed history transferred from a principal eg the citizens to an agent eg government there must be a mechanism in place for holding the agent accountable for their decisions and tools for sanction p Bovens writes that the most concise description of accountability would be the obligation to explain and justify conduct while also warning that as a concept however accountability is rather elusive It has become a hurrah-word like learning responsibility or solidarity to which no one can object p This core idea is with some variations deeply embedded into the fabric of liberal democracies From the idea that voters will hold politicians accountable for their performance to companies that are accountable to their shareholders to the legal system where wrongdoing is discouraged by the possibility of being held accountable for ones actions As such accountability rose to prominence in computer science together with the tight integration of computers into our societies and their increasing effect on daily life for example by managing medical records or controlling vehicles This long history of the term has led to many different definitions and meanings of accountability Lindberg for example identified twelve different subtypes of accountability and has also cautioned us that it cannot be assumed that findings in the area of one subtype of accountability are relevant for another p For example if we find an implementation of accountability that works well in a societal setting it is not a given that it will also work in a legal setting have found a tendency in computer science to not worry much about the underlying definitions of accountability Even recent works for example usually pick some definitions and declare it as typical This is in our opinion wrong Computer science should not try to pick winners and push one theory over another When talking about accountability it is important to be precise about the exact meaning As Lindberg puts it everything is not accountability it is but one of many possible ways to constrain the mis-use of power p Other means of limiting power are the devolution of power violence economic pressure public shaming and anarchy p However since accountability is a very old concept it has multiple meanings that often have subtle differences This is why when talking about accountability or making systems accountable we should always first try to define what we actually mean For example in some definitions sanctioning an actor for their action is considered part of accountability while in others a principal can only sanction an agent if they do not provide an account Such differences have a huge impact on the underlying system design and should thus be made explicit and not left ambiguous by just using the term accountability In our view all definitions of accountability have some merit in a specific context and computer science should strive to offer ways to implement any definition It is the purview of fields like sociology or the political sciences to debate the intricacies of the definitions themselves They have accumulated experience in debating these finer points and computer science should rely on their insights and offer ways to realize the theories developed there Here we will now present some approaches to accountability as an example of their wide variety and show how to formalize them later in Section Causes and accountability are also an important topic in law Since causality has only been formalized very recently relevant literature can be found under the term statistics eg Lindberg Staffen Lindberg surveyed the literature in the social sciences and distilled the following definition of accountability An agent or institution who is to give an account A for agent An area responsibilities or domain subject to accountability D for domain An agent or institution to whom A is to give account P for principal The right of P to require A to inform and explain/justify decisions with regard to D and The right of P to sanction A if A fails to inform and/or explain/justify decisions with regard to D p The first two points mean that there is an agent that has some power in a certain domain and knows that they need to give an account for their actions The third and the fourth condition imply that there is a third party that has the right to require A to explain and justify their decisions The last condition requires that P can sanction A Lindberg adds an important restriction often lost in other definitions he distinguishes between the right of P to sanction A for not providing the information and the right to sanction A for the content of effect of an action Another important implication is that there needs to be standard or measurable expectations p to have accountability Without a clear idea of what is acceptable and unacceptable behavior it cannot be evaluated and sanctioned As such we always need some form of evidence Bovens Mark Bovens definition became popular recently in computer science because it was used as the definition in the systematic literature review conducted by Bovens finds that accountability is hardly defined and he tries to counteract this vagueness and make it more amendable to empirical analysis p He focuses on public accountability and gives a short definition the obligation to explain and justify conduct p before giving the more detailed one as follows There is a relationship between an actor and a forum in which the actor is obliged to explain and justify his conduct the forum can pose questions pass judgement and the actor may face consequences p Following his definition actors can be individuals or organizations and a forum can also be a specific person an organization or even the general public The relationship between actor and forum will often but not always be a principal-agent relation in which the forum delegates power to the agent who is then held to account The obligation of the actor might be formal or informal The act of giving an account consists of three stages First the actor is obliged to inform the forum about his conduct by providing various sorts of data about the performance of tasks about outcomes or about procedures p Second there needs to be a possibility for the forum to interrogate the actor and to question the adequacy of the information or the legitimacy of the conduct p Finally the forum may pass judgement on the conduct of the actor p Additionally he also requires the possibility of consequences for the actor if they do not comply with the requests of the forum One fundamental difference between the definitions of Lindberg and Bovens is that Bovens requires the actor to regularly update the forum whereas Lindberg suggest that the principal can demand an account from the agent at any time Hall Hall et al look at accountability from the perspective of psychology As such they focus on what it means for an individual to feel accountable Their exact field of study is felt accountability In their overview they find that at the core of accountability is the expectation that ones actions will be evaluated They emphasize that it is not necessary that this evaluation does occur but that the possibility that an evaluation occurs must be present Furthermore the actor needs to believe that an account-giving ie an explanation might be required This account is then given to a salient audience that might reward or sanction the agents behavior In their review of models of accountability they find that accounts are often used by agents to protect their self-image and develop their social identity This underscores the important role accountability has in a society and supports the assumption that complex societies and social order necessarily need accountability to augment the reduced level of personal trust between individuals In their survey they describe four essential features of accountability First the accountability source describes to whom one feels accountable Second the accountability focus captures how things get done and how they relate to the results Third accountability salience expresses how important the task is for which an agent might be held accountable the idea is that agents will be more careful if their action is more significant Fourth and finally accountability intensity captures for how many things an agent is accountable here it is thought that being responsible for multiple things increases stress RACI The organizational sciences have developed several practical frameworks to understand accountability in an organization Here we present the Responsible-Accountable-Consult-Inform RACI framework Such frameworks sometimes called tools are used in practical settings to explicate accountability relationships in teams and organizations While these tools do not build on a sophisticated body of scientific literature they are nonetheless important in practical settings because they allow people to express their understanding and perception of accountability in a given setting We assume that such practical approaches will often be the basis for accountability expectations of systems and thus need to be considered in any attempt to formalize accountability for them RACI specifically tries to explicate the roles of people in an organization and helps to reconcile the conception of a role ie what a person thinks they are doing with the expectation of a role ie what others think the person is doing and with the behavior of a role ie what the person actually does Following having a RACI matrix helps to align these three aspects However they also point out that this is an ongoing process that needs to constantly realign those three aspects whenever they drift apart They list a few typical signs such as Questions over who does what or Concern over who makes decisions p that arise regularly during the design of systems Among other things one major difference of this definition from the others is the aspect of consultation once again underlining our finding in the introduction that definitions are manifold and different define the following four aspects in the RACI framework Responsible The individual who completes a task Responsibility can be shared Accountable The person who answers for an action or decision There can be only one such person Consult Persons who are consulted prior to a decision Communication must be bidirectional Inform Persons who are informed after a decision or action is taken This is unidirectional communication Computer Science A landmark publication on accountability in computer science was published by Weitzner et al They provided a definition for Information Accountability as an improvement on classic preventive data control measures Classically systems ensure a users privacy by ensuring that data cannot be accessed by unauthorized personnel and thus prevent data leaks Weitzner et al changed this premise and drawing parallels to law enforcement suggested to build systems in such a way that it is easy to trace data leaks and then leverage the existing legal system to punish misbehavior This idea was later refined and formalized by Feigenbaum et al albeit with a focus on security and not privacy Coinciding with the discussion on e-voting systems KÃ¼sters et al formalized accountability in relation to verifiability Here the main question is how to design an e-voting system such that the results can be trusted and any attempts to falsify the vote count or the votes will be detected and the perpetrator held to account The Acloud project coinciding with the spread of cloud services into society and questions about data protection has done extensive work on accountability in cloud environments with a focus on data protection and privacy They offer a reference architecture tools to complete certifications and risk assessment Looking at their website they defer the exact definition of accountability to contracts or service level agreements Furthermore Kacianka et al conducted a systematic mapping study to understand how accountability is understood and implemented in research tools In this study they identified a steady rise in publications on the subject and found that most research was either a solution proposal or an evaluation of an approach Analyzing the prominent application domains they found that cloud computing was clearly dominant followed by distributed data sharing and web applications The most prominent use cases were privacy focused in line with and the most popular techniques were cryptographic and network protocols with some dedicated accountability protocols as well Obviously the focus on information accountability makes related definitions different from those discussed before The core problem however was discussed much earlier Notable contributions technical solutions are Lamports logical time stamps in while Nissenbaums discussed the eroding accountability in computerized societies in p Algorithmic Accountability The term algorithmic accountability first gained prominence with the paper by Nicholas Diakopoulos where he discussed how journalists might investigate algorithms that started to make more decisions that affected human lives In this vein the literature on the subject usually focused on the understanding of machine learning algorithms Examples include algorithms used in court decisions policing and similar settings where human lives are directly affected by opaque computer systems Most of the literature is highly critical of these systems using terms like Weapons of Math Destruction or Algorithms of Oppression The general approach to counter the power of algorithms is to make the decisions of algorithms transparent and explainable Recently Maranke Wieringa surveyed the literature on algorithmic accountability and found that what is denoted with algorithmic accountability is this kind of accountability relationship where the topic of explanation and/or justification is an algorithmic system p Her survey also finds that typically algorithmic accountability follows the definition of Mark Bovens given above In earlier works transparency was often seen as a solution but Ananny and Crawford have shown that transparency alone is not sufficient for accountability Amongst other reasons a main point is that we also need someone to understand the output of such a transparency mechanism To alleviate this problem Wachter et al and later Tim Miller as well as Mittelstadt et al have proposed using contrastive explanations to make decisions understandable for humans Miller gives the example of a machine learning classifier that categorized an insect as a spider or a beetle An explanation it would give is that a result is categorizes as a spider because it has eight legs instead of six In contrast to the weights in a neural net or the layout of a decision tree such an explanation would be useful and understandable for a human CAUSALITY The study of causality is of a specific interest to the study of accountability our main subject matter because causality is a prerequisite for accountability In understanding how causal effects work we can improve the design of our systems to make sure that effects of causes are clearly understood and then in turn ensure that the causes of effects are easy to identify The first notion is prospective and the second one retrospective Both are deeply intertwined but often only studied separately In this paper we combine the study of both and build on the assumption that a good prospective causal model is also a good retrospective causal model For prospective models we can find structures and patterns that allow us to show that some variables are not relevant to certain outcomes and once a specific outcome comes to pass we can use retrospective reasoning to identify the concrete cause relative to the given context We closely follow the definition of SCMs as introduced and refined over the years by Judea Pearl and Joseph Halpern Other definitions of causality exist and might sometimes even be more suitable but Pearls SCM approach is the most widely adopted notion Even more discusses several incompatible notions of actual accountability and there are arguments that a single definition is not useful See also the notes in the first chapter of Texting Accident a A correlation between texting and accidents Texting Accident b Our model assumes that texting is the cause Texting Distraction Accident c We discover the mechanism by which texting causes accidents Figure From correlations to causal models Historically causality and causal relationships are tightly connected to statistics However as laid out by whereas statistical relations are epistemic and describe what we know or believe about the world causal relationships are ontological meaning that they describe objective constraints on the world This means that causal relationships are much more stable and should not change if the environment changes Still causality is tightly linked to statistics as many causal statements are uncertain and are thus often only true with some probability As such many prospective causal models will answer questions with a certain probability For accountability we often need exact and retrospective answers The question Did Alice cause the crash should have a clear retrospective answer For this we use Actual Causality It allows us to use a prospective causal model and reason about it in a specific context For example we might have a prospective model that shows that texting while driving causes accidents with a certain probability Then we might have the specific context of an accident in which we know that Alice was distracted because she was texting We can then set the context for this accident and use actual causality reasoning to find the cause of the accident in this case Alice caused the accident by being distracted In the literature as well already suggested using actual causality as a building block for accountability and show how causality can be useful to model accountability Type Causality Investigations of causality usually start with the identification of a correlation between two variables In Figure a for example we notice a correlation between texting while driving and the number of accidents A correlation has no direction and many correlations will turn out to be spurious so caused by some unknown third variable often called a confounder The goal of scientific investigations is now to find which correlations are the result of genuine causal mechanisms in the real world In the end such an investigation will yield a causal model In our example Figure b expresses the understanding that texting while driving causes accidents Maybe just with a certain probability and under certain assumptions but we clearly state that one is the cause of the other This is the modelers understanding of a mechanism in the world The advantage of stating it as a causal model is that all assumptions must be made For example smoking causes cancer is true but a single cigarette is very unlikely to cause cancer explicit and that it can be tested against actual data It might also be refined over time Figure c shows a causal model that assumes that texting does not directly cause accidents but that it does so via a mediator namely distraction Such details are often very important as they improve our understanding of the problem and allow us to develop ways to affect and often prevent specific outcomes by targeting the mediators directly Such causal relations can be formalized in so-called Structural Causal Models SCMs They are are derived from structural equation models SEMs eg but their relations have a direction Following Pearl an SCM consists of two sets of variables U and a set of functions that assigns each variable in V a value based on the value of the other variables in the model Formally Definition Structural Causal Model A structural causal model M is a tuple M where U is a set of exogenous variables V is a set of endogenous variables associates with each variable X V a function that determines the value of X given the values of all other variables Every SCM is associated with a graphical causal model called the graphical model or the graph Nodes are the variables edges represent a causal relationship between them While the graph does not include the details of its structure alone is enough to identify patterns and causes In an SCM exogenous variables denoted are external to the model meaning that we chose not to explain how they are caused They are the root nodes of the causal graph and are not a descendant of any other variable Endogenous variables denoted are descendants of at least one exogenous variable and model components of our system and the world for which we want to explain causes describes the relationships between all those variables If we knew the value of every exogenous variable we could use to determine the value of every endogenous variable In a graphical model every node represents an endogenous variable and arrows represent functions from between those variables Actual Causality Type causal models make predictions on how events will unfold This is useful because we want to build systems in a way that they will probably be accountable To achieve this we try to make sure that the causal effects within a system are clearly understood and that the structure ensures that as many components as possible are causally independent However what if an unwanted event has already happened In this case we do not care about the probabilities of events we know they happened but we want to find out why exactly they did happen For this we need the concept of actual causality It is backward-looking and stands in contrast to type causality which is forward-looking To illustrate actual causality Figure depicts an accident between two cars Imagine two drivers Alice and Bob breaking the Causal models are also often specified over probability distributions For simplicity we stick to the discrete definition in the paper and the examples Note that actual causality can also deal with probabilities for retrospective events see Ch This is based on the classic Suzy-Billy rock throwing example Example law at an intersection Alice is texting and thus distracted while Bob accidentally runs over a red light This example is designed to show that the simple but-for test is not adequate to attribute causality Here intuitively both Alice and Bob are necessary for the accident to happen However had Alice not been texting Bob would still have run the red light and caused the accident So the accident would have happened no matter what Alice did suggesting her behavior is not a cause which goes against our understanding of causality The goal of the Halpern-Pearl definition see Appendix A for its formalization of causality is to find a precise mathematical definition to enable algorithmic reasoning over such examples so that the result conforms with our human understanding of causality Alice texting Bob runs red light Accident Figure Two cars causing an accident Causality and Accountability It is now important to note that SCMs can describe purely technical systems They do not require a principal or any human at all For a system to be accountable however we require a natural or legal person that is not just a cause for an effect but is accountable for that effect In other words causality is necessary for accountability but by itself it is not sufficient for accountability The additional requirements are given by accountability definitions such as the ones introduced in Section In our work with SCMs we found the following differentiation of terms useful A cause is the actual cause in an SCM as determined by the Halpern-Pearl definition of actual causality Similarly to cause means that an endogenous variable in an SCM is the actual cause of another endogenous variable Causes are purely technical without any notion of intent or other social attributions Responsibility is the commitment of an entity to act a certain way and the ability to affect or change an outcome This entity is responsible for a certain outcome As such we have explicit notions of normality that are used to determine responsibility This entity is then responsible for an outcome when had it acted normally the outcome would not have happened Blame is a social process in which an agent has a specific notion of normality and will find fault with some entity for the fact that this normality is violated This agent blames that entity for some outcome even if that entity is not aware of this notion of normality and thus might not be responsible for the outcome The but-for test is a simple understanding of causality that reads A is a cause of B if but for A B would not have happened It is often used in the legal context called by its Latin name sine qua non test In this domain several improvements were developed such as the INUS an Insufficient but Necessary element of an Unnecessary but Sufficient set or the NESS Necessary Element of a Sufficient Set test For a detailed overview see However it is important to note that causes are always relative to a causal model The causal model might be biased and thus social attributions can leak into the model When this commitment is derived from moral reasons the term duty will often be used A transparent system will have an SCM available and log enough data to set the context of the causal model after some event Transparency indicates that an SCM is available although it makes no statement about the quality of the SCM Accountability finally means that we have a natural or legal person called an agent that is responsible for some outcome This responsibility is made transparent with an SCM and thus allows a dedicated principal to ask this agent for his her or their account and blame this agent for an unwanted outcome So we can actively question the agent and understand him her or them This means that our attribution of blame is no longer purely subjective but derived from objective facts An accountable system is a socio-technical system in which the responsibility for every outcome is linked to an agent so a natural or legal person It has an accountability mechanism which is an extension of the system that helps the principal to keep the agent accountable Lastly an accountability definition describes the necessary structures in the SCM We do not claim that our definitions fit all situations and it is not our intention to obfuscate the long history of these terms Yet we believe that it is helpful to clearly state our understanding so that it can be compared discussed and contrasted to others ACCOUNTABILITY STRUCTURES IN CAUSAL MODELS With SCMs as the means to formalize causal models we can now revisit the accountability definitions given in Section and look at them through a causal lens Unfortunately there is no automatic deterministic way of translating them into SCMs Causal models express the modelers understanding of the subject matter and their advantage is that they are unambiguous Here we do not argue that our translations to SCMs are perfect Our point is that they are easy to understand precise in their meaning and thus enable a discussion and review of a given definition of accountability SCMs are useful because they may exhibit patterns such as chains see Appendix B and specific structures such as the Front and Backdoor Criterion see Appendix B that allow us to show that some nodes will have no causal influence on a specific event To leverage this first the models need to contain actions taken by humans and as our purpose is the design of accountable systems also actions taken by machines This requires us to express the accountability relation as a set of variables that causally influence each other On the level of accountability definitions we do not care about the exact nature of this influence so we do not need to specify The reason for this is that if there is a causal influence accountability might be necessary and our system should provide data to ensure it It is only after something unwanted has happened that we need to understand the cause and answer questions of accountability Conversely if we can show that a specific variable representing the actions of an agent cannot contribute to a specific outcome this agent cannot be accountable for that outcome Lindberg Bovens and Hall Lindbergs definition of accountability requires an agent A that should give an account for some effect E caused by A Translated to an SCM this means we need to have at least the relation A E in the model A is a representation of the action taken by the agent Agent Action Mediator Effect Principal Figure The causal model for Lindbergs definition and the result of that action E will depend on some value A takes To allow for the fact that effects are often not caused directly but indirectly via a mediator M we would make the possible use of a mediator explicit by adding the relation A M E An example for a mediator is a power steering wheel that amplifies and translates the movements of the driver A into the actual angle of the wheels E Next Lindberg requires a domain that is subject to accountability This conveniently is captured very precisely by the model itself It reflects the context in which A is embedded as well as the effect A might cause Finally the definition contains a principal P that transferred power to A and thus has the right to demand information from A and should A not comply sanction A Figure now depicts the structure of the Lindberg pattern P will be causally affected by A and also might be affected by M and E In Lindbergs view the principal is not directly involved in the course of events Moreover typical actions by a principal such as helping in the design of the system or investigating an accident are beyond the scope of the technical system and part of the society the system is embedded in To reflect this the models shows no arrows originating from P Any action taken by P goes beyond the limits of the technical system In other words it is necessary for a system to exhibit the pattern in Figure but it requires additional facilities in the social world around the system to be accountable One interesting property of this pattern is that it seems to be at the core of several other definitions Despite the differences in the details such as the timing Bovens see Section shares the same causal model Where Lindberg calls for an agent giving an account to a principal Bovens considers an actor that explains his conduct to a forum Both the principal and the forum might sanction or judge the agent or actor This similarity of definitions suggests to us that the causal models should also be similar One pronounced difference is that Bovens requires the actor to regularly inform the principal about changes whereas Lindberg sees the principal as asking for information In contrast to both at the core of Halls definition see Section is an agents expectation that their action will potentially be evaluated by a third party As such it also suggests the A P relation but with the added twist that A only needs to believe this relation to exist It does not matter if it exists in reality Here the technical system does not have to provide any logging or data so long as A does not know this Examples are systems that promise to randomly audit certain transactions such as tax agencies or anti-cheat tools in online games While in a concrete instance there might be no technical means to evaluate A the system will deter A from misbehaving by introducing the fear of an evaluation RACI RACI see Section in contrast does not so much look at the individual but at an organization as a whole Similar to Lindberg Responsible Agents Mediator Effect Accountable Agent Discussions Consulted Agents Informed Agent Figure The RACI accountability pattern a A possible model of an accountability definition b A system model with the equivalent pattern highlighted in green Figure The first goal is to identify a specific accountability pattern in a given system model above it features agents that cause some effect and thus also exhibits the familiar causal chain A M E It specifically extends the pattern with an accountable agent AA that instructs A to do a specific task To reflect that in the model we need to extend it with an edge AA A Furthermore RACI requires any consulted agents C to be reflected in the model We would model this by adding a dedicated node D to the SCM to capture the outcomes of these discussions Lastly RACI considers dedicated agents I that are informed of the effects In contrast to the others RACI requires no dedicated principal Figure shows the complete model Designing an Accountable System The question now is how can we use these accountability structures in the development of a system Here we assume that we have an accurate SCM of the socio-technical system available We will call this M connoting that it is a model of the system m can be used to answer questions of causality of the system This however is not the same as answering questions of accountability For a system to be accountable it needs to conform to a given definition of accountability which we call D connoting that it is a definition For example if our notion of accountability is the driver is always accountable for what the car does a causal model that does not contain that the driver violates this notion of accountability Figure a shows the causal model for a fictitious definition of accountability D and Figure b shows a fictitious system model M The question now is if this given model fulfills the given definition of accountability Unfortunately we cannot simply compare the graphs Causal structures are more complex and as in this Getting the SCM is not easy however parts of it can be automated by using models of a system such as fault and attack trees or models of human behavior a The causal model for this definition of accountability requires a mediator b Adding the required node to the system model Figure Here the problem is to change a system model to comply with a given definition of accountability example additional intermediate nodes do not necessarily affect the equivalence of models see Appendix B for details Figure depicts another problem How do we need to alter a given system to comply with a given model of accountability Here the fictitious accountability definition in Figure a requires a mediator between the cause and an effect a real-world example would be a person confirming an order Figure b shows the model with such a node added Looking at this another way if we can show that the sociotechnical system m contains a causal structure D we can use the accountability definition connected to D to make M accountable according to this definition Conversely if M does not contain D it cannot be accountable according to this specific definition In our experience the following steps make for a useful guideline to map accountability definitions onto causal models of systems Identify the event for which accountability is desired Identify valid agents Choose the desired definition of accountability For the given definition of accountability check if the pattern is fulfilled for the desired agents a If the pattern is fulfilled stop here b If not change the model to fulfill the desired pattern EXAMPLE We now use the deadly crash of an Uber car as an example for the design decision in a system Here we will look at three different design choices for the control of the system and reason about their accountability implications In this accident an autonomous vehicle developed by Uber crashed into a pedestrian Elaine Herzberg crossing a road and is regarded as the first accident in which a pedestrian was killed by an autonomous vehicle Ms Herzberg was pushing a bicycle while crossing a dimly lit road and the software of the car repeatedly misclassified her ultimately hitting and killing her The safety driver on board the vehicle was distracted and did not brake in time In the aftermath of the crash the accountability of the parties was hotly contested At first the police claimed it was the pedestrians fault because at the site of the accident crossing the road was illegal But it might be accountable according to another definition However we have not proven these steps to be the best approach They are merely distilled from our experience other possibly better approaches probably exist Next the cars safety driver was blamed because she did not pay attention to the road The manufacturer of the cars chassis Volvo was quick to distance itself from any blame arguing that its chassis had a collision avoidance system which would have prevented the crash but it was turned off by Uber to test their own software Velodyne the manufacturer of the cars LiDAR also pointed out that their system was capable of detecting a pedestrian but that their system does not take the decision to brake The search for reasons went as far as criticizing Ubers development process the testing process of having only one driver in the car and even the car-friendly and pedestrian-hostile layout of the road in Arizona or the point of autonomous cars in general At the time of writing the safety driver is being indited with negligent homicide All these claims have in common that they ask counterfactual questions of a causal model Our goal is now to structure the causal model in such a way that accountability can clearly be attributed We focus on a simplified model that consists of three agents namely Uber who built the car Volvo who contributed the chassis and the safety driver who was supervising the car We show how different SCMs give us different possible agents and how certain structures of an SCM allow us to show that certain agents cannot be accountable for a given outcome Models of the System Once we have the SCMM of the socio-technical system we can then use it to evaluate it for its accountability To illustrate this we look at ways to design an autonomous car see Figure This example illustrates three ways that the control of such a system can be structured In Figure a the human can take over at any point in time Figure b depicts a scenario where any input by the human can be overridden by the machine and Figure c shows a setup where the human cannot influence the car at all In causal models the lack of arrows between two variables expresses the strong assumption that there is no causal connection between these two variables In these Figures we used rounded boxes for possible agents ie natural and legal persons and rectangular boxes for technical components Here we do not model any preemption Temporal ordering and the fact that one event might preempt another has a huge influence on the model For simplicity we assume this causal model to be binary The meaning of the variables is as follows collide w/ Pedestrian P is true if a collision with a pedestrian occurs and false otherwise Trajectory set T is true if an evasive maneuver is conducted and false otherwise Safety Driver D is true if the driver tries to change T and otherwise false Uber Software S is true if the cars software tries to change T and otherwise false Emergency Brake E is true if the chassis tries to change T and otherwise false Volvo V is true if E is enabled and otherwise false Uber U is true if Uber influences S or E This example is of course highly simplified In the real world most causal models will not be binary Uber Software collide w/ pedestrian trajectory set Emergency Brake Safety Driver Uber Volvo a The human can take over Uber Software collide w/ pedestrian trajectory set Emergency Brake Safety Driver Uber Volvo b Human influence is moderated by the machine Uber Software collide w/ pedestrian trajectory set Emergency Brake Safety Driver Uber Volvo c No human influence is possible Figure Three possible designs for a semi-autonomous car While the SCMs show social entities the system is not accountable as-is Here it is important to note that we have a very lax approach to levels of abstraction Uber and Volvo are companies with unfathomable complexities the safety driver is a single individual the software and the emergency brake are complex technical systems and the trajectory the outcome of multiple decisions However SCMs can be abstracted quite well and so this mixing of layers is easy to do formally Still it is important to bear in mind that these variables will in reality be complex SCMs in their own right Formally our model M looks like this U UV are the three exogenous variables V P T D U are the seven endogenous variables v U V Rv true since we assume a binary model the range is true for all variables Now we need to define so the structural equations for every endogenous variable U V UV D meaning that U V and D are set by some exogenous variables This might seem redundant but the point is that only endogenous variables can be identified as causes in a causal model In real models an endogenous variable will likely be influenced by several exogenous variables For example the safety driver might be influenced by blood alcohol level other influences are the weather or the road conditions E U U true V U so long as Uber is not disabling E it is the value of V S U for Figure a and Figure c says that the software will follow whatever Uber had in mind It is trickier for Figure b Here we need to decide or D can override the other and in what way The simplest model would be a model in which the car will break if either Uber or the safety driver wants to break In this case S U D would be the correct equation If one could preempt the other we would need to change the model to contain such a preemption relation For Figure a T S E D and for the others T S E if neither S nor E nor as in Figure a D influence T the car will hit the pedestrian P T if the car is on a collision course with the pedestrian it will always hit her if the trajectory is not changed Checking for Accountability We now have three distinct versions These models describe the causal relations but this alone is not sufficient for accountability Our goal now is to give a justification of why these options are accountable and decide between the three First we need to decide which events we are concerned about In this example we only care about potential collisions with pedestrians This means that we only care about causes that affect P Next we need to identify valid agents Since accountability only has a meaning for natural or legal persons we can exclude any technical components leaving us with three potential agents Uber U Volvo V and the safety driver D As the third step we need to decide on a notion of accountability D Here we have two possibilities Either D is prescribed by some law or standard or we want to find a D that is suitable for our socio-technical system If we look at the pattern given by the RACI definition it is easy to see that M cannot be accountable in that sense It is enough to look at the structure to see that M simply does not have the necessary endogenous variables to fulfill the RACI requirements This is not surprising as RACI is aimed at organizations and a discussion is not something that translates well to real-time systems like cars If we were under obligation to make that system RACI-accountable we can so show that this is impossible given the current M We would need to look for ways to extend M with the required nodes Making this system accountable according to Halls definition would requires us to convince the three agents that their behavior will potentially be evaluated and that any misbehavior will be punished In extremis this would mean that if we are convincing enough we do not need to add any technical means of accountability to our system In practice we would need to find a trade-off between the cost of supervision and the compliance of the agents For example it might make sense to collect all vehicle data to create a plausible scenario of evaluation To give an example most people respect speed limits despite the fact that speed traps are quite rare In many technical systems the notions of Lindberg and Bovens will appeal to the developers In contrast to RACI they do not require many additional nodes and in contrast to Hall they both define a clear relationship between an agent or actor and a principal or forum To apply their notion we first need to identify the basic structure they prescribe Mediator Effect In Figure a we can find the following causal chains leading to the crash u S T P U E T P V E T P and D T P Here the structure of the chain including the safety driver is an exact match with this structure For Uber and Volvo we have two steps in the mediator and would need to show that they can be treated as a single node For Figure a and Figure c the argument is straightforward because S U so these two nodes could be joined into one In Figure b we need to clarify if U or D has the final say over the matter As the model is given here it would not be Lindberg accountable because and D would be accountable for T Similarly Volvo only has an influence on the trajectory if Uber is allowing them to do so So they fulfill this pattern If we now take another look at T we can see that its equation is as follows T S E D Again we have the problem that we cannot disentangle the effects from U E and D However given will always disable V because E U true we can at least rule as an eligible agent In Figure a and Figure b it is unclear if the safety driver or Uber are accountable because both have a causal effect This agrees with the investigations in the aftermath of the accident as there it was also unclear at first who was to be held accountable for the deadly crash Figure c causality is much clearer because D has no causal influence on T at all Lastly we will notice that none of the models has a principal So we need to determine whoA is accountable to In the real world this role will be filled with the authorities so we would need to make sure that they have all the evidence they need to understand the actions So far just judging from the SCMs we could either pick Lindbergs or Bovens definition Here we would pick Lindbergs because is has the notion that P might inspect the behavior of A on demand Bovens suggests regular reports which seem unnecessary for a car because accidents are rare Bovens would be more suitable if we were for example checking for violations of the speed limit Given the three possible designs for the system which would be the easiest to make Lindberg accountable Figure c where D has no causal influence and V is inhibited by U is attractive because only U is left as an agent There would be no confusion about accountability However Figure a or Figure b might be attractive in practice because keeping the human-in-the-loop allows the technical system to make wrong decisions without clear accountability of the manufacturer Leveraging the Structure Specific structures in causal models allow us to prove that one variable cannot affect another in a specific way even if there is a path between those two variables Two such structures are the Front and the Back-Door Criterion see Appendix B Knowing that a specific variable has no causal influence on another is invaluable for the design of a system because this means that we do not need to measure or log it This is helpful from an engineering perspective because it allows us to not store specific data and thus Note that in general showing that two causal models are equivalent is not trivial and cannot be treated as a graph problem see Appendix B save the cost for storage and the development of the data logging functionality It is also often desirable from a privacy perspective because it allows to justify not storing specific sensitive data So if we can for example show that skin color or religion have no causal influence we can justify not logging them without compromising a systems accountability To return to our example in Figure c we might just be interested in the effect of S The question now is what other values do we need to control for to calculate the effect of S on T Employing the Back-Door Criterion we can see that we do have an open backdoor path namely S U E T that will confound our estimate of the effect of S on T To de-confound this reading we could either control E or both of them This now allows us to justify not logging one of these variables provided we are only interested in the effect of S on T Actual Causality So far we used our models in a type causal manner that is we looked at the future ensuring that the accountability for a specific event was clear and easy to attribute How would we now use this model after an accident has actually happened For this we can employ actual causality reasoning see Section First we need to ensure that we can set the context correctly So in our real-world system we need sensors and logs that can tell us what has actually happened Here we can leverage the fact that a causal model is determined by its exogenous variables Looking at Figure we have three exogenous variables for Uber UV for Volvo and for the safety driver All can be either true or If we measure and assume the model in Figure a will a crash occur E S U T S E D P T true Here we can read this as Neither Uber nor the driver nor Volvo tried to change the trajectory therefore the car crashed into the pedestrian However this sentence already includes the counterfactual assumption Had either Uber the driver or Volvo done something else the crash would not have happened We can now formally check if this is correct Since we assumed the model to be binary there is only one other thing the agents could have done namely whatever makes their measurement true So we can change the value in the model and see if the result changes E S U true T S E D true true true P T Here binary models are not the best examples It is easier to think of S and E contributing different amounts to a real valued function For ways to automatically check these models see In this example we ignore any uncertainty The reasoning works similarly but the results would be probability distributions This simplicity makes binary models so popular for textbook examples However it is obvious that in the real world defining something else will be tricky Note that we just change the endogenous variable not the exogenous variable This setting can be read as Uber changed the trajectory and therefore the car did not crash into the pedestrian So we can say that because there is a counterfactual world in which U could have prevented the crash u is a cause for the crash to happen If we now look at the causal model for Figure c we see that T S E so D has no influence or any other variable in the model If D we would get the same result as above However if in the SCM above we were to set D true T would be true and the crash would be prevented What would now happen in Figure c E S U T S E P T true Despite the fact that we set D true so the driver tried to prevent the accident in the counterfactual world the accident still happens This means that D is not a possible cause for the accident and since causality is a requirement for accountability can also not be held accountable for the crash CONCLUSION Accountability is embedded deep into the fabric of society Algorithmic systems need to be designed in a way that conforms with these societal expectations This means that such important design decisions cannot be hidden deep within the system They need to be made explicit communicated and discussed SCMs are uniquely suitable for that because they allow us to formalize causality the necessary core of all definitions of accountability In the current literature SCMs are mainly used in scientific studies The models there are small and communicate assumptions about mechanisms in a study Developing SCMs for socio-technical systems is despite some early work still a hard problem Similarly no clear-cut ways of identifying principals or express definitions of accountability as SCMs exists Despite these open problems we are convinced that SCMs offer the clarity that is a requirement to make meaningful design decisions While SCMs are not sufficient to ensure accountability a correct understanding of the underlying causal mechanisms is necessary for any notion of accountability Expressing this as an SCM allows us to realize that we need certain structures in systems to enable accountability Without these structures a system cannot be accountable Once we have identified a specific structure we can utilize existing definitions of accountability and reuse the knowledge that comes with them we do not need to invent our own notions of accountability SCMs are a powerful tool to analyze systems and if they are not accountable provide a well-reasoned argument why this is the case and how the system should be improved