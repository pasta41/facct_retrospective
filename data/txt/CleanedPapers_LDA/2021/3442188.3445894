Leave-one-out Unfairness We introduce leave-one-out unfairness which characterizes how likely a models prediction for an individual will change due to the inclusion or removal of a single other person in the models training data Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary they should not be based on the chance event of any one persons inclusion in the training data Leave-one-out unfairness is closely related to algorithmic stability but it focuses on the consistency of an individual points prediction outcome over unit changes to the training data rather than the error of the model in aggregate Beyond formalizing leave-one-out unfairness we characterize the extent to which deep models behave leave-one-out unfairly on real data including in cases where the generalization error is small Further we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness which sheds light on the relationships between robustness memorization individual fairness and leave-one-out fairness in deep models Finally we discuss salient practical applications that may be negatively affected by leave-one-out unfairness INTRODUCTION Deep networks are becoming the go-to choice for challenging classification tasks due to their remarkable performance on many high-profile problems they are used everywhere from recommendation systems to medical research and increasingly in even more sensitive contexts such as hiring loan decisions and criminal justice Their continued rise in adoption has led to growing concerns about the tendency of these models to discriminate against certain individuals or otherwise produce outcomes that are seen as unfair There are several definitions that aim to formalize fair behavior in machine learning contexts group-based notions such as demographic parity and equalized odds stipulate that different demographic groups should be treated similarly in aggregate on the other hand individualized notions focus on how each person is treated such as individual fairness which requires similar outcomes for similar people and counterfactual fairness which argues that people should be treated the same as their hypothetical counterpart who takes a different protected attribute Fundamentally these fairness criteria depend on a comparison of how one group or individual is treated versus another However there are also situations where the decision-making mechanism is unfair not because of how its behavior varies across defined groups or individuals but rather because its decisions cannot be justified by consistent intelligible criteria In other words decisions may be unfair because they are arbitrary In this paper we study the extent to which instability can lead to such fairness issues Intuitively when a persons outcome hinges on the presence of another single individual in the training data the outcome that follows may be viewed as unfair Take for example a person in reasonable financial health who applies for an auto loan Suppose that whether their application is approved or not depends on whether another unrelated person had applied for a loan from the same bank and was subsequently included in the training data Such a decision may be viewed as unfair as it depends on the willingness and availability of another person to provide their data for training a chance occurrence rather than a well-justified set of criteria Even beyond its potential unfairness this behavior may be especially undesirable in applications which come with a right to explanation Measuring leave-one-out Unfairness To formalize this intuition we introduce leave-one-out unfairness LUF the chance that an individuals outcome will change due to the presence of any one instance in the training data Section Definition To the best of our knowledge this is the first attempt to formalize unfairness as stemming from the arbitrary nature of decision rules and in particular the stability of the underlying learning algorithm Certainly there are other random choices made during model development that may lead to an arbitrary change in model outcome for an individual changes in the random initialization or architecture for example which we explore in Section However we focus on instability with respect to training data in particular due to its connections to other areas of machine learning literature such as stability privacy and robustness We find that in many cases the use of deep models can lead to this type of unfair outcome with surprising frequency and can result in different outcomes for seemingly unrelated individuals To gain an intuition for why this might be Figure depicts the decision boundaries of two low-dimensional binary classifiers whose training data differs only on the presence of the point highlighted in red Notice that the boundary near the left-out point remains fairly consistent but there are non-trivial differences in both the boundary locations and the confidence of the models predictions in regions away from the point While this low-dimensional example provides some intuition we systematically characterize the extent to which deep models behave as such on real data Section We find that it occurs often enough to be a concern in some settings ie up to of data is affected that it occurs even on points for which the model assigns high confidence and is not consistently influenced by dataset size test accuracy or generalization error Figure Table Figure Classification boundaries of a deep model with three hidden layers trained on two-dimensional data with uniform-random binary labels before left and after right the point highlighted in red is removed from the training data Lighter regions correspond to predictions with less confidence While the model remains largely unchanged in the area around the left-out point its boundary changes significantly in other far-away areas For example the middle-right region assigns greater confidence to white points even flipping its prediction on one such point Connections Leave-one-out unfairness has useful connections to other fields such as stability privacy and robustness We show that while LUF is strictly stronger than some prior notions of leave-one-out stability Section Proposition it is weaker than differential privacy Proposition Thus one can achieve bounded levels of leave-one-out unfairness by satisfying differential privacy but it may also be possible to do so via relaxations that allow greater flexibility in the selection of learning rules Recent work has related robust classification to desirable properties beyond mitigating adversarial examples such as the encoding of more human-interpretable features and individual fairness on weighted metrics These results may seem to suggest that robust models would also be less susceptible to leave-one-out unfairness Evaluating two common techniques for producing robust models adversarial training and randomized smoothing we find that these methods in fact have vastly different effects on leave-one-out unfairness Whereas randomized smoothing tends to have no effect adversarial training amplifies the problem resulting in up to a factor of five more affected points Section These results suggest that although LUF and robustness are not inherently tied to each other certain types of models may prove beneficial for both Summary In a similar vein to the oft-cited lack of interpretability leave-one-out unfairness complicates the responsible application of deep models to sensitive decisions Particularly in settings where a right to explanation is pertinent these complications may need to be weighed against the benefits that deep models provide over less complex alternatives This paper presents the first steps towards a better understanding of this issue and points to several intriguing directions for future study To summarize we present the following contributions We introduce and formalize leave-one-out unfairness which characterizes a possible source of unfair arbitrary outcomes in ML applications We relate leave-one-out unfairness to well-known prior notions of stability shedding light on when models may suffer Figure From left to right Individual removed from the dataset z When is included in the training set the two individuals to the right are labeled as a match with confidence When is not in the dataset and predicted as not a match with confidence from leave-one-out unfairness and techniques that might help to mitigate it Finally we present an extensive evaluation of how prevalent LUF is when deep neural networks are trained on a variety of datasets and compare it to other sources of instability such as random initialization and choice of architecture In Section we provide two examples of machine learning applications where leave-one-out unfairness may lead to unjust model behavior along with experimental results demonstrating that LUF indeed may occur in these contexts Following this in Section we formally define leave-one-out unfairness and explore its relationships to LOO-stability and differential privacy In Section and Section we present our experimental results of the extent of leave-one-out unfairness on real datasets for conventional and robustly trained machine learning models CONTEXTUALIZING LEAVE-ONE-OUT UNFAIRNESS Leave-one-out unfairness may not pose a problem in all machine learning applications If the models outcome is of little consequence to peoples lives or if the application context does not require consistency across data samples for adequate justification then arbitrary predictions may be acceptable Determining whether or not leave-one-out unfairness leads to fairness issues requires considering this context In this section we motivate examples of how leave-one-out unfairness constitutes a fairness issue in two contexts facial recognition use by law enforcement and loan application decision models used by financial institutions Facial Recognition Facial Recognition Technology FRT has proliferated in recent years as a method of verifying identity at scale Its use in law enforcement and the potential harms that may follow have gained particular attention due to the potentially dire consequences of misidentification matches for facial recognition matches have been used as evidence for arrest Moreover the use of this technology in this context is becoming prevalent according to a study from at least one in four police agencies in the United States have made use of it Background The use of FRT by law enforcement relies primarily on face-matching models where two face images are provided as input to determine whether they depict the same individual Note that this differs from face classification models which aim to identify the person depicted in a face image from a pre-determined set of individuals A typical workflow proceeds as follows given an image of a suspect law enforcement queries a face-matching model against a large set of images in a database which also contains identifying information The face-matching model provides a binary label with a confidence score and the most confident matches are provided to the operator for further review Many police agencies use ready-made third-party models For example one such third-party ClearviewAI reportedly contracts with approximately law enforcement agencies Such third-party models are often trained on images obtained from public sources like the Internet in particular by taking advantage of Creative-Commons licenses widely used on social media websites The database of images on which these models are run during inference are often obtained from public records such as drivers license databases Notably these databases may largely consist of individuals with no prior criminal record Impact of Instability The results FRT are increasingly being used by law enforcement as evidence to justify arrest According to US law an individual must be arrested for a justifiable reason ie probable cause a police officer must have evidence leading them to believe that the person arrested likely did commit the crime in question Thus when FRT results are cited when justifying probable cause the factors that lead a particular face-matching model to its predictions must be scrutinized In particular if it is likely that a matching outcome can change due to the inclusion of a particular image unrelated to the suspect or the potential matchout of tens of thousands in the models training set then it may be argued the evidence used to justify the eventual arrest is based on a chance occurrence rather than on convincing facts relevant to the case In short such an outcome would be unfair due to the arbitrary nature of the supporting evidence We aim to formalize this behavior and investigate its prevalence on models trained on real datasets including face-matching models Experimental Confirmation We trained a face-matching model on Labeled Faces in the Wild LFW consisting of unconstrained pictures of different individuals To measure the effect of individual images on prediction outcomes we trained models both with and without a randomly sampled individual controlling for all sources of non-determinism eg parameter initialization and GPU operations We repeated this experiment for different randomly sampled individuals and measured the effects on prediction behavior Further details of our methodology are given in Section We found that the predictions given by the face-matching model change across datasets with single-image differences with surprising frequently One such example of this behavior is shown in Figure When person is included in the dataset persons are labeled as a match but when person is removed they are not Persons are clearly different from one another and aside from gender share few salient characteristics More surprisingly both predictions are made with high confidence and far from a baseline random guess Such behavior was not limited to these images but rather we observed that of the models predictions changed across datasets differing in one image while the change in accuracy remained less than Moreover this behavior was consistent across changes in random initialization and choice of architectures including a residual network resembling ResNet age education occupation sex capital gain model Affected point Bachelors Self-employed LOO point th Grade Service Industry M Table Selected feature values for a point treated leave-one-out unfairly in a deep model on the Adult dataset and the point whose removal resulted in the change in prediction Confidence refers to the raw output of the models prediction in the model with Consumer Finance Machine learning is also finding uses in consumer finance Not surprisingly the predictions made by these models too can greatly impact peoples lives potentially playing a decisive role in their ability to obtain buy a car a house or start a business Impact of Instability Models used in this context may be expected to have consistent justifiable reasons for the predictions that they make A salient example is credit models used to inform lending decisions where in Europe the General Data Protection Regulation GDPR requires that creditors using automated decision systems release meaningful information about the logic involved to applicants Similar regulations are relevant in the US through the Federal Deposit Insurance Corporation FDIC consumer protection law which provides a right to explanation in lending decisions Some interpretations argue that the right to explanation provided by the GDPR requires that it should be possible to trace a decision back to pertinent details of an individuals loan application and further that the information about the logic must be meaningful to the applicant notably a human and presumably without particular technical expertise This suggests that if the explanation is not legible to the applicant based on prevailing norms eg if it seems to be made based on incomprehensible or arbitrary facts such as the incidental makeup of the models training data then such a decision infringes upon their rights and freedoms After receiving an explanation the GDPR provides the applicant the right to contest such a decision and request human review Experimental Confirmation As with the face-matching model in the previous subsection we conducted experiments on models trained to predict a proxy for creditworthiness using datasets differing in a single instance We used the UCI Adult dataset consisting of a subset of US census data and trained one-hidden-layer neural networks with internal units to predict income from demographic education and employment information details in Section Our results suggest that the predictions of these models are often sensitive to the presence of single instances indicating the potential for leave-one-out unfairness Looking more closely at the results one of these models was trained with the point shown in Table included in the training set a -year-old man with an th-grade education who works in the service industry This model predicts that a -year-old college-educated self-employed woman makes more than confidence whereas a model trained on the same data without made the opposite prediction Mirroring our findings with the FRT models there is no apparent connection between the features that represent these individuals see Table and the models predict the womans outcome with high confidence The removal of this one individual does not just affect this -year-old woman but rather we find that approximately of the entire data set predictions are changed LEAVE-ONE-OUT UNFAIRNESS In this section we introduce the definition of leave-one-out unfairness and discuss its connections to prior notions of stability leave-one-out stability differential privacy and individual fairness We prove that leave-one-out unfairness is a stronger notion than leave-one-out stability and weaker than differential privacy Our formalization of LUF allows us to measure its prevalence objectively on real data and our investigation of its connections to other forms of stability suggest mitigation techniques as well potential middle ground for achieving gains in privacy Notation and Preliminaries We assume a typical supervised learning setting Let be a data point where represents a set of features a response Points are drawn from a distribution D as are datasets from the iid product ie We assume that learning rules are randomized mappings from datasets to models which are functions mapping features to responses in other words is the model obtained by learning with on data We to refer to the uniform distribution over the integers Given sampled and index we denote the sample with the element removed as Leave-one-out Unfairness Leave-one-out unfairness is based on the notion that a models treatment of an individual should not depend too heavily on the inclusion of any other single training point This is related to the concept of algorithmic stability which measures the effect that a small change in input has on an algorithms output For example a machine learning algorithm is stable if a small change to its input training set causes limited change in its output a trained model Usually the change in output is measured in the form of model error Definition formalizes this as leave-one-out LOO stability but we note that there are several variants that quantify over pointwise replacement instead of leave-out and use different types of aggregation in their bound Definition Leave-one-out LOO Stability Let stable be a monotonically-decreasing function Given a training set and a training set with a learning rule is leave-one-out-stable or LOO-stable on loss function with rate stable if E stable LOO-stability records the average effect of removing an individual from the training set on the absolute loss on that individuals prediction Quantifying the effect model of instability on the fairness of predicted outcomes however calls for a definition focusing on different aspects of model behavior LOO-stability is a predicate on a learning rule that can be satisfied in order to achieve an acceptable level of model stability in expectation over all draws of a training set However in this paper we are interested in quantifying the extent of arbitrariness in a particular individuals prediction to capture this we need a metric of unfairness rather than a fairness guarantee Pursuant of capturing an particular individuals real-life experience with a particular model we are interested in a quantifying arbitrary behavior in relation to a particular model context ie on a fixed training set To focus the effect of instability on the experience of the population on which it is deployed rather than a measure of model performance we need a metric which accounts for the instability that arises for any person from the inclusion of a given point in the training set rather than the impact that the changed point has on the error its own prediction Even with this focus on the experience of the individuals an aggregate calculation such as in LOO-stability may hide the experiences of an unlucky few who may encounter particularly high arbitrariness in their outcome To ensure that model behavior on every individual is considered a worst-case metric is more suitable Further appealing to the intuition that a model acts unfairly if it is arbitrary the consistency of its prediction rather than its loss is the target consistent predictions even when incorrect suggest that the models decision is not arbitrary Definition below reflects these considerations Definition Leave-one-out Unfairness LUF be the distribution from which the training set is drawn and let be in the support We define the leave-one-out unfairness LUF experienced by under learning rule and training set to be luf max The randomness in this expression is over the choices made by Note that in cases of a deterministic learning rule is or In other words given a learning rule and a training set the LUF experienced by a person is the worst-case probability receives a different prediction in a model trained on and one trained on with a single point removed Intuitively this is one way of quantifying the arbitrariness of the models decision at If LUF is high then the models decision is brittle under small potentially irrelevant changes ie a one-point change in the models training set casting doubt on the reason behind the models decision In certain situations such as when evaluating various models during development it may be useful to understand the extent of leave-one-out unfairness across the entire population under a given learning rule ie understanding how likely it is any individual in the distribution will experience an arbitrary decision This motivates the concept of expected leave-one-out unfairness defined below As most of our experiments aim to measure the frequency and severity of arbitrary behavior across real datasets we will focus most heavily on this definition throughout the paper Definition Expected Leave-one-out Unfairness be the distribution from which the training set is drawn and let be drawn randomly from We define the expected leave-one-out unfairness LUF experienced by under learning rule and training set to be luf max Where the randomness in the expectation is taken over samples of x from D Connections to Existing Stability Notions While our introduction of Definition above is clearly motivated by LOO stability in this section we explore the connections to this and other forms of stability in greater depth Specifically we demonstrate that while learning rules that are already known to be leave-one-out-stable may still be susceptible to leave-one-out unfairness strategies for ensuring stronger notions of stability such differential privacy can be used to mitigate luf we also explore the connection between LUF and other individual-based fairness notions ie individual fairness LOO Stability Leave-one-out stability is a coarser notion than leave-one-out unfairness as it records the average change in a models error on a given point when that same point is removed from the training set Meanwhile LUF focuses on how a certain points model outcome can change as a result of any point in the training set being removed A LOO-stable model may still treat points leave-one-out unfairly a model can exhibit similar error on a given point before and after that point is removed from the training set but it may treat other points differently We demonstrate this point on the simple learning rule and distribution in Figure Additionally the fact LOO-stability is averaged over the entire training set can obscure the fact that some individual points are strongly affected by a small change in the training set Proposition formalizes this showing that LOO-stability is strictly weaker than LUF Proposition be a learning rule be loss and be a montonically-decreasing function such that is leave-one-out stable with rate for all Then there exists a training set such that LUF stable and Proof Consider a binary classification problem a discrete distribution with three points as pictured in Figure are of class and is of class shown in red and blue We define a learning rule according to the different classifiers learned with each possible training set shown in Figure Notice that this learning rule is LOO-stable with stable as when each point is removed the classification error on that point remains the same this is shown by construction in Figure when and in all other cases the learning rule is constant as shown in the figure Thus However notice that eg is removed experiences a change in classification outcome Thus luf See that in fact that every point is susceptible to a change in prediction as the result of different point being removed from the dataset thus luf Proposition shows that models with bounded LUF are also LOO-stable the proof is given in the supplementary material Proposition be a learning rule be loss and be a montonically-decreasing function such that luf for all and is leave-one-out stable with rate Differential Privacy Privacy and fairness are related in various ways as others have illustrated before Like differential privacy leave-one-out unfairness is a stability property of learning rules but differential privacy is stronger In particular differential privacy Definition quantifies universally over all pairs of related training data and limits the probability of any change in outcome On the other hand Definitions and fix a training set and require stability of the models response on points from the target distribution Figure Left A learning rule that satisfies LOO-stability but not expected LUF over the distribution of the three points pictured In each box we see the decision boundary learned with a specified training set thus fully defining The proof is explained in Proposition Right Visual intuition for how a model can have but not satisfy differential privacy Consider a KNN model on a binary classification problem over the distribution pictured above two perfectly separated uniform distributions over circles The diameter of each circle is and the distance between the centers of the two circles is Consider any training set drawn from this distribution that has at least two data points from each class See that luf for all removing any point from cannot change the classification of any point in the distribution ie within the circles pictured above However KNN is not differentially private as it is a deterministic non-constant learning rule Specifically see that adding or removing a point in S can shift the boundary sufficiently far to change the models behavior on points not such as point pictured which is a violation of differential privacy Definition -Differential privacy An algorithm X Y satisfies ùúñùõø-differential privacy for and if for all that differ in a single row and all Y Differential privacy is stronger than leave-one-out-unfairness as any change to the model even if it does not actually affect prediction of any point in the distribution can potentially leak information and is therefore a violation of differential privacy This makes sense in the context of privacy as it concerns an adversarial setting where an attacker is free to interact with a model as-needed to extract information The focus of fairness is how people receiving an outcome model are treated and thus leave-one-out unfairness focuses on the models behavior on the data distribution drawing attention to how changes in the model could affect those who are its likely subjects Leave-one-out unfairness does not require randomization in the models learning rule whereas differential privacy does Figure shows an intuitive example of this where the deterministic learning rule may yield models with unstable outcomes but only on points with vanishing probability for points with non-zero probability the models predictions will remain consistent across unit changes to the training data Moreover because Definition depends a learning rule may have little leave-one-out unfairness on some distributions and more on others However as Proposition shows differential privacy implies bounded LUF A proof can be found in the supplementary material Proposition be an ùúñùõø-differentially private learning rule and D be a point Then luf Individual Fairness Individual Fairness is a Lipschitz condition that aims to formalize the maxim similar people ought to be treated similarly Importantly in the context of supervised learning this is typically construed as a constraint on models rather than learning rules This stands in contrast to Definitions and which impose a constraint on the latter Additionally our definitions do not relate the treatment of individuals to others but instead measure the degree to which ones treatment by the model may be arbitrarily decided by the composition of the training data While there is no reason that individual fairness and leave-one-out fairness cannot coincide there is no a priori reason to believe that they will In Section we present experimental results on models trained with random smoothing which has been shown to guarantee individual fairness shedding further light on the relationship between these two fairness concepts We note that leave-one-out unfairness is also related to the definition of memorization introduced by Feldman which we discuss in greater detail in Section LUF IN deep models We characterize the prevalence of leave-one-out unfairness across models trained on several types of data tabular time-series and image data Importantly we find that a non-trivial fraction of data from to experiences LUF and moreover that the prevalence does not appear to depend on model generalization test accuracy or dataset size Datasets We perform all of our experiments over five datasets UCI german credit Adult Seizure FashionMNIST and Labeled Faces in the Wild The German Credit data set consists of individuals financial data with a binary response indicating their creditworthiness The Adult dataset consists of a subset of publicly-available US Census data with a binary response indicating annual income of The Seizure dataset comprises time-series EEG recordings for individuals with a binary response indicating the occurrence of a seizure Fashion MNIST contains images of clothing items with a multilabel response of classes Labeled Faces in the wild consists of unconstrained pictures of individuals faces with labels connoting the identity of the individual in each picture Further information about these datasets and the preprocessing steps we apply can be found in the supplementary material Table contains the accuracy and generalization error for each baseline model for all datasets Setup For all experiments we train models using Keras with TensorFlow In keeping with common practice we set the random seeds used by Python numpy and Tensorflow Beyond this in order to isolate the effect of leave-one-out unfairness from other sources of instability we use the same random initialization of model parameters across models in the same experiment and we turn off non-determinism in GPU operations This effectively makes the learning rule deterministic so that when measuring LUF the probabilities in Definition are We note that in the case of LFW an additional source of instability remains in the process that produces pairs of faces dynamically during training This is necessary in order for the model to encounter a sufficiently high number of face pairs during training while being bound to memory constraints We provide results of the same experiments over a smaller static dataset in the supplementary material with similar LUF behavior but lower accuracy As it would be prohibitively expensive to train models for the datasets listed above we instead measure differences over a fixed number of training sets obtained by randomly deriving from each dataset a training set a set of size that consists of points drawn randomly from test data ie with which to create different and a test set We train a baseline deep model with which to calculate the differences in prediction resulting from removing a point from from For each we train by removing from For each we estimate luf for all in the dataset by measuring the differences between and and taking the maximum difference over the sample of leave-one-out point Since the distribution that each training set comes from is a uniform distribution over the entire dataset this is measuring luf for each training set S and learning rule A step-by-step explanation of this calculation is given in the supplementary material Due to the cost for LFW we train models ie in this case we set To verify that the leave-one-out unfairness is a property of the models and not an unavoidable consequence of training a machine learning model on the presented datasets we also train linear models on the same datasets with the same method and compare the leave-one-out unfairness of these linear models to their deep counterparts The majority of our results displaying the extent of expected LUF in deep models center around the use of one architecture seed and set of hyper-parameters per dataset in order to keep as many variables controlled as possible To ensure that the behavior described is consistent we present experiments displaying the effect of changing architecture and random seed on our main results in Figure The main set of models for German Credit and Seizure datasets have three hidden layers of size and Models on the Adult dataset have one hidden layer of neurons The fmnist model is a modified LeNet architecture This model is trained with dropout The LFW face-matching model consists of a concatenation layer composing the two input images a -layer convolutional stack followed by a dense layer and a Sigmoid output German Credit Adult and Seizure models are trained for epochs FMNIST and lfw models are trained for German Credit models are trained with a batch size of FMNIST and Adult Seizure and LFW used batch sizes of German Credit Adult Seizure and lfw models were trained with Adam lr and FMNIST with SGD lr The experiments outlined above were also performed on models with two other architectures per dataset in order to compare results across architecture presented in Figure For German Credit and Seizure datasets one additional architecture was a shallower model of a -hidden layer model of size and the other a narrower model Deep PGD Trades Smoothed Linear dataset base acc gen err base acc gen err base acc gen err base acc gen err base acc gen err German Credit Adult Seizure FMNIST LFW Table Test accuracy and generalization error for models Confidence German Credit Confidence Adult Smooth Trades Deep Linear PGD Confidence Seizure Confidence FMNIST Confidence LFW Points Flipped N u m b e r o Points Flipped Points Flipped Points Flipped Points Flipped Figure Top row Prediction confidence on the horizontal axis percentage of stable points experiencing LUF on the vertical axis For FMNIST confidence is calculated as the absolute difference between the two most confidently predicted classes for other datasets confidence is Note the differences in scale between the graphs adversarial German Credit and Adult models display especially high leave-one-out unfairness as well as LFW Bottom Row A bar chart displaying what percentage of points in the dataset are affected by each one of the points taken out Each bar shows the number of points left-out points whose absence changed the prediction of the percentage of points shown on the axis Notably every single point that was taken out of the dataset affected at least one other individuals prediction Note the difference in scale on the axis of hidden layers of sized and For the Adult dataset the additional models were a narrower -hidden layer of size and a deep model with the same architecture as the main German Credit models For FMNIST we trained a shallower model with one set of layers removed as well as a model with no dropout Finally for LFW we compare with a ResNet model pre-trained on ImageNet and modified to take in two inputs and have a Sigmoid output aswell as a model whose filters are twice the size of the original model For experiments comparing the extent of expected LUF across models seeded differently we perform the main experiments outlined in the paragraphs above over different random seeds for all tabular and time series datasets and three different random seeds for image datasets Further details on model construction can be found in the appendix LUF in Deep Models Figure shows the prevalence of leave-one-out unfairness on all five datasets The first row plots the percentage of individuals experiencing luf ranging over the confidence of the baseline models prediction On every dataset examined deep models display nontrivial expected LUF ranging from to The second row shows the number of points in out of that lead to a given percentage of individuals having their predictions changed when only is removed from the dataset The percentage per point on axis and the number of points that change this percentage of outcomes is on axis Notably the removal of each point sampled lead to an model that changed the predictions of at least one other point suggesting that leave-one-out unfairness is in fact very common The results show that leave-one-out unfairness cannot be reliably predicted given test accuracy and more notably generalization error shown in Table While it may seem natural that models with higher accuracies display less LUF the deep model on the Adult dataset has an accuracy higher than the german credit dataset yet the German Credit dataset has approximately fewer individuals experiencing LUF Even more impressively the lfw model has higher accuracy than both German Credit and Adult models by and respectively yet has a much higher expected LUF of compared to and Similarly following intuitions from model stability lower generalization error may naturally seem to coincide with lower levels of LUF However the German Credit model has a generalization error of yet has lower LUF than both the Adult model with generalization error of just and the LFW model with generalization error of Indeed while these results will be further discussed in the next section it is worthy of note that the PGD model on the Adult dataset has essentially zero generalization error yet has a very high percentage of individuals experiencing leave-one-out unfairness while the deep model on the Adult dataset has generalization error of and has around of individuals experiencing LUF While we did not explicitly control for accuracy or generalization error these results are evidence that LUF does not depend on these metrics Also of note is that LUF does not decrease with dataset size FMNIST and German Credit are the largest and smallest datasets with training set sizes of and respectively yet FMNIST displayed similar LUF to German Credit within The Adult dataset is also larger than German Credit and displays more expected LUF Perhaps most importantly confidently-predicted points are not immune from leave-one-out unfairness in deep models on the majority of the datasets a substantial portion of points with high LUF were predicted with confidence greater than by the baseline model This is illustrated by the fact that the curves displaying the number of points versus baseline model confidence do not drop off sharply in all models except for those on the Adult dataset This is an interesting manifestation of miscalibration in deep models some confident decisions may still be somewhat arbitrary in that they are sensitive to the specific makeup of the training set Consistency Under Varying Conditions We provide calculations of expected LUF over all datasets in deep models where the architecture and random seed differ in order to ensure that the results are consistent across different modeling choices The results are presented in Figure While there is some variation in expected LUF no modeling choice explored eradicates the behavior Interestingly certain architectures seem to exacerbate or diminish LUF a deeper model increases LUF in the Adult dataset by nearly and removing dropout from the fmnist model as well as increasing the filter size on LFW have a similar effect This may warrant further study to find potential mitigation techniques through architecture selection however no pattern is immediately noticeable for example while a shallower model exhibited lower expected LUF on the German Credit dataset than the baseline model the same shallow architecture exhibited more expected LUF than the baseline on the Seizure dataset which shares the same architecture as the German Credit baseline model Random seed also affects the prevalence of expected LUF to a slightly lesser extent for all models but LFW Broadly however the results show that LUF is not an artifact of any one particular set of training conditions Linear Models We also provide the results for the same experiments on linear models to calibrate against a more stable learning rule that yields less complex models observe the green line in Figure These results show that LUF is not inherent to the data While there are points that are treated leave-one-out unfairly they are substantially fewer with the exception of LFW where the learning task is markedly more complex than the other datasets and unsuitable for a linear model Additionally the overwhelming majority of points treated leave-one-out unfairly in linear models are not confidently predicted in fact in all models but FMNIST there are no points treated leave-one-out unfairly that are predicted with a difference of more than from confidence This result agrees with intuition linear boundaries are smooth and linear regression is stable If the introduction of a point does shift the boundary it is likely that only points already close to the decision boundary ie low-confidence points are affected Deep models can have arbitrarily complex decision boundaries which appears to be closely-related to LUF As the phenomenon of memorization suggests and these results support deep models have the capacity to overreact to the presence of individual entries in their training data Figure illustrates this further in a low-dimensional setting Not only can the region around the left-out point potentially change but there are may also be far-reaching effects on the decision boundary beyond the neighborhood of the left-out point These changes will affect not just the predicted label of new points but also their assigned confidence score While intuitions that are valid in low-dimensional settings do not always transfer to high dimension this may nonetheless provide some intuition behind the factors that contribute to leave-one-out unfairness LUF AND ROBUST CLASSIFICATION Calls to mitigate adversarial examples have motivated a significant amount of research aimed at producing robust classifiers Recent results have shown that some of these techniques can even be repurposed to ensure individual fairness and moreover that they often produce deep models that admit more interpretable feature attributions Intuitively these findings could suggest that robust prediction methods rely on robust features that align more closely with human understanding of the problem domain and whose presence in the model may be accordingly less dependent on individual points in the training data In this section we explore this conjecture by measuring the incidence of leave-one-out unfairness with two robust classification methods adversarial training and randomized smoothing We find that models trained adversarially using projected gradient descent PGD aswell as models trained with the TRADES algorithm have significantly higher rates of LUF in most cases approximately doubling the number of unstable points over standard training On the other hand models that are made robust by post-hoc smoothing with Gaussian noise almost always have similar rates of expected LUF Taken together these results suggest that LUF and robustness are not inherently tied to one another but that certain classes of models may provide beneficial properties for both warranting further study Setup We use the same experimental setup as in Section for measuring leave-one-out unfairness In these experiments we only train deep models For adversarial training we use PGD with an radius and PGD steps on FMNIST and Seizure datasets For the Adult and German Credit datasets we use radius On the German Credit dataset we use the norm The radius remained Confidence German Credit Confidence Adult Arch Deep Linear Arch Confidence Seizure Confidence FMNIST Confidence LFW Figure Effect of random seed and architecture on LUF results in deep models from Figure The red and green plots show LUF for models of slightly different architecture as described in the experimental setup and the bars on the blue line show the minimum and maximum LUF values over random seeds on the main architecture shown in main results Notice the difference in scale across the graphs the same between PGD and TRADES training We determined the radius for adversarial training by finding the minimum distance with respect to the adversarial norm between any two points of different classes over a large sample of the dataset If this was impossible because this distance was zero we chose a distance smaller than that between over of cross-class pairs of points in the sample For TRADES training we used all of the same hyperparameters as PGD training with the addition of the TRADES parameter which was for Adult and German Credit and for Seizure and FMNIST Notice that for face-matching problems the threat model for finding adversarial examples is less clear eg it is not obvious if the attacker has access to individual images or pairs of images As we are unaware of an established threat model for face-matching we do not evaluate LFW in this section For randomized smoothing we take Gaussian samples with for the Adult and Seizure datasets samples with for FMNIST and samples with for German Credit While Cohen et al report needing more smoothing samples to achieve strong adversarial guarantees our goal in these experiments is to measure LUF which we found to be insensitive to additional samples beyond the numbers reported above The accuracy of these models is shown in Table Results and Discussion The results are shown in Figure The most immediate trend is the degree to which PGD and TRADES adversarial training worsens LUF approximately by a factor of two across all datasets and by a factor of nearly three on the German Credit dataset Seizure is a partial exception in that the PGD training does not worsen LUF but TRADES training does While adversarial training produces models that are more invariant to small changes in their inputs these results show that the training procedure itself can be unstable This may be related to prior work demonstrating that adversarially-trained models are more vulnerable to membership inference a privacy attack that exploits memorization to leak information about training data While membership vulnerability does not necessarily imply greater LUF these experiments show that in many cases the two phenomena may be related We also note that these results do not necessarily contradict the robust feature hypothesis proposed by Ilyas et al as robust learned features need not generalize across large portions of the dataset Turning to the curves labeled Smooth in Figure it is clear that randomized smoothing leads to qualitatively different leave-one-out unfairness results On most datasets smoothing had little effect difference on expected LUF Beyond suggesting that leave-one-out unfairness is independent of robustness these results also point to the fact that individual fairness and LUF are related but separate notions Randomized smoothing guarantees individual fairness for weighted metrics but has a negligible effect on leave-one-out unfairness Looking at the geometry of these models can shed further light on the differences in results between PGD training and randomized smoothing As suggested by Figure deep model decision boundaries have the potential to be very sensitive to individual points and this sensitivity may affect regions of the decision boundary far beyond the local neighborhood of the point in question This could contribute to leave-one-out unfairness as the predictions of points in regions shifted by a training points addition or removal will change Adversarial training may in some cases intensify the boundaries sensitivity to training points by penalizing inconsistent predictions in any direction within away Alternatively a smoothed model returns the expected prediction over a continuous distribution centered at each point rather than the value of the underlying model at only one point While this does not remedy larger boundary changes stemming from instability it likely does not exacerbate them as evidenced by the effects in Figure DISCUSSION Our study focused on instability to changes in training data as this type of stability is particularly well-studied due to its relevance to generalization and privacy However there are other potential sources of instability that may lead to arbitrary outcomes as well for example random initialization batching order and model architecture If a difference in any of these choices results in a difference in outcome for an individual eg if a change in random initialization frequently leads to a change in predicted credit risk for someone then this too could be seen as unfair as it would call into question the robustness of any supposed justification To establish a preliminary understanding of the degree to which these sources introduce changes in outcome similar to luf we experimentally investigate the percentage of changed outcomes resulting from varying the random seed prior to initializing and training models as well as from the choice of model architecture Figure shows these results for all of the datasets studied in Section alongside the corresponding measurements for LUF The experimental setup largely follows that described in Section We isolate the effect of each potential variable causing instability unfairness architecture Confidence I n d i v i d u a l s l i p p e d German Credit Confidence Adult Arch Deep LUF Linear LUF Seed Confidence Seizure Confidence FMNIST Confidence LFW Figure Arbitrariness in decision outcome as a result of changes in random seed and small changes in architecture are presented alongside expected LUF ie arbitrariness from small changes in the training set Calculation methods are described in We present these results to motivate a wider connection between learning algorithm stability and fairness beyond LUF Notice the difference in scale across graphs random seed and leave-one-out unfairness in its own experiment keeping other sources of instability controlled For the random seed experiments we train the same model with different random seeds and calculated the effects of instability in the same manner as calculating LUF described in Section for the experiments calculating the fairness effects of changes in architecture we train the model on three different architectures as described for the experiments verifying consistency in LUF in Section Further information on the architectures considered can be found in the supplementary material As Figure shows any of these aspects in a model can affect model behavior over a substantial percentage of the overall dataset Interestingly LUF seems to have a more consistent effect across points with high prediction confidence than arbitrariness resulting from a change in architecture LUF seems to have a similar effect to changing random seed and initialization as changing seed produces a larger effect in FMNIST and German Credit but a smaller effect in Adult and Seizure models While these other sources of instability unfairness are interesting avenues for future work we focus on leave-one-out unfairness in this paper due to its useful connections to other areas of the machine learning literature bridging the fields of fairness to those of stability and privacy as discussed in Section and also to the field of robustness as explored in Section RELATEDWORK Leave-one-out unfairness views the problem of learning instability from a fairness perspective While deep learning is generally understood not to enjoy strong stability properties our results are among the few systematic studies of the extent and potential ramifications of their instability Hardt et al show that even nonconvex models trained using Stochastic Gradient Descent remain stable over a small number of iterations and that popular heuristics like dropout and regularization help and provide some experimental demonstrations Towards achieving stability in deep learning Kuzborskij et al develop a screening protocol for choosing random initalizations that improve stability Memorization as defined by Feldman is a symptom of model instability where a model predicts the correct output on a given point if it is in the training set and incorrectly otherwise There has been much recent work unearthing the potential for memorization in deep neural networks discussion about the extent of the phenomenon in practice as well as arguments for its usefulness Memorization is closely related to leave-one-out unfairness in it is a measure of stability and crucially focuses on how instability affects a given point rather than an average However leave-one-out fairness is much broader than memorization Memorization quantifies how much removing a given point from the training set affects that whether that particular point is predicted correctly Leave-one-out fairness quantifies how the consistency not the error of a given points prediction is affected by any other point A well-known meeting point of stability and privacy is differential privacy which quantifies privacy risk in terms of a uniform information-theoretic notion of stability Leave-one-out fairness is related to but weaker than differential privacy as shown in Section Instability also worsens concrete privacy attacks over-sensitivity to the training set can affect a models parameters which can be leveraged to perform membership inference Our experiments in Section may suggest that this phenomenon has a connection to leave-one-out unfairness in that adversarial training increases both LUF and the potential for membership inference attacks There is little work that connects fairness and stability leave-one-out fairness is an individual-based fairness notion While there are several definitions of individualized fairness they are rarely operationalized in common fairness testing platforms as they can be difficult to calculate In addition to already-noted differences from prior notions of fairness expected LUF can be effectively measured on real datasets to give insight into whether an individual may be subject to unfair treatment at inference time CONCLUSION We present leave-one-out fairness a connection between algorithmic stability and fairness We demonstrate the extent to which deep models are leave-one-out unfair and experimentally showed that this behavior does not depend on generalization error Interestingly adversarial training worsens leave-one-out unfairness in deep models while random smoothing often mildly mitigates it showing that leave-one-out fairness is not dependent on robustness or individual fairness These results may suggest an interesting geometric intuition of deep networks sensitivity to their training points Finally we note that LUF may be undesirable in sensitive applications as it casts doubt on the justifiability of a models decision