High Dimensional Model Explanations An Axiomatic Approach Complex black-box machine learning models are regularly used in critical decision-making domains This has given rise to several calls for algorithmic explainability Many explanation algorithms proposed in literature assign importance to each feature individually However such explanations fail to capture the joint effects of sets of features Indeed few works so far formally analyze high dimensional model explanations In this paper we propose a novel high dimension model explanation method that captures the joint effect of feature subsets We propose a new axiomatization for a generalization of the Banzhaf index our method can also be thought of as an approximation of a black-box model by a higher-order polynomial In other words this work justifies the use of the generalized Banzhaf index as a model explanation by showing that it uniquely satisfies a set of natural desiderata and that it is the optimal local approximation of a black-box model Our empirical evaluation of our measure highlights how it manages to capture desirable behavior whereas other measures that do not satisfy our axioms behave in an unpredictable manner INTRODUCTION Machine learning models are applied in a variety of high-stakes domains such as healthcare insurance credit decisions and more In order to offer high prediction accuracy over high-dimensional data one must adopt increasingly complex models Complex models are however more difficult to understand Explainability has been recently identified as an important design criterion and has received widespread attention within the ML community of particular note is research into generating model explanations Broadly speaking model explanations are based on a labeled dataset as well as other potential sources of information Most model explanation techniques focus on feature-based explanations for each feature the model explanation outputs a value which signifies the importance of the feature in determining model predictions The value can be thought of as a score Alices high income is highly indicative of her receiving a loan or as a recourse had Alices income been lower by /year her loan would have been rejected Either way the basic premise of attribute-based model explanations is to explain complex model decisions via a list of numerical values where is the number of data features Crucially this approach fails to capture feature interactions Features are often strongly interdependent especially in complex ML models For example consider a black-box model that predicts sentiments associated with a paragraph of text In such texts there can be a high negative interaction effect between not and bad which attribute-based model explanations will fail to capture assigning influence to bad and not individually can be misleading Consider the following review It isnt the greatest scifi flick Ive every seen but it is not a bad movie The above review is classified as a positive sentence LIME a type of feature-based explanation assigns high negative influence to the words bad and not However reviews omitting the word bad or not will be classified as negative Explanation methods that indicate feature interactions rather than just standalone scores offer a much clearer picture of how the model makes its decisions The current model explanation landscape offers a wide variety of model explanation methods how should we pick the right one In this paper we propose an axiomatic approach rather than starting with a candidate solution and then offering post-hoc justifications for using it start with the properties one would like to have in a high-dimensional model explanation and derive a solution that satisfies these properties This is the approach we take in this paper We axiomatize a high dimensional model explanation method called the Banzhaf Interaction Index or BII This method faithfully captures how feature interactions influence model decision-making Not only does our proposed model explanation satisfy a set of desirable properties it is the only model explanation that satisfies all these properties Our Contributions In this work we propose a method for high-dimensional explanations for black-box models Our main goal is to axiomatically capture higher-order feature interaction Our main contribution is twofold first we propose a high-dimensional black-box model explanation method axiomatically capturing higher-order feature interaction Our characterization uses properties that are very natural in the ML context such as feature symmetry and effect monotonicity Section we obtain a new axiomatization of the Banzhaf interaction index which uniquely satisfies symmetry limit condition general-efficiency and a newly proposed axiom in the context of Banzhaf indices monotonicity Monotonicity is a rather general property which essentially means that the model explanation should change in a manner faithful to the underlying data This is very fundamental property for an interaction measure which states that the net contribution of the subset of features for the machine learning model is more than that for the model then the interaction measure for those features for model should be more than the interaction measure for those features for model Second we extend the idea of feature-based model explanations which can be thought of as a local linear approximations of blackbox models to higher-order polynomial approximations Especially we show that our proposed measure can be obtained by approximating the black-box model by a higher-order polynomial Section We evaluate our approach on real datasets In particular our experiments show that feature-based model explanations may fail to capture feature synergy moreover we show that other model explanations which fail to satisfy our axioms tend to behave in an undesirable manner Section Related Work While some model explanations provide record-based explanations or generate explanations from source code the bulk of the literature on the topic focuses on feature-based model explanations Ancona et al offer an overview of feature based-model explanations for deep neural networks The connection to cooperative game theory has been widely discussed and exploited in order to generate model explanations with a particular focus on the Shapley value and its variants Interaction Indices for Cooperative Games Two widely accepted measures of marginal influence from cooperative game theory the Shapley value and the Banzhaf value are uniquely derived from a set of natural axioms see also These measures do not capture player interactions rather they assign weights to individual players Owen proposes the first higher-order solution for a cooperative game for pairwise players Grabisch and Roubens extend it to interaction between arbitrary subsets and axiomatically derive the Shapley and the Banzhaf interaction indices In a recent paper Agarwal et al propose a new axiomatization for interaction among players which is inspired by the Taylor approximation of a Boolean function Feature Interactions Feature interaction has been studied by different communities Statistics offers classic ANOVA based feature interaction analysis Some recent work in the deep learning literature discusses feature interaction Tsang et al learns interactions by inspecting the inter-layer weight matrices of a neural network Tsang et al construct a generalized additive model that contains feature interaction information In another line of work Cui et al Greenside et al compute the interaction among features by computing the expected Hessian this can be thought as an extension of gradient-based influence measures for neural networks Datta et al also propose an influence measure for a set of features called Set-QII It essentially measures the change in model output when we randomly change a fixed set of features Lundberg et al propose the Shapley interaction index specifically for tree-based models PRELIMINARIES We denote sets with capital letters and use lowercase letters for functions scalars and features To minimize notation clutter we try to omit braces for singletons pairs etc eg we write instead of and instead of Let be the set of features A black-box model is a function mapping a set of n-dimensional input vectors X to R Our objective is to generate a model explanation for a given point of interest POI this explanation should ideally offer stakeholders some insight into the underlying decision-making process that ultimately resulted in the outcome they receive In this work we are interested in measuring the extent to which features and their high-order interactions affect model decisions In order to measure feature interaction effects we adopt the baseline comparison approach in other words we assume the existence of a baseline vector to which we compare an input vector in order to generate a model explanation For example in the automatic loan acceptance/rejection domain could correspond to an all-zero vector eg measuring the effect of an applicant having no money in their bank account as opposed to the true amount they have or a vector of mean values eg comparing the applicants true income to the average population income In order to generate model explanations we need to formally reason about the effect of changing features in the POI to their baseline values Generally speaking changing a single feature may have no significant effect on the model prediction For example if income debt it may well be the case that changing either the applicants low income or their high debt would not result in them receiving the loan however it is unreasonable to claim that neither had an effect on the outcome To formally reason about the joint effect of features we define a function measuring their value as a set Given a set of features a point of interest a baseline and a model we define a set function as In other words the value we assign to a set of features is the extent to which they cause the model prediction to deviate from the baseline prediction as a sanity check we note that and This formulation induces a cooperative game where features correspond to players We refer to the game defined in as the feature effect game We omit and they will normally be fixed focusing solely on the set of features We often replace sets of features with a single feature demarcating the entire set given a set denotes a single feature corresponding to the set The reduced game the nonempty is defined on the features with the characteristic function R otherwise Our objective is to generate high dimensional model explanations ie functions that assign a value to every subset of features To do so we define a feature interaction index for as In other words under the game namely the game defined in should roughly capture the overall effect that the set of features has on the value of Going back to the feature effect game defined in should measure the degree to which switching the value of features in back to their baseline values affects the prediction for A key idea in our analysis is marginal effect consider a single feature Its marginal effect on a set equals the extent to which the value of changes when joins The marginal effect of on is denoted In the context of the feature importance game is the marginal effect of knowing the feature given that the values of features in the set are known This is similar to other definitions considered in the literature When considering a pair of features how would one define their marginal effect on a coalition One very natural definition is to offset the effect of adding both features to by the marginal effects of adding and separately ie We can define the marginal contribution of a general in a similar manner Let be This is also known as the discrete derivative of at echoing the inclusion-exclusion principle We note that when is the Harsanyi dividend of a well-known measure of the synergy or surplus generated by can be thought of as the added value of having the coalition form given that the players in have already committed to joining More generally represents the marginal interaction between features in within the set of features AXIOMATIC CHARACTERIZATION OF GOOD MODEL EXPLANATIONS As previously discussed our objective is to identify high-quality model explanations When pursuing quality metrics for a model explanation one can take one of two approaches either show that the model explanation is the optimal solution to some target eg minimizes some loss function or that it satisfies a set of desirable properties We take both approaches in this work resulting in a unique and optimal high-dimensional explanation this is the Banzhaf interaction index BII Given a cooperative game the Banzhaf interaction index for a subset is BII In words BII equals s expected marginal contribution to a set sampled uniformly at random Grabisch and Roubens show that BII uniquely satisfies Linearity Symmetry Dummy the Recursive Property Generalized-Efficiency and the Limit Condition Theorem proposes a leaner axiomatization of BII which as we argue is more sensible in the model explanation setting We show that BII is the unique measure which satisfies four natural axioms Symmetry-Generalized-Efficiency the Limit Condition and Monotonicity The first three axioms are fairly standard assumptions in identifying good solutions generalized to interaction indices by Symmetry S for any permutation over we have that Here equals and is the game where the value of a coalition equals Symmetry is a natural property for any interaction measure intuitively it simply stipulates that features interaction value is independent of their identity and depends only on their intrinsic coalitional worth Generalized-Efficiency GE for any and for any Intuitively merging two features into one feature encoding the same information results in no additional influence Generalized-Efficiency extends the Efficiency axiom proposed by to characterize the Banzhaf value Limit Condition L if is the set of players of the game then In other words the interaction value of the set equals exactly the added value of it forming given that no subsets of players have pre-committed themselves to joining Next let us introduce the notion of monotonicity for interaction indices Monotonicity M If and for some strict inequality holds then Moreover if then This idea extends the strong monotonicity axiom proposed by which states that if for all then Datta et al argue that the monotonicity axiom is better suited for charactering model explanations than the more standard linearity axiom used in the classic characterization of the Shapley value as well as the original BII characterization Characterization Result First we show that the four axioms we propose imply a generalized version of the dummy property in Lemma The main result of this section is Theorem that characterizes our explanation method uniquely Lemma If satisfies S GE L and M then then Proof We begin by showing a weaker claim if is a null game where for all then for all For a given null game we have for all and By symmetry for all with because for any permutation Also by the Limit Condition L Similarly for all In fact this property holds for all Now we use GE property for by sequentially removing all until it becomes a singleton First for all The second equality holds because of symmetry S property for the game Now for we similarly use the GE property to obtain We repeat this argument until only one element is left We get This equality holds for all and all Which shows that for all Now to prove the second part of the lemma consider any game If for all then for all therefore by the Monotonicity property M which concludes the proof Before we prove the main theorem we define important terms which will be helpful to prove the theorem Given a set of features we define the primitive game as if else The set of all Boolean functions forms a vector space and the set of primitive games of all primitive games is an orthonormal basis of the vector space Therefore any Boolean function In this context a cooperative game is uniquely represented by a linear combination of primitive games In other words given a cooperative game R we can uniquely write as a linear combination of primitive games The unique decomposition of cooperative games is useful for our characterization of high-dimensional model explanations We also require some technical claims which will be useful for our characterization result for proofs see supplementary material Theorem BII is the only high-dimensional model explanation satisfying and M Proof We first show that BII satisfies all the properties BII trivially satisfies S L and M To show that it satisfies GE take any BII A similar calculation for shows that BII Thus BII BII equals Equation shows that BII satisfies the four axioms to show that it uniquely satisfies them we use the fact that can be uniquely expressed as the sum of primitive games We define the index of a cooperative game to be the minimum number of terms in the expression of the form We prove the theorem by induction on For in Lemma for all which coincides with the Banzhaf interaction index If then for some Consider Proposition A in the supplementary material implies that for all which in turn implies BII By Lemma A in the supplementary material which equals BII by Proposition A in the supplementary material To complete the proof for the first inductive step we need to show that for all BII If and then by symmetry we can define a permutation over such that bijectively maps to some and all are invariant By the Symmetry property however because Now consider any and define by the GE property we can write which implies for with The reduced game is if else By Lemma A in the supplementary material and This property holds for all By inductively using the GE property in a manner similar to Lemma A in the supplementary material we show that By Proposition A in the supplementary material this coincides with Banzhaf interaction index concluding the first inductive step To complete the proof assume that coincides with the Banzhaf interaction index whenever the index of the game is at most Suppose that has an index and expressed as Let and suppose that We define another game Since the index is strictly smaller than We claim that for all Indeed consider any equals The second-last equality holds by Proposition A in the supplementary material hence by induction on and monotonicity M coincides with BII for all It remains to show that coincides with BII when for any consider any and define Take any such that By the GE property we can write In Equation therefore as previously shown coincides with BII for the game Consider the restricted game Consider By the GE property In Equation therefore as we have shown before BII Let us denote and By repeating this argument for all and exploiting the GE property for each we can write as All of the summands in coincide with BII because for all We can write the reduced game as Thus the index of the reduced game is strictly smaller than By induction coincides with BII for the reduced game can be written as By using the GE property inductively BII can also be written in the same form which implies that coincides with the Banzhaf interaction index for all Explaining Our Model Explanations Does the BII measure make sense in the model explanation domain This is purely a function of the strength of the axioms we set forth Symmetry is natural enough if a model explanation depends on the indices of its features then it fails a basic validity test The index in which a feature appears has no bearing on the underlying trained model ideally nor does it affect the outcome Recall that Generalized Efficiency requires that model explanations should be invariant under feature merging In other words artificially treating a pair of features as a single entity while maintaining the same underlying model should not have any effect on how feature behaviors are explained Interestingly Shapley values are not invariant under feature merging a result shown by Lehrer The following examples illustrate what this entails in actual applications Example Consider an sentiment analysis task where a model predict if a movie review was positive In a preliminary step the text is parsed by a parser to be machine readable This can be done in many different ways For example the sentence This isnt a absolutely terrible movie Can be parsed as This isnt a absolutely terrible movie or as This isnt a absolutely terrible movie Generalized Efficiency ensures that the influences of is and nt in the second version add up to the influence of isnt in the first In other words Generalized Efficiency ensures that the influence of features generated through different parsers behaves in a sensible manner Example Features might be merged in another situation when features that were readily available during the training of a model end up being costly to obtain during its deployment If additionally these features are highly correlated with other features they might just be coupled Eg generally birds can fly so the features is_bird and can_fly may simply be merged at prediction time to make it easier to enter information into a classifier Again Generalized Efficiency ensures that the influence of the merged feature relates in a natural way to the influence of the original features The Limit condition normalizes the overall influence to be the discrete derivative of with respect to In other words the total influence distributed to sets of features equals the total marginal effect of reverting features to their baseline values This is an interesting departure from other efficiency measures shapley-based measures require efficiency with respect to or variants thereof ie the total amount of influence should equal the total value the classifier takes at the point of interest or the difference between the classifier and the baseline value We require that the total influence equals the discretized rate in which features change the outcome This makes BII more similar in spirit to gradient based model explanations which are often used for generating model explanations in several application domains The authors are aware of the existence of ostriches emus penguins and the fearsome cassowary Monotonicity is a very natural property in the model explanation domain if a set of features has a greater effect on the value this should be reflected in the amount of influence one attributes to it This has already been established in prior works for shapley-based measures However this property does not naturally generalize when using Shapley-based high-dimensional model explanations Agarwal et al propose a novel generalization of the Shapley value to high-dimensional model explanations which fails monotonicity for smaller interactions size of for k-th order explanations however interactions of size follow monotonicity Example highlights issues that may arise when monotonicity is not preserved Example Given a function with defined on binary input space for example is the result of an image classification task where denotes the presence/absence of particular super-pixel We assume that the baseline is Thus if resulting in and What is the interaction value between and Intuitively offer some degree of interaction that monotonically grows as increases Moreover it is easy to see that whenever Set-QII fails to satisfies the monotoncity and fails to capture the interaction between for any Set-QII for all Similarly the Shapley-Taylor interaction index for and is as it does not follow the monotonicity property however for it satisfies monotonicity and interaction value for is The BII value for is In Section we show that BII can be interpreted as a polynomial approximation offering additional intuition as to why our explanation method is optimal GEOMETRICAL INTERACTION AND BII The geometry of model explanations is relatively well understood for attribute-based methods Ribeiro et al view linear explanation methods as local linear approximations of around a point of interest Linear model explanations can be thought of as functions taking the following form where captures the importance of feature Taking this interpretation linear model explanations can be objectively bad explanations they are a poor local approximation of the underlying model as black-box model can be highly non-linear around In order to better capture the behavior of a black-box model we approximate it using higher-order polynomials For better visualization we first assume that the black-box model R takes a binary input vector mainly referred to as the humanly understandable feature representation First we start with quadratic approximation of the black-box model In the above equation captures the interaction between features and capture the importance of and Thus it is not unreasonable to assume that captures the pure interaction effect of and we can delegate the singular effects to having the resultant coefficient of capture the pure interaction between and This also connects with the idea of the Statistical Interactions For instance consider a sentiment analysis problem both the tokens bad and not have negative influence on the prediction However when they are present together as not bad their influence is positive In this simple example it would be desirable to have not bad The idea of higher order interactions can be extended similarly Consider a global polynomial approximation of by a k-degree polynomial Again to capture interaction among the set of features such that we should remove all internal interaction effects captured by for as we intend to capture the The polynomial is meant to locally approximate around the POI what is the best approximation Finding the best fitting polynomial of the highest possible degree seems like a natural objective However taking this approach runs the risk of ignoring lower order feature interactions and their possible effects this is shown in Example Example Consider the degree polynomial studied in Example with the baseline set to rather than as was the case in Example The best approximation to is clearly itself However if we do so then the interaction coefficients for variable pairs will be zero This is arguably undesirable for example if then has virtually no impact it is already set at the baseline Similarly and have little individual effect but do have significant joint effect it is only when both are set to that we observe any change in label Now we formally define the optimization problem to find the best k-degree polynomial approximation of the black-box model globally Let be the set of k-degree polynomials We are interested in finding a polynomial which globally minimizes the quadratic loss between and for all ie argmin The interaction among features in with is measured as the coefficient of in the least square approximation of with a polynomial of degree Theorem shows that this geometrical definition of feature interaction coincides with the Banzhaf interaction index Theorem Let be the k-degree solution of the optimization problem in Equation Then the coefficients of for in is given by BII The proof of the Theorem is a direct corollary of Theorem Theorem shows that k-th order interaction obtains via least square approximation of the black-box model using k-th order polynomial coincides with BII and extend a geometrical argument for good feature-based explanation to feature interactions Moreover the least square approximation of the model via polynomial also resonates with the Statistical interaction among features as we already discussed Therefore BII is not only obtained from strong axiomatic but also derived from an intuitive optimization problem EXPERIMENTAL EVALUATION In this section we experimentally illustrate the need for high dimensional model explanations and the axioms discussed above We consider two-well known benchmark datasets the UCI Adult dataset tries to predict whether an individuals income exceeds /year based on census data and the IMDB movie review dataset tries to predict the positive/negative sentiment of movie reviews based on their word content For the Adult dataset we train a -level decision three see Figure which achieves an test accuracy We use this easily comprehensible model to show the shortcomings of existing methods and the importance of our axioms We also train a random forest with estimators and a maximum depth of that achieves a slightly higher accuracy of For the IMDB dataset we train the BERT language model which achieves evaluation accuracy the BERT-Base model we use has M parameters making it virtually incomprehensible to humans Like many other model explanation methods BII assumes the existence of a baseline ie a vector having typical values which can be used as a reference for the point of interest when creating the explanation In the Adult dataset we assume the baseline for continuous features to be their median value categorical features are one-hot encoded and the baseline is assumed to be zero For the IMDB dataset each word is a distinct feature and the baseline is an empty sentence ie all features set to Baseline Explanation Frameworks We compare BII with an interaction measure based on the Shapley interaction index SII and a metric that falls into the framework proposed in which we call Set-QII For all methods we consider a version using the baseline ie the BII and SII values are the respective game-theoretic measures when playing the game of setting features to the baseline Set-QII measures the influence of a set of features by measuring the difference of the function value on the point of interest and a single another point where the values of this set of features is set to the baseline For all three metrics we consider model explanations capturing single feature influences and pairwise feature interactions We briefly discuss the computational complexity in the supplementary material To further illustrate the importance of measuring interactions we also study the well known LIME method which only generates individual feature importance We also consider a naive approach to create feature interactions which we call the additive expansion The additive expansion of a feature-based explanation methods is computing joint influence of feature and interaction among features and by simply adding individual influence of features and This is a naive way to compute the joint effect of features when only individual feature importance is provided Via additive expansion we can create a feature-interaction measure from LIME as well as We use keras library to train decision tree and random forest We use pretrained BERT model for the IMDB dataset available at artemisart/bert-sentiment-IMDB ad-hoc feature-interaction measures from the feature-importance versions of Banzhaf Shapley and QII The Importance of Interactions While seemingly obvious we begin our empirical investigation with a simple question are there instances where high-dimensional model explanations are needed In other words is it possible that feature-wise explanations offer sufficient explanations in practice Our first experiment indicates that high-dimensional explanations matter In Figure we visualize interactions for the movie review described in the Introduction Section We compute pairwise interactions using BII among features for the BERT model We represent the explanations generated by BII as a symmetric matrix where the diagonal element represents the influence of feature in the decision tree and element of the matrix for represents the interaction between and Comparing Figures and it is immediately clear why feature importance offers additional insights The values generated by LIME are indicative of a negative review whereas the review we examine is positive and look somewhat odd not has a strong positive influence On the other hand the interactions make it clear that BERT has picked up on the fact that not bad indicates a good review We compare additional BII feature interactions for the BERT model and the IMDB classification task in the supplementary material Figures to LIME and BII mostly agree for the individual feature importance however BII captures non-trivial interactions among pairs of words that are impossible to observe only from feature-based explanation In Figure we plot the explanation generated by BII and LIME for individuals with Marital Status yes Capital Gain and Education Level from the adult dataset for the simple tree classifier see Figure LIME suggests that Capital Gain is the most important feature and the other two features have minor importance However for any coalition of features activated features are set to their POI value and the others to their baseline value Marital Status No and Capital Gain changing only Marital Status or Capital Gain to their POI value does not significantly change the prediction value but changing both of them concurrently to their POI value results in a significant increase in prediction probability see Table BII successfully captures these insights in Figure and suggests that the high positive interaction between Capital Gain and Marital Status is responsible for the positive label When considering feature interaction there is high positive interaction between Capital Gain and Marital Status an insight that linear explanations do not capture as seen in Table Figure summarizes the explanations generated by BII SII LIME and QII for a married woman with a Capital Gain CG of with Education Level in the adult test data for the simple tree classifier Figure Column a is the additive expansion of the feature-importance versions of these measures and Column b shows the methods that explicitly consider interactions Both BII and SII indicate a negative synergy between Education Level and Capital Gain whereas all methods indicate positive individual importance Hence additive expansions fail to capture negative interactions even for relatively simple models Marital Status Capital Gain Education Level Education Level Age Capital Gain Capital Gain No Yes No Yes No Yes No Yes No Yes No Yes No Yes Figure A three-level decision tree trained on the adult dataset achieving test accuracy Table This example shows the joint importance of the interactions between Capital Gain and Marital Status in the coalition of Age Education Level and the monotonicity property in the tree Figure The prediction probability becomes when Marital Status and Capital Gain both join the coalition Age Education Level however when the Marital Status or Capital Gain join individually in the coalition Age Education Level the prediction probability becomes and respectively This shows that the features have synergistic effect which is not captured by linear explanations this holds for other coalitions not just Age Education Level Features POI Base Coalition AEL Marital Status MS Yes No No Yes No Yes Capital Gain CG Age A Education Level EL Prediction Monotonicity Matters Having established the importance of measuring feature synergy let us turn to evaluating the importance of our proposed axioms To do so we consider a modified version of the tree depicted in Figure The output in the second-to-last leaf is changed from to ie married people with high Education Level but low Capital Gain are now classified as not having a high paying job This change does not affect the prediction of the two points discussed above However this increases the interaction between Capital Gain and Marital Status having a lower Capital Gain or being unmarried both lead to a negative outcome More formally we denote the cooperative game induced by the original tree by and the one induced by the modified tree by Under the joint marginal gains increase Age Education Level Age Education Level for any other coalition According to the monotonicity property the interaction index value among Age Education Level should be higher under than This change is reflected in Shapley interactions and BII as both measures satisfy monotonicity see Figure column b for actual interaction and column c for interaction after the modification Set-QII however is unchanged it fails to satisfy monotonicity and even for this small example cannot distinguish between the two trees Similarly the interaction between Capital Gain and Education Number should also increase which is again reflected by SII and BII Set-QII fails to show this see Figure The Effects of Efficiency Table highlights how failure to satisfy generalized efficiency can lead to counterintuitive interactions when features are merged We calculated the SII values for Education Level and Education Number for a random forest with Marital Status left and Capital Gain right on some sampled points we recalculated them after merging the features to Education Level Education Number These features can be naturally merged as they are in fact identical Education Number is just a numerical representation of Education Level In some instances the interaction values of Education Level and Education Number with other features differ in sign ie one has positive interaction and another has a negative interaction In this case under efficiency the interaction value of the merged feature Education Level Education Number should lie somewhere between the two In Table we show some of the points where the Shapley interaction of Marital Status and Capital Gain with merged feature deviates from the expected interactions For example for Point the SII of Education Level with Marital Status is and Education Number with Marital Status is therefore the interaction after merging Education Number and Education Level should be less than however the SII interaction value between Marital Status and the merged feature is more than Table Failure to satisfy generalized-efficiency leads to counterintuitive interactions after features are merged We merge Education Level and Education Number which are actually identical Shapley interaction values with Marital Status left and Capital Gain right before and after the features are merged tend to overestimate the expected merged value as compared to the expected value under generalized-efficiency Interaction with Marital Status Interaction with Capital Gain Pt Pt Pt Pt Pt Pt Pt Pt Pt Pt Education Level EL Education Number EN Expected Difference Negative interaction No interaction Positive interaction It It isnt t the th e greatest gr ea te st scifi sc flick Ive Iv e every er y seen se en but t it it is is not no t a a bad ba d movie m ov ie Negative interaction No interaction Positive interaction Figure Feature importance and interactions for the movie review given in the introduction generated via BII BII is able to pick up on the strong positive interaction between not and bad that leads to a positive prediction for this review Note that the individual importance of not and bad is rather weak their synergy is assigned importance In other words SII placed far too much influence on two identical merged features We show additional examples in Table Since BII satisfies generalized-efficiency influence is preserved under merging and thus matches the expected outcome every isnt bad greatest not Influence Figure LIME feature importance for the movie review given in Section The measure assigns significant weight to the word not but little weight to bad BII LIME CG CG EL EL MS MS Negative Positive CG MS EL Influence Figure The explanation generated by BII and LIME for another point in the dataset LIME fails to capture interactions DISCUSSION AND FUTURE WORK Designing provably sound higher-order explanations for machine learning models in high stake domains is an important problem a Additive expansion b Actual interactions c After monotone shift Banzhaf CG CG EL EL MS MS CG CG EL EL MS MS CG CG EL EL MS MS Shapley CG CG EL EL MS MS CG CG EL EL MS MS CG CG EL EL MS MS Qii CG CG EL EL MS MS CG CG EL EL MS MS CG CG EL EL MS MS Negative interaction No interaction Positive interaction Figure The explanations for a married woman with a Capital Gain CG of with Education Level ie Masters in the adult test data for the above tree a Linear explanations highlight the importance of Marital Status MS and Capital Gain CG they fail to show how these to features interact b The positive interaction between Capital Gain CG and Marital Status MS highlights that both features a needed for a positive outcome c After the tree gets modified to strengthen the interaction the explanation proposed in Set-QII does not change failing monotonicity Ideally we want to design explanations we can trust We offer a variety of reasons to trust in BII it uniquely satisfies a set of natural properties if we believe that these are sensible then our job is done That said the authors believe that a normative approach is critical in the design of fair transparent AI solutions Rather than debating approaches one should debate the fundamental properties they are guaranteed to satisfy Deriving model explanations from norms offers an intuitive justification for the chosen interaction measure which fosters trust in the explanation method As shown in Section some of our fundamental properties are absolutely critical for the design of sensible explanations Even if one adheres to an optimization-based approach to explanation design we show that BII is the optimal solution to a natural objective While BII certainly satisfies several design criteria it is not without issues The first challenge is computational Feature-based explanations are computationally intensive as it is so it should be no surprise that computing higher-order interactions comes at a higher cost There are however good reasons to believe that BII can be computed efficiently on simpler ML model classes and that low error approximations can be found quickly This we believe is an important direction for future work