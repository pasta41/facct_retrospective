Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness Decision-making systems increasingly orchestrate our world how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance one that is substantially complicated by the context-dependent nature of fairness and discrimination Modern decision-making systems that involve allocating resources or information to people eg school choice advertising incorporate machine-learned predictions in their pipelines raising concerns about potential strategic behavior or constrained allocation concerns usually tackled in the context of mechanism design Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity in some complex decision-making systems neither framework is individually sufficient In this paper we develop the position that building fair decision-making systems requires overcoming these limitations which we argue are inherent to each field Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making teasing out the lessons each field has taught and can teach the other and highlighting application domains that require a strong collaboration between these disciplines INTRODUCTION Centralized decision-making systems are being increasingly automated through the use of algorithmic tools user data is processed through algorithms that predict what products and ads a user will click on student data is used to predict academic performance for admissions into schools and universities potential employees are increasingly being filtered through algorithms that process their resume data and so on Many of these applications have traditionally fallen under the umbrella of mechanism design from auction design to fair allocation and school matching to labor markets and online platform design However recent pushes towards data-driven decision-making have brought together the fields of mechanism design MD and machine learning ML creating complex pipelines that mediate access to resources and opportunities Increasingly learning algorithms are used in the context of mechanism design applications by adopting reinforcement learning techniques in auctions or general machine learning algorithms in combinatorial optimization and transportation systems As such applications do not directly focus on fairness and discrimination they are not the central focus of this paper The growing impact of these decision-making and resource-allocating systems has prompted an inquiry by computer scientists and economists are these systems fair and equitable or do they reproduce or amplify discrimination patterns from our society In building fair and equitable systems the question of fairness and discrimination is often a contested one Paraphrasing Dworkin People who praise or disparage fairness disagree about what they are praising or disparaging The causes of these philosophical debates include divergent value systems and the context-dependent nature of fairness and discrimination However even when we do agree on the types of harms and discrimination we seek to prevent mechanism design and machine learning often provide different sets of techniques and methodologies to investigate and mitigate these harms A key goal of this work is to identify the gaps between how machine learning and mechanism design reason about how to treat individuals fairly and detail concrete lessons each field can learn from the other Our hope is that these lessons will enable more comprehensive analyses of joint MLMD systems Where do the gaps between machine learning and mechanism design come from Crucially each field tends to make assumptions or abstractions that can limit the extent to which these interventions perform as desired in practice This limitation is not specific to machine learning and mechanism design in general any field must choose an appropriate scope in which to operate ie a reducibility assumption it is assumed that the issue at hand is reducible to a standard domain problem and that if the solution to this problem is fair and equitable then so too will be the overall sociotechnical system Under the reducibility assumption fairness and discrimination can be addressed by an intervention that operates within the frame of the field in question whether that be a constraint on a machine learning algorithm or a balance between the utilities of various agents in a mechanism Yet in practice complex algorithmic decision-making systems rarely satisfy any sort of reducibility assumption not only do these systems require the combination of ideas from both disciplines they also depend heavily on the social and cultural contexts in which they operate Our goal here is not to argue that it is sufficient to consider machine learning and mechanism design in conjunction with one another rather we argue that it is necessary to do so Working within each field in isolation will ultimately lead to gaps in our broader understanding of the decision-making systems within which they operate making it impossible to fully assess the impact these systems have on society Of course broadly construed sociotechnical systems cannot be fully understood just through these technical disciplines our hope is that a more robust understanding of the strengths and weaknesses of machine learning and mechanism design will allow for a clearer view into how they can be integrated into a broader study of these sociotechnical systems As an illustrative example consider the problem of online advertising Most modern online advertising systems perform a combination of prediction tasks eg how likely is a user to click on this ad and allocation tasks eg who should see which ad Moreover these advertising systems significantly impact individuals lives including their access to economic opportunity information and housing and new products and technologies see eg Thus advertising platforms must consider the social impact of their design choices and actively ensure that users are treated fairly In isolation techniques to ensure fair ad distribution from either machine learning or mechanism design fail to fully capture the complexity of the system On the mechanism design side auctions typically take learned predictions as a given as a result they can overlook the fact that algorithmic predictions are trained on past behavior which may include the biased bidding and targeting decisions of advertisers On the other hand while evaluation tools from fair machine learning would help to ensure that the predictions of interest are good for everyone by some definition they may fail to capture the externalities of competition between ads that might lead to outcome disparities For example a job ad may be shown at a higher rate for men than for women because it must compete against a different set of ads targeted at women than at men As each field has only a partial view of the overall system it might be impossible to reason about the systems overall impact without taking a broader view that encompasses both the machine learning and mechanism design considerations This disconnect is not limited to the ad auction setting described above Due to their historically different applications and development both machine learning and mechanism design tend to make different sets of assumptions that do not always hold in practice especially in pipelines that combine tools from both fields On the one hand machine learning traditionally treats people as data points without agency and defines objectives for learning algorithms based on loss functions that depend either on deviations from a ground truth or optimize a pre-defined metric on such data points Thus machine learning definitions of fairness tend to ignore complex preferences long-term effects and strategic behavior of individuals On the other hand as mechanism design often assumes known preferences and more generally that information comes from a fixed and known distribution without further critique and measures utility as a proxy for equality it tends to miss systematic patterns of discrimination and human perceptions see also Section While recent works have started to address these gaps between machine learning and mechanism design approaches to fairness by embedding welfare notions in measures of accuracy and fairness and using learning algorithms to elicit preferences many open questions remain on what each field can learn from the other to improve the design of automated decision-making systems In this paper we formalize these ideas into a set of lessons that each field can learn from the other in order to bridge gaps between different theories of fairness and discrimination In doing so we aim to provide concrete avenues to address some of the limitations of machine learning and mechanism design under the acknowledgement that bridging these fields is only an initial step towards a comprehensive analysis of sociotechnical systems We make the following contributions We review definitions of fairness and discrimination in machine learning and mechanism design highlighting historical differences in the way fairness has been defined and implemented in each Section We define several lessons that can be learned from mechanism design and machine learning in order to create an encompassing framework for decision-making Specifically we highlight the gap between fairness and welfare the potential of long-term assessment of decision making systems group versus individual assessment of fairness and the effect of human perception of fairness among others Section Finally we highlight different application domains and survey relevant works in which both mechanism design and machine learning tools have been deployed such as advertising education labor markets and the gig economy criminal justice health insurance markets creditworthiness and social networks We discuss advances and limitations of current techniques and implementations in each of these domains relating to the lessons from the previous section Section DIFFERENCES BETWEEN MECHANISM DESIGN AND MACHINE LEARNING Machine learning has been increasingly used to supplement human decisions drawing attention to biases rooted in learning from historically prejudiced data Fair machine learning often defines fairness conditions eg parity for legally protected groups without considering core mechanism design concerns such as welfare and strategic behavior Yet mechanism design often fails to conceptualize the impact of decisions for different social groups While both fields incorporate quantitative notions of fairness into optimization they differ in the roles those notions play in machine learning fairness is typically a constraint to be satisfied hence the learning algorithms are not optimizing for the most fair solution in contrast mechanism design typically defines and directly optimizes a fair utility-based objective eg social welfare This is only one of many high-level differences between the two fields Abebe and Goldner and Kasy and Abebe indirectly observe that understanding those differences and bridging different notions of fairness is essential in improving access to opportunity for different communities as well as extending the purpose of each field to encompass the causal effect of algorithmic design on inequality and distribution of power Fairness in machine learning Multiple definitions of fairness have been proposed interestingly their common characteristic seems to be that they agree to disagree Mehrabi et al collect the most common fairness definitions most of them fall into two main categories individual and group fairness Group fairness notions assess the large-scale effect of an algorithmic system on different demographic groups often defined by legally protected classes Individual fairness however compares outcomes between each individual in a population requiring people who are similar to each other to receive a similar outcome and is therefore typically a stronger constraint Individual fairness Inspired by Rawls fair equality of opportunity in political philosophy Dwork et al formalize the notion of individual fairness as a constraint in a classification setting where one wants to treat similar individuals similarly based on a pairwise similarity metric of their features partially designed by domain experts However defining similarity metrics is not easy especially between individuals belonging to sub-populations with different characteristics Subsequent work though limited mainly aims to overcome this obstacle by either learning feature representations that conceal the individuals membership to a protected group or by selecting individuals based on how they compare in terms of qualification with other members of their own sub-group However individual fairness is not equivalent to meritocracy since qualified covariates might be more difficult to obtain for disadvantaged people meaning one person may have worked harder to be recognized as similar by the algorithm Overall individual fairness is reminiscent yet different from the individual perspective that utility measures in mechanism design often take eg the notion of envy-freeness from mechanism design and can be used to compare metrics that assess the individual experience in an algorithmic setting While individual fairness does not take into account ones preferences often assumed in mechanism design recent works re-design this definition by taking into account individual preferences Group fairness Numerous definitions have been proposed for group fairness suggesting to impose either simple statistical parity conditions between groups or more complex classification constraints some aim to equalize each groups opportunity to positive outcomes balance the misclassification rates among groups or provide similar classifications under counterfactual group memberships Kleinberg et al and Chouldechova show that tensions arise when trying to simultaneously achieve multiple notions However as Madaio et al emphasize if one is to strive for quantitative fairness the notion one optimizes for should be context-dependent and developed in partnership with stakeholders Despite the variety of individual and group fairness definitions it becomes apparent that they lack expressiveness Most of these definitions focus solely on the inputs and outputs of the algorithm without taking into account how those outputs ultimately impact real-world outcomes For example the most common assumption is that a positive classification output is an equally valuable outcome for everyone As we discuss in Sections and mechanism design can offer the tools and definitions to overcome such limitations and successfully incorporate important aspects such as individual and group-level utilities resource constraints as well as strategic incentives to the design of decision-making models Fairness in mechanism design The mechanism design literature shifts the focus away from fairness towards welfare and discrimination We review i the classic theories of taste-based and statistical discrimination ii utilitarianism and the idealized objective of maximum social welfare and iii fairness in social choice theory Economic Theories of Discrimination There are two prevalent economic theories of discrimination taste-based and belief-based The key difference between them is the effect of information taste-based discrimination arises due to pure preferences and persists even with perfect information about individuals This theory has often been criticized as being simplistic since it is based on the discriminatory principle that decision-making agents derive higher utility from certain social groups however empirical evidence is rather inconclusive and application-dependent The latter theory of belief-based discrimination can be particularly informative for the design of fair machine learning systems as the true attribute of an agent is often not observed directly but only through a proxy From this theory statistical discrimination generally assumes that differences are exogenous but exist Other papers attribute discrimination to coordination failure agents are born unqualified but can undertake some costly skill investment which may lead to asymmetric equilibria Finally another belief-based discrimination theory is mis-specification unaware of their own bias decision-makers may hold mis-specified models of group differences which in the absence of perfect information lead to false judgment of an individuals abilities Such economic models offer useful insights on how to design a system aware of inequality due to i equilibrium asymmetries ii information limitations and iii human behavioral biases For example different social groups may differ in their skill level due to systematic inequalities of opportunity when certain equilibria arise but not due to inherited differences in their true ability This may be in sharp contrast to human decision-makers or even algorithms who due to imperfect information or other biases may incorrectly infer that perceived differences among individuals can be perfectly explained by observed characteristics Utilitarianism and normative economics Beyond discrimination theories utilitarianism and normative economics have been extensively used in mechanism design to motivate using utility functions as a synonym for social welfare Although these two terms are used interchangeably and welfare economics is often viewed as applied utilitarianism their origin differs As Posner writes utilitarianism is a philosophical system which holds that the moral worth of an action practice institution or law is to be judged by its effect on promoting happiness of society On the other hand normative or welfare economics holds that an action is to be judged by its effects in promoting the social welfare In contrast to machine learning and its multiple definitions of fairness weighted social welfare is the most accepted measure of broader social good in mechanism design but not necessarily of fairness or equity Typically utilitarian approaches capture equity by assigning appropriately defined weights to the utility of each agent Nevertheless a major limitation remains as welfare economics models rarely explain how to come up with these weights and how to interpret the relative difference between two agents weights Fairness in Social Choice Theory Social choice theory deals with collective decision making processes and fairness is of great significance in such processes particularly in resource allocation problems and voting In fair allocation the goal is to divide a resource or set of goods among agents that is somehow fair The literature tends to focus on three primary notions of fairness proportional division every agent receives at least of her perceived value of resources equitability every agent equally values their allocations and envy-freeness every agent values their allocation at least as much as anothers While these notions capture fairness of allocations at an individual level they treat all individuals equally in contrast to individual fairness which relies on some similarity metric to ensure similar outcomes only for similar individuals Moreover in many real-world problems in healthcare finance education relaxed notions of fairness are used due to the hardness of the absolute notions Conitzer et al kaplow points out that one deficiency of relaxed notions of fair allocation is that they fail to capture group-level disparities and often leave room for group unfairness see Section Finally another difference is that unlike machine learning settings where all individuals prefer positive or higher outcomes social choice theory can naturally capture different preferences of agents over the possible outcomes PAST AND FUTURE LESSONS We enumerate several lessons that mechanism design MD and machine learning ML are able to learn from each other We denote by a lesson that has been or can be taught by field to MD ML Tension between fairness and welfare Kaplow and Shavell are among the first to argue from a legal and economic point of view that the pursuit of notions of fairness results in a needless and at root perverse reduction in individuals well-being and that welfare should be instead the primary metric for the effectiveness of a social policy Optimizing for fairness instead of welfare can actually cause harm in social decision-making processes eg by leading to a violation of the Pareto principle This is later supported for quantitative fairness metrics by Hossain et al Hu and Chen who show that adding group parity constraints can decrease welfare for every group Voting theory deals with aggregating individual preferences We exclude discussions on voting while we acknowledge the existence of substantial works on fair voting Recent works also propose fairness-to-welfare pathways that transform utility-based metrics into comparing probability of outcome showing that fairness definitions do not automatically imply equitable outcomes from a mechanism design perspective but on the contrary Kasy and Abebe formalize some of these tensions arguing that machine learning definitions fail to acknowledge inequality within protected groups as well as perpetuate it through notions of merit This is further complicated by the fact that while notions of fairness in machine learning often treat outcomes as binary with a single desirable outcome the real world is far more complex different individuals may have different preferences over a wide range of outcomes While individual fairness is often incorporating stronger constraints to ensure that individuals receive a good outcome given their features their preferences are not directly taken into account Recent works are addressing this gap by re-designing notions of fairness with preferences in mind Using the lens of welfare economics as well as economic theories of discrimination to assess the equitability of machine learning systems can be useful for designing just systems but it is no panacea An important question that arises is whether the prevalent utilitarian view of mechanism design is already problematic A common criticism of utilitarianism is that it is not clear whose utilities we should maximize and how much weight each individual should receive in the optimization objective For example should an algorithm ensure the average utilities of both protected and unprotected groups be the same or should each group contribute to the total welfare proportionally to its size in society If we search beyond economics and computer science we soon realize that practical difficulties and tensions in philosophy political science history sociology and other disciplines are similar to some of the tensions we currently see in machine learning For example borrowing from political philosophy Binns introduces new notions of fairness that challenge both the common concept of social welfare maximization and fair machine learning definitions by asking questions such as should we minimise the harms to the least advantaged In the end while there may be no universal notion of welfare that adequately captures societys beliefs about whose welfare to prioritize mechanism design provides the tools to begin to interrogate these welfare trade-offs in a way that machine learning has yet to fully reckon with MD ML Long-term effects of fairness Because mechanism design considers outcomes for an entire population of agents the machine learning community has started to adopt mechanism design techniques ranging from equilibria analysis in games to dynamic models of learning agents in order to study the effects of machine learning algorithms on different subpopulations For example the decisions made by an algorithm and the strategic participants can change the population data over time requiring learning to be dynamic rather than one-shot Economics has long studied such dynamic effects but without a machine learning perspective However several useful lessons can be extracted from recent works First and foremost dynamic effects over time are crucial and if neglected they can worsen rather than improve inequality and discrimination in large-scale decision-making systems Indeed even simple two-stage models show that it is impossible to achieve full equality and have the potential of causing harm due to fairness constraints interestingly such models and subsequent works are strongly influenced by the classic economic models such as Coate and Loury and Phelps Second the type and complexity of interventions needed to achieve long-term fairness may vary significantly For example Hu and Chen build upon the labor market model in Levin and showcase the positive effect of simple short-term restrictions via a group demographic parity constraint on improving long-term fairness However other systems may require a more complex approach Wen et al study fairness in infinite-time dynamics by using a Markov Decision Process to learn a policy for decisionmaking that achieves demographic parity or equalized odds in the infinite time dynamics From a technical perspective increasing leaning on popular mechanism design tools such as large market models mean-field equilibria analysis and dynamic programming techniques seems to be a promising direction for the design of effective and fair policies in machine learning-driven systems Finally most machine learning models focus solely on algorithmic bias and are oblivious to the existence of the social bias that is coming from human agents making complex dynamic decisions as a response to the systems algorithmic decisions The interplay between social and algorithmic bias over time may in fact prove itself useful in explaining dynamic patterns of discrimination in sociotechnical systems Bohren et al introduce the discrimination theory of mis-specification and show both theoretically and empirically that contradicting patterns of discrimination against womens evaluations in online platforms can be well explained by users mis-specified bias in sequential ratings Monachou and Ashlagi build upon this theory to study the long-term effects of social bias on worker welfare inequality in online labor markets while Heidari et al also use observational learning to study the temporal relation between social segregation and unfairness MD ML Strategic agents The economists basic analytic tool is the assumption that people are rational maximizers of their utility and most principles of mechanism design are deductions from this basic assumption Therefore as machine learning algorithms are increasingly used in prescriptive settings like hiring or loan approval it becomes necessary to consider the incentives of the agents who are affected from those algorithmic decisions As transparency laws regarding algorithmic decision-making are gradually being introduced individuals are now more than ever capable to use insights about the deployed classifiers and accordingly alter their features in order to game the system and receive a beneficial outcome This observation has initiated a line of work on strategic classification which focuses on incentive-aware machine learning algorithms that try to reduce misclassification caused by transparency-induced strategic behavior The ability to manipulate their features naturally raises several fairness questions For example Hu et al contextualize strategic investment in test preparation to falsely boost scores that are used as a proxy to quantify college readiness as well as the disparate equilibria that could potentially emerge in the presence of social groups with disproportionate manipulation capabilities Additionally Milli et al utilize credit scoring and lending data to show that there is a trade-off between the utility of a decision-maker who tries to protect themselves from the agents who modify their features strategically and the social burden different groups of agents incur as a consequence On a more positive note recent work has argued that this strategic modification of features does not always correspond to an agents attempt to game the system but could also represent a truthful investment of effort towards improvement depending on the features being used and the extent to which they can be maliciously manipulated This idea has become apparent both in the mechanism design literature on evaluation mechanisms and the machine learning literature on the design of transparent decision policies that aim to incentivize the individuals improvement Relaxing our initial assumption about strict individual rationality we can easily see that transparent decision policies based on features prone to manipulation may prove themselves substantially unfair by equally rewarding seemingly similar individuals with dissimilar effort profiles in direct opposition to definitions of individual fairness as those dissimilarities may have ethical behavioral or cultural origins For ease of exposition consider a simple example of admitting graduate students solely based on their undergraduate GPA Even if two students share the same observable features GPA that could reflect different mixtures of manipulating the undergraduate evaluation rules or achieving truthful academic excellence a behavior often depending on their cultural background In this context the uncertain relation between features and individual qualifications gives rise to a need for strategy-proofness in order to make prediction-based decision-making systems transparent and fair Apart from simple classification settings the interplay between machine learning and mechanism design also needs to be considered in more complex systems where the stakeholders have more diverse incentives and predictive models of different forms also appear For example in health insurance markets machine learning is used to predict the expected costs of individuals and proportionally compensate insurers with strategic up-coding by the latter favorably skewing subsequent predictions and disincentivizing all insurers from offering attractive insurance plans to people with specific medical conditions Moreover the retrieval and recommender systems well-known downstream applications of machine learning are also vulnerable to strategic behavior leading to disparate effects even in the absence of model transparency specifically strategic manipulation in recommendations and search engines often results in skewed information delivery leading to disproportional opportunity or exposure for the users Such disparate effects of machine learning highlight the need for further research towards the direction of developing models aware of the strategic environment in which they operate as well as the effects of their predictions on different people and groups ML MD Defining and Diagnosing Unfairness Under Uncertainty Definitions of fairness from the mechanism design literature tend to be centered around preferences and utilities As discussed earlier the fair machine learning literature has yet to fully adopt this perspective typically operating at the level of model outputs as opposed to the values for individuals produced by those outputs However a key assumption necessary for mechanism designs preference-based notions of fairness is that individuals preferences are known or can be in some way communicated to a central decision-maker In many mechanism design applications like traditional auctions or school choice this assumption can be reasonable In more complex systems like online advertising preferences are often unknown a priori and must be estimated in practice Thus questions of fairness necessarily involve reasoning about uncertainty and who bears the burden of errors In this way ideas about fairness from machine learning can be useful Because machine learning treats uncertainty as a first class concept many conceptions of fairness from the machine learning literature explicitly consider errors and their impact on different sub-populations Uncertainty can also manifest itself with respect to outcomes not just to preferences Many application domains utilize probabilistic models for example labor market models from mechanism design often consider two-stage processes in which noisy signals provide information about whether a worker is qualified or not Importantly while these models do incorporate uncertainty the designer knows the true relationship between observed signals and true outcomes even though this relationship is probabilistic This style of analysis is less suited to deal with cases where the relationship between signals and ground truth is unknown and can only be learned about through data The lack of ground-truth information greatly complicates any analysis of the impact of a mechanism but it is precisely this lack of information that machine learning techniques are designed to handle Many of the challenges that arise during learning including data scarcity for certain groups feedback loops preference elicitation and explore-exploit trade-offs implicate serious fairness concerns By integrating lessons from machine learning on how to define and measure disparities that learning produces mechanism design can gain a deeper understanding of real-world systems Using fairness definitions as a diagnostic tool for potential harms and societal issues is a powerful application of computing as Abebe et al argue As such the various group fairness definitions from machine learning focus on illustrating output differences between different legally protected groups using error measurements to quantify such differences eg false positive/negative rates A single definition is thus not feasible nor desirable but the process of defining fairness has been expanding both conceptually and practically from early computer science works that defines fairness through observations or representations to understanding causal relationships between features While satisfying multiple definitions may not always be possible the different definitions of fairness in machine learning offer an opportunity to become more intersectional in defining sensitive groups and in assessing power differentials More than that they shift the purpose of defining fairness from a normative one to a diagnostic one a purpose that mechanism design can learn from when assessing the utility of a system Together with a plethora of works from economics that assess differences in welfare at a group level recent works in mechanism design propose adapting individual notions of envy-freeness into group-level definitions through stability eg no group of people should prefer the outcome of another group The need to assess the outcome differences between groups becomes more pressing as machine learning tools are increasingly being used in traditional mechanism design applications as previously discussed Recent works increasingly adapt group fairness methods inspired from machine learning to design fair voting procedures and advertising bridging the gap between the individual perspective of mechanism design methods and group-level definitions of fairness from machine learning Beyond transferring lessons from machine learning to mechanism design we argue that future design must encompass perspectives other than the purely computational one from sociological understandings of harm and power to economic discrimination and theories of justice MLMD Human perceptions and societal expectations of fairness Early studies on fairness in both mechanism design and machine learning propose various mathematical formulations of fairness and normatively prescribe how fair decisions should be made However given the impossibility to simultaneously satisfy multiple fairness notions decision-making systems need to be restricted to only selected principles of fairness a process that becomes challenging in certain applications such as criminal justice finance and lending self-driving cars and others Given such applications and their potential for harm it is essential for the chosen design and principles to be socially acceptable Thus there is a need to understand how people assess fairness and how to infer societal expectations about fairness principles in order to account for all voices in a democratic design of decision-making systems A line of work in machine learning research has taken steps towards this democratization goal through participatory sociotechnical approaches to fairness by studying human perceptions and societal expectations of fairness Three major questions emerge from this line of work which we argue are central in developing participatory mechanism design tools that incorporate preferences We discuss them next First whose perceptions or assessment of fairness should be considered While Awad et al and Noothigattu et al used crowdsourced preferences from lay humans in the famous moral machine experiment Jaques and Yaghini et al have argued that preferences should be taken only from relevant individuals eg primary stakeholders ethicists domain experts citing context-dependent aspect of fairness and the possible vulnerability of lay humans to societal biases Second what options and information should be made available to the participants Some studies directly asked participants to choose the model with the best fairness notion or the best outcomes whereas others asked indirect questions to infer the acceptable fairness principles eg whether they approve of certain differences in decision outcomes for pairs of individuals from different groups or the overall outcome distribution In a different approach Grgić-Hlača et al and Van Berkel et al study the validity of using certain input features in the decision-making process in order to achieve procedural fairness Finally how should the individual preferences be aggregated Even though most of the literature has followed some variant of majority rule for this Noothigattu et al and Kahng et al have argued for tools like score-based bloc voting or Borda count from voting theory for better representation of participants choices These studies have also shown the need of model explainability transparency and context-specific feature selection in improving societal fairness perceptions which mechanism design has traditionally considered as out of scope or assumed to be known leading to a recent surge in explainability and transparency studies in machine learning Future work in mechanism design can learn from such studies in challenging current assumptions about preferences perceptions and values APPLICATION DOMAINS In this section we discuss several application domains of machine learning and mechanism design to illustrate the lessons of Section underscore the complex interplay between these domains point out gaps as well as potential ways of bridging these gaps We note that many of the applications are open to critique One might object to the idea of deciding which students are qualified or unqualified to receive an education in college admissions More fundamentally one might argue that the overall social system eg criminal justice in which an application eg recidivism prediction is embedded is unjust and further that this cannot be remedied by any technical fairness intervention We discuss applications merely as an illustration of the lessons we have articulated and reiterate our position that it is necessary though not sufficient to bridge machine learning and mechanism design for algorithmic fairness Online Advertising Auction design a subfield of mechanism design deals with the optimal design of allocation and payment rules when a number of agents bid for a resource As online ad auctions run in a high-frequency online setup that demands automated and precise bidding from the agents many ad platforms have deployed machine learning models to estimate the relevance of an ad to a customer while using some high-level preferences about advertisers budget bidding strategies and target audiences Using the automated bids derived from these relevance predictions ad allocation mechanisms are run to place specific ads every time a user visits a webpage thus making the system a complex mix of interdependent components from both machine learning and mechanism design Recent studies show that the resulting ad delivery may be problematically skewed users who differ on sensitive attributes such as gender age race may receive very different types of ads For example search queries with Black-sounding names are highly likely to be shown ads suggestive of arrest records In another study women were shown relatively fewer advertisements for high-paying jobs than men with similar profiles When ads are about housing credit or employment such disparities can harm equality of opportunity One cause of problematically skewed ad delivery is explicit targeting of users based on sensitive attributes which can be tackled by disallowing ad targeting based on sensitive attributes especially for housing credit and employment ads Although major ad platforms like Google and Facebook had disallowed targeting of opportunities ad based on sensitive attributes the advertisers could still exploit other personally identifiable information such as area code or using a biased selection of the source audience in the Lookalike audience tool by Facebook Following a lawsuit Facebook removed targeting options for housing credit and employment ads Other studies again reveal that ad delivery mechanisms could still result in skewed audience distribution based on sensitive attributes even in the absence of any inappropriate targeting These are often the results of competitive spillovers relative competition between general opportunity ads and category-specific ads for items like womens fashion can result in opportunity ads being shown to more male audiences This issue has been tackled from both advertisers side and auctioneers side Solutions on the advertisers side include running multiple ad campaigns for different sensitive groups with parity-constrained budgets or using different bidding strategies for different demographics groups However such type of targeting has been disallowed by the platforms because of earlier exploitation by discriminatory advertisers Moreover rational advertisers may not want to adopt solutions that decrease utility On the auctioneers side the allocation mechanism can be redesigned to ensure fair audience distribution Along with the welfare optimization goal group fairness constraints can be used to ensure fair audience distribution and individual fairness or envy-freeness constraints can be adopted to ensure similar individual satisfaction of the users Most of these papers have focused on the mechanism design of online ad delivery Yet all components advertisers strategies platforms relevance prediction ad allocation mechanism may be responsible for unfair ad delivery While the mechanism design components take the relevance predictions from machine learning models as inputs they often overlook the possibility of biases in these predictions Thus to build a fair online ad ecosystem there is a need to study the role of relevance prediction models and their role in the mechanism design pipeline In this regard a line of work in machine learning that studies preference elicitation in auction settings can be explored and extended to online advertisements Admissions in education Schools and universities increasingly use machine learning to inform admissions decisions Mechanism design has traditionally studied problems such as school choice college admissions and affirmative action eg In general most of these papers adopt similar assumptions and approaches At their baseline they model the problem as a two-sided market of strategic agents schools or colleges on the one side and students on the other side In school choice the assignment decisions are usually centralized eg all public schools in Boston may commit to a common matching process while in college admissions each university decides independently which applicants to admit In both cases explicit fairness considerations are rarely taken into account The only exception is of course affirmative action which is imposed as an additional external constraint on the market Most economics papers have mostly considered two categories of policies with respect to protected attributes group-unaware and group-aware policies Both policy schemes usually translate to demographic parity constraints and similar quota rules Interestingly explicit notions of fairness and equity are less commonly considered This may be due to various reasons For example in a decentralized system such as college admissions it is unclear whether and most importantly how to optimize social welfare But even in more centralized applications such as school choice several dilemmas arise Given that both market sides have heterogeneous preferences and strategic incentives should the central planner prioritize the students or schools welfare How is social welfare even practically defined in this case Indeed several papers have offered a broader critique of the approaches used by market designers pointing to the gap between translating theoretical assumptions to practical solutions Machine learning algorithms are increasingly being used in this area as well for the purpose of parsing data at a large scale more efficiently and embedding missing notions of fairness The machine learning literature usually poses the admissions problem as a classification task to predict whether an applicant is qualified or unqualified to attend their university-based on covariates given in the students application standardized test scores demographic information etc When framed as a machine learning problem the task at its core is to accept students who are qualified and reject those who are not However when one widens the scope of the problem one soon realizes that universities have finite capacity for accepting students which creates market competition and thus strategic incentives among schools and applicants This latter problem is studied through a dual MLMD lens by Emelianov et al who consider admission policies under implicit bias and show how affirmative action in the form of group-specific admission thresholds can improve diversity and academic merit at a capacity-constrained university Finally Kannan et al highlight another interesting dimension in the intersection of mechanism design machine learning and policy downstream effects of affirmative action The paper draws upon the mechanism design literature to explore how the effects of different policy schemes propagate across education and labor when sequential decisions are made by utility-maximizing agents with potentially conflicting goals universities vs employers They show that fairness notions such as equal opportunity and strong irrelevance of group membership can be achieved only in the extreme case where the college does not report grades to the employer Thus the problem of intersectionality occurs again in complex decision pipelines where different fairness metrics may be required yet it may be infeasible to satisfy all simultaneously the question of what is an acceptable trade-off between utility maximization and various notions of fairness persists Labor markets and gig economy Discrimination has been a perennial problem in labor markets Decades of research has shown that hiring decisions are subject to bias against disadvantaged communities More recently techniques from both machine learning and mechanism design have been brought to bear in the labor market and in particular the gig economy leading to a fresh wave of concern that the persistent discrimination found in traditional labor markets will manifest itself in new and unexpected ways In particular we focus on two use cases employee selection and employee evaluation Both of these use cases blend techniques from machine learning and mechanism design and as we will argue it is impossible to adequately deal with issues of discrimination and bias without drawing upon ideas from both fields Emerging data-driven techniques for employee selection have begun to employ techniques from machine learning to evaluate and sort candidates While some contend that quantitative tools might help to reduce discrimination others warn that hiring discrimination will not be solved by machine learning alone However hiring cannot be treated as a purely predictive problem it requires consideration of factors like allocation incentives externalities and competition all of which feature more prominently in the mechanism design literature Consider for example the case of salary prediction platforms like LinkedIn use machine learning techniques to predict a jobs salary While this might appear to be a straightforward application of machine learning it creates strategic incentives that may produce unintended consequences If a candidate applies to a new position their potential employer may be able to infer their current salary based on these predictions enabling the new employer to reduce the salary they offer Similar consequences can arise from efforts to predict a candidates likelihood to leave a job Moreover many predictions about candidates are ultimately used in contexts where there is a limited hiring capacity As a result predictions about candidates are often later used to rank or filter candidates a type of mechanism To avoid the explicit consideration of demographic characteristics efforts to ensure that candidates are treated fairly usually through constraints similar to demographic parity often come at the prediction stage but fail to make guarantees about the eventual outcomes produced by downstream mechanisms A more complete effort to prevent discrimination in algorithmic hiring pipelines must leverage the flexibility provided by machine learning to implement anti-discrimination solutions while taking into account the effects of downstream hiring mechanisms Beyond issues of discrimination in hiring recent technological developments have fundamentally changed how labor markets work particularly with regards to the gig economy and thus led to a plethora of recent works in this space For example Rosenblat et al and Monachou and Ashlagi describe how mechanisms that use customer ratings to evaluate workers can internalize customers discriminatory tastes Barzilay and Ben-David call attention to the ways in which platform design can be used to create or reduce wage disparities Similarly Hannák et al document the existence of linguistic and other biases in employers reviews for gig workers on two online labor platforms TaskRabbit and Fiverr and the negative effects of gender and racial bias on the number of reviews rating search and ranking Edelman et al find evidence of discrimination against African-American guests on Airbnb highlighting the role that Airbnbs design choices play in facilitating this discrimination Spurred in part by this work Airbnb recently launched an initiative to study racial discrimination on their platform Crucially this body of work combines insights from economics mechanism design and machine learning to better understand how discrimination can manifest in the gig economy Criminal justice Recent popularity of the use of machine learning techniques in prescriptive settings has motivated several attempts to analyze the fairness aspects of predictive and statistical models especially in the context of a critical application domain like criminal justice Unsurprisingly relying on such models in practice can end up reinforcing underlying racial biases as it has been shown in studies about neighbourhood surveillance and recidivism prediction The latter ProPublica study has raised a heated discussion leading many to advocate that the deployed system independent of the larger criminal justice system in which it is situated is plainly unfair While that was apparent in this instance a rigorous explanation was not trivial several responses argued that their claims of discrimination were mainly caused by differences in methodology like the statistical measure of discrimination Since the deployment of predictive models in the criminal justice system is a contested idea knowledge about their potential advantages and pitfalls regarding fairness is crucial in order to perform a fruitful debate on their applicability As already mentioned in Section the proposed theoretical notions of fairness seem to present significant trade-offs while some of them are impossible to simultaneously satisfy Those contradictions naturally raise a major question regarding the criminal justice system and the automated decision-making systems within it What do people consider truly fair Since there doesnt seem to be a one-sizefits-all answer to this question a natural step forward is a more participatory approach to the definition of context-dependent notions of fairness As discussed in Section some first approaches have been made towards studying human perceptions of fairness but questions regarding who are the relevant stakeholders in criminal justice what notions are more appropriate in that field and how to aggregate preferences still need to be answered But even under a perfect fairness definition humans involved in the judicial decision-making process might be inherently biased In this context machine learning can be leveraged to mitigate these human biases and mechanism design can be proven useful in studying the welfare implications and effects on inequality of decisions in criminal justice Moreover merely focusing on the task of fair recidivism prediction might be considered an oversimplification because the assessment of a ML system regarding innocence and guilt ignores both human incentives in the criminal justice pipeline and the humanity of the criminal justice system as a whole In the United States a defendant only needs to prove their innocence when their case goes to trial in court Yet of felony convictions in the United States are obtained through guilty pleas and of known exonerees pleaded guilty or did not contest to crimes they did not commit Machine learning techniques could be applied in conjunction with the critical perspective of mechanism design to better comprehend the racial disparities in both sentence and charge bargaining as documented by Berdejó It is worth noting that any theoretical techniques used to examine the criminal justice system should be wary of the common mechanism design assumption that people are rational expected utility maximizing agents while desperation selflessness or fear often counter this assumption in the real world Though enlightening theoretical understanding of fairness in risk assessment and its aforementioned aspects is not sufficient to suggest adopting the use of such systems The ultimate decision should be made by the respective stakeholders considering the practical issues that need to be addressed and the particular context in which risk assessment tools are utilized Health insurance markets Interactions between machine learning and mechanism design are salient for fairness in healthcare For example prior work studied how machine learning formulations may underpredict black patients health care needs Here we draw attention to problems at the intersection of the two fields in health insurance markets The Patient Protection and Affordable Care Act ACA was designed in part to defuse health-insurer incentives to refuse or avoid coverage to individuals with higher healthcare costs ie selection incentives One way the ACA addresses this is through risk-adjustment based transfer payments premiums are transferred from plans with lower expected costs to plans with higher expected costs compensating insurers and fairly spreading costs Thus issues of fairness in mechanism design inhere at the policy-design level A key component of risk adjustment is estimating individual actuarial risk inaccurate estimates can create selection incentives for insurers The Centers for Medicare Medicaid Services Hierarchical Condition Category HCC model is a widely-used risk adjustment model The HCC predicts an enrollees expected costs using demographic and diagnosis information the HCC is therefore group-aware for some protected classes eg sex age but is otherwise group-unaware particularly to groups of enrollees with specific healthcare patterns related but not limited to diagnoses treatments and prescription-drug use Although there is evidence that the HCC accurately predicts expected costs for many groups of enrollees there is also evidence that the HCC makes systematic errors for some groups and that insurers often engage in benefit design to exploit the resulting selection incentives Thus healthcare-policy designers approaching the problem from a mechanism design perspective encounter the lesson from fair machine learning that it is in general necessary to be group-aware ML practitioners also encounter the lesson from mechanism design that it is necessary to take into account the strategic behavior of stakeholders A natural machine-learning oriented response to the systematic error observed in the HCC could be to incorporate more information about enrollees into the model but because the HCC data are provided by the health insurers there are concerns that insurers or healthcare providers might then strategically up-code enrollees to favorably skew subsequent predictions Recent work seeks to address these issues in risk adjustment by incorporating fairness interventions to learn a regression model that equalizes systematic error across groups The proposed fair regression models can bring average predicted costs significantly more in line with average historical costs without a commensurately large penalty to the traditional evaluation metric of We see each disciplines techniques applied in a component-wise fashion towards a competitive health-insurance market that achieves socially optimal outcomes Notably neither discipline can independently achieve this goal selection incentives cannot be defused without accurate risk adjustment behavior cannot be changed by predictions without appropriately designed incentives Determining creditworthiness The Financial Technology FinTech industry increasingly decides to whom a home business loan should be awarded and at what interest rate When one considers banks have finite liquid assets determining an individuals creditworthiness quickly becomes one piece of a larger problem Saunders notes that FinTech can help streamline the application process for loans among other benefits However concerns including disparate impacts of disadvantaged communities overcharging the poor the unintelligibility of such algorithms and protection under consumer laws emerge from the use of machine learning Overcharging the poor particularly appears to be in part a corollary of determining creditworthiness as a machine learning problem in isolation from mechanism design In determining creditworthiness the true label ability to pay back the loan faces two shortcomings first it is only observed if the loan is given and second it might be a function of the given interest rate The Pew Research Center revealed of Black applicants and of Hispanic applicants were denied mortgages compared with about of White and Asian applicants Moreover when granted a loan of Black applicants were charged an interest rate over compared to only of White applicants This in turn makes repaying the loan more difficult exacerbating financial insecurities resulting from historical financial and housing oppression such as loan denial and redlining of neighborhoods Resulting from the historical imbalance of loan acceptance Kallus and Zhou observe that algorithms might still yield bias in bias out phenomena even with fairness constraints and online machine learning approaches aim to face this issue by incentivizing exploration As transparency increasingly becomes a legal obligation of financial institutions such technology is particularly susceptible to disparities largely because of two tensions shaped by the incentives of different stakeholders First individuals who gain insight about the institutions decision-making processes might have disproportionate recourse abilities based on their current financial status and access to opportunity Since financial institutions are typically for-profit organizations aiming to maximize their utility Milli et al note that a lack of strategy-proofness can disproportionately harm disadvantaged groups in the population The second tension arises from the fact that financial institutions are asked to find a balance between transparency towards customers and protection of their intellectual property Tsirtsis and Gomez-Rodriguez argue that maximizing utility in this context may provide limited recourse to disadvantaged populations and they propose methods to counteract such disparities Those tensions reinforce the need to incorporate the study of incentives in automated decision-making systems before they can be effectively used in financial environments As Saunders concludes their report The key to FinTech is Understand first Proceed with caution Social networks Social networks have received scrutiny in the way they reinforce patterns of social inequality and discrimination Inequality at the level of individual connections is often reinforced by algorithms that use these connections for learning in opinion diffusion recommendation clustering and others Often such inequality arises from the individual preference for establishing new connections as well as from pre-defined communities Recent papers discuss these patterns through the lens of welfare economics or equilibrium strategies with Avin et al analyzing the utility function for which preferential attachment is the unique equilibrium solution in a social network Thus understanding the incentives behind network creation patterns is crucial for designing better algorithms that learn from relational data and tackling bias at its root cause as Section teaches us Beyond this several works argue that ranking and retrieval algorithms not only reinforce existing bias but also cause changes in peoples behavior To tackle this a recent line of work takes into consideration the post-ranking and post-recommendation effects in a game-theoretical framework considering users as players and assigning highly ranked/recommended items to a high pay-off The lesson from Section of modeling individuals as rational agents has started a whole subfield in recommendation systems starting with Bahar et al who focus on finding stable equilibria for which users get the best pay-off for their desired items Ben-Porat and Tennenholtz propose new methods such as the Shapley mediator to fulfill both fairness and stability conditions as defined by mechanism design in cases where content providers are strategic to maximize utility and assume a rational behavior of their users based on their preferences To account for the incentives of users in post-recommendation settings Basat et al Ben Basat et al account for users attempting to promote their own content in information retrieval describing it as an adversarial setting The main results point to an increase in general utility when accounting for such incentives as non-strategic design presents limitations in truly fulfilling individual preferences Mladenov et al directly tackle the problem of welfare by considering recommendations as a resource to be allocated Incorporating the preferences of the users of a social network in a fair way is thus a subsequent question Recent works tackle this by adapting tools from social choice theory specifically by proposing a voting mechanism called Single Transferable Vote to aggregate inferred preferences votes of users and achieve better recommendations This kind of tools can be used to operate in adversarial settings for example in non-personalised recommendation systems like Twitter or Youtube trending topics which can be manipulated by flooding the network with bot-created content that can become viral Methods from mechanism design can be used to protect against strategic behavior that could game the underlying machine learning system as well as incorporate individual preferences in a meaningful way CONCLUSION While the literature is rapidly growing many open questions at the intersection of mechanism design and machine learning remain motivating the need for developing a lingua franca of fairness identifying knowledge gaps and lessons and ultimately bridging the two fields to work towards a fair pipeline in decision making However both communities must acknowledge that making the pipeline fair from a technical perspective does not mean the system is ipso facto perfect or just More interdisciplinary work is needed beyond mechanism design and machine learning to create interventions that improve access to sociotechnical systems and design algorithms for critical application domains