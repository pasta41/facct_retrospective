Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images We nd that state-of-the-art unsupervised models trained on ImageNet a popular benchmark image dataset curated from internet images automatically learn racial gender and intersectional biases We replicate previously documented human biases from social psychology from the innocuous as with insects and flowers to the potentially harmful as with race and gender Our results closely match three hypotheses about intersectional bias from social psychology For the rst time in unsupervised computer vision we also quantify implicit human biases about weight disabilities and several ethnicities When compared with statistical patterns in online image datasets our endings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web CONCEPTS Computing methodologies Unsupervised learning Transfer learning Machine learning KEYWORDS implicit bias unsupervised learning computer vision INTRODUCTION Can machines learn social biases from the way people are portrayed in image datasets Companies and researchers regularly use machine learning models trained on massive datasets of images scraped FAccT March Virtual Event Canada ACM ISBN Figure Unilever using AI-powered job candidate assessment tool HireVue from the web for tasks from face recognition to image classification To reduce costs many practitioners use state-of-the-art models pre-trained on large datasets to help solve other machine learning tasks a powerful approach called transfer learning For example HireVue used similar state-of-the-art computer vision and natural language models to evaluate job candidates video interviews potentially discriminating against candidates based on race gender or other social factors In this paper we show how models trained on unlabeled images scraped from the web embed human-like biases including racism and sexism Where most bias studies focus on supervised machine learning models we seek to quantify learned patterns of implicit social bias in unsupervised image representations Studies in supervised computer vision have highlighted social biases related to race gender ethnicity sexuality and other identities in tasks including face recognition object detection image search and visual question answering These algorithms are used in important real-world settings from applicant video screening to autonomous vehicles but their harmful downstream effects have been documented in applications such as online ad delivery and image captioning Our work examines the growing set of computer vision methods in which no labels are used during model training Recently pretraining approaches adapted from language models have dramatically increased the quality of unsupervised image representations With fine-tuning practitioners can pair these general-purpose representations with labels from their domain to accomplish a variety of supervised tasks like face recognition or image captioning We hypothesize that like their counterparts in language these unsupervised image representations also contain human-like social biases and these biases correspond to stereotypical portrayals of social group members in training images Results from natural language support this hypothesis Several studies show that word embeddings or representations learned automatically from the way words co-occur in large text corpora exhibit human-like biases Word embeddings acquire these biases via statistical regularities in language that are based on the co-occurrence of stereotypical words with social group signals Recently new deep learning methods for learning context-specic representations sharply advanced the state-of-the-art in natural language processing NLP Embeddings from these pre-trained models can be fine-tuned to boost performance in downstream tasks such as translation As with static embeddings researchers have shown that embeddings extracted from contextualized language models also exhibit downstream racial and gender biases Recent advances in NLP architectures have inspired similar unsupervised computer vision models We focus on two state-of-the-art pre-trained models for image representation iGPT and SimCLRv We chose these models because they hold the highest fine-tuned classification scores were pre-trained on the same large dataset of Internet images and are publicly available iGPT or Image GPT borrows its architecture from GPT- a state-of-the-art unsupervised language model iGPT learns representations for pixels rather than for words by pre-training on many unlabeled images SimCLRv uses deep learning to construct image representations from ImageNet by comparing augmented versions of the training images Do these unsupervised computer vision models embed human biases like their counterparts in natural language If so what are the origins of this bias In NLP embedding biases have been traced to word co-occurrences and other statistical patterns in text corpora used for training Both our models are pre-trained on ImageNet the most widely-used dataset of curated images scraped from the web In image datasets and image search results researchers have documented clear correlations between the presence of individuals of a certain gender and the presence of stereotypical objects for instance the category male co-occurs with career and oce related content such as ties and suits whereas female more often co-occurs with flowers in casual settings As in NLP we expect that these patterns of bias in the pre-training dataset will result in implicitly embedded bias in unsupervised models even without access to labels during training This paper presents the Image Embedding Association Test iEAT the rst systematic method for detecting and quantifying social bias learned automatically from unlabeled images We nd statistically significant racial gender and intersectional biases embedded in two state-of-the-art unsupervised image models pre-trained on ImageNet iGPT and SimCLRv We test for previously documented human and machine biases that have been studied for decades and validated in social psychology and conduct the rst machine replication of Implicit Association Tests IATs with picture stimuli In tests our machine results match documented human biases including of biases also found in large language models The tests which did not show significant human-like biases are from IATs with only small samples of picture stimuli With novel tests we show how embeddings from our model confirm several hypotheses about intersectional bias from social psychology We compare our results to statistical analyses of race and gender in image datasets Unsupervised models seem to learn bias from the ways people are commonly portrayed in images on the web We present a qualitative case study of how image generation a downstream task utilizing unsupervised representations exhibits a bias towards the sexualization of women RELATEDWORK Various tests have been constructed to quantify bias in unsupervised natural language models but to our knowledge there are no principled tests for measuring bias embedded in unsupervised computer vision models Wang et al develop a method to automatically recognize bias in visual datasets but still rely on human annotations Our method uses no annotations whatsoever In NLP there are several systematic approaches to measuring unsupervised bias in word embeddings Most of these tests take inspiration from the well-known IAT Participants in the IAT are asked to rapidly associate stimuli or exemplars representing two target concepts eg flowers and insects with stimuli representing evaluative attributes eg pleasant and unpleasant attribute Assuming that the cognitive association task is easier when the strength of implicit association between the target concept and attributes is high the IAT quantifies bias as the latency of response or the rate of classification error Stimuli may take the form of words pictures or even sounds and there are several IATs with picture-only stimuli Notably Caliskan et al adapt the heavily-validated IAT from social psychology to machines by testing for the mathematical association of word embeddings rather than response latency They present a systematic method for measuring language biases associated with social groups the Word Embedding Association Test WEAT Like the IAT the WEAT measures the effect size of bias in static word embeddings by quantifying the relative associations of two sets of target stimuli eg woman female and man male that represent social groups with two sets of evaluative attributes eg science mathematics and arts literature For validation two WEATs quantify associations towards flowers vs insects and towards musical instruments vs weapons both accepted baselines Greenwald et al Greenwald et al refer to these baseline biases as universally accepted stereotypes since they are widely shared across human subjects and are not potentially harmful to society Other WEATs measure social group biases such as sexist and racist associations or negative attitudes towards the elderly or people with disabilities In any modality implicit biases can potentially be prejudiced and harmful to society If downstream applications use these representations to make consequential decisions about human beings such as automated video job interview evaluations machine learning may perpetuate existing biases and exacerbate historical injustices The original WEAT uses static word embedding models such as wordvec and GloVe each trained on Internet-scale corpora composed of billions of tokens Recent work extends the WEAT to contextualized embeddings dynamic representations based on the context in which a token appears May et al insert targets and attributes into sentences like This is an word and applying WEAT to the vector representation for the whole sentence with the assumption that the sentence template used is semantically bleached such that the only meaningful content in the sentence is the inserted word Tan and Celis extract the contextual word representation for the token of interest before pooling to avoid confounding effects at the sentence level in contrast Bommasani et al nd that pooling tends to improve representational quality for bias evaluation Guo and Caliskan dispense with sentence templates entirely pooling across word-level contextual embeddings for the same token extracted from random sentences Our approach is closest to these latter two methods though we pool over images rather than words APPROACH In this paper we adapt bias tests designed for contextualized word embeddings to the image domain While language transformers produce contextualized word representations to solve the next token prediction task an image transformer model like iGPT generates image representations to solve the next pixel prediction task Unlike words and tokens pixels do not explicitly correspond to semantic concepts objects or categories as words do In language a single token eg love corresponds to the target concept or attribute eg pleasant But in images no single pixel corresponds to a semantically meaningful concept To address the abstraction of semantic representation in the image domain we propose the Image Embedding Association Test iEAT which modifies contextualized word embedding tests to compare pooled image-level embeddings The goal of the iEAT is to measure the biases embedded during unsupervised pre-training by comparing the relative association of image embeddings in a systematic process Chen et al and Chen et al show through image classification that unsupervised image features are good representations of object appearance and categories we expect they will also embed information gleaned from the common co-occurrence of certain objects and people and therefore contain related social biases Our approach is summarized in Figure The iEAT uses the same formulas for the test statistic effect size and -value as the WEAT described in Section Section summarizes our approach to replicating several different IATs Section describes several novel intersectional iEATs Section describes our test statistic drawn from embedding association tests like the WEAT Replication of Bias Tests In this paper we validate the iEAT by replicating as closely as possible several common IATs These tests fall into two broad categories valence tests in which two target concepts are tested for association with pleasant and unpleasant images and stereotype tests in which two target concepts are tested for association with a pair of stereotypical attributes eg male vs female career vs family To closely match the ground-truth human IAT data and validate our method our replications use the same concepts as the original IATs listed in Table Because some IATs rely on verbal stimuli we adapt them to images using image stimuli from the IATs when available When no previous studies use image stimuli we map the non-verbal stimuli to images using the data collection method described in Section Figure Example iEAT replication of the Insect-Flower IAT which measures the differential association between flowers vs insects and pleasantness vs unpleasantness Many of these bias tests have been replicated for machines in the language domain for the rst time we also replicate tests with image-only stimuli including the Asian and Native American IATs Most of these tests were originally administered in controlled laboratory settings and all except for the Insect-Flower IAT have also been tested on the Project Implicit website at projectimplicitorg Project Implicit has been available worldwide for over years in the site had collected more than million IATs The average effect sizes which are based on samples so large the power is nearly for these tests are reproduced in Table To establish a principled methodology all the IAT verbal and original image stimuli for our bias tests were replicated exactly from this online IAT platform We will treat these results along with the laboratory results from the original experiments as ground-truth for human biases that serve as validation benchmarks for our methods Section Intersectional iEATs We also introduce several new tests for intersectional valence bias and bias at the intersection of gender stereotypes and race Intersectional stereotypes are often even more severe than their constituent stereotypes Following Tan and Celis we anchored comparison on White males the group with the most representation and compared against White females Black males and Black females respectively Table Drawing on social psychology we pose three hypotheses about intersectional bias Intersectionality hypothesis tests at the intersection of gender and race will reveal emergent biases not explained by the sum of biases towards race and gender alone Race hypothesis biases between racial groups will be more similar to differential biases between the men than between the women Gender hypothesis biases between men and women will be most similar to biases between White men and White women Embedding Association Tests Though our stimuli are images rather than words we can use the same statistical method for measuring biased associations between image representations to quantify a standardized effect size of bias We follow Caliskan et al in describing the WEAT here Let and be two sets of target concepts embeddings of size C and let and be two sets of attribute embeddings of size For example the Gender-Career IAT tests for the differential association between the concepts male and female and the attributes career and family Generally experts in social psychology and cognitive science select stimuli that are typically representative of various concepts In this case contains embeddings for verbal stimuli such as boy father and man while contains embeddings for verbal stimuli like oce and business These linguistic visual and sometimes auditory stimuli are proxies for the aggregate representation of a concept in cognition Embedding association tests use these unambiguous stimuli as semantic representations to study biased associations between the concepts being represented Since the stimuli are chosen by experts to most accurately represent concepts they are not polysemous or ambiguous tokens We use these expert-selected stimuli as the basis for our tests in the image domain The test statistic measures the differential association of the target concepts and with the attributes and B G B G B where B is the differential association of with the attributes quantified by the cosine similarity of vectors B mean mean We test the significance of this association with a permutation test over all possible equal-size partitions to generate a null hypothesis as if no biased associations existed The one-sided -value measures the unlikelihood of the null hypothesis A B B and the effect size a standardized measure of the separation between the relative association of and with and is mean G B G mean B std B A larger effect size indicates a larger differential association for instance the large effect size in Table for the gender-career bias example above indicates that in human respondents male is strongly associated with career attributes compared to female which is strongly associated with family attributes Note that these effect sizes cannot be directly compared to effect sizes in human IATs but the significance levels are uniformly high Human IATs measure individual peoples associations embedding association tests measure the aggregate association in the representation space learned from the training set In general significance increases with We use an exact non-parametric permutation test over all possible partitions There are no normality assumptions about the distribution of the null hypothesis the number of stimuli an insignificant result does not necessarily indicate a lack of bias One important assumption of the iEAT is that categories can be meaningfully represented by groups of images such that the association bias measured refers to the categories of interest and not some other similar-looking categories Thus a positive test result indicates only that there is an association bias between the corresponding samples sets of target images and attribute images To generalize to associations between abstract social concepts requires that the samples adequately represent the categories of interest Section details our procedure for selecting multiple representative stimuli following validated approaches from prior work We use an adapted version of May et al s Python WEAT implementation All code pre-trained models and data used to produce the figures and results in this paper can be accessed at githubcom/ryansteed/ieat COMPUTER VISION MODELS To explore what kinds of biases may be embedded in image representations generated in unsupervised settings where class labels are not available for images we focus on two computer vision models published in summer iGPT and SimCLRv We extract representations of image stimuli with these two pre-trained unsupervised image representation models We choose these particular models because they achieve state-of-the-art performance in linear evaluation a measure of the accuracy of a linear image classifier trained on embeddings from each model iGPT is the rst model to learn from pixel co-occurrences to generate image samples and perform image completion tasks Pre-training Data Both models are pre-trained on ImageNet a large benchmark dataset for computer vision tasks ImageNet contains million annotated images of object classes including a person class even if the annotated object is not a person a person may appear in the image For this reason we expect the models to be capable of generalizing to stimuli containing people While there are no publicly available pre-trained models with larger training sets and the people category of ImageNet is no longer available this dataset is a widely used benchmark containing a comprehensive sample of images scraped from the web primarily Flickr We assume that the portrayals of people in ImageNet are reflective of the portrayal of people across the web at large but a more contemporary study is left to future work CIFAR- a smaller classification database was also used for linear evaluation and stimuli collection Image Representations Both models are unsupervised neither use any labels during training Unsupervised models learn to produce embeddings based on the implicit patterns in the entire training set of image features Both models incorporate neural networks with multiple hidden layers each learning a different level of abstraction and a projection layer for some downstream task For linear classification tasks features can be drawn directly from layers in the base neural network As a result there are various ways to Both models were tested on the Tensorflow version of ILSVRC available at extract image representations each encoding a different set of information We follow Chen et al and Chen et al in choosing the features for which linear evaluation scores are highest such that the features extracted contain high-quality general-purpose information about the objects in the image Below we describe the architecture and feature extraction method for each model iGPT The Image Generative Pre-trained Transformer iGPT model is a novel NLP-inspired approach to unsupervised image representation We chose iGPT for its high linear evaluation scores minimalist architecture and strong similarity to GPT- a transformer-based architecture that has found great success in the language domain Transformers learn patterns in the way individual tokens in an input sequence appear with other tokens in the sequence Chen et al apply a structurally simple highly parameterized version of the GPT- generative language pre-training architecture to the image domain for the rst time GPT- uses the contextualized embeddings learned by a transformer to predict the next token in a sequence and generate realistic text Rather than autoregressively predict the next entry in a sequence of tokens as GPT- does iGPT predicts the next entry in a flattened sequence of pixels iGPT is trained to autoregressively complete cropped images and feature embeddings extracted from the model can be used to train a state-of-the-art linear classifier We use the largest open-source version of this model iGPT-L x with layers and embedding size All inputs are restricted to x pixels the largest model which takes x input is not available to the public Original code and checkpoints for this model were obtained from its authors at githubcom/openai/imagegpt iGPT is composed of blocks layer_norm multihead_attention mlplayer_norm where is the input tensor to the th block In the final layer called the projection head Chen et al learn a projection from layer_norm to a set of logits parameterizing the conditional distributions across the sequence dimension Because this final layer is designed for autoregressive pixel prediction the final layer may not contain the optimal representations for object recognition tasks Chen et al obtain the best linear classification results using embeddings extracted from a middle layer specifically somewhere near the th layer A linear classifier trained on these features is much more accurate than one trained on the next-pixel embeddings Such high-quality features from the middle of the network are obtained by average-pooling the layer norm across the sequence dimension i Chen et al then learn a set of class logits from for their fine-tuned supervised linear classifier but we will just use the embeddings In general we prefer these embeddings over embeddings from other layers for two reasons they can be more closely compared to the SimCLRv embeddings which are also optimal for fine-tuning a linear classifier we hypothesize that embeddings with higher linear evaluation scores will also be more likely to embed biases since stereotypical portrayals typically incorporate certain objects and scenes eg placing men with sports equipment In Appendix C we try another embedding extraction strategy and show that this hypothesis is correct SimCLR The Simple Framework for Contrastive Learning of Visual Representations SimCLR is another state-of-the-art unsupervised image classifier We chose SimCLRv because it has a state-of-the-art open source release and for variety in architecture unlike iGPT SimCLRv utilizes a traditional neural network for image encoding ResNet SimCLRv extracts representations in three stages data augmentation random cropping random color distortions and Gaussian blur an encoder network ResNet mapping to a latent space for contrastive learning which maximizes agreement between the different augmented views These representations can be used to train state-of-the-art linear image classifiers We use the largest pre-trained open-source version the model with the highest linear evaluation scores of SimCLRv obtained from its authors at githubcom/google-research/simclr This pretrained model uses a -layer ResNet with width and selective kernels which have been shown to increase linear evaluation accuracy and it was also pre-trained on ImageNet As with iGPT we extract the embeddings identified by Chen et al as high-quality features for linear evaluation Following let G and G be two data augmentations random cropping random color distortion and random Gaussian blur of the same image The base encoder network is a network of layers G ResNet G where R is the output after the average pooling layer During pre-training SimCLRv utilizes an additional layer a projection head that maps to a latent space for contrastive loss The contrastive loss function can be found in After pre-training Chen et al discard the projection head using the average pool output for linear evaluation Note that the projection head is still necessary for pre-training high-quality representations it improves linear evaluation accuracy by over but Chen et al nd that training on rather than I also improves linear evaluation accuracy by more than We follow suit using the average pool output of ResNet to represent our image stimuli which has dimensionality High dimensionality is not a great obstacle association tests have been used with embeddings as large as dimensions STIMULI To replicate the IATs we systematically compiled a representative set of image stimuli for each of the concepts or categories listed in Table Rather than attempting to specify and justify new constructs we adhere as closely as possible to stimuli defined and employed by well-validated psychological studies For each category eg male or science in each IAT eg Gender-Science we drew representative images from either the original IAT stimuli if the IAT used picture stimuli the CIFAR- dataset or a Google Image Search This section describes how we obtained a set of images that meaningfully represent some target concept eg male or attribute eg science as it is normally or predominantly portrayed in society and on the web We follow the stimuli selection criteria outlined in foundational prior work to collect the most typical and accurate exemplars For picture-IATs with readily available image stimuli we accept those stimuli as representative and exactly replicate the IAT conditions with two exceptions the weapon-tool IAT picture stimuli include outdated objects eg cutlass Walkman so we chose to collect an additional modernized set of images the disability IAT utilizes abstract symbols so we collected a replacement set of images of real people for consistency with the training set For IATs with verbal stimuli we use Google Image Search as a proxy for the predominant portrayal of words expressed as search terms on the web described in Section Human IATs employ the same philosophy for example the Gender-Science IAT uses common European American names to represent male and female because the majority of names in the US are European American We follow the same approach in replicating the human IATs for machines in the vision domain One consequence of the stimuli collection approach outlined in Section is that our test set will be biased towards certain demographic groups just as the Human IATs are biased towards European American names For example Kay et al showed that in search results for powerful occupations like CEO systematically under-represented women In a case like this we would expect to underestimate bias towards minority groups For example since we expect Gender-Science biases to be higher for non-White women a test set containing more White women than non-White would exhibit lower overall bias than a test set containing an equal number of stimuli from white and non-White women Consequently tests on Google Image Search stimuli would be expected to result in under-estimated stereotype-congruent bias scores While under-representation in the test set does not pose a major issue for measuring normative concepts we cannot use the same datasets to test for intersectional bias For those iEATs we collected separate equal-sized sets of images with search terms based on the categories White male White female Black male and Black female since none of the IATs specifically target these intersectional groups Verbal to Image Stimuli One key challenge of our approach is representing social constructs and abstract concepts such as male or pleasantness in images A Google Image Search for pleasantness returns mostly cartoons and pictures of the word itself We address this difficulty by adhering as closely as possible to the verbal IAT stimuli to ensure the validity of our replication In verbal IATs this is accomplished with buckets of verbal exemplars that include a variety of common-place and easy-to-process realizations of the concept in question For example in the Gender-Science IAT the concept male is defined by the verbal stimuli man son father boy uncle grandpa husband and male To closely match the representations tested by these IATs we use these sets of words to search for substitute image stimuli that portray one of these words or phrases For the vast majority of exemplars we were able to nd direct visualizations of the stimuli as an isolated person object or scene For example Figure depicts sample image stimuli corresponding to the verbal stimuli orchid for category flower centipede insect sunset pleasant and morgue unpleasant We collected images for each verbal stimulus from either CIFAR or Google Image Search according to a systematic procedure detailed in Appendix B This procedure controls for image characteristics that might confound the category we are attempting to define eg lighting background dominant colors placement in several ways we collected more than one for each verbal stimulus in case of idiosyncrasies in the images collected for stimuli referring to an object or person we chose images that isolated the object or person of interest against a plain background unless the object filled the whole image when an attribute stimulus refers to a group of people we chose only images where the target concepts were evenly represented in the attribute images for the picture-IATs we accepted the original image stimuli to exactly reconstruct the original test conditions We also did not alter the original verbal stimuli relying instead on the construct validity of the original IAT experiments For each verbal stimulus Appendix B lists corresponding search terms and the precise number of images collected All the images used to represent the concepts being tested are available at githubcom/ryansteed/ieat Choosing Valence Stimuli Valence the intrinsic pleasantness or goodness of things is one of the principal dimensions of affect and cognitive heuristics that shape attitudes and biases Many IATs quantify implicit bias by comparing two social groups to the valence attributes pleasant vs unpleasant Here positive valence will denote pleasantness and negative valence will denote unpleasantness The verbal exemplars for valence vary slightly from test to test Rather than create a new set of image stimuli for each valence IAT we collected one large consolidated set from an experimentally validated database of low and high valence words eg rainbow morgue commonly used in the valence IATs To quantify norms asked human participants to rate these non-social words for pleasantness and imagery in a controlled laboratory setting Because some of the words for valence do not correspond to physical objects we collected images for verbal stimuli with high valence and imagery scores We used the same procedure as for all the other verbal stimuli described above in Section The full list of verbal valence stimuli can be found in Appendix A In the original IATs the category set sizes C and range from exemplars We collected images for each exemplar such that C and are significance could be increased by including more stimuli at the risk of diluting the test set with less-representative images from farther down in the search results We rst check for test images in CIFAR- because iGPT performs well in out-of-sample linear evaluation on this dataset For example for the family attribute in the Gender-Career test we chose only images of families with equal numbers of men and women One exception the Gender-Career IAT used specific male- and female-sounding names rather than general exemplars like man or father as in the Gender-Science IAT We use the general exemplars for both tests EVALUATION We evaluate the validity of iEAT by comparing the results to human and natural language biases measured in prior work We obtain stereotype-congruent results for baseline or universal biases We also introduce a simple experiment to test how often the iEAT incorrectly finds bias in a random set of stimuli Predictive Validity We posit that iEAT results have predictive validity if they correspond to ground-truth IAT results for humans or WEAT results in word embeddings In this paper we validate the iEAT by replicating several human IATs as closely as possible as described in Section and comparing the results We nd that embeddings extracted from at least one of the two models we test display significant bias for of the ground-truth human IATs we replicate Section The insignificant biases are likely due to small sample sizes We also nd evidence supporting each of the intersectional hypotheses listed in Section which have also been empirically validated in a study with human participants Baselines As a baseline we replicate a universal bias test presented in the rst paper introducing the IAT the association between flower vs insects and pleasant vs unpleasant If human-like biases are encoded in unsupervised image models we would expect a strong and statistically significant flower-insect valence bias for two reasons as Greenwald et al conjecture this test measures a close-to-universal baseline human bias our models described in Section achieve state-of-the-art performance when classifying simple objects including flowers and bees The presence of universal bias and absence of random bias suggests our conclusions are valid for other social biases specificity Prior work on embedding association tests does not evaluate the false positive rate To validate the specificity of our significance estimation we created random partitions of from the flower-insect test to evaluate true positive detection Our false positive rate is roughly bounded by the -value of these random tests resulted in a false positive at were statistically significant false positives at EXPERIMENTS AND RESULTS In correspondence with the human IAT we nd several significant racial biases and gender stereotypes including intersectional biases shared by both iGPT and SimCLRv when pre-trained on ImageNet iEATs effect sizes and -values from the permutation test for each bias type measurement are reported in Table and interpreted below Widely Accepted Biases First we apply the iEAT to the widely accepted baseline Insect-Flower IAT which measures the association of insects and flowers with pleasantness and unpleasantness respectively As hypothesized we nd that embeddings from both models contain significant positive biases in the same direction as the human participants associating flowers with pleasantness and insects with unpleasantness with Table Notably the magnitude of bias is greater for SimCLRv effect size than for iGPT effect size In general A linear image classifier trained on iGPT embeddings reaches accuracy on CIFAR- SimCLRv embeddings reach accuracy SimCLRv embeddings contain stronger biases than iGPT embeddings but do not contain as many kinds of bias We conjecture that because SimCLRv transforms images before training including color distortion and blurring and is more architecturally complex than iGPT its embeddings become more suitable for concrete object classification as opposed to implicit social patterns Racial Biases Both models display statistically significant racial biases including both valence and stereotype biases The racial attitude test which measures the differential association of images of European Americans vs African Americans with pleasantness and unpleasantness shows no significant biases But embeddings extracted from both models exhibit significant bias for the Arab-Muslim valence test which measures the association of images of Arab-Americans vs others with pleasant vs unpleasant images Also embeddings extracted with iGPT exhibit strong bias large effect size effect size for the Skin Tone test which compares valence associations with faces of lighter and darker skin tones These endings relate to anecdotal examples of software that claim to make faces more attractive by lightening their skin color Both iGPT and SimCLRv embeddings also associate White people with tools and Black people with weapons in both classical and modernized versions of the Weapon IAT Gender Biases There are statistically significant gender biases in both models though not for both stereotypes we tested In the Gender-Career test which measures the relative association of the category male with career attributes like business and oce and the category female with family-related attributes like children and home embeddings extracted from both models exhibit significant bias iGPT effect size SimCLRv effect size This ending parallels Kay et al s observation that image search results for powerful occupations like CEO systematically under-represented women In the Gender-Science test which measures the association of male with science attributes like math and engineering and female with liberal arts attributes like art and writing only iGPT displays significant bias effect size Other Biases For the rst time we attempt to replicate several other tests measuring weight stereotypes and attitudes towards the elderly or people with disabilities iGPT displays an additional bias effect size towards the association of thin people with pleasantness and overweight people with unpleasantness We found no significant bias for the Native American or Asian American stereotype tests the Disability valence test or the Age valence test For reference significant age biases have been detected in static word embeddings the others have not been tested because they use solely image stimuli Likely the target sample sizes for these tests are too low all three of these tests use picture stimuli from the original IAT which are all limited to fewer than images Replication with an augmented test set is left to future work Note that lack of significance in a test even if the sample size is sufficiently large does not indicate the embeddings from either model are definitively bias-free While these tests did not confirm known human biases regarding foreigners people with disabilities and the elderly they also did not contradict any known human-like biases Table iEAT tests for the association between target concepts vs represented by C images each and attributes vs represented by images each in embeddings generated by an unsupervised model effect sizes represent the magnitude of bias colored by conventional small medium and large Permutation -values indicate significance Reproduced from Nosek et al the original human IAT effect sizes are all statistically significant with they can be compared to our effect sizes in sign but not in magnitude C Model iEAT iEAT IAT Age Young Old Pleasant Unpleasant iGPT SimCLR Arab-Muslim Other Arab-Muslim Pleasant Unpleasant iGPT SimCLR Asian European American Asian American American Foreign iGPT SimCLR Disability Disabled Abled Pleasant Unpleasant iGPT SimCLR Gender-Career Male Female Career Family iGPT SimCLR Gender-Science Male Female Science Liberal Arts iGPT SimCLR Insect-Flower Flower Insect Pleasant Unpleasant iGPT SimCLR Native European American Native American US World iGPT SimCLR Race European American African American Pleasant Unpleasant iGPT SimCLR Religion Christianity Judaism Pleasant Unpleasant iGPT SimCLR Sexuality Gay Straight Pleasant Unpleasant iGPT SimCLR Skin-Tone Light Dark Pleasant Unpleasant iGPT SimCLR Weapon White Black Tool Weapon iGPT SimCLR Weapon Modern White Black Tool Weapon iGPT N/A SimCLR N/A Weight Thin Fat Pleasant Unpleasant iGPT Originally a picture-IAT image-only stimuli Originally a mixed-mode IAT image and verbal stimuli SimCLR Intersectional Biases Intersectional Valence Intersectional valence tests with the iGPT embeddings are the most consistent with social psychology exhibiting results predicted by the intersectionality race and gender hypotheses listed in Section Overall iGPT embeddings contain a positive valence bias towards White people and a negative valence bias towards Black people effect size as in the human Race IAT As predicted by the race hypothesis the same bias is significant but less severe for both White males vs Black males iGPT effect size and White males vs Black females iGPT effect size and the White female vs Black female bias is insignificant in general race biases are more similar to the race biases between men We hypothesize that as in text corpora computer vision datasets are dominated by the majority social groups men and White As predicted by the gender hypothesis our results also conform with the theory that females are associated with positive valence when compared to males but only when those groups are White iGPT effect size there is no significant valence bias for Black females vs Black males This insignificant result might be due to the under-representation of Black people in the visual embedding space The largest differential valence bias of all our tests emerges between White females and Black males White females are associated with pleasant valence and Black males with negative valence iGPT effect size Intersectional Stereotypes We nd significant but contradictory intersectional differences in gender stereotypes Table For Gender-Career stereotypes the iGPT-encoded bias for White males vs Black females is insignificant though there is a bias effect size for male vs female in general There is significant Gender-Career stereotype bias between embeddings of White males vs White females iGPT effect size even higher than the general case this result conforms to the race hypothesis which predicts gender stereotypes are more similar to the stereotypes between Whites than between Blacks The career-family bias between White males and Black males is reversed embeddings for images of Black males are more associated with career and images of White men with family iGPT effect size One explanation for this result is under-representation there are likely fewer photos depicting Black men with non-stereotypical male attributes Table iEAT tests for the association between intersectional group vs represented by C images each and attributes vs represented by images each in embeddings produced by an unsupervised model effect sizes represent the magnitude of bias colored by conventional small medium and large Permutation -values indicate significance C Gender-Career MF Male Female Career Family Gender-Career White Male Black Female Gender-Career Black Male White Male Career Family Gender-Career White Male White Female Gender-Science MF Male Female Science Liberal Arts Gender-Science White Male Black Female Gender-Science White Male Black Male Science Liberal Arts Gender-Science White Male White Female Valence Black Female Black Male Pleasant Unpleasant Valence White Black Valence FM Female Male Pleasant Unpleasant Valence White Female Black Female Valence White Female Black Male Pleasant Unpleasant Valence White Male Black Female Valence White Male Black Male Pleasant Unpleasant Valence White Female White Male Unexpectedly the intersectional test of male vs female with equal representation for White and Black people reports no significant Gender-Science bias though the normative test with unequal representation does Table Nevertheless race-science stereotypes do emerge when White males are compared to Black males iGPT effect size and to an even greater extent when White males are compared to Black females iGPT effect size confirming the intersectional hypothesis But visual Gender-Science biases do not conform to the race hypothesis the gender stereotype between White males and White females is insignificant though the overall male vs female bias is not Origins of Bias Bias in Web Images Do these results correspond with our hypothesis that biases are learned from the co-occurrence of social group members with certain stereotypical or high-valence contexts Both our models were pre-trained on ImageNet which is composed of images collected from Flickr and other Internet sites Yang et al show that the ImageNet categories unequally represent race and gender for instance the groom category may contain mostly White people Under-representation in the training set could explain why for instance White people are more associated with pleasantness and Black people with unpleasantness There is a similar theory in social psychology most bias takes the form of in-group favoritism rather than out-group derogation In image datasets favoritism could take the form of unequal representation and have similar effects For example one of the exemplars for pleasantness is wedding a positive-valence high imagery word if White people appear with wedding paraphernalia more often than Black people they could be automatically associated with a concept like pleasantness even though no explicit labels for groom and White are available during training Likewise the portrayal of different social groups in context may be automatically learned by unsupervised image models Wang et al nd that in OpenImages also scraped from Flickr a similar benchmark classification dataset a higher proportion of female images are set in the scene home or hotel than male images male is more often depicted in industrial and construction scenes This difference in portrayal could account for the GenderCareer biases embedded in unsupervised image embeddings In general if the portrayal of people in Internet images reflects human social biases that are documented in cognition and language we conclude that unsupervised image models could automatically learn human-like biases from large collections of online images Bias in Autoregression Though the next-pixel prediction features contained very little significant bias they may still propagate stereotypes in practice For example the incautious and unethical application of a generative model like iGPT could produce biased depictions of people As a qualitative case study we selected male and female-appearing artificial faces from a database generated with StyleGAN We decided to use images of non-existent people to avoid perpetuating any harm to real individuals We cropped the portraits below the neck and used iGPT to generate different completions with the temperature hyperparameter set to following Chen et al We found that completions of woman and men are often sexualized for female faces of completions featured a bikini or low-cut top for male faces of completions were shirtless or wore low-cut tops while wore suits or other career-specific attire One held a gun This behavior might result from the sexualized portrayal of people especially women in internet images and serves as a reminder of computer visions controversial history with Playboy centerfolds and objectifying images To avoid promoting negative biases Figure shows only an example of male-career associations in completions of a GAN-generated face DISCUSSION By testing for bias in unsupervised models pre-trained on a widely used large computer vision dataset we show how biases may a Cropped image of an artificial White male face b random autoregressive completions of the cropped image depict career-related attire Figure Example of career associations in image completion of a male face with iGPT pre-trained on ImageNet be learned automatically from images and embedded in general-purpose representations Not only do we observe human-like biases in the majority of our tests but we also detect of the human biases replicated in natural language Caliskan et al show that artifacts of the societal status quo such as occupational gender statistics are imprinted in online text and mimicked by machines We suggest that a similar phenomenon is occurring for online images One possible culprit is confirmation bias the tendency of individuals to consume and produce content conforming to group norms Self-supervised models exhibit the same tendency In addition to confirming human and natural language machine biases in the image domain the iEAT measures visual biases that may implicitly affect humans and machines but cannot be captured in text corpora Foroni and Bel-Bahar conjecture that in humans picture-IATs and word-IATs measure different mental processes More research is needed to explore biases embedded in images and investigate their origins as Brunet et al suggest for language models Tenney et al show that contextual representations learn syntactic and semantic features from the context Voita et al explain the change of vector representations among layers based on the compression/prediction trade-off perspective Advances in this direction would contribute to our understanding of the causal factors behind visual perception and biases related to cognition and language acquisition Our methods come with some limitations The biases we measure are in large part due to patterns learned from the pre-training data but ImageNet does not necessarily represent the entire population of images currently produced and circulated on the Internet Additionally ImageNet is intended for object detection not distinguishing peoples social attributes and both our models were validated for non-person object classification The largest version Recently Yang et al proposed updates to improve fairness and representation in the ImageNet person category that could change our results of iGPT not publicly available was pre-trained on million additional web images Given the financial and carbon costs of the computation required to train highly parameterized models like iGPT we did not train our own models on larger-scale corpora Complementary iEAT bias testing with unsupervised models pretrained on an updated version of ImageNet could help quantify the effectiveness of dataset de-biasing strategies A model like iGPT pre-trained on a more comprehensive private dataset from a platform like Instagram or Facebook could encode much more information about contemporary social biases Clearview AI reportedly scraped over billion images from Facebook YouTube and millions of other sites for their face recognition model Dosovitskiy et al recently trained a very similar transformer model on Googles JFT-M a million image dataset scraped from the web Further research is needed to determine how architecture choices affect embedded biases and how dataset filtering and balancing techniques might help Previous metric-based and adversarial approaches generally require labeled datasets Our method avoids the limitations of laborious manual labeling Though models like these may be useful for quantifying contemporary social biases as they are portrayed in vast quantities of images on the Internet our results suggest the use of unsupervised pre-training on images at scale is likely to propagate harmful biases Given the high computational and carbon cost of model training at scale transfer learning with pre-trained models is an attractive option for practitioners But our results indicate that patterns of stereotypical portrayal of social groups do affect unsupervised models so careful research and analysis are needed before these models make consequential decisions about individuals and society Our method can be used to assess task-agnostic biases contained in a dataset to enhance transparency but bias mitigation for unsupervised transfer learning is a challenging open problem CONCLUSIONS We develop a principled method for measuring bias in unsupervised image models adapting embedding association tests used in the language domain With image embeddings extracted by state-of-the-art unsupervised image models pre-trained on ImageNet we successfully replicate validated bias tests in the image domain and document several social biases including severe intersectional bias Our results suggest that unsupervised image models learn human biases from the way people are portrayed in images on the web These endings serve as a caution for computer vision practitioners using transfer learning pre-trained models may embed all types of harmful human biases from the way people are portrayed in training data and model design choices determine whether and how those biases are propagated into harms downstream