Reasons Values Stakeholders A Philosophical Framework for Explainable Artificial Intelligence The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions such as welfare allocation and criminal justice have generated a lively debate among multiple stakeholders including computer scientists ethicists social scientists policy makers and end users However the lack of a common language or a multi-dimensional framework to appropriately bridge the technical epistemic and normative aspects of this debate nearly prevents the discussion from being as productive as it could be Drawing on the philosophical literature on the nature and value of explanations this paper offers a multifaceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems Computing methodologies Philosophical/theoretical foundations of artificial intelligence Machine learning Human-centered computing KEYWORDS Explainable AI Explainable Artificial Intelligence Explainable Machine Learning Interpretable Machine Learning Ethics of AI Ethical AI Machine learning Philosophy of Explanation Philosophy of AI