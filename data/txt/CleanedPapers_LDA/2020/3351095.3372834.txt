The relationship between trust in AI and trustworthy machine learning technologies To design and develop AI-based systems that users and the larger public can justifiably trust one needs to understand how machine learning technologies impact trust To guide the design and implementation of trusted AI-based systems this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products We start from the ABI Ability Benevolence Integrity Predictability framework augmented with a recently proposed mapping of ABI on qualities of technologies that support trust We consider four categories of trustworthiness technologies for machine learning namely these for Fairness Explainability Auditability and Safety FEAS and discuss if and how these support the required qualities Moreover trust can be impacted throughout the life cycle of AI-based systems and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle In so doing we establish the ways in which machine learning technologies support trusted AI-based systems Finally FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international principled AI policy and technology frameworks that have emerged in recent years CONCEPTS Applied computing Sociology Social and professional topics Computing technology policy Security and privacy Human and societal aspects of security and privacy Computing methodologies Artificial intelligence Machine learning KEYWORDS trust trustworthiness machine learning artificial intelligence INTRODUCTION Growing interest in ethical dimensions of AI and machine learning has led to the focus on ways of ensuring trustworthiness of current and future practices eg European Commission IBM nd The current emphasis on this area reflects recognition that maintaining trust in AI may be critical for ensuring acceptance and successful adoption of AI-driven services and products This has implications for the many AI-based services and products that are increasingly entering the market How trust is established maintained or eroded depends on a number of factors including an individuals or groups interaction with others data environments services products and factors which combine to shape an individuals perception of trustworthiness or otherwise Perceptions of trustworthiness impact on AI and consequently influence a persons decision and behaviour associated with the service or product In this paper we research the connection between trust and machine learning technologies in a systematic manner The aim is to identify how technologies impact and relate to trust and specifically identify trust-enabling machine learning technologies AI and machine learning approaches are trustworthy if they have properties that one is justified to place trust in them see for this manner of phrasing It is important to highlight the difference between studying trust in AI and studying ethics of AI and data science Trustworthy AI is related to normative statements on the qualities of the technology and typically necessitates ethical approaches while trust is a response to the technologies developed or the processes through which they were developed and may not necessarily or entirely depend on ethical considerations Ethical considerations behind the design or deployment of an AI-based product or service can impact perceptions of trust for instance if trust depends on having confidence in the service not discriminating against the trusting entity or in general However there may be cases where ethics is not a consideration for the trusting entity when placing trust in a service or more frequently if ethics is one of the many concerns the trusting entity has in mind In what follows we will also see that trust-enhancing machine learning technologies can be related to various Principled AI frameworks such as Asilomar AI Principles introduced in Montr√©al Declaration for Responsible Development of Artificial Intelligence in and IEEE Ethically Aligned Design Document The aim of this paper is to identify the ways in which classes of machine learning technologies might enhance or impact trust based on trust frameworks drawn from ethics social sciences and computing and algorithm design literature on technological trust qualities Figure outlines our approach At the centre of Figure is the end product of this paper trust-enhancing technologies in machine learning and their classification in technologies for Fairness Explainability Auditability and Safety FEAS The downwards arrows indicate that these technologies are derived from trust frameworks from social science literature particularly organisational science The upward arrow indicates that the FEAS-classification of technologies was informed by the various Principled AI frameworks that shape the ethics and policy discussion in many nations this is discussed in Section As indicated in Figure we base our discussion on the widely accepted ABI Ability Benevolence Integrity principles underlying trust as introduced by Mayer et al and extended to include Predictability by Dietz and Den Hartog aka ABI We add to this a temporal dimension from initial trust to continuous trust as discussed by Siau et al This gives us a base to understand trust in general and we augment this further by integrating Siaus perspective on trust in technology which identifies that trust is impacted by Human Environmental and Technological qualities referred to as the technologies HET qualities in what follows We will discuss these steps to go from the ABI model to HET qualities of trustworthy technologies in Section To summarise the contributions of this paper are as follows We draw on social science literature particularly from organisational science to apply established principles of trust to examine the qualities for technologies to support trust in AI-based systems primarily based on the ABI and ABI framework and the HET qualities We identify how trust can be enhanced in the various stages of an AI-based systems life-cycle specifically the design development and deployment stages We therefore introduce the concept of an AI Chain of Trust to discuss the various stages and their interrelations We introduce a FEAS Fairness Explainability Auditability Safety classification of machine learning technologies that support and enable trust and establish the relation between these trust-enhancing technologies and the HET qualities We discuss how our technology classification and trustworthy machine learning techniques relate to various Principled AI framework considered by policy makers and researchers in ethics and associated topics TRUST This section discusses trust frameworks we can use to classify and identify trustworthy machine learning technologies It discusses the top half of the paper contribution provided in Figure the box with Trust and with Trust in Technology In Section we introduce the ABI framework to describe trust in general ie not restricted to trust in technology Section reflects on trust in technology and science recognising that AI-based services are based on science and manifest as technologies The final sections provide the framework developed by Siau which includes a discussion on time-sensitivity of trust Section and recognises three types of qualities technologies may exhibit that impact trust Section Trust is discussed across many diverse social science literature leading to an abundance of definitions and frameworks available through which to examine the concept It is a concept which in everyday conversation is routinely and intuitively used and yet remains challenging to define and study Andras et al summarise some of the ways that trust has been approached across different disciplines In the social world trust is about the expectation of cooperative supportive and non-hostile behaviour In psychological terms trust is the result of cognitive learning from experiences of trusting behaviour with others Philosophically trust is the taking of risk on the basis of a moral relationship between individuals In the context of economics and international relations trust is based on calculated incentives for alternative behaviours conceptualised through game theory A comprehensive review of literature relating to trust is beyond the scope of this paper Here we focus on established models to examine the nature of trust and discuss how this relates to technology The ABI Framework Ability Benevolence Integrity and Predictability The ABI framework introduced by Mayer et al suggested that the three main attributes which will shape an assessment of the trustworthiness of a party are Ability Benevolence and Integrity The model discusses the interactional relationship between a trustor the entity that trusts and a trustee the entity to be trusted Building on the work of Mayer et al Dietz and Den Hartog outline three forms of trust trust as a belief a decision and an action While many studies have focused on trust as a belief in isolation from actions Dietz and Den Hartog regard the three forms as being the constituent parts of trust which are most usefully examined together In the ABI model ability is defined as the perception of that group of skills competencies and characteristics that enable a party to have influence within some specific domain The specific domain is crucial as assessments of a partys ability will vary according to particular tasks or contexts Benevolence is defined as the extent to which a trustee is believed to want to do good to the trustor To be considered to possess Integrity a trustee must be perceived to adhere to a set of principles that the trustor finds acceptable This requires confidence that the trustee will act in accordance to a set of principles and that those align with the values of the trustor Trustworthy Machine Learning Technologies this paper Fairness Technologies Explainability Technologies Auditability Technologies Safety Technologies Ability Integrity Benevolence Trust in Technology HET technology qualities Humane Qualities Technological Qualities Environmental Qualities Initial Trust Continuous Trust Trust ABI and ABI frameworks Principled AI Frameworks various governments and other organisations Predictability Figure The contribution of this paper identification of trust-enhancing Machine Learning technologies based on social sciences literature and relating these with Principled AI frameworks Since its inception Mayer et al s framework has been adapted and expanded to acknowledge the importance of Predictability or Reliability in shaping perceived trustworthiness Dietz and Den Hartog developed the ABI model suggesting that the four key characteristics on which judgements of trustworthiness are based are Ability Benevolence Integrity and Predictability Predictability will reinforce perceptions of the Ability Benevolence and Integrity of the trustee Considering the role of Predictability draws attention to the importance of trust being sustained overtime through ongoing relationships While each of the attributes are related and may reinforce one another they are also separable One party may trust another even if they perceive one or more of these attributes to be lacking As such trust -and trustworthiness- should not be thought of in binary terms but rather trust exists along a continuum As such evaluating one partys trust in another is more complex than a straightforward assessment of whether or not they trust that party and statements such as A trusts B are overly-simplistic instead trust is described in statements reflecting the conditional nature of trust for example A trusts B to do X or not to do Y when Z pertains p As well as being shaped by judgements of trustworthiness the act of trusting also depends on a range of external and contextual factors personal attributes and traits of the trustor An individuals predisposition or ideological position will impact on the extent to which they trust particular individuals/organisations and these positions will shape how they receive interpret and respond to information about the other party As the context changes for example relating to cultural economic political or personal developments so levels of trust and perceptions of trustworthiness also change Therefore trust is characterised as an ongoing relationship rather than a static concept Moreover trust can be strengthened or conversely weakened through interactions between trustors and trustees outcomes of trusting behaviours will lead to updating of prior perceptions of the ability benevolence and integrity of the trustee p See also Section for a discussion of the time- -sensitive nature of trust Trust in Science and Technology There is a significant body of literature in the field of Science and Technology Studies STS examining public relationships with science and technology and in particular the role of trust in these relationships In the UK House of Lords Science and Technology Committee published a landmark statement which continues to be widely cited stating that there was a crisis of trust in science This reflected wider discourses suggesting that a series of high-profile scientific controversies and scandals eg BSE thalidomide and the MMR triple vaccine together with the rapid pace of scientific progress had resulted in an erosion of public trust in science This led to considerable attention directed at improving public trust in science typically through efforts to increase public understanding of science on the assumption that where the public is sceptical or mistrusting this can be explained by ignorance or lack of understanding and as such can be corrected through better dissemination of scientific knowledge or facts However such approaches are now widely discredited as it is recognised that they overlook the role of members of the public in actively engaging with scientific knowledge rather than being passive recipients of scientific knowledge p Members of the public critically assess deconstruct question and evaluate claims to scientific knowledge in line with their own ideologies experiences and the contexts in which the information is received This active process shapes peoples trust as beliefs as well as informing the trust decisions and actions that are taken The publics relationship with science and technology is too sophisticated to be characterised by a simple trust/distrust binary relationship Rather in many cases the public adopts an ambivalent form of trust described by Wynne as an as if trust This takes account of the publics knowingly inevitable and relentlessly growing dependency upon expert institutions p With regard to AI-based technologies the dependence on knowledge and behaviours of experts is clear and trust is increasingly conditional This implies that people do not automatically have confidence in particular innovations scientists or scientific institutions but equally lack of absolute trust does not mean that innovations will be met with public opposition There can be dissonance between the trust beliefs held and the decisions and actions taken based on contextual personal or organisational factors In particular even where people do not fully trust the technology they may use a service driven by AI if they feel there is no alternative option Trust Technology Qualities Humane Environmental and Technological To further understand how technology interfaces with trust Siau et al identify qualities of technologies that relate to trust and the concepts in the ABI framework of Section The authors recognise three types of conditions to demonstrate the potential for a technology to be perceived as trustworthy humane environmental and technological qualities Humane Qualities Humane qualities refer to the actions that attract individuals possessing a risk-taking attitude The effectiveness of this quality depends on the personality type past experiences and cultural backgrounds of the individuals This is linked to the ability of the trustee to satisfy the curiosity of the trustor in testing a desired task In other words if cultural background resonates and if testing a product or service is feasible this will typically enhance trust Environmental Qualities Environmental qualities consider elements that are enforced by the qualities of the technology provider First it heavily relies on the nature of the task that the technology handles The sophistication of the task has a potential to attract trustworthiness or cause distraction The pattern of the establishment of trustworthiness differs in various places depending on the education system level of their accessibility to novel modern advancements and subtle inherent cultural backgrounds Yuki et al discussed such cultural impact on the trust establishment pattern Institutional factors are another environmental parameter for trust Siau et al defined it as the impersonal structures that enable one to act in anticipation of a successful future endeavor They collected two aspects for this concept the institutional normality and structural assurance The first deals with efficiency of the organisational structure and the later refers to the ability of an institution to fulfil the promises and its commitments Technological Qualities Finally technological qualities determine the capacity of the technology itself to deliver the outcome as promised This commitment is multi-dimensional First the technology needs to yield the results efficiently Thus it needs to establish an agreed performance metric and assure its outcome yields into the desirable range of the metric Second the technology should define concrete boundaries for their solution The user as potential trustors of the technology should be provided with enough information to infer the purpose of the technology and set their expectations sensibly based on such understanding Lastly The process of the technology outcome is another factor in trustworthiness The technology should be able to reply to potential queries about how they concluded such outcome and why it led to the such performance This aspect outlines the relation between the performance and purpose aspects Time-domain Initial and Continuous Trust A final element to support our understanding of trustworthy technologies is understanding trust as it develops over time as also discussed in Highlighting the importance of predictability in the ABI model the dynamics of relationship between trustor and trustee is an ongoing process Usually it requires initial preconditions to be satisfied provided through first impressions which is referred to as initial trust After the initial phase trust levels may change for a variety of reasons and this is referred to as continuous trust in the literature Typical examples that may impact continuous trust in AI-based services and products are data breaches privacy leaks or news items on unethical business or other practices In what follows we will consider an additional time-sensitive element for trustworthy AI-based technologies namely that of the service and product life cycle both in terms of moving between the stages of design development and deployment as well as in terms of the machine learning pipeline which includes data input algorithm selection and output presentation See the next section and particularly Section MACHINE LEARNING AND THE CHAIN OF TRUST In this section we introduce the concept of a Chain of Trust which connects trust considerations in the stages of the machine learning pipeline and when considered over time the AI-based service or product may iterate through these stages effectively expending the chain into a cycle as illustrated in Figure Before introducing the Chain of Trust in Section we briefly review the basics of machine learning as well as the notion of a machine learning pipeline Basics of Machine Learning A machine learning algorithm is basically a function as y x with exception of a few algorithms such as nearest neighbour In this equation represents the function that maps input to the output ie the machine learning model In this function denotes a set of values that is tuned for the optimal operation of is calculated based on a pre-defined loss function that measures the similarity or dissimilarity between samples The input of a model x correspond to a set of features which is a vector of values that represents to data aka dataset Finally y represents the output of the algorithm to fulfil the task that it meant to undergo ie supervised unsupervised and reinforcement learning In supervised learning the output y is meant to be an assignment of the input x to a pre-defined label set Supervised Learning algorithms are mainly used in object recognition machine translation filtering spams etc For example a fraud detection classifier would assign two labels fraud or benign to an input feature which is derived from a transaction Unsupervised Learning methods are used when the input x and output y are both unlabelled In these methods the task is to determine a function that takes x as input and detects a hidden pattern representation as output y The problems that unsupervised learning tackles include grouped a dataset based on a similarity metric aka clustering projecting data to a reduced dimension space eg PCA methods and pretraining algorithms for the other tasks ie preprocessing methods Reinforcement Learning methods maps x to a set of policies as y In these techniques determines an action an observation or a reward y that should be taken into account when situation x is observed Reinforcement learning techniques are concerned with how an agent suppose a rescue robot should behave under certain circumstances to maximise the chances of fulfilling a purpose Machine Learning Pipeline Regardless of the task the use of any machine learning algorithm implies activities in various stages called the pipeline This is depicted in Figure through the chain of circles First one collects the data from a source and store the digital representation in a database Data Collection in Figure Then such data undergoes pre-processing methods to extract certain features as x in the machine learning equation labelled Data Preparation and Feature Extraction respectively When x is ready to be processed for the supposed task the features are divided into at least two groups to attain two purposes The first group of features Training in Figure are used to tune to optimise the output y of the function The group of features for this purpose is called the training data The training process can be offline or online depending on how static the training data is with respect to the life-cycle of a model The online fashion deals with dynamic training in which the model re-tunes when new training data arrives In contrast in offline mode the training stage only operates once on a static training dataset The second group of features is used for the purpose of verifying the generalisation of the parameters when the model faces an unknown parameters when new training data appears in the process Testing in Figure Verification is done by assessing the efficiency of the performance through some chosen metric For example in classification a typical accuracy metric is the proportion of the test data that has been mapped to their original label correctly by The features and data used at the testing stage are Trustworthiness Data Collection Data Preparation Feature Extraction Training Testing Inference Figure Various stages in the machine learning pipeline motivating the notion of a Chain of Trust called test data When the model passes the verification stage with a sufficiently good performance then they are applied in the wild This stage is known as Inference in which the trained model is deployed to face unseen data In this context is fixed and y is computed for x when x is unknown ie not contained in either test and training datasets In a realworld context a machine learning system is designed and implemented for a certain use case Let us consider face recognition as an example of how the pipeline functions The recognition system is based on a machine learning algorithm x that distinguishes facial properties and maps them on a predefined set of authorised people This is the supervised learning task Before the deployment of the system a large number of facial samples is collected selected and prepared for the system For instance images with corrupt or blurred faces may be removed or the lightening of the images is re-balanced This is done in the data collection and data preparation stages of the machine learning pipeline The input to the algorithm x is a set of features derived from the facial image samples using image processing techniques This is the feature extraction stage Then the algorithm is trained optimising with a set of prelabelled facial images from the sample collection the training set After that in the testing stage the trained algorithm is tested with another unknown set of facial image samples verifying the accuracy of its recognition Finally in the inference stage the trained and tested system is deployed in a real-life setting categorising images of new faces captured in realtime In what follows we group some of the stages in the machine learning pipeline The first group of stages concerns data-centric aspects involving data collection methods pre-processing techniques and extraction of useful features for the analysis The second group of stages is model-centric the stages in Figure that deal with the tuning the model to the best performance Training stage evaluating the trained model for confirmation of the desirable performance Testing stage and deployment of the model for the real-world application Inference stage Chain of Trust We are now in a position to introduce the notion of Chain of Trust Based on the machine learning pipeline depicted in Figure it becomes clear that technologies may impact trust in the resulting service or product in various stages of the pipeline For instance better methods to clean the data during Data Preparation may avoid bias in the output of algorithms which in turn helps to enhance trust once it becomes visible to users or the public There are a number of important dimensions to the Chain of Trust each of which demonstrates the importance of continuous trust as discussed in Section Stages may impact on each other in terms of the level of trust they are able to propagate Romei et al reviewed various cases studies that led to biased decisions and analysed the causes There is an important specific case of this namely that trust impact may only manifest itself in later stages or at a later time For instance in the above example of improving the Data Preparation this enhancement will only impact trust by users if it is made visible to these users This may for instance be through better results when using the services but possibly more likely may also only become visible if news article or long-term statistics are presented to users that explain that results are say less biased and that therefore can be trusted However where trust is established or damaged based on visible outcomes at later stages the resulting levels of trust will have implications for trust in all stages of the development of future technologies The second dimension present in the Chain of Trust results from the fact that a service or product may iterate through the stages during its lifecycle possibly multiple times This occurs for instance when new data is being introduced to improve the Training and through this the Inference In this case effectively the service cycles through the chain depicted in Figure A third dimension within the notion of Chain of Trust is the development of trust through the stages of design development and deployment of the AI-based service or product Trust will be impacted by technology decisions in all stages of the lifecycle In the above examples we mainly consider the deployment stage in which trust is considered from the perspective of a running service or existing product Even simply making an AI-based service available may run the risk of introducing new biases or exacerbating existing ones because changing the way a service is offered may imply it is less useful or effective for certain groups For instance a recent study demonstrates inherent discrimination in automated mortgage advising in the FinTech industry Therefore to establish trusted AI-based solutions it will be critical to consider trust from the initial stages starting from the design of a new service or product In so doing trust is considered a priori before it is being deployed and does not come as a surprise once the service is running A final dimension to consider in the context of the Chain of Trust is that of accidents sudden breakdowns of trust or failures Typical examples of such trust failures are security breaches that impact trust reports about data loss of the service or similar services or the discovery of bias in the machine learning results that drive a service Such accidents can take place through all pipeline and life cycle stages discussed above and may have severe impact on the level of trust users place in AI-based systems However the ways in which an organisation responds to such accidents can be equally or more important for determining the impact on trust Dietz and Gillespie have shown that scandals or crises which risk damaging the reputation of an organisation can also act as catalysts for culture change bringing about and reinforcing new ethical/trustworthy practice They are opportunities to forge new relationships with stakeholders positive or negative Technology solutions that continuously monitor and possibly transparently share data about service bias are trustworthy technologies that may assist in avoiding or mitigating the impact of trust failures The Chain of Trust denotes the stages in which one can and should consider the trust qualities of machine learning technologies both for initial trust and continuous trust The chain of Trust also identifies the opportunities to maintain the trustworthiness of the system given the evolving nature of the relationship between the system and its users Finally the Chain of Trust provides guidance for experts and the public how and when to evaluate the trustworthiness of the system in relation to the ABI framework not only in the final outcome of the system but also in all the inner stages that leads to such outcome TRUST IN AI-BASED SYSTEMS We now aim to establish the connection between trust in the AI-based solution and trustworthiness of the underlying technologies We first discuss in Section various trust related issues one may encounter in AI-based services and products We then discuss in Section a variety of existing Principled AI policy and technology frameworks that have emerged in recent years Based on these frameworks we will propose in Section a technology-inspired Principled AI variant namely FEAS Technologies that is technologies for Fairness Explainability Auditability and Safety Trust Considerations in Stages of the Chain of Trust In this section we discuss trustworthiness aspects in the various stages identified in the Chain of Trust We divide the discussion in data-related concerns with the focus on data collection data pre processing and feature selection Section and model-related concerns with the focus on the stages of model training testing and inference Section Data Related Trust Concerns Data Collection and Pre-Processing Stages Data collection involves reading data from various sources reliably eg sensors to collect environmental data a smart speaker listening to audio commands or website that stores session-related data from the users browser or etc secure transmission of the data from the collection point to a machine for the purpose of storage or online analysis storing data in server The preprocessing stage includes mechanisms to clean the dataset by removing null duplicate or noisy data Usually preprocessing solutions combines with the other ones in datarelated stages One of the main trust concerns in data is in protection against privacy violations and other forms of illegitimate use of ones data Legislative support eg in Europe via GDPR regulation has an important role to play GDPR is a perfect illustration of several data-related trust issues For instance the collection process should be transparent and requires explicit consent from users for many data uses Moreover the subject keeps the right to challenge the ability to collect data has control over their collected data and maintains the right to be forgotten GDPR-type concerns and solutions provides a basis to identify technologies that enhance trust In general the perceived intrusive nature of instances of data collection has convinced common users to be more cautious in the situations that their data is being collected This is a threat to trustworthiness of a service that functions based on collected data particularly AI-based services and products Technological solutions for trustworthy data collection preprocessing as well as storage have been well established Trustworthiness usually relies on factual declaration of the good faith benevolence and abiding to it in action integrity The current trust challenges are therefore less in the development of technology solutions than in identifying ways of interacting working and regulating aspects that impact trust Model-Related Trust Concerns Feature Extraction Training Testing and Inference Stages Model-related trust concerns trust in the working of the models and algorithms This set of concerns gets to the heart of trust challenges we phase in a world in which AI becomes omni-present fundamentally changing the way people live their lives Many of the trust concerns relate to FAT a fear that algorithms may harm society or individuals because they are unfair without accountability and non-transparent As for data Section GDPR is useful as an illustration of the issues important for society GDPR enforces a requirement to explain results of AI-based solutions to end users which implies a desire for more transparent machine learning GDPR also contains substantial accountability measures to implement a mechanism to challenge the outcome of AI-based technology The question is what the role of technologies in dealing with model-related trust concerns At their core machine learning models and algorithms are optimised for accuracy of results and efficiency in obtaining these While the accuracy and speed of results might be considered to demonstrate basic functionality ability in ABI terminology Section they do not necessarily satisfy or align with other trust qualities In most cases algorithms are considered as a black-box which implies algorithms can be assessed only in relation to the outcomes they produce Assessments of the benevolence or integrity again using ABI terms can therefore only be done in indirect manners through that of the entity that develops or applies the AI-based solution Of course justifying such trust is problematic especially in the time of high profile data breaches and scandals relating to mishandling or misuse of personal data eg Facebook Cambridge Analytica In Section we will introduce a number of technologies to enhance or impact trust of two types The first type is to establish a mechanism to verify the outcomes of the model In this case an agent would be responsible to function in parallel to the model and the model outcome are not accessible until the agent and the model are both satisfied in a predefined criteria In the second type one endeavours to redesign a model or choice of algorithms into something that is inherently more trustworthy For example the design of a fair SVM model refers to embedding a fairness constraints into its definition so that the model functions with the built-in consideration of that notion This set of approaches we will discuss in detail in Section Principled AI Policy Frameworks The trust concerns discussed in the previous section are related to concerns that have been raised widely about the impact on society of the proliferation of AI This has resulted in the emergence of a large amount of policy frameworks that relate to Principled AI frameworks that is policy frameworks to enhance and regulate Fairness Accountability and Transparency of particularly AI-based services and products Principled AI frameworks are particularly relevant to trust as well Therefore as depicted in Figure the bottom box it is opportune to relate Principled AI frameworks to the trustworthy technology classification we introduce in Section It is important to note that in this paper we use FEAS to classify technologies while many of the Principled AI frameworks inform policy and do not provide much detail in terms of specifying or restricting technology implementations to achieve the policy objectives Principled AI frameworks have been introduced by various stakeholders technology companies professional standardisation governmental and legislator bodies academic researchers as illustrated by Table These Principled AI frameworks present varying sets of qualities that AI-based systems should follow to be considered trustworthy some Principled AI frameworks also apply to technologies other than AI In general these documents present high-level definitions for the objectives and qualities of the involved science and technology but do not go into the specifics of technical implementation guidelines There is emerging literature reviewing Principled AI frameworks Whittlestone et al provide a critical analysis of frameworks for ethical machine learning and highlights a number of challenges some of which require attention from a technical perspective For instance frameworks may confuse the interpretation of qualities present conflicting definitions and/or qualities may be different across different documents Particularly relevant also for the underlying technologies is that the frameworks often fail to realise dependencies between policy objectives eg addressing discrimination issues might lead to unexpected privacy leakages Current frameworks are focused on privacy transparency and fairness issues but this needs to be shifted toward understanding such tensions and re-framing core research questions In yet unpublished work Fjeld et al analyse currently available Principled AI frameworks from industry governments general public civil societies and academic bodies Table contains many of the Principled AI frameworks considered by see Section for an explanation of how we compiled Table Interestingly they recognised qualities categorised them into eight groups which are a combination of the qualities identified by Siau ie humane promotion of human values professional responsibility human control of technology and technological qualities fairness and nondiscrimination transparency and explainability safety and security accountability privacy The authors have available a graphical demonstration of their findings We note again that most of the frameworks do not focus on trust but on ethics privacy and related concerns Moreover the terms ethical and trustworthy machine learning are at times used interchangeably in these frameworks This would effectively imply that trustworthiness is achieved through abiding with the ethical concepts such as human rights or non-discrimination approaches However while ethical considerations are inevitably related to perceptions of trust ethical machine learning and trustworthy machine learning are not necessarily the same thing Specifically in terms of ABI ethical machine learning would necessarily emphasise the benevolence aspects of trust while the other two aspects critical for trust ability and integrity are insufficiently represented FEAS Fair Explainable Auditable and Safe Technologies We propose to classify trustworthy technologies in Fair Explainable Auditable and Safe Technologies FEAS This is in part motivated by a desire to align our discussion of trustworthy technologies with the Principled AI frameworks that are available in the literature as discussed in Section We stress that the implementation of such technological solutions in itself will not make the system trusted Clearly trust is only to a limited extend a technology challenge which is the reason we provide in this paper the linkage of technologies with non-technological perspectives on trust Moreover even if one considers only technology FEAS technologies are not the only ones that impact trust For instance a user-friendly Graphical User Interface or thoughtful presentation of the results through meaningful plots may have an impact on the overall trust in the system too However FEAS technologies represent technological advances focused specifically on machine-learning and are therefore the focus of our attention The question remains to what extent the implementation of FEAS technologies which aim to enhance trustworthiness actually also enhance peoples trust This is an open problem and should be investigated thoroughly in the future We converged on FEAS based on our knowledge and understanding of the technologies involved classified in manner we believe will be comprehensible illustrative and natural for technologists Note that the FEAS technological qualities are in addition to the essential technological qualities of accuracy and efficiency and performance of the algorithms without which trustworthiness is not possible Fairness Technologies technologies focused on detection or prevention of discrimination and bias in different demographics Explainability Technologies technologies focused on explaining and interpreting the outcome to the stakeholders including end-users in a humane manner Auditability Technologies technologies focused on enabling third-parties and regulators to supervise challenge or monitor the operation of the models Safety Technologies technologies focused on ensuring the operation of the model as intended in presence of active or passive malicious attacker FEAS Related to Principled AI Table provides the relationship between the FEAS technology classes and the Principled AI frameworks identified in We reviewed each of the frameworks with respect to the FEAS technology classes required to establish the qualities mentioned in the framework We mark frameworks that refer to fairness explainability safety and auditability qualities using the symbols explained in the caption of Table As one sees immediately from Table all frameworks are related to FEAS technologies and would be able to make use or even require FEAS technologies to be available Note that there is a considerable difference in the granularity of the discussions in the Principled AI frameworks compared to that of the computing literature Hence the precise technology needs for each framework would need deeper investigations and may not be completely specified within the existing framework documents For instance the policy frameworks refer to the general existence of discrimination caused by bias in machine learning algorithms but in the technological literature there are at least mathematical definitions for fairness and a wide range of solutions to prevent/detect bias in the algorithm The technology discussion in Section is therefore at a much deeper level of detail than that of the Principled AI frameworks of Table FEAS Related to Trust Qualities Table provides the relationship between trust qualities humane environmental and technological see Section and FEAS technologies Table is based on the authors understanding of the qualities and technologies the latter to be discussed deeper in Section Fairness strongly requires technologies that are strong in humane and environmental qualities as does explainability since their effectiveness strongly depends on individuals and the culture or setting Safety is dominated by technological quality associated with security and reliability of the systems TRUSTWORTHY MACHINE LEARNING TECHNOLOGIES This section discuss technologies for trustworthy machine learning using the FEAS grouping introduced in the previous section We introduce the FEAS classes discuss challenges and provide some examples of existing approaches A full review of technologies is beyond the scope of this paper Fairness Technologies This group of technologies is concerned about achieving fair nondiscriminating outcomes The ethical aspects of fairness constitute a structural assurance in the service or product that enhances or at least impacts trust Fair machine learning is a difficult challenge A first challenge is to identify if how to measure unfairness typically in terms of bias or related notions Narayanan has identified at least definitions of fairness in the literature which cannot necessarily all be obtained at the same time To enhance trust the metrics used by machine learning experts needs to relate to how it impacts trust by individuals and the public posing an additional challenge Various solutions have been proposed to establish a subset of fairness notions One approach is to reduce the bias in the dataset Table Trustworthy technology classes related to FAT frameworks no mention mentioned emphasised Framework Year Document Owner Entities Country Fairness Explainability Safety Auditability Top principles of ethical AI UNI Global Union find Switzerland Toronto Declaration Amnesty International Gov find Canada Future of work and Education For the Digital Age T Think Gov Argentina Universal Guidelines for AI The public voice coalition find Belgium Human Rights in the Age of AI Access Now Gov find United States Preparing for the Future of AI US national Science and Technology Council Gov find Acad United States Draft AI RD Guidelines Japan Government Gov Japan White Paper on AI Standardization Standards Administration of China Gov find China Statements on AI Robotics and Autonomous Systems European Group on Ethics in Science and New Technologies Gov find Acad Belgium For a Meaningful Artificial Intelligence Mission assigned by the French Prime Minister Gov find France AI at the Service of Citizens Agency for Digital Italy Gov find Italy AI for Europe European Commission Gov find Belgium AI in the UK UK House of Lords Gov find United Kingdom AI in Mexico British Embassy in Mexico City Gov Mexico Artificial Intelligence Strategy German Federal Ministries of Education Economic Affairs and Labour and Social Affairs Gov find Germany Draft Ethics Guidelines for Trustworthy AI European High Level Expert Group on AI Gov find Civ AI Principles and Ethics Smart Dubai find UAE Principles to Promote FEAT AI in the Financial Sector Monetary Authority of Singapore Gov find Singapore Tenets Partnership on AI Gov find Acad United States Asilomar AI Principles Future of Life Institute find United States The GNI Principles Global Network Initiative Gov find United States Montreal Declaration University of Montreal Gov find Civ Canada Ethically Aligned Design IEEE find United States Seeking Ground Rules for AI New York Times find United States European Ethical Charter on the Use of AI in Judicial Systems Council of Europe CEPEJ Gov France AI Policy Principles Gov find United States The Ethics of Code Sage find United States Microsoft AI Principles Microsoft find United States AI at Google Our Principles Google find United States AI Principles of Telefonica Telefonica find Spain Guiding Principles on Trusted AI Ethics Telia Company find Sweden Declaration of the Ethical Principles for AI Latam find Chile Table Trustworthy technology classes versus trust qualities less important important very important Fair Explainable Auditable Safe Humane Qualities Technological Qualities Environmental Qualities known as debiasing data However it is not sufficient or even helpful to simply ignore or remove features associated with unfairness eg gender ethnics Luong et al assigned a decision value to each data sample and adjusted this value to eliminate discrimination Kamishima et al proposed a model-specific approach by adding a regulating term for a logistic regression classifier which eventually leads to unbiased classification Other fairness mitigation solutions focus on the inference stage They enforce the output of the model to generate a specific notion of fairness ≈Ωliobaite et al identify two conditions for non-discriminating machine learning data with the same non-protected attributes should give the same outcomes and the ability to distinguish outcomes should be of the same order as the difference in the nonprotected attribute values This provides a more generic understanding helpful to design fairness technologies This paper does not aim to review all techniques but it is clear that many challenges remain in achieving fair machine learning technologies Explainability Technologies Explainability refers to relating the operation and outcomes of the model into understandable terms to a human In the machine learning literature notions of explainability transparency intelligibility comprehensibility and interpretability are often used interchangeably Lipton provides a thorough discussion on these terms and the differences He also concludes that explainability increases trust Doshi-Velez et al suggested two types of explainability namely explainability of an application eg a physician being able to understand the reasons for a classifiers medical diagnostic and explainability to understand the way in which a classifier is coming up with its outputs mainly by using intelligible models There are two main approaches in explainable solutions The first one known as ex-ante refers to the use of highly intelligible models to obtain the desired predictions The second one known as ex-post refers to the use of a second model to understand the learned models behaviour In ex-ante approach an explicit prediction function analyses feature coefficients to understand their impact over a decision decision trees or decision lists Ex-post explanations are categorised into local and global explainers The ex-post approach uses explain There is a fast growing body of literature including local explainers eg and the unified framework for local explainers which generalises these and other existing methods global explainers eg There are many challenges left with respect to explainable machine learning technologies including trading off fidelity to the original model and explainability Auditability Technologies Auditability technologies refer to methods that enable third parties to challenge the operation and outcome of a model This provides more transparency to blackbox characteristics of machine learning algorithms Enabling machine learning lineage provides insights into how they have operate which gives a greater degree of transparency compared to explainability of current processes or outcomes More specifically auditability in machine learning may involve assessing the influence of input data in the output of the model This ensures predictability a process that is also referred as decision provenance in the literature Singh et al argue that decision provenance have two objectives it should provide the history of particular data and it should provide a viewpoint on the systems behaviour and its interactions with its inner components or outside entities Much of the literature around auditability relates to data provenance research in database and cloud concepts and model provenance approaches It involves proposals on how to store provenance data summarisation of provenance data specific query language for the provenance data query explainability Natural language processing for provenance data and cryptographic solutions to verify models behaviours without exposing user privacy or revealing models intellectual properties Safety Technologies Data and the machine learning model could both be the target to adversarial operations The extent of attackers access to the data and model depends on the intention of the attack and the weaknesses in the systems architecture The attacker can execute a targeted attack to harm an individual or perform a indiscriminate attack Moreover the attacker can perform the attack stealthily for the intention of information gathering or intelligence aka exploratory attack or he/she can actively engage into the functioning of the system for the purpose of manipulation aka causative attack The security and privacy foundations of a ML model is not different from classical security model of Confidentiality Integrity and Availability CIA model We omit availability since its relevance is general not just or specifically for AI-based services Careless preparation of the stored data would leak information to an attacker but confidentiality can be enhanced in many ways for instance through approaches for differential privacy homomorphic encryption or cryptography integrated in machine learning algorithms Integrity can be enhanced by either preventing tampering such as in pre-processing or by discovering and possibly repairing tampered data These methods deal only with data but it is also relevant to consider integrity during the execution of the algorithm eg in the training stage CONCLUSION This paper established the connection between trust as a notion within the social sciences and the set of technologies that are available for trustworthy machine learning More specifically we related the ABI framework and HET technology qualities for trust with categories of machine learning technologies that enhance trustworthiness We identified four categories of technologies that need to be considered Fair Explainable Auditable and Safe FEAS technologies These need to be considered in various interrelated stages of a system life cycle each stage forming part of a Chain of Trust The paper shows a close relationship between technologies to improve the trustworthiness of AI-based systems and those that are being pursued in ethical AI and related endeavours We illustrated this by mapping of FEAS technologies on concerns in a large set of international Principled AI policy and technology frameworks