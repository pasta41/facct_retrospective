Counterfactual Risk Assessments Evaluation and Fairness Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings such as medicine criminal justice and education In each of these cases the purpose of the risk assessment tool is to inform actions such as medical treatments or release conditions often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism Problematically most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy These tools thus reflects risk under the historical policy rather than under the different decision options that the tool is intended to inform Even when tools are constructed to predict risk under a specific decision they are often improperly evaluated as predictors of the target outcome Focusing on the evaluation task in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context We introduce a new method for estimating the proposed metrics using doubly robust estimation We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold Consequently fairness-promoting methods that target parity in a standard fairness metric may and as we show empirically do induce greater imbalance in the counterfactual analogue We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice INTRODUCTION Much of the activity in using machine learning to help address societal problems focuses on algorithmic decision-making and algorithmic decision support systems In settings such as health education child welfare and criminal justice decision support systems commonly take the form of risk assessment instruments RAIs which distill rich case information into risk scores that reflects the likelihood of the case resulting in one or more adverse outcomes Prior literature has raised significant concerns regarding the fairness transparency and effectiveness of existing RAIs Yet RAIs remain very popular in practice and there is a large body of research on fairness and transparency promoting methods that seek to address some of these concerns eg This paper highlights a different issue one that has not received sufficient attention in the discussion of RAIs but that nonetheless has significant implications for fairness RAIs are typically trained and evaluated as though the task were prediction when in reality the associated decision-making tasks are often interventions Models trained and evaluated in this way answer the question What is the likelihood of an adverse outcome under the observed historical decision process Yet the question relevant to the decision maker is What is the likelihood of an adverse outcome under the proposed decision When decisions do not impact outcomes when we are in what call a pure prediction setting these are one and the same However many decisions take the form of interventions specifically designed to mitigate risk RAIs for these settings must be developed and evaluated taking into account the effect of historical decisions on the observed outcomes Failure to do so will result in RAIs that despite appearing to perform well according to standard evaluation practices underperform on cases such as those that have been historically receptive to intervention In this paper we propose an approach to counterfactual risk modeling and evaluation to properly account for these intervention effects Counterfactual modeling has been proposed for medical RAIs and prior work has used counterfactual evaluation for o-policy learning in bandit settings However the question of adapting counterfactual evaluation for risk assessments and in particular for predictive bias assessments remains open In this paper we propose a new evaluation method for RAIs that uses doubly-robust estimation techniques from causal inference We also argue that fairness metrics that are functions of the outcome should be defined counterfactually and we use our evaluation method to estimate these metrics We theoretically and empirically characterize the relationship between the standard fairness metrics and their counterfactual analogues Our results suggest that in many cases achieving parity in the standard metric will not achieve parity in the counterfactual metric Our contributions are as follows We define counterfactual versions of standard predictive performance metrics and propose doubly-robust estimators of these metrics We provide empirical support that this evaluation outperforms existing methods using a synthetic dataset and a real-world child welfare hotline screening dataset We propose counterfactual formulations of standard fairness metrics that are more appropriate for decisionmaking settings We provide theoretical results that only under strong conditions which are unlikely to hold in general does fairness according to standard metrics imply fairness according to counterfactual metrics We demonstrate empirically that applying existing fairness-corrective methods can increase disparity in the counterfactual redefinition of the metric they target BACKGROUND AND RELATEDWORK Counterfactual learning and evaluation Literature on contextual bandits has considered counterfactual learning and evaluation of decision policies While this literature is methodologically relevant as we discuss below it addresses a different problem In the decision support setting we are considering human users will ultimately decide what action to take The goal of the learning and evaluation task is not to learn a decision policy but rather to learn a risk model that will inform human decisions That is the risk assessment task is to accurately and fairly estimate the probability of an outcome under a given intervention While the underlying task is different the statistical methods used in evaluation are related use propensity score weighting a form of importance sampling to correct for the effect of the historical treatment on the observed outcome and they propose learning the optimal policy based on the minimization of the propensity-score weighted empirical risk Propensity-score methods are a good candidate when one has a good model of the historical decisionmaking policy but may otherwise be biased Doubly robust DR methods by contrast are robust to parametric misspecication of the propensity score model if instead one has the correct specification of the model of the regression outcome X where Y is the outcome and X are the features/covariates In a non-parametric setting DR methods have faster rates of convergence than propensity-score methods DR methods have been used for policy learning in the online bandit setting The policy learned minimizes a DR estimate of the loss Prior work has considered counterfactual RAIs when the outcomes are continuous-time trajectories In this work the trained model is evaluated on real data using the observed outcomes and on simulated data Evaluating against the observed outcomes can be misleading in settings in which treatment was not assigned randomly We propose instead to adapt DR techniques to evaluate counterfactual RAIs Counterfactual learning in the causal inference literature uses model selection based on DR estimation of counterfactual loss Whereas this approach evaluates counterfactual metrics implicitly our approach does so explicitly providing the estimators for standard classification metrics in A line of work focuses on counterfactual learning in the presence of hidden confounders propose policy learning via minimax regret learning over uncertainty sets Their method is not immediately applicable to decision-support settings where RAIs are more informative to decision-makers than a policy recommendation propose using deep latent variable models to model hidden confounders via proxies in the data of our paper assumes no hidden confounders and future work could attempt to incorporate these techniques for handling hidden confounders Our theoretical analysis in holds even in the presence of hidden confounding Fairness and causality A growing literature on counterfactual fairness has offered notions of fairness based on the counterfactual of the protected attribute or its proxy In this work a policy is considered fair if it would have made the same decision had the individual had a different value of the protected attribute and hence potentially different values of features affected by the attribute In this setting the treatment decision is the outcome and the protected attribute is the treatment By contrast we consider counterfactual treatment decisions and consider a future observation to be the outcome Another line of work considers unfair causal pathways between the protected attribute or its proxy and the outcome variable or target of prediction These papers characterize discrimination via path-specific effects which are defined by interventions on the protected attribute We do not consider interventions on ie counterfactuals of the protected attribute rather we propose methods that account for interventions on treatment decisions Fairness definitions based on the counterfactual of the protected attribute are not widely used in RAI settings for two reasons First the assumptions required to estimate these counterfactual metrics prohibit the use of important features such as prior history or require full specification of the structural causal model SCM These requirements are too restrictive for our settings where we have insufficient domain knowledge to construct the SCM and where we are unable to disregard important predictors like prior history Second and more significantly these definitions are ill-suited for RAI settings like child welfare screening As we discuss in decisions made based on the counterfactual protected attribute may cause further harm to the protected groups Our work bears conceptual similarity to the analysis of residual unfairness when there is selection bias in the training data that induces covariate shift at test time as discussed in In settings where cases are systematically screened out from the training set such as loan approvals they nd that applying fairness-corrective methods is insufficient to achieve parity We consider a different but related setting in which we observe outcomes for all cases but these outcomes are under different treatments We propose fairness definitions that account for the effect of these treatments on the observed outcomes and analyze the conditions under which existing methods can achieve this notion of counterfactual fairness COUNTERFACTUAL MODELING AND EVALUATION Before introducing the learning approaches and evaluation methods considered in this work we pause to clarify the types of risk-based decision policies to which our evaluation strategy as presented is tailored RAIs typically inform human decisions either by identifying cases that are the most or least risky or by identifying cases that are the most or least responsive The evaluation metrics we consider are relevant in the paradigm where human decision-makers wish to intervene on the riskiest cases Our method can be adapted as discussed in for paradigms based on responsiveness This distinction is also made in a survey of fairness literature The motivating application for our work is child welfare screening Child welfare service agencies across the nation eld over million child abuse and neglect calls each year Call workers must decide whether to screen in a call which refers to opening an investigation into the family The child welfare system is responsible for responding to all cases where there is significant suspicion that the child is in present or impending danger The standard of practice is therefore to identify the riskiest cases Jurisdictions in California Colorado Oregon and Pennsylvania are all in various stages of developing and integrating RAIs into their call screening processes The RAIs are trained on historical data to predict adverse child welfare outcomes such as re-referral to the hotline or out-of-home foster care placement The decision to investigate a call can affect the likelihood of the target outcomes Notation We use Y to denote the observed binary outcome and for exposition we assume Y is the unfavorable outcome T denotes the decision which for simplicity we take to be binary We note that DR estimation methods can be used in any treatment setting including for continuous treatments such as dosing Throughout the remainder of the paper we will use the term decision and treatment interchangeably In describing counterfactual learning and evaluation we rely on the potential outcomes framework common in causal inference In this framework Y t denotes the outcome under treatment t For any given case we only get to observe Y or Y We will take T to be the baseline treatment the decision under which it is relevant to assess risk Most risk assessment settings have a natural baseline which is often the decision to not intervene For instance in education one might assess the likelihood of poor outcomes if a student is not offered support in child welfare it is natural to assess risk if the call is not investigated We refer to the baseline treatment as control and the non-baseline treatment as treatment X X Rd denotes the covariates or features which may include a protected or sensitive attribute A X PT X denotes the propensity score whose estimate we denote by X In the child welfare setting X contains call details and historical information on all associated parties T is whether the case is screened-in for investigation and Y is whether the case is re-referred to the hotline in a six-month period We use subscripts i to index our data eg Xi are the features for case i We use Y X to denote our predicted label and s X to denote the predicted score which is the models estimate of the target outcome our RAI Learning models of risk In this section we introduce observational standard practice and counterfactual forms of model training Observational The observational RAI produces risk estimates by regressing Y on X for the entire observed dataset ie this RAI estimates X This model answers the question What is the likelihood of an adverse outcome under the observed historical decision process The observational RAI is ill-suited for guiding future Y X is typically obtained by thresholding decisions it will for instance underestimate baseline risk for cases that were historically responsive to treatment Counterfactual The counterfactual model of risk estimates the outcome under the baseline treatment Our counterfactual model of risk targets X Even though we only observe Y or Y for any given observation we may nevertheless draw valid inference about both potential outcomes under a set of standard identifying assumptions These assumptions hold by design in our synthetic dataset and we discuss why they may be reasonable in the child welfare setting under each point Consistency Y TY T Y This assumes there is no interference between treated and control units This is a reasonable assumption in the child welfare setting since opening an investigation into one case will not likely affect another cases observed outcome Exchangeability Y T X This assumes that we measured all variables that jointly influence the intervention decision T and the potential outcome Y This is an untestable assumption it may be reasonable in the child welfare setting where the measured variables capture most of the information the call screeners use to make their decision see Weak positivity requirement P X requires that each example have some non-zero chance of the baseline treatment This can hold by construction in decision support settings We can filter out cases that violate this assumption since the decision for these cases is nearly certain Our assumptions identify the target X X T The counterfactual model estimates X by computing an estimate of X T We can train such a model by applying any probabilistic classifier to the control population Since the control population may have a different covariate distribution than the full population reweighing can be used to correct this covariate shift This may be useful in a setting with limited data or where model misspecication is a concern Evaluation To evaluate how well our models of risk might inform decisionmaking in the paradigm targeting the riskiest cases we assess precision true positive rate TPR false positive rate FPR and calibration Since the task is to evaluate how well the model predicts risk under a baseline intervention we specify the performance metrics in terms of Y The target counterfactual TPR is Y The target counterfactual precision is Y identification is the process of using a set of assumptions to write a counterfactual quantity in terms of observable quantities We set the treatment to be the same value for all children in a family Risk assessments are unnecessary for these cases since the decision-maker already knows what to do In the paradigm where interventions are to be targeted at the most responsive cases performance metrics such as discounted cumulative gain DCG or Spearmans rank correlation coefficients are more natural choices for evaluation DR estimates can be constructed for these metrics as well The target counterfactual FPR is Y A model is well-calibrated in the counterfactual sense when E Y r r r r where r r define a bin of predictions Observational Evaluation A standard approach evaluates the model against the observed outcomes An observational Precision Recall PR curve plots observational precision Y against observational TPR Y An observational ROC curve plots observational TPR against observational FPR Y An observational calibration curve plots r r the observational outcome rate for scores in the interval r r The observational evaluation answers the question Does the RAI accurately predict the likelihood of an adverse outcome under the observed historical decisions This evaluation approach can be misleading since Y Y For instance it will conclude that a valid counterfactual model of risk under baseline performs poorly because its predictions will be systematically inaccurate for cases that are responsive to treatment Evaluation on the Control Population The standard counterfactual approach to evaluation computes error metrics on the control population The PR curve evaluated on the control population plots Y T against Y T and the ROC and calibration curves are similarly defined by conditioning on T When the control population is not representative of the full population ie T X as is the case in nonexperimental settings this evaluation may be misleading since T T A method that performs well on the control population may perform poorly on the treated population or vice-versa In child welfare cases where the perpetrator has a history of abuse are more likely to be screened in Since there is more information associated with these cases a model may be able to discriminate risk better for these cases than on cases in the control population with little history Doubly-robust DR Counterfactual Evaluation We propose to improve upon the control population evaluation procedure by using DR estimation to perform counterfactual evaluation using both treated and control cases This ensures that performance is assessed on a representative sample of the population Our method estimates the counterfactual outcome for all cases and evaluates metrics on this estimate Other approaches such as inverse-probability weighing IPW or plug-in estimates could be used for a counterfactual evaluation but DR techniques are preferable because they have faster rates of convergence for non-parametric methods and for parametric methods they are robust to misspecication in one of the nuisance functions ie the treatment propensity X and the outcome regression X Under sample splitting and convergence rates in the nuisance function error terms these estimates are p n-consistent and asymptotically normal This enables us to compute confidence intervals see Appendix B We rst consider estimates of the average outcome under control Under our causal assumptions in Section TPR and recall are equivalent X T The plug-in estimate is n i where X denotes the score of our counterfactual model The IPW estimate uses the observed outcome on the control population and reweighs the control population to resemble the full population i Ti Xi DR estimators combine the plug-in estimate with an IPW-residual bias-correction term for the control cases DRY n i Ti Xi i Next we consider estimators for the counterfactual targets in Equations We emphasize that s in is the score of any model we wish to evaluate while s refers specifically to our counterfactual model in TPR Recall Counterfactual TPR is identified as Y E X T E X T i The DR estimate for the numerator is n i Ti Xi i The DR estimate for the denominator is DRY in Equation Precision The target counterfactual precision is identified as Y X T Y The DR estimator for precision is i Ti Xi i where I denotes the indicator function FPR The target counterfactual FPR is identified as Y E YE Y X T i E E Y X T i The DR estimator for the numerator is n i Ti Xi i For the denominator we use DRY where DRY is in Calibration The target in Equation is identified as E X T r r The DR estimate for calibration is i Ti Xi i r Pr r We show how to compute confidence intervals in Appendix B In survey inference this is known as the generalized regression estimator Results Synthetic example We begin with a synthetic dataset so that we can compare methods in a setting where we observe both potential outcomes We specify two groups with different treatment propensities where the treatment is constructed to be equally effective at reducing the likelihood of adverse outcome Y for both groups We generate data points Xi Y i Y where Xi Ai and N a normal distribution with mean and variance Ai Bern a Bernoulli with mean Y Y where c controls the treatment Bern kAi where describes the bias in treatment assignment toward group A We set Y TY T Y We use logistic regression to train both the observational X and counterfactual models X as well as the propensity model ET X Under this choice of model the propensity model and counterfactual model are both correctly specified and accordingly the plug-in and IPW estimates are both consistent in this setting However in practice there is no way to know whether the models are correctly specified so DR estimates are preferable for real-world settings Figure displays PR ROC and calibration curves DR evaluation most closely aligns with the true counterfactual evaluation Notably the observational evaluation suggests the observational model outperforms the counterfactual model The true counterfactual evaluation shows the counterfactual model performs better Child Welfare We also apply counterfactual learning and evaluation to the problem of child welfare screening The baseline intervention is screen-out which means no investigation occurs The data consists of over calls to the hotline in Allegheny County Pennsylvania each containing more than features describing the call information as well as county records for all individuals associated with the call The call features are categorical variables describing the allegation types and worker-assessed risk and danger ratings The county records include demographic information such as age race and gender as well as criminal justice child welfare and behavioral health history The outcome is re-referral within a six month period Our approach contrasts to prior work which used placement out-of-home as the outcome This outcome is only observed for cases under investigation therefore it cannot be used to identify Y the risk under no investigation We use random forests to train the observational and counterfactual risk assessments as well as the propensity score model We used reweighing to correct for covariate shift but did not observe a boost in performance likely because we have sufficient data and we used a non-parametric model We present the PR ROC and calibration curves in Figure The observational evaluation suggests that the observational model performs better The control evaluation suggests that the counterfactual and observational models of risk perform equally well Our DR evaluation suggests the counterfactual model has both better We present results for alternative values of c and in Appendix E The offset is to roughly balance the number of treated/control units In Appendix E we include T as a feature in the observational model to see if this can appropriately control for treatment effects but we nd that it does not discrimination and calibration in estimating the probability of rereferral under screen-out The observational evaluation suggests that the observational model is well-calibrated whereas the counterfactual model is overestimating risk this is expected because the counterfactual model assesses risk under no investigation whereas the observed outcomes include cases whose risk was mitigated by child welfare services The control evaluation suggests that the two models are similarly calibrated The DR evaluation shows that the counterfactual model is well-calibrated and the observational model underestimates risk This makes intuitive sense because the observational model is not accounting for that fact that treatment reduced risk for the screened-in cases We see further evidence that the observational model performs poorly on the treated population in the drop in ROC curves between the control evaluation and DR evaluation in Figure Deploying such a model would mean failing to identify the people who need and would benefit from treatment The observational and control evaluations do not show this significant limitation DR evaluation is the only evaluation that illustrates the poor performance of the observational model on the treated population We also evaluate the different models according to whether they are equally predictive in the sense of being equally well calibrated across racial groups Research suggests child welfare processes may disproportionately involve black families Here we ask whether the observational or counterfactual model is more equitable We compare calibration rates by race in Figure The observational evaluation suggests that the counterfactual model of risk is poorly calibrated by race The DR evaluation shows that the counterfactual model is well-calibrated by race and indicates that the observational model underestimates risk on both black and white cases Overall the observational evaluation suggests that the observational model performs better whereas the DR evaluation suggests the counterfactual model performs better Since we do not have access to the true counterfactual to validate these results we further consider how well the models align with expert assessment of risk Expert Evaluation At various stages in the child welfare process social workers assign treatment based on their assessment of risk Social workers decide whether to screen in a case for investigation whether to oer services for a case under investigation whether to place a child out-of-home after an investigation Assuming that social workers are competent at assessing risk we expect the group placed out-of-home to have the highest risk distribution followed by those offered services and we expect those screened out to have the lowest risk Figure shows that the counterfactual model exhibits this expected behavior whereas the observational model does not The observational model assesses the screened out population to have more high risk cases than any other treatment group The observational model underestimates risk on the treated groups These cases should be assigned treatment but the observational model would suggest they should be screened out Such a mistake can have cascading effects We are particularly concerned about screening out cases that had they been screened in would have been accepted for services or placed out-of-home Figure shows the recall for placed cases and serviced cases as we vary the proportion of cases classified as high-risk The counterfactual model has much higher recall for both services and placement Control Doubly robust True Counterfactual Pr is io n Model Counterfactual Observational Control doubly-robust True Counterfactual R al l Control doubly-robust True Counterfactual O ut co m e ra te Figure Synthetic data results From top to bottom the rows give PR curves ROC curves and calibration curves with pointwise confidence bounds Each column is an evaluation method Colors denote the learning method DR evaluation most accurately represents the true counterfactual evaluation Observational evaluation erroneously suggests the observational model performs better than the counterfactual model because it includes units whose risk was mitigated by treatment Control evaluation produces inaccurate curves because it does not assess how well the models perform on the treated population See Task adaptation Another way to evaluate the models is to assess performance on related risk tasks While the counterfactual risk models X we can assess how well it estimates X the risk under investigation If we have reason to believe there will be common risk factors for risk under no investigation and risk under investigation then we expect our model to perform well on this task We use placement out-of-home an adverse child welfare outcome that is observed for cases under investigation Table shows that the observational model performs worse than a random classifier on the placement task whereas the counterfactual model shows some degree of discrimination This suggests that the counterfactual model is learning a risk model that is useful in related risk tasks whereas the observational model is not model model Random AUROC AUPR Table Area under ROC and PR curves using our re-referral models to predict a related risk task out-of-home placement confidence intervals given in parentheses The observational model performs worse than a random classifier The counterfactual learns a model of risk that transfers to related risk tasks See The comparison to expert assessment of risk and the performance on a downstream risk task support the conclusions of our Control doubly-robust Pr is io n Model Counterfactual Observational Control doubly-robust R al l Control doubly-robust Average risk score R er er ra l r at e Figure Child welfare results From top to bottom the rows give PR curves ROC curves and calibration curves with pointwise confidence bounds Each column is an evaluation method Colors denote the learning method Observational evaluation suggests the observational model has better discrimination and calibration than the counterfactual model because it evaluates against the observed outcomes which include cases whose risk was mitigated by child welfare services Control evaluation suggests the two models perform similarly on cases that did not receive treatment DR evaluation shows that the observational model does not perform well on treated cases See and DR evaluation In decision-making contexts failure to account for treatment effects can lead one to the wrong conclusions about model performance leading to the deployment of a model that underestimates risk for those who stand to gain most from treatment COUNTERFACTUAL FAIRNESS Standard observational notions of algorithmic fairness are subject to the same pitfalls as observational model evaluation In this section we propose counterfactual formulations of several fairness metrics and analyze the conditions under which the standard observational metric implies the counterfactual one We motivate the importance of defining these metrics counterfactually with an example Suppose teachers are assessing a model that predicts who is likely to fail an exam which they intend to use to assign tutoring resources Suppose anyone tutored will pass The tutoring session conflicts with girls sports practice so only male students are tutored A model that perfectly predicts who will fail without the help of a tutor will have a higher observational FPR for men than women because some male students were tutored which enabled them to pass It would be wrong to conclude that this model is unfair with regards to FPR Someone who would have been high-risk had they not been treated but whose risk was mitigated under treatment should not be considered a false positive Failure to make this distinction could lead to unfairness not only in settings where the treatment assignment varies according to the protected attribute but also in settings where the risk under treatment varies according to the protected attribute For instance suppose now both girls and boys are tutored but the tutor is only effective in preparing male students to pass The model that perfectly predicts who will Control doubly-robust C m O m Average risk score R er er ra l r at e Race Black White Figure Calibration curves by race for child welfare Counterfactual model top row is well-calibrated by race according to the control and DR evaluations but shows inequities according to the observational evaluation because black cases were more likely to get treatment which mitigates risk see for more details The observational model bottom row is poorly calibrated for both black and white cases according to the DR evaluation C m O m Risk de Treatment Screened out Investigated Services Placed Figure Child welfare risk distributions by treatment type for counterfactual and observational models We expect risk to increase with the severity of treatment assigned with Placed out-of-home having the highest risk distribution and Screened out of investigation having the lowest see The counterfactual model displays this trend The observational model does not underestimating risk on cases where child welfare effectively mitigated the risk fail without a tutor has a higher observational FPR for men but as before it is wrong to conclude that the model is unfair We distinguish our notion of counterfactual fairness from prior work which considered counterfactuals of the protected attribute an approach which is counterproductive in our settings of interest Consider a female student who is at high risk of failing because of gender discrimination at home or in the classroom eg Placement Services Proportion classified as high-risk R al l Model Counterfactual Observational Figure Recall for downstream child welfare decisions At current screen-in rates the observational model would screen out nearly of very high risk cases that were placed out-of-home The counterfactual model has higher recall at The gap is even larger for cases that were accepted for services See parents or previous teachers have not given her the support they would have had she been male Treating this student counterfactually as if she had been male all along may suggest that we should not assign this student a tutor In fact we must assign her a tutor in order to correct historical discrimination Similar arguments can be made in settings like child welfare screening and loan approvals Theoretical results For three definitions of fairness parity we show that observational parity implies counterfactual parity if and only if a balance condition holds We show that an independence condition is sufficient for observational parity to imply counterfactual parity We discuss why it is generally unlikely that the independence condition holds and even more unlikely that the ner balance condition holds when the independence condition fails Appendix C contains the proofs Base Rate Parity Base rate plays a core role in statistical definitions of fairness also known as group fairness Base rate parity is similar to the fairness notion of demographic parity which requires Y A In we perform experiments on a fairness corrective method that targets base rate parity in order to encourage demographic parity A related fairness notion prediction-prevalence parity requires a a Satisfying both prediction-prevalence parity and demographic parity requires parity in the base rates We distinguish observational base rate parity Y A from counterfactual base rate parity which requires Y A where Y is the potential outcome under the baseline treatment T B R P Assume PT a If holds then holds if and only if the following balance condition holds C BP PY PT Y PY aPT Y a PY PT Y PT Y a holds under the following independence conditions which provide sufficient conditions for to imply C BP T A Y Y T A It is unlikely that holds in many contexts In child welfare and criminal justice research suggests that even when controlling for true risk certain races are more likely to receive treatment cannot hold since T A Y Even in settings where there is no such bias will not hold if the risk distributions under treatment vary by protected attribute since requires Y A also requires T A Y which forbids discrimination in treatment assignment when controlling for risk under treatment If does not hold it is possible that still holds if the conditional and marginal probabilities are such that all terms exactly cancel however there is no semantic reason why this should hold Theorem assumes PT a a positivity-like assumption that holds in all settings that are suitable for algorithmic RAIs Violations of this assumption indicate perfect or imperfect treatment assignment historically for a group Predictive parity Base parity and demographic parity may be ill-suited for settings where base rates differ by protected attribute due to disparate needs Here we may instead desire parity in an error metric such as precision Positive predictive parity requires the precision also known as positive predictive value to be independent of the protected attribute and negative predictive parity requires the negative predictive value to be independent of the protected attribute We define observational Predictive Parity oPP as Y A Y and counterfactual Predictive Parity as Y A Y where corresponds to negative predictive parity and corresponds to positive predictive parity T P P Assume PT a If oPP holds then holds if and only if the following balance condition holds C PY PT Y PY a PT Y a PY PT Y PT Y a is satisfied under the following independence conditions which provide sufficient conditions for oPP to imply C T A Y Y Y T A Y will not hold in many settings Note that Y T A Y T A Y Y and Y A Y Conditions T A Y t Y require Y to contain all the information that A tells us about treatment assignment that is not contained in Y t Since Y is typically trained to predict Y and not T it is quite unlikely that these conditions will hold in settings where there is bias in treatment assignment even when controlling for true risk Condition Y A Y allows differences in the risk distribution under treatment if we can fully explain these differences with Y In the best case Y Y but it is unlikely that the observed outcome which is not causally well-defined would explain differences in the risk distribution under treatment As above even if does not hold may hold but it is difficult to reason why this should hold in any setting Like Theorem Theorem assumes a mild positivity-like assumption Equalized odds In settings where TPR and FPR are more important than predictive value we may desire parity in TPR and FPR a fairness notion known as Equalized Odds Observational Equalized Odds requires that Y A Y and counterfactual Equalized Odds cEO requires that Y A Y T E O Assume PY a and PT a If holds then cEO holds if and only if the following balance condition holds C PY Y PT Y Y PY PY PY Y aPT Y Y a PY a PY Y PT Y Y a PY a PT Y Y PY PY The balance condition is satisfied under the following independence conditions which comprise sufficient conditions for to imply cEO C Y A Y A T A Y Y Y Y T A The rst two conditions of require and so requires to hold In settings where there is discrimination in treatment assignment even when controlling for true risk is unlikely to hold Even if there is no such discrimination will not hold if there are differences in the risk distributions under treatment since the last condition of requires Y A requires further conditions such as parity in the TPR/FPR against the outcome under treatment If these conditions are not met could imply cEO if holds but it is difficult to reason about why this would hold when the independence conditions do not Our theoretical analysis suggests that in many settings equalizing the observational fairness metric will not equalize the counterfactual fairness metric We conclude by noting that the theorems hold when conditioning on any features X and in this context these theorems are relevant to individual notions of fairness Original Reweighed C ou nt er fa l O er va na l Ba se R at e Group A A Figure Counterfactual and observational base rates before and after applying a fairness-corrective method that reweighs training data X-axis controls the bias of treatment assignment toward group A Before reweighing Original counterfactual base rates are equal holds but observational base rates are different doesnt hold for since group A is more likely to get treated Reweighing achieves but no longer holds Experiments on synthetic data We empirically demonstrate that equalizing the observational metric can increase disparity in the counterfactual metric Reweighing One approach to encourage demographic parity reweighs the training data to achieve base rate parity Figure shows that without any processing Original the counterfactual base rates are equal while the observational base rates show increasing disparity with Reweighing the observational outcome achieves but induces disparity in the counterfactual base rate Theorem suggested this result For A T Y then it is unlikely that implies Post-processing We evaluate a method that modifies scores to achieve a generalized version of equalized odds This method targets parity in the generalized FNR/FPR where GFPR is Y and GFNR is E Y We refer to these observational rates as and define their counterfactual counterpart Y and E Y We use the scores of the counterfactual model as inputs We compute the and using our DR method from Table shows that post-processing to equalize and induces imbalance in and In Figure we see that the original model achieved cEO but post-processing induced disparity to the detriment of the group that was less likely to be treated We perform these experiments on the synthetic data We do not use the child welfare data since it is balanced in terms of base rates and FPR/TPR with respect to race We use the Pleiss implementation on and_calibration that extends the method in to probabilistic classifiers The estimator is nearly identical to the estimators for FPR/FNR if we use in place of the predicted label Y X We use c and We report results for other values in Appendix Group Method A Original A Original A A Table Counterfactual and observational generalized FNR/FPR before and after post-processing to equalize odds using threshold Before post-processing Original the counterfactual generalized rates and are the same for both groups Post-processing equalizes the observational rates and but induces noticeable disparity in both and Original Post-Processed R al l Group A A Figure Counterfactual ROC curves before and after postprocessing to equalize odds Before post-processing ROC curves are identical for both groups indicating that counterfactual equalized odds cEO holds Post-processing induces imbalance harming group A and compounding initial unfairness in treatment assignment Since treatment is beneficial this fairness adjustment actually compounded the discrimination in the treatment assignment CONCLUSION This paper demonstrates that training and evaluating models using observed outcomes can lead to the misallocation of resources due to the misestimation of risk for those most receptive to treatment Furthermore fairness-correcting methods that seek to achieve observational parity can lead to disparities on the relevant counterfactual metrics and may further compound inequities in initial treatment assignment The counterfactual approaches to learning evaluation and predictive fairness assessment introduced in this paper provide more accurate and relevant indications of model performance