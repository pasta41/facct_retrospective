Model Agnostic Interpretability of Rankers via Intent Modelling A key problem in information retrieval is understanding the latent intention of a users under-specified query Retrieval models that are able to correctly uncover the query intent often perform well on the document ranking task In this paper we study the problem of interpretability for text based ranking models by trying to unearth the query intent as understood by complex retrieval models We propose a model-agnostic approach that attempts to locally approximate a complex ranker by using a simple ranking model in the term space Given a query and a blackbox ranking model we propose an approach that systematically exploits preference pairs extracted from the target ranking and document perturbations to identify a set of intent terms and a simple term based ranker that can faithfully and accurately mimic the complex blackbox ranker in that locality Our results indicate that we can indeed interpret more complex models with high fidelity We also present a case study on how our approach can be used to interpret recently proposed neural rankers INTRODUCTION In the context of data-driven models interpretability can be defined as the ability to explain or to present in understandable terms to a human Recently in the machine learning ML and NLP communities there has been growing interest in the interpretability of ML models but there has been limited work on interpreting retrieval/ranking models considered central to information retrieval IR Ranking models are used in a variety of domains The most traditional use is in a variety of search engines for differing or mixed modalities images video Web news web pages social media posts patents scientific papers and data In these scenarios the ranking model is responsible for ranking search results given a query Ranking models are now pervasive in recommender systems as well where no user generated query is apparent Instead they are used for ranking similar items in application areas like shopping movies music news and social media or prioritizing information for a day like in a news feed Machine learned ranking models are also used in systems where a ranking is not the final output For example ranking models are used to order candidate answers in question answering systems which are then consumed by other models to select or generate answers When using machine learning models to rank results the training data clicks human annotations informs how signals/features should be combined Various models have been employed ranging from linear regression and decision trees to deep neural networks more recently Document relevance ranking also known as adhoc retrieval or search is the task of ranking documents from a large collection using the query and the text of each document only Ranking based on textual features is particularly important in cold-start learning scenarios when there is no existence of click-logs and features like link-structure are non-informative Examples include various domains in digital libraries eg patents or scientific literature enterprise search and personal search In this paper we tackle the problem of post-hoc interpretability of adhoc rankers Post-hoc here means that we try to explain a trained ranking model There are major challenges when explaining ranking models in particular that we seek to address in this work Ranking models ideally output a single ranked list of documents for a given query In practice however rankings are an aggregation of decisions i individual document relevance scores are first assigned and then the documents are sorted to get a list or ii pairwise document preferences are combined using approaches such as This in turn makes evaluating the explanation of a ranking model more difficult What kind of explanation is best suited to a ranked list of documents given a query How do we measure if this explanation is accurate Interpreting intents in rankings We first ponder on what interpreting a ranking really means Are we only interested in explaining why a document was relevant or ranked above another Although that is certainly interesting what we are really interested in is if a model is performing in accordance with the information need of the user who issues the usually under-specified query a key concept in IR In other words what is the actual query intent as understood by the trained ranking model There are two key benefits to uncovering the learned intent First users can immediately identify biases induced by the training data or learning procedure For example consider the query how to find the mean and two different rankers Ranker Top Doc Find what they mean in the following songs Ranker Top Doc Tutorial on using SPSS to find mean and other stats Both rankers seemingly prefer very different documents even though they both contain the query terms However why they choose different top documents is unclear If we present the user with terms that encode the intent then this becomes easier to discern The intent of Ranker is actually say want meant and that of Ranker x statistics plus know Ranker prefers documents which are semantically similar to a different sense of the word mean whereas Ranker accurately captures the query intent The second benefit is being able to help identify over-fitting or under-performance If spurious patterns like copyright messages are identified as part of the intent developers can quickly rectify the data and improve the model While terms are easy to understand as explanations it may not be enough to accurately answer two important questions Why is A relevant Why is A ranked above B Hence we additionally need an explanation model that can easily show how a document is relevant to the query using the intent terms Our Contribution We take a local view to the problem of posthoc interpretation of a black-box ranking model ie interpreting the output for a single query at a time Our approach is to learn a simple ranking model with an expanded query that approximates the original ranking In doing so we argue that the expanded query forms the intent or explanation of the query as perceived by the black-box ranker In coming up with the explanation or the intent we hypothesize that an expanded query along with a simpler and interpretable model is an accurate interpretation of the black-box model if it produces a similar ranking to the ranking of the black-box model Towards this we exploit the pairwise preferences between documents in the original ranking to extract relevant terms to be considered for expansion by postulating a combinatorial optimization problem called the Maximum Preference Coverage Experimental Evaluation To quantitatively evaluate the goodness of our explanations we need to collect ground truth explanations given a model Note that this is not always possible for ranking models that do not have a query understanding procedure that describes the inferred intent In fact most learning to rank and neural models do not have an explicit query understanding component In order to overcome these hurdles we devised blackbox ranking models that explicitly perform query understanding using term expansions The expansion sets then serve as ground truth explanations We conducted extensive experiments using queries and documents from the TREC test collections Our results indicate that we can interpret well understood explicit query understanding based retrieval models with good accuracy and high fidelity a measure of how closely we can reproduce the behavior of a model In a case study we also show how our framework can be employed to explain the results of neural ranking models with high fidelity in various scenarios PRELIMINARIES Intents A query in IR is an encoding of a users intent A subtle point is that the relevance of a document to a query is driven by the intent and not its encoding In traditional relevance evaluation like TREC trained judges are first presented with a topic description that describes the users information need clearly rather than just the keyword query submitted to the search engine For example in the TREC web track dataset from query id is hayrides in pa The intent/topic description is Where can I go on a hay ride in Pennsylvania which is the information assessors use to judge document relevance Note here that the description details the true intention pa stands for Pennsylvania and the user is more interested in where to go for a hay ride rather than a history of hayrides for instance Keyword queries tend to be short and often ambiguous and hence identifying the intent behind them is key to determining relevance and in turn a good ranking Query Expansions Early on IR researchers used query expansion techniques such as to explicitly model the intent of the query through terms The under-specificity is treated by adding more terms to the query unbeknownst to the user For the aforementioned example terms such as location distance philadelphia pennsylvania hayrack wagon can provide crucial context to aid the retrieval model These terms can be mined from an external source or from the pseudo relevant documents ie driven by the collection More recently neural approaches to query modelling have been proposed either by using word embeddings to identify terms for expansions or converting the discrete query to a vector representation in a continuous space The objective behind all of this work is to be able to get a better specification of the query so that i more context is available to estimate document relevance and ii simpler term based retrieval models can be used for ranking ML ranking models Machine learned models rank documents based on a representation of queries and documents that can be either hand crafted as is the case for traditional learning to rank or learned as in the case of neural ranking models These models learn query-document interaction patterns from the training data in order to estimate the relevance or ranked order of documents given a query Here the intent behind a query is derived/learned from the training data ie the documents marked relevant or irrelevant for keyword queries The use of neural ranking models in adhoc search has shown significant improvements but we lack the ability to explain individual decisions made by them These models in particular learn a new representation space for queries and documents which is difficult to understand This however is not an issue with the more explicit query modeling approaches where a query is expanded to better specify the intent In this work we detail how to locally explain complex ranking models by estimating a simpler query expansion based model where the intent behind the output ranking is made explicit RELATEDWORK ON INTERPRETABILITY Interpretability in machine learning has been studied extensively in classical machine learning However the success of neural networks NN and other expressive yet complex ML models have only intensified the discussion On one hand they have largely improved performance but on the other they tend to be opaque and less interpretable Consequently interpretability of these complex models has been studied in various other domains to better understand decisions made by the network image classification and captioning sequence to sequence modeling recommender systems etc Post-hoc methods for interpretability can be categorized into two broad classes model introspective and model agnostic Model introspection refers to either interpretability by design or access to all the model parameters In interpretability by design more inherently interpretable models such as decision trees rules additive models and attention-based networks are utilized Instead of supporting models that are functionally black-boxes such as an arbitrary neural network or random forests with thousands of trees these approaches use models in which there is the possibility of meaningfully inspecting model components directly eg a path in a decision tree a single rule or the weight of a specific feature in a linear model When using deep neural networks or other complex models where we have access to the model parameters but direct introspection is not possible approaches such as explanations through influential training examples and explanations through input attributions are more useful We however operate in the model agnostic regime where we do not assume any access to the ranking models parameters Recent work in this space has tended to focus on classification and regression models but not on ranking models For other notions of interpretability and a more comprehensive description of the approaches we point the readers to In information retrieval there has been limited work on interpreting rankings In the authors estimate the feature importance of a document in a ranked list given a learned ranker This becomes unwieldy if the number of features is large or un-interpretable itself Singh and Anand tried to approximate an already trained learning to rank model by a subset of the original features interpretable features using secondary training data from the output of the original model Recently shows how a model introspective method meant for computer vision can be applied to interpreting the relevance score of a single query document pair Firstly these works do not focus on interpreting intents and secondly they are not model agnostic limiting their usability Closest to our work is which propose using LIME directly for the task of explaining the relevance between a single query document pair While the results of are anecdotal evaluate how close complex ranking models are to the topic description as provided by TREC Note that their objective is to evaluate how close the explanation terms are to the terms in the human created intent description for a query This is different from our objective which is to accurately determine the intent terms that drive the ranking output of the model not just a single document and not how close it is to the intended intent for a given query In terms of approach we utilize similar perturbation techniques as the aforementioned works but only to identify a set of candidates Instead of training a local classification or regression model we optimize for ranking preferences directly for better intent determination Tangential but related to our line of work is the explainability of recommender systems We point readers to for an overview of the field A key difference to ranking models is that explanations here focus on explaining why a product was recommended to a user who has a rich interaction history Recent works such as detail how a transparent and scrutable model can be built directly instead of explaining it posthoc EXPLAINING RANKINGS When explaining rankings we wish to understand why for a query Q does a given ranker BB output the list BB Q as the top documents from the set of documents retrieved from the index The explanation is a simple human understandable representation that will help justify the behavior of BB for the given query Q to the user We consider a post-hoc model agnostic setting to understand ranking decisions ie we do not assume access to the learning algorithm or model parameters such as coefficients encoding feature weights or neural network parameters Doing so allows us to be independent of the underlying ranking model Our assumption is that we can approximate a complex ranker BB operating on an under-specified query Q with a simple ranker E with an over-specified query ie with additional expansion terms I We consider the expansion terms as the intent representation as understood by the E and hence intent terms forms our explanation for the intent of BB Terms words or phrases in adhoc retrieval are not only central to devising retrieval models but are also intuitive and understandable for humans Retrieval models or rankers that are explicit functions of well-understood IR features like term-level statistics document lengths and proximity like BM statistical language models are both interpretable and have been shown to be essential indicators of textual relevance We chose as our explanation model both because its easy to understand and because it naturally supports query expansions The PDR framework outlines main desiderata for evaluating interpretability methods The first is the predictive accuracy or fidelity of the explanation model For an explanation to be convincing it should faithfully mimic the underlying model Given a ranking BB Q to interpret or explain the explanation model should produce a prediction ranking in this case that approximates BB Q We measure the fidelity of the mimiced ranking by the rank correlation measure Kendalls The second aspect to evaluating an explanation model is its descriptive accuracy Here we are interested in measuring how accurately the explanation model has learned the same data relationships as the blackbox model In essence we evaluate if the explanation model is right for the right reasons Note that descriptive accuracy can only be measured in cases where the data relationships learned by the blackbox model can easily be determined In Section we describe how we constructed blackbox models that explicitly expand the query under-the-hood This allows us to directly quantify descriptive accuracy We denote descriptive accuracy as accuracy henceforth for the sake of simplicity The final aspect when evaluating explanations is the relevance of the explanation to an audience for a given task In Section we briefly show use cases where our explanations can help legal and non-technical users quickly identify biases in the learned intents Section is dedicated to measuring the fidelity and accuracy of our approach on a variety of blackbox rankers in uncovering the intent I Problem Statement Formally for a given keyword query Q and ranking BB Q produced by a complex black box ranker BB we wish to identify a high fidelity local explanation I The fidelity of I is measured by the rank correlation metric between the rankings BB Q and E Q I APPROACH To explain the top-k documents we should be in principle be able to explain all preference pairs i where i We denote a document at position i in a ranking as i In what follows we explain how to identify terms I and an easy-to-understand E such that an expanded query Q I using E preserves most of the preference pairs in Approach Overview Our approach to explaining rankings produced by arbitrary blackbox ranking models consists of major steps i First we identify candidate terms from the documents retrieved Section ii Next we construct a preference matrix that is a data structure we use to compute the influence of each candidate term in preserving pairwise preferences according to a given E Section iii Finally for a given E we chose an I that maximizes fidelity modelled as Maximum Preference Coverage with the original ranking using a greedy procedure described later in this section described in Section Figure provides a visual overview of our approach Candidate Term Selection The initial input to our approach is the query and resulting ranked list of documents In this section we propose approaches to meaningfully chose a set of terms that should be considered as candidates for I Candidate pre-selection We pre-select an initial set of terms from the top- documents of BB Q after removing common and generic terms using tf-idf filtering In our experiments we chose terms Not all chosen terms will contribute to the explanation and many of them would be false positives Document Perturbation We assume that for a term to be an intent term its presence or absence in a document influence the relevance of the document Exploiting this hypothesis we systematically perturb documents by adding and removing terms from the pre-selected terms Similar notions of perturbation have been used for text classification question answering and machine translation Adhoc retrieval models are often sensitive to document lengths Consequently in our document perturbation procedure that we ensure that document lengths are maintained The Median of a Set of Data Math Goodies To find the median the data should be arranged in order from least to greatest If there is an even number of items in the data set then the median is found by taking the mean average of the two middlemost numbers What is statistical mean median mode and range Definition from In statistics that single value is called the central tendency and mean median and mode are all ways to describe it To find the mean add up how to find the mean Blackbox Ranking Position d d d d average statistics found describe Proximity d d d d average found statistics describe LM d d d d average found statistics describe Preference Pairs How to find the mean median statistics Explanation Ranking d d d d d d d and preference pairs from the ranking produced by the blackbox Construct preference matrix using candidate terms and pre-selected interpretable rankers E Choose the set of terms I and E that maximizes coverage of preference pairs in the target ranking The explanation model E is a combination of I and E that tries to mimic the rank order produced by the blackbox BB Q We estimate the importance of a pre-selected term by substituting all instances of the term in a result document by an out-of-vocabulary OOV term We call this perturbation by removal Similarly in perturbation by term addition we add terms absent from the document from the vocabulary or pre-selected terms Additive perturbation allows us to control not only the frequency of the added candidate term but also the position and order The relevance contribution of a term on the document is estimated as the score difference computed using the original ranking model with and without the perturbation We retain the top terms in our experiments that ensure a substantial recall from the pre-selected set that have the maximum relevance contribution over the top-k in the result set In our experiments we use a combination of the two to efficiently select a set of candidates with minimal potential false positives We call this refined subset of terms from the pre-selected terms as the candidate set d d d d d d d d d d le handle doctor invert medicine- Sum Expansion Terms doctor medicine health hazards d d d d d Explain top Preference Pairs or Features Figure preference Matrix for the ranking for the query health hazards Modeling Preference Pairs Until now we were concerned with the terms that are relevant to the query However not all terms are responsible for preference pairs induced by the original ranking In what follows we describe a procedure on how to select terms from the candidate set that preserve the preference pair ordering in BB Q In principle for a ranking of size we have concordant pairs However only considering the preference pairs from the top-k documents could result in higher likelihood of false positive terms being included in the selection set False positive terms tend to be general terms that co-occur with relevant terms resulting in an increased local fidelity local here refers to the top ranked documents We counteract the effect of false positives by sampling additional preference pairs from outside the top-k and requiring that the algorithm covers a larger number of preference pairs Carefully sampling pairs can help focus on alternate yet important query aspects and prevent overfitting Conversely improper sampling or sampling too many pairs from the tail could lead to good global rank fidelity but poor local rank fidelity Sampling Preference Pairs/Features We use multiple strategies to sample preference pairs random randomly select preference pairs from the target ranking rank biased sample preference pairs that are weighted by rank Each pair i is weighted by rank di rank top-k rank random construct preference pairs based on a combination of rank and random sampling In this method for a preference pair i di is rank bias sampled but is randomly sampled top-k random consider all pairs from the top-k results to explain and a fixed number of randomly sampled pairs We contrast these against top-k that are all preference pairs in top-k results in the experiments Constructing the Preference Matrix The next step in our approach is to construct an n m preference matrix for a given candidate E where n is the number of candidate terms is the number of preference pairs For each preference pair feature we compute a score that encodes the degree of concordance the candidate term maintains for the pair We employ E to first estimate the importance of a term for each document The score for the term given a pair i is computed as SE i SE SE is the relevance score estimated by E as is to BB The score for each cell is computed by then multiplying it with a weight corresponding to each preference pair We make the assumption that more distant concordant pairs rank vs rank as opposed to close ranked document pairs rank vs rank contains stronger evidence of relevant intent terms We weight a given preference pair i i In the next section we show how this choice allows us to directly optimize for fidelity if we select an E akin to a language model where q are terms in a query SE P d q q P d q d Optimizing Preference Pair Coverage Once we construct a good set of candidates we then build the preference matrix corresponding to a set of pre-selected E In this section we describe how to find the set of terms I given a single preference matrix We start with a set of candidate expansion terms X X n where each expansion term t X is described by a feature vector thus t has a vector p pd Rd and feature vectors in X Rd Each feature corresponds to a preference i and its value determines to what degree is the preference pair satisfied if t is chosen described earlier as SE i SE We build the preference matrix P from the term vectors and intend to find a minimal set of terms I X as expansion terms Preference Coverage Given a selection set represented as a Boolean vector s the preference coverage over the aggregate vector y is given by i The best selection of expansion terms naturally is the set that maximizes the preference coverage or explanation fidelity We pose the Maximum Preference Coverage problem as choice of a set of terms where maximum preferences are covered Writing it as an Integer Linear Program we have max s t si i n in si Pi wi Note that Pi SE i SE The Maximum Preference Coverage problem is NP hard We do not include the proof in the paper for space reasons but we note that the proof sketch follows from the fact that the Maximum Preference Coverage is a generalization of the well known Set Cover problem Not only is the Maximum Preference Coverage problem NP hard it is also easy to see that it is not sub-modular The Maximum Preference Coverage with concordant document pairs as features naturally tries to maximize fidelity as defined by Kendalls Now selecting terms that maximize coverage of concordant pairs is equivalent to selecting terms that when used as expansions closely reproduces BB q A positive score for a feature indicates that a term is more relevant i according to E where i For a set of terms simply adding the scores for that feature will indicate if concordance is maintained and in turn maximize fidelity Greedy Algorithm Although Maximum Preference Coverage does not induce submodularity we propose a heuristic greedy algorithm that intends to maximize the preference coverage of the input At each iteration the algorithm greedily chooses the term into the selection set that provides the maximum utility The utility of a term t at any stage of the algorithm U is the increase in the preference coverage when t is added to the selection set T or U t This is clearer from the example in Figure Say the term medicine is chosen into the selection set with a medicine Now choosing the term handle in fact reduces medicine handle In case of ties the t that has the highest column sum denoted by of the covered features are considered ie titi t ti EXPERIMENTAL SETUP The major challenge in evaluation is measuring accuracy descriptive accuracy according to PDR In order to do so we must obtain a ground truth of perceived intents of black-box rankers note that this is different from the actual user or query intent which is difficult for complex neural models In order to quantify the quality of our explanations we resort to black-boxes whose intents are fully understood This in fact is common practice in evaluation of local posthoc-interpretable approaches with the underlying assumption that if an explanation model can correctly locally interpret a simple well understood model then it can faithfully locally interpret other complex models For the scenarios where the rankers intents are unknown we resort to measuring the explanation performance using fidelity Dataset and Queries For all experiments we use the web track Clueweb TREC test collection category B for adhoc retrieval We use the first queries only Note that in our experiments we are more interested in showing that our approach can be applied to a variety of retrieval models trained on a large training set rather than the same retrieval model across multiple test collections For that reason we consider only one Web scale collection with a large set of queries rather than more datasets Metrics We use Kendalls to measure fidelity between the explanation ranking and the original ranking ie the predictive accuracy in order to establish our approachs ability to extract intents I that produce a similar output as BB q The descriptive accuracy of the explanation is computed as the fraction of intent terms that overlap with the set of ground truth intents whenever available We select the top- documents to explain for a given q and BB and set max I to in all our experiments Blackbox Ranking Models We consider three black box rankers term and DESM where we know the actual intent terms due to the explicit modelling of relevance computation for the expansion terms These are the ground truth terms used in computing the metrics We set the number of ground truth terms to term Using the term algorithm we determined a set of relevant expansion terms from the top-k documents for a given query We then re-ranked the results using the aforementioned language model We use Instead of using pseudo-relevant documents to find expansion terms an external collection is used for the expansions We use glove embeddings dimensions trained on English Wikipedia dump to find semantically related terms We first average the embeddings of the words in the query to create a query vector Next we search the embedding space for the nearest terms that also occur in the top documents DESM models the relevance score of d given q as a parameterized sum between the syntactic relevance and semantic similarity between a learned query vector representation and the document vector representation We select terms closest to the query vector that are also present in the top documents of the initial result list as expansions They are used to compute the syntactic relevance whereas all terms are used for the semantic similarity The expansion terms are considered as ground truth terms but this set is not exact The two relevance components are combined linearly with a parameter set to Here the objective is to uncover the term expansions in-spite of the noise To compute the vectors we employ the same glove embeddings from In Section we consider three neural retrieval models that have been shown to have improvements over BM DRMM P-DRMM and M-Pyramid DRMM operates on query document term similarity histograms whereas M-Pyramid uses a query-document term similarity matrix as input and then learns hierarchical representations P-DRMM is based on the PACRR model that operates on query-document interaction matrices along with considering the position of terms in a document All models were trained on an adhoc search test collection Robust Here we focus on measuring fidelity and use cases where our intent based explanations are useful Explanation Model Settings While accuracy is measured against a ground truth set of terms fidelity however is sensitive to the choice of E used to estimate BB For the explanation model we fix E as a language model estimated using MLE with additive smoothing ie P d count d d We had to be careful in choosing a language model that is sufficiently different from the ranking function in R since we are only concerned with bag of words based models RESULTS In order to establish the effectiveness of our approach we i evaluate the quality of our explanation approach in terms of fidelity and accuracy ii performance under different sampling and feature generation strategies and finally iii present scenarios where explanations can be used to debug rankers Effectiveness of Explanations Table reports the accuracy and fidelity values of all the models under consideration for the best individual setting of our explanation method Local fidelity measures the rank correlation between our explanation ranking and original ranking for only the top results while global considers all retrieved documents ie in our experiments Accuracy vs Fidelity We find that for these simpler models were able to achieve an accuracy greater than In figure we see the correlation between accuracy and fidelity in a scatter plot We uniformly sampled query explanations in total from our devised models to create this plot We find that global fidelity is strongly correlated with accuracy whereas local fidelity shows relatively weak correlation This result is useful in estimating how well we do on neural models that do not have a ground truth Note that a indicates that we can faithfully reproduce over of concordant pairs with our simple interpretable model ignoring score ties How many pairs to sample Creating preference matrices with many pairs can be computationally expensive however we observe diminishing returns after sample pairs For all our presented results we set the number of pairs to For the neural models we found that increasing the number of pairs improves fidelity We set the number of pairs to for the neural models Which sampling strategy works best In Table we see that sampling pairs at random is the worst option top-k random and top-k rank random substantially improve accuracy and in turn global fidelity Both of these techniques allow for documents towards the bottom of the ranking to be selected The benefit of doing so is observed when comparing against rank biased and top-k Sampling pairs only from the top-k leads to highest overall local fidelity as expected but poor accuracy This indicates that sampling pairs from the top and bottom of the ranked list gives us the best regularization effect Effect of Perturbation False positive terms have a large impact on the quality of our approach that we try to address by exploiting the optimizing preference pair coverage For the non-DESM rankers chosen for our experiments we can remove all false positive terms this way since score changes will only occur due to the terms in the ground truth Table However for DESM false positives are harder to determine due to soft matching between query and document terms This according to us is also hard to evaluate because of lack of access to all terms that are indicators of relevance Hence the accuracy score reported for DESM is a lower bound on our actual performance Perturbation helps reduce the number of false positives with no effect on recall in DESM Nonetheless our approach can still locally explain DESM with an accuracy of when using top-k rank random sampling which is similar to Figure Scatter plot of Accuracy vs Fidelity Kendalls Global Fidelity is highly correlated with Accuracy Local Fidelity is less correlated Global Fidelity is a better indicator of accuracy Data points represent queries sampled from all models where ground truth is available Model Accuracy local global fidelity fidelity DESM DRMM P-Drmm M-Pyramid Table and DESM features top-k rank random DRMM P-DRMM and M-Pyramid features top-k rank random Note that A negative implies there are more discordant pairs than concordant Usually is considered to be strong correlation EXPLAINING NEURAL MODELS The goal of this case study is to demonstrate the utility of our explanation framework when interpreting complex models like DRMM Match Pyramid and DESM described in Section where ground truth is absent in a posthoc model agnostic setting Here we use the full fledged approach with multiple candidate E We chose simple rankers based on well studied features that are known to improve adhoc text retrieval i term frequency and document length language model ii position and iii proximity of terms Sampling Accuracy Local Global fidelity fidelity random top-k Random Rank Random rank biased Table Interpreting DESM features Comparing against TREC intents In the previous sections we evaluated the ability of our approach in uncovering the blackbox models intent which is the true test of explanation accuracy Now we turn towards evaluating which neural models inferred intent is most human like by comparing against hand crafted query intents As described in Section TREC assessors estimate relevance given a clearly described intent Table shows a selection of queries and their corresponding TREC descriptions We see that queries tend to be under-specified but with the aid of the description the context becomes clearer We generated explanations for DRMM and Match Pyramid and compared them against the TREC descriptions to identify if the rankers do indeed capture the human defined intent provided by TREC for the given query We found that both rankers tend to capture the intended intent accurately albeit with slightly differing terms For the query churchill downs DRMM is able to better capture the schedule part of the intent as can be seen by the terms year date and hour Similarly DRMM is also able to capture more of the intent for the query bph treatment Note that we are able explain over of the concordant pairs in both cases global fidelity of approximately and for both models respectively indicating that the explanations are more likely to be accurate Detecting Bias through Intent Terms Training Data Bias Table highlights queries that illustrate the power of our explanations in this scenario DESM uses wikipedia embeddings which is reflected in the more generic intent explanation terms nurses as opposed to war for alexian brothers hospital Since DRMM was trained on Robust which is a news collection from we find more terms related to news-worthy events This is particularly evident for afghan flag USA was involved in many conflicts in Afghanistan in the early s and is promptly the reason why documents related to the USA get ranked higher for DRMM DESM on the other hand favors documents more directly related to the concept of a flag We also find evidence for temporal bias in the queries fidel castro and electoral college results DRMM ranks documents related to events in higher Vice President and brother of Fidel Raul Castro was handed control in evidenced in DESM due to Fidel Castros illness which was a more prominent topic in Similarly DRMM considers documents related to Al Gore more relevant as compared to DESM for electoral college results Model Bias The explanation also gives us insight into the nature of the ranker For the query how to find mean even though the semantics of the query terms is resilient to temporal shifts DRMMs gating mechanism helps capture the right semantics of the query DESM on the other hand computes semantic similarity in a more simplistic manner relying heavily on the pretrained word embeddings to capture the right semantics Explaining Rankings for DRMM Can we mimic DRMMs ranking We generated the explanations for randomly selected queries from the dataset we used in the quantitative experiments for DRMM We found that our approach could effectively explain nearly half of all preference pairs and could also explain nearly the same fraction in the top global fidelity and local fidelity Based on the quantitative results where we showed that global fidelity is correlated with accuracy we can be confident that the explanations produced are accurate Why is d relevant to q Once we have identified the intent terms we can reason why a given document is relevant to the query The simplest way to visualize this explanation is to highlight terms in the document For the query fidel castro and document clueweb en according to our explanation for DRMM the terms cuba and intestine are indicators of relevance whereas the same document for DESM is also ranked in a similar position but for different reasons Here raul and communist are the reasons why clueweb en is considered relevant This difference can be visualized using snippets as shown in Figure to help users easily discern why a document is relevant to a query Why is di more relevant than The intent explanation terms when used with E can further help us understand why a document was considered more relevant than another I can effectively explain document pairs that are concordant in both target and explanation ranking Figure shows an anecdotal document pair explanation for the query fidel castro and DRMM Due to our choice of an E that scores terms independently we can construct an easy-to-understand visual explanation that is a composition of term scores In our experiments we found that our approach chose terms along with the language model TF and document length based E to best explain the ranking produced by DRMM local fidelity and global fidelity In this ranking d and d seem to be similar when just looking at the explanation terms havana domestic cuba invest intestine medical real both are related to medical issues However on closer inspection using E it becomes clear that intestine is a key term that is more prominent in d than in d Similarly d is ranked considerably lower since it only matches a few intent terms APPLICATION TO OTHER DOMAINS In our experiments we demonstrated the applicability of our approach to models trained on a document ranking task where queries tend to be under-specified and documents tend to be at least a few paragraphs Query TREC Topic Description DRMM Intent Explanation M-Pyrmaid Intent Explanation eggs shelf life What is the shelf life of a chicken egg that is how old can it be and still be safe to eat store chicken month year actual eat fat fish tend product develop air pick chemical old food remove bph treatment What are the treatment options for BPH benign prostatic hyperplasia Which medications are used for BPH What are the symptoms of BPH What are the side effects of BPH treatments prostatic symptom grow inflammation bladder red medical urologist prostatic hyperplasia urine bladder medicine drug hormone psa churchill downs Find information on the horse racing schedule at Churchill Downs horse winner track begin hour year date patron registration radio run horse oak park win cup fair rider anderson jones reed bart sf Find information about the BART Bay Area Rapid Transit system in San Francisco transit service metro rider ac oakland san transit station train downtown union castro san Table Comparing intent descriptions from TREC with explanations produced by our approach for DRMM and Match Pyramid Query DRMM Intent Explanation DESM Intent Explanation how to find x statistics actually say the mean plus know want meant alexian brothers war person nurses father hospital patient course physical doctors afghanistan flag US official symbol nation inscription time flagpole hoist fidel castro havana domestic cuba havana cuba invest dictator communist electoral college president popular statistic following results senate nominee gore election outcome expected Table Anecdotal explanations The Intent Explanation terms I are terms that optimize for Maximum Preference Coverage according to our approach Figure Explanation for why clueweb is relevant for fidel castro according to DRMM and DESM Snippets containing the most important terms that were found as intent terms using our approach are highlighted QA Models Rankers are also used in question answering QA tasks where the query is a question and the answers can be an arbitrary piece of text A typical QA system first retrieves a set of candidate answers and then ranks them before displaying/generating the final answer We successfully applied our approach to a QA model similar to a BERT based method to explain anecdotal Figure Explanation for d d d for DRMM for the query fidel castro The length of a cell in the bar indicates term importance tod as estimated by our explanation model Since E is known we can also explain how term importance is computed questions from the MSMarco dataset Table shows explanations for randomly selected questions from the dev set Here we can see how the BERT based model perceives the question when ranking candidate answers passages in this dataset which can further shed light on answers delivered to the end user Recommender Systems A subtle point to note in our work is that the approach itself does not require a query as input Given Question BERT Based Model Intent Explanation Ludacris Net Worth million actor rapper atlanta Explain what a bone scan is and what it is used for body abnormalities joint radionuclide Does Suddenlink Carry ESPN espn computer service games Androgen receptor define estrogen assay positive therapy levels of government in canada and their responsibilities commons queen laws federal Table Anecdotal explanations for a BERT based QA model any ranking produced by a black box model we can select a set of terms from the mined candidates and a simpler ranking model that serve as the explanation This implies that our approach can also be applied to ranking models used in recommender systems where the query is not a set of terms input by the user but the user itself the date or an item being viewed The key is the ability to mine candidates via document perturbation For instance consider the ranking of similar products in a shopping application Here the query can be considered to be the product currently being viewed The ranked list of similar items can be explained with our approach by i mining candidate terms from the product descriptions and titles ii constructing the preference matrix based on concordant product pairs iii selecting terms using the greedy approach suggested earlier Now the intent based explanation can inform the user as to why certain products are recommended first for the product she is currently viewing CONCLUSION In this paper we detailed our framework for post-hoc explanations of adhoc black box rankers Our setting enables us to tackle a multitude of text based retrieval models predominantly used in news search medical search patent retrieval product search etc irrespective of the underlying learning algorithm or training data Note that web search tends to rely heavily on network and behavioral signals in most cases but textual relevance is crucial for tail queries rarely seen or unseen queries From the quantitative results we gathered that using preference pairs only from the top-k results leads to high local fidelity but low accuracy Sampling additional pairs from lower in the ranked list on the other hand can substantially increase accuracy since it indirectly optimizes for global fidelity at the cost of local fidelity We find that top-k rank random is usually the best sampling method which shows that taking pairs of documents that are mostly from the top with with finer differences rank biased or just randomly selected random is not the best strategy Qualitatively we have seen how our approach can be used to identify temporal and model biases We also how we could use intent terms to explain pairwise rank differences In the future we would try to incorporate evidence from model-introspective approaches to improve our explanation fidelity