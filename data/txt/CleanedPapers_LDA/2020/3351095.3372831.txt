An Empirical Study on the Perceived Fairness of Realistic Imperfect Machine Learning Models There are many competing definitions of what statistical properties make a machine learning model fair Unfortunately research has shown that some key properties are mutually exclusive Realistic models are thus necessarily imperfect choosing one side of a tradeoff or the other To gauge perceptions of the fairness of such realistic imperfect models we conducted a between-subjects experiment with Mechanical Turk workers Each participant compared two models for deciding whether to grant bail to criminal defendants The first model equalized one potentially desirable model property with the other property varying across racial groups The second model did the opposite We tested pairwise trade-offs between the following four properties accuracy false positive rate outcomes and the consideration of race We also varied which racial group the model disadvantaged We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy Nonetheless no preferences were overwhelming and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants We observed nuanced distinctions between participants considering a model unbiased and considering it fair Furthermore even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants we did not observe consensus that a machine learning model was preferable to a human judge Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations CONCEPTS Human-centered computing Empirical studies in interaction design Computing methodologies Machine learning KEYWORDS Fairness Accountability Machine Learning Survey Data Science INTRODUCTION As machine learning ML is used to automate increasingly significant decisions such as predicting criminal defendants risk of recidivism activists and journalists have raised the alarm about issues of bias and a lack of fairness Researchers have responded by attempting to define fairness mathematically There are dozens of competing definitions that embed different notions of what is fair Recent work has noted that these fairness definitions address a relatively narrow set of considerations In light of this some have suggested that these definitions can be deployed to understand and address specific problems Unfortunately some key definitions of fairness are incompatible Therefore in reality building models that embed some of these definitions will require making difficult trade-offs between competing notions of fairness We conducted an empirical user study to understand attitudes about these sorts of potentially difficult trade-offs Consider the well-worn example of COMPAS In ProPublica reported that COMPAS an ML tool used to assess criminal defendants fitness for parole or bail had a false positive rate for black defendants nearly twice that for white defendants The tools makers responded by noting that the accuracy of the tool was equalized between the two racial groups and that the tool was therefore not biased Subsequent work has shown that one cannot achieve both conditions simultaneously In other words there was a necessary trade-off between equalizing either false positive rates or accuracy across racial groups It is not clear whether the makers of COMPAS were aware of the disparate false positive rates prior to ProPublicas reporting so it is not clear if preferring equalized accuracy was a conscious decision Regardless COMPAS implicitly embeds the notion that equalizing accuracy rather than the false positive rate is the correct definition of fairness A number of studies in recent years have aimed to quantify attitudes about the fairness of particular ML models However few studies investigate attitudes about the difficult choices and fairness-related trade-offs inherent in realistic applications of ML Understanding attitudes about these trade-offs can inform technical and regulatory interventions Documenting what people outside of specialized fields think about these problems can help inform and facilitate future multi-stakeholder processes We conducted a survey-based experiment of Mechanical Turk workers In the context of making bail decisions this experiment tested a fairness-related trade-off randomly assigned to each participant Each pair captured one side of the type of trade-off that an ML developer might face when trying to make a model as fair as possible in realistic circumstances For example one set of participants was prompted to choose between a model that equalized accuracy at the expense of disparate false positive rates across racial groups and one that did the opposite We tested trade-offs of all pairwise combination of four desirable fairness properties equalized accuracy equalized false positive rates equalized outcomes and explicitly excluding race as a model input Each model in a pair had a disparity in one model property and equality in the other Through this study we investigated the following questions When choosing between models exhibiting the two sides of a difficult trade-off which do people prioritize We observed a statistically significant trend but not full consensus in participants prioritizing the equalization of false positive rates across groups rather than the equalization of accuracy Notably this preference is the opposite of what the COMPAS tool did which may partially explain the controversy around the tool For many other trade-offs we saw a wide distribution in participants preferences We observed a non-trivial fraction of strong opinions preferring each side of each trade-off we tested highlighting the difficulty of building ML models with broadly acceptable fairness characteristics in realistic circumstances What models that encapsulate difficult yet realistic trade-offs do people perceive as fair or biased Participants tended toward rating most models we tested as not biased However participants tended to consider it biased when outcomes were equalized at the expense of disparate false positive rates that disadvantaged African-American defendants Only one model we tested one in which more African-American defendants than White defendants were granted bail even though race was not used as an input to the model was overwhelmingly considered fair In many other cases opinions were mixed and fairly polarized A models perceived lack of bias did not necessarily imply a perception of fairness We observed a number of cases in which participants considered a model not to be biased yet also did not consider it to be fair In some cases this was because of high false positive rates that were nonetheless equal across racial groups Do people prefer to use an imperfect model or rely on a human judge For most trade-offs we investigated we found a preference for a human judge over either ML model the participant saw Even when the participant considered one or both of these models unbiased they often still preferred a human judge They justified this preference based on a human judges accountability capacity for ethical reasoning and ability to make individualized decisions To what extent do responses vary based on which racial group the model disadvantages For each trade-off we randomly assigned whether White or African-American defendants would be disadvantaged by the disparate property We observed some potential yet not statistically significant differences in the distribution of responses In sum our empirical user study is a first step in unpacking how people view the bias and fairness of ML models encoding difficult trade-offs related to fairness RELATEDWORK We first summarize some of the most straightforward definitions of fairness including those we investigate in this study Sec Afterwards we present prior studies that like ours empirically investigate humans perceptions of the fairness of ML models Sec Fairness Definitions and Trade-offs Researchers have proposed dozens of definitions of fairness for ML models Many of these definitions center on one or more properties of the model In this section we provide a brief overview of two classes of fairness definitions most relevant to our study Definitions in the first class group fairness concern a models comparative treatment of different groups eg demographic groups Definitions in the second class procedural fairness concern the process by which individuals are judged While other classes of fairness definitions have been proposed including definitions concerned with the treatment of individuals all four definitions we test in our study fall into one of these two classes We conclude this section by highlighting how some of these definitions are mutually exclusive motivating our study of the trade-offs between definitions Group Fairness Group fairness refers to the class of definitions that examine differences across groups such as the different demographic groups represented in a dataset These definitions frequently propose that some property such as the models accuracy or false positive rate should be equal across groups As highlighted in the COMPAS controversy accuracy and false positive rates are two of the most prominent examples of model properties used to evaluate group fairness Another example of a group fairness definition is disparate impact or demographic parity Originating from employment discrimination law disparate impact is the idea that a sufficiently large difference in favorable classification rates is rebuttable evidence of discrimination In the ML context disparate impact is frequently parameterized as attempting to equalize outcomes across groups More complex group fairness definitions have also been proposed Under equalized odds the true positive and false positive rates must be equalized between groups Equalized opportunity is less strict requiring only the true positive rates be equalized Both equalized odds and opportunity encode the idea that fairness consists of being right at the same rate across groups Refinements to these definitions often look at discrimination through a causal lens The intuition behind causal definitions is that the same distribution could be caused by a fair or an unfair social process In our study we test conditions in which accuracy false positives and outcomes vary across groups see Sec We do not test more complex definitions because they are difficult to explain succinctly Procedural Fairness In contrast procedural fairness focuses on how a model makes decisions For instance a procedural fairness approach might argue that a model that makes decisions using race as an input variable is always unfair regardless of the models outcome The input variables most widely discussed in procedural fairness eg race gender and age are protected classes under employment discrimination law Researchers have also proposed other ways of determining which variables are fair to use in ML models Some have argued that for a model to be fair it must give individuals the ability to change a models decision Actionable variables might include employment status while immutable variables would include race or the age of first arrest Others have proposed the notion of process fairness in which fair variables are identified by surveying the public In our experiment we vary whether or not race is included as an explicit model input The explicit consideration of a data subjects race would violate almost all procedural fairness definitions While process fairness considers the explicit use of variables unfair variables may be implicitly encoded in data through combinations of seemingly fair variables A large literature proposes techniques for auditing models for such correlations Tensions Between Fairness Definitions Unfortunately certain key fairness definitions are mutually exclusive or otherwise incompatible Kleinberg et al and Chouldechova et al both independently argue that it is only possible to achieve equally accurate risk scores across groups and equally balanced risk quantiles across groups under very specific conditions Grgić-Hlača et al find that one can sometimes achieve both process fairness and outcome fairness but at an accuracy cost Furthermore the use of race may be necessary during model development to audit the model or to achieve group fairness Indeed the lack of data labeled with unfair attributes can prevent the analysis of fairness in practice Because many definitions of fairness can be mutually exclusive realistic ML models often cannot simultaneously satisfy all desired definitions In light of these inherent trade-offs we design the models in our survey-based experiment to be imperfect satisfying one definition of fairness yet violating another Empirical Studies of Fairness Researchers have begun to investigate humans attitudes toward ML systems Qualitative studies have found that people from marginalized communities do have experiences with algorithmic discrimination though they do not always use that term for it Similar to us a handful of empirical studies have collected attitudes about specific model properties For example Srivastava et al ask participants to choose between a succession of pairs of models to identify which group fairness definition best captures peoples perceptions of fairness Through twenty comparisons generated through an adaptive algorithm they converge upon a given respondents preferred notion of fairness In both risk prediction and medical diagnostic contexts their participants prefer demographic parity equality across groups in the percentage predicted to receive a positive classification over other definitions As we discuss in Section using a different protocol we come to a different conclusion Within process fairness Grgić-Hlača et al investigate attitudes about what features people consider fair to use In follow-up work Grgić-Hlača et al investigate why people consider those features fair or not They find a features perceived relevance reliability and volitionality drive assessments They also find that support for the consideration of race increases from to when participants are told the use of race increases accuracy Two additional empirical studies focus on questions complementary to ours In a loan-allocation context Saxena et al quantify attitudes about individual fairness They ask participants to rate the fairness of three different ways a loan officer could divide between two individuals with different repayment rates and in one iteration different races Participants rated giving the entire sum to the candidate with the higher repayment rate as more fair than dividing it equally only when the candidate with the higher rate was black Kennedy et al investigate the relationship between trust and model properties They use an experiment in which participants choose between pairs of risk assessment models The models vary at random in overall rates eg true/false positives/negatives the size of the training data the number of features the weight of features the algorithm source and more While they investigate trust they do not investigate fairness or test whether differences in model properties across groups affect trust Awad et al use a viral game to study variations across cultures in how participants would solve a variation of the trolley problem involving different demographic characteristics of the parties Our work is distinct from Awad et al both in that we seek to understand a different form of decision and that we do not attempt to make any kind of generalized claims about universal moral characteristics METHODOLOGY To investigate perceptions of fairness in imperfect yet realistic ML models we conducted an online between-subjects survey-based experiment We asked participants to rate the fairness bias and utility of two models that exhibited both sides of a specified trade-off between two fairness-related model properties randomly selected from among the following four accuracy false positive rate outcomes and the explicit consideration of race We graphically presented the properties encapsulated in this trade-off Participants then chose which of the two models they preferred overall as well as whether they preferred a human judge to either ML model On Amazons Mechanical Turk we recruited workers years old and located within the United States We limited recruitment to workers with a rating over HITs We paid for the survey which took a median time of minutes We excluded data from participants whose free responses were off-topic or nonsensical eg discussing the Tesla Model X car in their response Exclusion happened after data collection and all participants were paid regardless of whether we excluded their data from analysis Our Institutional Review Board approved this experiment We used Mechanical Turk for participant recruitment because it provided a means of reaching participants who were largely non-technical and were unlikely to have formally engaged with academic debates about fair machine learning We do not claim that this participant pool is representative of some larger public Our participant pool largely did not have technical backgrounds Below we detail the structure of the survey Sec and the specific trade-offs we tested Sec We then describe our quantitative Sec and qualitative Sec analyses The supplementary appendix includes the full survey instrument Survey Structure To familiarize participants with the topic of machine learning the survey began by giving a high-level description of how ML models can be used to make predictions We then told them a city was considering using an ML model to decide whether to grant bail to defendants charged with non-violent crimes Each participant was randomly assigned a pair of models exemplifying a trade-off between two fairness-related model properties see Sec Each model satisfied one definition of fairness but violated the other We presented these properties visually see Fig Figure The comparison presented to participants in the condition testing accuracy and false positives FP-Acc Property Disparate Rates Equalized Rate Accuracy Acc vs Outcomes vs False Positives FP vs Race usage Is a model input Is not a model input Table The properties investigated The equal accuracy rate as well as disparate rates for outcomes and false positives were based on the COMPAS dataset We first presented the models individually in randomized order Participants rated the fairness bias and utility of each on a five-point Likert scale Afterwards we presented the two models side-by-side as in Figure asking participants to rate on five-point Likert scales which was more fair biased or useful Also on five-point Likert scales we asked participants to select which model they would prefer to see implemented as well as whether they would prefer a human judge to their choice of either model Model Properties and Trade-offs Each participant evaluated and compared two models that were imperfect in opposite ways We randomly assigned each participant a pair of models representing the trade-off between two of the following four properties accuracy false positives outcomes and explicit race usage We chose these properties because they have been widely discussed and can be expressed succinctly In contrast we chose not to test more complex definitions like equalized odds which requires equalizing both the true and false positive rates Table summarizes these properties and their associated disparate and equalized rates We tried to capture realistic trade-offs by using rates taken from ProPublicas analysis of COMPAS data Because some of our properties are counter-factuals we fabricated the unknown rates disparate accuracy and equalized false positive and outcome rates Below we list the properties and in italics the abbreviations we use throughout the paper as we explained them to participants Accuracy Acc The accuracy of the model is the rate at which the model makes correct predictions A prediction is correct if the model either predicts that a defendant will show up for their trial and they would or that they will not show up for their trial and they would not have Outcomes The probability of bail is the likelihood of a defendant being granted bail if the model is used False positive rate FP A defendant who is mistakenly denied bail is one that the model predicts would not show up for their trial when they would have We explained false positives without using the term false positives to avoid confusion about what a positive classification meant Race usage If the model does not use race The model makes decisions with no knowledge of the race of the subjects Other features like type of offense and number of previous offenses are used as input to the model If the model uses race The model makes decisions with knowledge of the race of the subjects Other features like type of offense and number of previous offenses are used as input to the model We considered a model equalized if it did not use race as an explicit model input We considered a model to be disparate if it did use race This corresponds to the notion that considering race is generally undesirable For each participant we randomly selected one of the six possible pairwise combinations of these four properties In all models we showed how the two properties varied across two subgroups White defendants and African-American defendants We also randomly assigned whether African-Americans were disadvantaged or whether White defendants were disadvantaged in the disparate rates doubling the total number of conditions to twelve Terminology As we present our results we refer to the trade-off participants saw using the abbreviated names of the two properties involved as well as the disadvantaged group For example FP-Outcome-Maj refers to the trade-off between false positives and outcomes in which the majority group White defendants is disadvantaged At some points we need to refer to the particular model within the pair We use and to indicate the equalized and disparate property respectively For example the FP-Outcome-Maj condition includes the model Outcome FP Maj and the model Outcome FP Maj When we quote participants we identify them by participant number and condition trade-off Quantitative Analysis We mapped answers on five-point Likert scales to For example participants could rate whether they would definitely or probably prefer a model that they were unsure or that they would probably or definitely prefer a human judge For such answers we tested whether participants answers tended toward one answer model the other human judge or neither We did so using the unpaired Wilcoxon Signed-Rank test which tests whether a distribution is skewed around zero Significance indicates answers tended toward one answer or the other For questions where participants individually rated each model eg on fairness we tested whether they tended to rate one model higher than the other As each participant rated both models the data was not independent We thus used the paired Wilcoxon Signed-Rank test which measures whether the distribution of differences between pairs of ratings is symmetric Significance indicates one model was seen as more fair biased or useful than the other For each trade-off pair eg FP-Outcomes some participants saw a version where White defendants were disadvantaged -Maj while others saw a version where African-American defendants were disadvantaged -Min We compared the -Maj and the -Min versions of each pair using the Mann-Whitney U test a non-parametric analog of the ANOVA test for comparing two groups For each of the above families of tests we corrected for multiple testing with the Benjamini-Hochberg method As both fairness and bias ratings were ordinal Likert-scale data we calculated the correlation of these ratings with Kendalls Qualitative Analysis We thematically coded free-response explanations participants gave for their choice of one model over another their preference for a human judge over either model and their ratings of fairness and bias Two coders collaboratively developed a codebook from a sample of answers Two coders then independently used that codebook to code the remaining answers We allowed multiple codes per answer to capture the compositionality of responses We created three distinct codebooks The first was for explanations of bias It contained ten high-level codes six of which had sub-codes For this codebook Cohens The second was for why a participant chose one model over the other It had seven codes no sub-codes and The last codebook assessed explanations of why humans or models were preferred It had eight high-level codes five of which had subcodes For this codebook The coders met to resolve disagreements We saw a wide variation across participants in the length of these responses For example free-response explanations of why the participant preferred one model over the other ranged in length from to words with a median of Most free-response questions had a similar distribution in terms of length RESULTS We begin by describing our participants Sec We detail which trade-offs participants preferred Sec and whether they ultimately preferred a human judge Sec We then unpack participants ratings of fairness and bias as well as how those concepts correlate Sec Finally we delve into the impact of varying which racial group was disadvantaged Sec Participant Demographics We surveyed individuals in a convenience sample from Mechanical Turk Table summarizes our participants demographics which we compare to those of a recent study by Redmiles et al comparing sampling methods Our participant population skewed male Consistent with Redmiles et al participants were more highly educated than the overall population Political affiliation was split of our participants described themselves as Democrats as Independent and as Republican A large portion reported no experience with computer science A roughly similar percentage reported no experience with probability Most respondents reported having no experience with machine learning In spite of this most participants appeared to understand the mathematical concepts we presented to them We tested participants understanding of the graphs in our survey with a series of Ours Redmiles et al Gender Male Female Gender non-binary Race/Ethnicity White Black/African-American Hispanic/Latinx Other Asian Native American Hawaiian/Pacific Islander Income Education High School High School Some college/Two-year degree Four-year degree or above Table Participants demographics which we compare to Redmiles et als analysis of MTurk workers A dash indicates that the study did not use that particular category graph comprehension questions at the end of the survey These graph understanding questions were imperfect and highly limited However a majority of participants were able to correctly identify valid and invalid inferences Furthermore we administered a cognitive reflection test CRT a series of three quantitative questions shown to correlate with quantitative reasoning ability More than half of participants got all three questions correct got two and got one The CRT was at the end of the survey and the answers are free-response integers Participants good CRT performance suggests both that they were taking the survey seriously and that they had decent quantitative reasoning skills Preferred Trade-offs We investigated six pairs of trade-offs between properties each with variants in which majority White and minority African-American groups were disadvantaged We observed a preference for equalizing false positives over equalizing accuracy As shown in Figure this preference was statistically significant yet not overwhelming Comparing false positives against accuracy of participants probably or definitely preferred the model that equalized false positive rates when White defendants were disadvantaged and of participants favored the model that equalized false positives when African-American defendants were disadvantaged In contrast only and of participants respectively probably or definitely preferred the model that equalized accuracy at the expense of disparate false positive rates The rest were unsure That we observed this preference toward equalized false positive rates is particularly notable because ProPublicas reporting centered on COMPAS doing the opposite equalizing accuracy at the expense of disparate false positive rates In short this result highlights that COMPAS equalized the property that our participants preferred significantly less potentially explaining some of the public controversy about the COMPAS system Definitely Probably Dont know Probably Definitely Maj Outcome Race used Min Maj Acc Race used Min Maj Acc Outcome Min Maj FP Race used Min Maj FP Outcome Min Maj FP Acc Min Z p Z p Z p Outcome Race not used Acc Race not used Acc Outcome FP Race not used FP Outcome FP Acc Figure Participants preferences for one model within a trade-off pair over the other The left and right arrows indicate preferences toward the side of the trade-off listed on the left or right axes of the graph respectively Among the remaining five trade-offs again shown in Figure we also observed a statistically significant preference toward equalizing false positives over equalizing outcomes when White defendants were disadvantaged FP-Outcome-Maj Here of participants preferred the model that equalized false positives while only preferred the model that equalized outcomes This effect disappeared when African-Americans were disadvantaged Notably for each trade-off a non-trivial fraction of participants preferred each side of the trade-off see Fig Recall that participants were assigned to one of twelve conditions six trade-off pairs multiplied by two different groups that could be disadvantaged For the nine conditions in which we did not observe a statistically significant preference at least of participants probably or definitely preferred each side of the trade-off Furthermore at least of participants definitely preferred each side of the trade-off In other words a non-trivial fraction of participants would be unhappy no matter which side of the trade-off was chosen Our qualitative analysis of participants free-text justifications for their choice emphasized that participants successfully identified the characteristics we were varying yet did not shed much insight beyond this confirmation When participants articulated specific reasons for their choice they often did so by saying that equalizing the particularly property captured by the model they chose was more fair or less biased of all explanations For example P FP-Outcome-Min said they chose the model that equalized outcomes because in the alternative model White people are unfairly being given bail more than blacks An additional of explanations were too vague to provide additional insight For example P Acc-RaceUsage-Maj wrote A combination of both can provide valuable information Not including race changes the results significantly Testing both models and determining how much each factor should weigh would help improve both models While few participants described fully what they meant by the terms accuracy mentioned in of all answers the use of race and equality were all invoked Explanations invoking equality tended to further mention evenness and consistency For example P FP-Outcome-Min preferred to equalize outcomes because the probability of bail is consistent across race When race usage was part of the trade-off justifications often mentioned this property For example P FP-RaceUsage-Min preferred to equalize false positives even when explicitly considering race because it is much more accurate overall In contrast P Acc-RaceUsage-Maj wrote Id rather not have race as an issue for the computer to factor in because it is irrelevant to the decision being made Participants who wrote that the model they saw was fair because it did not use race generally did not expand on what they thought not using race meant We intentionally limited our explanation for the use of race condition to a simple statement that race was not an explicit variable in the model thus leaving the door open to participants identifying the issue of proxy variables This reflects the perspective of process fairness which is not explicitly concerned with whether some combination of variables can combine into another impermissible variable Our qualitative responses indicate that participants largely seem to have assumed that not using race meant race did not factor into the classification at all For the models where there was some disparity and race was not used of participants qualitative responses pointed to the disparate quantity to indicate either that some other sensitive variable played a role in the models predictions or that the disparity indicated race was used For example one participant explained that a model that exhibited differences in outcomes yet did not use race was not at all fair by stating It states that race is not a factor but clearly it is as its showing that white have a higher probability of paying bail than African Americans P Outcome-RaceUse-Min Human Judges vs Models After participants saw both sides of the trade-off we asked them whether they preferred a human judge or their choice of either of the two models they had seen For eight out of the twelve conditions we observed a statistically significant preference for human judges Figure shows this graphically Among these eight conditions the percentage of participants probably or definitely preferring a human ranged from Acc-RaceUsage-Maj to FP-Acc-Maj We did not observe a statistically significant preference for using an ML model in any condition The Outcome-RaceUsage conditions had the highest percent of participants who favored the model over a judge for Outcome-RaceUsage-Maj and for Outcome-RaceUsage-Min Furthermore these same conditions also had the highest proportion of people who strongly favored the model over a judge and for -Maj and -Min respectively That Definitely model Probably model Unsure Probably human Definitely human Maj Race usage Outcomes Min Maj Acc Race usage Min Maj Acc Outcomes Min Maj FP Race usage Min Maj FP Outcomes Min Maj FP Acc Min Z p Z p Z p Z p Z p Z p Z p Z p Figure Participants preferences for a human judge versus either model seen human judges were fallible and models were more objective appeared in of participants free-text justifications in these cases where the participant preferred a model over a judge For example P Outcome-RaceUsage-Maj wrote Human beings are biased and easily manipulated Similarly P Outcome-RaceUsage-Min wrote Models lack bias and personal experiences that they will not be able to use to make decisions However being able to use individualized judgment was also sometimes expressed positively In contrast participants expressed a number of reasons for preferring a human judge of justifications mentioned a judges ability to make individualized case-by-case decisions For example P FP-Acc-Maj wrote I would use a human judge because I think that they would be able to see past the race of the offender and recognize outliers in their personalities and past that would make them more risky to not appear before trial Similarly P FP-Acc-Maj wrote A human judge would be able to take race out of the equation An additional of justifications discussed the judge as either more accountable or more ethical Finally of justifications expressed that judges have ethical reasoning capabilities beyond those of models According to P FP-RaceUsage-Min I still think human judges can see things that a model cannot such as morals values and attitude These cannot be taken into account by the model as it only looks at conditions known to it Similarly P Outcome-RaceUsage-Min wrote Programmers arent trained to at least try not to be biased Even though participants preferred equalizing false positive rates to equalizing accuracy as described in the previous section they nonetheless still significantly preferred a human judge In particular of FP-Acc-Min participants and of FP-Acc-Maj participants preferred a human judge to either model they saw Fairness and Bias Prior to having participants compare the two models in their tradeoff we asked them to rate the fairness bias and usefulness of each model individually Across most models participants tended to lean toward rating the model as not at all biased or only somewhat biased For only one model did participants significantly lean toward rating it as biased Nonetheless participants tended not to rate these models as fair In particular for only one model did participants significantly lean toward rating it as fair In contrast for six others they leaned towards rating it as not at all fair or only somewhat fair In spite of these trends there were no models where participants opinions were unanimous and many where opinions were strongly divided Bias Participants rated the bias of each of the two models they saw on a five-point scale from completely biased to not at all biased There were six trade-off pairs two possible disadvantaged groups and two models per pair yielding twenty-four individual models For sixteen of these twenty-four participants significantly tended towards rating the model as not biased as shown in Figure For these sixteen models the percentage of participants who rated the model as either not at all biased or only somewhat biased ranged from to In contrast for only one model FP Outcome Min were responses significantly skewed toward rating the model as biased That is participants tended to consider it biased when outcomes were equalized at the expense of disparate false positive rates that disadvantaged African-American defendants We also tested differences between bias ratings within a tradeoff pair Consistent with the results of the preferred side of tradeoffs we found a significant difference in bias assessments between models in the FP-Acc-Min trade-off p That is participants felt that equalizing accuracy was more biased than equalizing false positive rates Fairness As shown in Figure we found fewer significant trends in participants fairness ratings For six of the twenty-four models participants significantly tended toward rating the model as not fair For these six models between and of participants thought the model was not at all fair or only somewhat fair In contrast for only one of the twenty-four models did we observe a significant trend toward considering a model fair Overall of participants rated as mostly fair or completely fair the model in which more African-American defendants than White defendants were granted bail even though race was not used as an input to the model Outcome Race not used Maj This perception of fairness did not persist when White defendants were favored see Figure Differences based on which group was disadvantaged are discussed further in Section Like bias ratings fairness ratings were polarized of participants rated at least one of the two models they saw as either completely fair or not at all fair We also tested for differences in perceived fairness between models in a pair The model equalizing false positives was rated as more fair than the model equalizing accuracy when Whites were Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Perceptions of Bias Perceptions of Fairness Maj Outcome Race not used Min Maj Outcome Race used Min Maj Acc Race not used Min Maj Acc Outcome Min Maj Acc Race used Min Maj Acc Outcome Min Maj FP Race not used Min Maj FP Outcome Min Maj FP Acc Min Maj FP Race used Min Maj FP Outcome Min Maj FP Acc Min Completely biased Mostly biased Dont know Somewhat biased Not at all biased Not at all fair Somewhat fair Dont know Mostly fair Completely fair Z p Z p Z p Z p Z p Z p Z p Figure Participants ratings of each models bias left and fairness right The bold axis in the center indicates the condition Not at al l se d hat se d D on t ow M os se d se d Not at al l r hat r D on t Know M os r r Fairness/Bias Correlation of bias for each model Percentages are of total answers disadvantaged p This echoes a similar finding in differences of their bias ratings The Relationship Between Bias and Fairness While one might assume that models that are not biased are thereby fair we observed a much more complex and nuanced relationship between participants ratings of bias and fairness Participants tended to rate the model with equal outcomes and unequal false positive rates disadvantaging African-Americans FP Outcome Min as biased and not fair Participants tended to rate the model where race was not used but outcomes were unequal disadvantaging White defendants Outcomes Race not used Maj as both fair and not biased Participants tended to rate the model with equal false positives and unequal accuracy disadvantaging African-American defendants FP Acc Min as not biased yet also not fair We found that participants tended to rate models they considered biased as unfair Graphically the triangular nature of Figure shows that models that were considered biased were almost never considered to be fair The most common combination was that a model was not at all biased and also completely fair However a model that they rated as not biased was not necessarily rated as fair of participants rated at least one model as either not at all or only somewhat biased yet not at all or only somewhat fair As shown in the leftmost column of Figure some participants rated models as not at all biased yet only mostly somewhat or not at all fair That is not to say that there is not a strong association between bias and fairness The Kendalls correlation coefficient between fairness ratings and bias ratings was p Ratings of a model being not biased but also not fair were most frequent in response to FP Outcome Maj composing of all responses to that model Similarly they were of all responses to the FP Acc Maj model Such answers tended to express ambivalence For example P FP-Outcome-Maj wrote that the model seems somewhat fair but mistakenly denying bail to of Whites and African Americans still seems like a high error rate They then explained that they did not think the model was biased because Model X mistakenly denies bail equally across both Whites and African Americans In other words high error rates could make a model unfair even though these rates were equal across racial groups making the model unbiased The Relationship Between Bias and Judge Preferences Initially we had expected that participants who rated a model they saw as not at all biased would prefer a model over a human judge since they had an unbiased option However most of these participants nonetheless preferred a human judge over either model they saw Figure shows little difference in the distribution of preferences for a human judge versus a model between participants who rated both models they saw as unbiased and those who rated one model as biased and the other as unbiased About half of participants in Neither model is biased One model is biased Both models are biased Definitely model Probably model Unsure Probably human Definitely human Figure Preference for a human or a model broken down by the participants ratings of the individual models bias each case probably or definitely preferred a human judge Participants who reported both models they saw as at least somewhat biased were even more likely to prefer a human judge It is also noteworthy that even though of participants thought that the Outcome Race not used Maj model was mostly or completely fair only of participants probably or definitely preferred a model instead of a human judge This suggests that even relative consensus about the fairness of a model may be insufficient to produce consensus about whether an ML model is more appropriate to use than a human judge Qualitative coding of the reasons for bias ratings showed that participants largely understood the trade-offs yet did not provide much deeper insight Most frequently participants reported finding a model biased because of disparate false positive rates of responses the explicit consideration of race disparate accuracy and disparate outcomes Usefulness Participants ratings of a models usefulness were largely redundant to those of bias and fairness In the majority of models where we observed a statistically significant trend toward one side the trends were toward the model not being useful The one model where there was a statistically significant trend toward a model being useful was the already discussed Outcome-Race not used-Maj model The results for models perceived usefulness are shown in the supplementary appendix The Impact of Who Was Disadvantaged For each of the six trade-offs half of participants saw a model where the majority group Whites were disadvantaged by the disparate rate whereas the other half saw a model where the minority group African-Americans were disadvantaged We observed several instances in which there appear to be differences in participants reactions between the -Maj and -Min variants Despite these apparent differences statistical testing did not distinguish between the -Maj and -Min variants The model in which outcomes were equalized at the expense of disparate false positive rates was rated as mostly biased or completely biased by of participants when African-Americans were disadvantaged yet only by of participants when Whites were disadvantaged Similarly in the model where race was not considered and outcomes were disparate across groups of participants rated the model as mostly fair or completely fair when Whites were disadvantaged yet only did so when African-Americans were disadvantaged Though not statistically significant the differences we observed suggest a more targeted study may have found significant differences In particular our statistical power in comparing the -Maj and -Min variants was relatively low given that each condition had roughly participants Furthermore we were also making many comparisons and therefore correcting for multiple testing A few participants discussed which group was disadvantaged in their free-text justifications In total ten of the participants justified their choice of one model over the other in terms of the disadvantaged group For example P FP-Acc-Maj wrote I would choose the model with higher accuracy for Whites since I am a white American and that model has a higher success rate for white Americans As P FP-Outcome-Maj wrote the model disproportionately impacts white people On the other hand the whole bail system disproportionately impacts Black folks so it may be a wash Overall of participants volunteered in one of their free-text justifications that they considered the justice system as a whole to be unjust DISCUSSION After discussing our protocols limitations Sec we compare our findings with those of prior empirical studies on perceptions of fairness Sec We conclude by recapitulating our works key lessons and proposing future work Sec Limitations Our experiment was limited in a number of ways First we studied a convenience sample that is not necessarily generalizable to any larger group We asked about differences in how models treated White and African-American defendants but did not have a sufficiently large number of non-White participants to meaningfully determine whether there was an interaction between the participants own demographics and what group was disadvantaged In addition we only examined fully automated models and fully manual human judges whereas a hybrid approach a human who relies in part on an automated model is a potential compromise Furthermore we investigated only a single visualization of the model properties and differences While we piloted these visualizations using cognitive interviews to verify their intelligibility for participants without particular statistics expertise other work has used different visualizations including simple text statements of percentages tables and novel visualizations We used straightforward bar graphs annotated with the text percentage We chose bar graphs because they communicate difference and magnitude Because we used a convenience sample the precise percentages reported are unlikely to generalize However we expect the broad patterns we saw to generalize with implications for the design of fairness interventions We found that even outside of the ethics and machine learning community there exist strong nuanced views about the acceptability of different approaches to fairness Additionally participants expressed significant differences of opinion Comparison with other empirical studies As noted in the previous section prior empirical studies of how humans perceive the fairness of machine learning models vary from each other and from our work in how they visualize the properties of these models Our supplementary appendix discusses this confound further While this limits our ability to directly compare with prior work in this section we highlight similarities and differences in conclusions among these studies Future work could investigate how the visualization of models impacts perceptions of fairness Grgić-Hlača et al found that the fairness of explicitly considering race to predict recidivism risk depended on how doing so impacted model accuracy While only of their participants thought it unconditionally fair to consider race thought it fair to consider race if it improved accuracy We investigated the fairness of explicitly considering race in a model that equalized accuracy across racial groups rather than strictly increasing accuracy While we observed high variance in fairness ratings for this model most participants who rated the model as not at all fair mentioned the unfairness of using race regardless of equalized accuracy Among those who rated the model as fair most referenced equalized accuracy as a mitigating factor eg P wrote Its equally accurate for people of different races so I think that makes the use of the data justified Our findings are therefore consistent with Grgić-Hlača et als findings suggesting that people sometimes find explicitly considering race justified if doing so improves performance Saxena et al found the race of the person advantaged in loan allocation affected perceptions of fairness Specifically participants found it more fair to give the entire sum to the candidate with the higher loan repayment rate than to divide it equally but only when the candidate with the higher repayment rate was Black In contrast we did not observe any statistically significant effects when we compared conditions in which Whites were negatively impacted to those in which African-Americans were negatively impacted This suggests that race may be more significant when examining individuals as in Saxena et al rather than groups Srivastava et al found a preference for demographic parity equalizing the percentage of people who receive a positive classification which was our Outcome condition over other definitions of fairness like equalized false positive rates or false negative rates In contrast we found that equalizing the false positive rate at the expense of having disparate outcome rates across groups was preferred over the opposite One possible explanation for this lies in the different ways the two studies visualized the properties of a model which we discuss further in the supplementary appendix Kennedy et al found the size of the training data the false positive and false negative rates and the institutional source most impacted which model participants trusted We also found that equalizing false positive rates was generally valued over equalizing accuracy Whereas Kennedy et al found that their participants generally expressed trust in algorithmic methods our participants expressed a general preference for a human judge A possible explanation for this difference is that while we showed differences between model properties by racial group Kennedy et al investigated only the overall false positive rate false negative rate and accuracy Had their participants been aware of differences across racial groups they may have been less likely to trust algorithms Conclusions and Future Work Our survey-based experiment asked participants to comparatively evaluate two models that exemplified the two sides of the realistic trade-offs between fairness-related properties We observed a marginal yet statistically significant preference for equalizing false positives across demographic groups over equalizing accuracy For other trade-offs we observed at most a weak and non-significant preference Notably though each side of each trade-off was strongly preferred by a non-trivial fraction of participants This result casts doubt on the possibility of achieving broad acceptance across society that the right fairness decision was made among mutually exclusive yet seemingly desirable statistical definitions of fairness Furthermore we observed a general yet not often not overwhelming preference for a human judge over models capturing either side of the realistic trade-offs we examined Even when participants thought that neither side of the trade-off was biased over half of them still preferred a human judge over the model We also found that just because a participant felt a model was not at all biased did not imply that they considered the model fair Our findings are a starting point for future investigations of algorithmic fairness from samples of participants without specialized knowledge or deep previous engagement with algorithmic justice For example a similar instrument could be used to understand how participants with first-hand experience of the criminal justice system approach these questions Are the trade-offs similarly contentious Do the explanations those participants provide differ Though we did find a pattern of marginal support for equalization of false positives over accuracy even in this a bare majority of participants supported one side over the other Nonetheless our results should not be read as an attempt to make definitive claims about which sides of trade-offs should be favored A machine learning developer confronted with the tough types of trade-off decisions we investigated might be tempted to crowd-source the decision by surveying the public Our findings suggest that crowdsourcing is unlikely to produce consensus or full clarity about the decision Deciding a trade-off on the basis of will likely leave a significant portion of people unhappy Furthermore artificially curtailing options to be just between models rather than leaving options to not have a model at all is also unlikely to elicit true preferences Future interventions should promote the visibility of design decisions Data scientists are not well-situated to resolve the tensions and disagreements we have identified Instead data scientists should make clear the decisions they have made and allow experts and the public to deliberate about whether the model should be used To that end future work should investigate how to facilitate public involvement in decisions concerning fairness In particular future research into tools and mechanisms for identifying decisions during the data science workflow should be emphasized Understanding the role the visualization of model properties plays in discourse about design decisions is key