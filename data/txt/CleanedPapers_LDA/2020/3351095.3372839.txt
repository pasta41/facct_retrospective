Fairness Warnings and Fair-MAML Learning Fairly with Minimal Data Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools we propose two algorithms Fairness Warnings and Fair-MAML The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness ie training a fair model on a new task with only data points Then we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair CONCEPTS Computing methodologies Machine learning KEYWORDS machine learning fairness meta-learning covariate shift INTRODUCTION As machine learning tools become more responsible for decision making in sensitive domains such as credit employment and criminal justice developing methods that are both fair and accurate become critical to the success of such tools Correspondingly there has been an increasing amount of academic interest in the field of fair machine learning for surveys see Research on fairness is often concerned with identifying a notion of fairness developing an approach that mitigates the notion of fairness and applying the approach to a variety of data sets in a supervised learning setting see eg However we ask where this leaves fairness-concerned practitioners who are interested in using fair tools for their particular applications but have access to minimal or no training data In particular we introduce the following questions When can a practitioner rule out the use of a fair tool trained in a similar but slightly different context How can a practitioner who has access to only a few labeled training points successfully train a fair machine learning model We suggest the relevance of these questions through the motivating scenario of recidivism prediction There have been calls for and extensive action towards the proliferation of criminal risk assessment tools in the United States However there is often a disconnect between the intended use of these tools and how they are used in practice which can lead to undesirable or ineffective results The LJAF a foundation that focuses on addressing societal issues through data driven approaches argues for a risk assessment tool that can be adopted by judges and jurisdictions anywhere in America and has released such a tool that has been widely used We observe that minor demographic differences in the distribution of data can lead to broad effects on statistical notions of group fairness on fairly trained machine learning models These results could impact the ways in which fair risk prediction tools are used because such results suggest the proliferation and transfer of such methods between precincts can lead to their unreliability The most related work to the proposed questions is fairness applied to transfer learning and the covariate shift problem in machine learning Covariate shift deals with situations where the distribution of data in application differs from the distribution of data in training Covariate shift is a well studied field and there are numerous methods that attempt to train supervised learning classifiers that are robust to test distribution shifts with respect to accuracy Related methods have been developed to address fairness in the covariate shift setting Kallus et al address the problem of systematic bias in data collection and use covariate shift methods to better compute fairness metrics under such conditions Coston et al consider the situation where there are sensitive labels available in only the source or target domain and propose covariate shift methods to solve such problems Additional work focuses on transferring fair machine learning models across domains Madras et al propose a solution called LAFTR that uses an adversarial approach to create an encoder that can be used to generate fair representations of data sets and demonstrate the utility of the encoder for fair transfer learning Similarly Schumman et al provide theoretical guarantees surrounding transfer fairness related to equalized odds and opportunity and suggest another adversarial approach aimed at transferring into new domains with different sensitive attributes Lan and Huan observe that the predictive accuracy of transfer learning across domains can be improved at the cost of fairness Related to fair transfer learning Dwork et al use a decoupled classifier technique to train a selection of classifiers fairly for each sensitive group in a data set We argue that our proposed questions are different than the existing work in the following ways While methods exist that address fairness and covariate shift such methods do not address the problem of communicating to practitioners and policy makers what domain specific factors might cause a fairly trained model to fail to be fair in practice Additionally the problem of training fair machine learning models with very little task specific training data is relatively unstudied Practitioners might have access to minimal training data in one task and sufficient data from other related tasks This data might be minimal or skewed in terms of which sensitive attribute or label the data belongs to because of data collection issues associated with sensitive data sets like those discussed in Kallus et al Though LAFTR offers a way to transfer machine learning models between tasks we observe it is unsuccessful in very data light situations In this paper we propose two different methods to address the proposed problems First we discuss the situation where a practitioner has no training data and must decide whether to use a fair machine learning tool trained in another similar but slightly different context We introduce Fairness Warnings a model agnostic approach that provides interpretable boundary conditions on fairness for when not to apply a fair model in a different but related context because the model may behave unfairly Fairness Warnings provide an interpretable model that indicates what distribution shifts to a data sets features may cause a fairly trained classifier to act unfairly in terms of a user specified notion of group fairness While the covariate shift problem setting allows for arbitrary changes to the testing distribution we only consider mean shifts in this paper We discuss the limitations imposed by this problem restriction in section To provide intuition if Fairness Warnings were trained on a recidivism classifier with respect to the rule of demographic parity the model would provide conditions such as what mean shifts to the features age and priors count would cause the model to score demographic parity lower than Because law enforcement agencies report general covariate crime information it is likely the case that precinct specific practitioners have access to these high level details and can effectively use fairness warnings to assess whether unfairly trained machine learning model may behave fairly in their application Second we consider a better route to transfer a fair machine learning model through meta-learning We introduce a meta-learning approach Fair-MAML to quickly train models that are both accurate and fair with respect to different notions of fairness with very minimal task specific data Fair-MAML is based on a meta-learning algorithm called Model Agnostic Meta Learning or MAML that has shown success in reinforcement learning and image recognition Fair-MAML encourages the learning of more general notions of fairness and accuracy that allow it to achieve strong results on new tasks with only minimal data available In this way Fair-MAML can escape the negative inter-distributional effects of sharing a fair machine learning model by providing a model that can be quickly fine-tuned a specific task We connect Fairness-Warnings and Fair-MAML by applying Fairness Warnings as boundary conditions on the fine-tuned fair meta-model Besides offering better ways for practitioners to implement fair machine learning models these methods also provide ways for involved parties to question and assess the results of fair machine learning models Considering recidivism prediction the absence of success surrounding the adoption of recidivism tools in the United States has been explained in part by judges lack of trust in algorithm recidivism tools Indeed such scores are sometimes given to judges without any context By including Fairness Warnings in assessments users may be able to better understand under what conditions the algorithm will fail to be fair This could help increase judge understanding of such tools as well as defense attorneys ability to challenge its results Additionally by fine-tuning recidivism prediction to specific precincts using Fair-MAML users may more readily trust that such algorithms are delivering relevant predictions than if they were only trained on disparate localities BACKGROUND Fairness We consider a binary fair classification setting with features X labels Y and sensitive attributes A Our goal is to train a model that outputs predictions Y such that the predictions are both accurate with respect toY and fair with respect to the groups defined by A We consider the positive outcome being labeled at low risk of recidivism and the negative outcome being labeled high risk Within the sensitive attribute one label is protected denoted and the other unprotected denoted The protected group might be historically disadvantaged groups such as women or African-Americans There are three often-used ways to define group fairness in this setting The first demographic parity or statistical parity can be formalized as PY A PY A This is also known as a lack of disparate impact or discrimination A value closer to indicates fairness The second group fairness definition equalized odds requires that Y have equal true positive rates and false positive rates between groups where values closer to indicate fairness PY A Y y PY A Y y y This is also known as error rate balance or disparate mistreatment Equal opportunity or equal true positive rates introduces relaxed constraints on and requires the equivalence to hold only on the positive outcome in Y As compared to equalized odds equal opportunity often allows for increased accuracy Meta-Learning Meta-learning is concerned with training models such that they can be trained on new tasks using only minimal data and few training iterations within a domain Meta-learning can be phrased as learning how to learn because such methods are trained on a range of tasks with the goal of being able to adapt to new tasks quickly Metaphorically this can be likened to finding a base camp meta-model from which you can quickly ascend to multiple nearby peaks optimized per-task models In the supervised learning setting each task T where D is a data set containing pairs X Y and L is a loss function We consider a distribution over tasks PT which we train the meta-model to adapt to Supposing the meta-model is a parameterized function with parameters its optimal parameters are argmin This states that the optimal parameters of the model are those that minimize the loss with respect to both L and D Intuitively the model parameters should be such that they are nearly optimal for a range of tasks Ideally this will mean that optimizing for any new task is quick and requires minimal data In the meta-learning scenario used in this paper we train to learn a new task T PT using examples drawn from T Additionally we assume can be optimized through gradient descent During the meta-training procedure examples are drawn from T The model is trained with respect to and L and the test performance is evaluated with new examples The use of only training examples for learning a new task is often referred to as K-shot learning and such methods have generally been applied to image recognition and reinforcement learning Based on the test performance is improved The meta-model is evaluated at the end of meta-training through a set of tasks that are not included in the meta-training procedure METHODS Fairness Warnings Framework Similar to the formalization of LIME in Ribeiro et al we define fairness warnings as an interpretable model G where G is a class of interpretable models such as decision trees or logistic regression Further is a function Rd where Rd is a set of distribution shifts applied to the features labels and sensitive values of some test data set D X Y A under which a fair model is evaluated We assume is fair with respect to some notion of group fairness such as equation and the codomain of represents whether the potential shift may result in fair classifications according to that notion of group fairness Additionally we assume that group fairness can be evaluated as fair or unfair according to some binary notion of fairness success such as the rule of demographic parity We assume access to a function D that maps between a data set and whether acts fairly on that data set according to the binary notion of group fairness Problem Restrictions In typical covariate shift settings the testing distribution can be changed in any number of ways including being drawn from an entirely different distribution altogether In this application we only consider shifts to the mean of the distribution of data that is available for training Under this assumption there could be more complex changes to the distribution that affect the mean but are not captured by this summary statistic and that may affect fairness Because we only consider a subset of the possible changes to the testing distribution Fairness Warnings only indicate what mean shifts may lead a classifier to not be fair and do not strongly indicate fairness if no warning is issued Additionally it could be the case that Fairness Warnings predict unfairness for certain mean shifts but due to other changes to the testing distribution the classifier actually behaves fairly Because of these challenges Fairness Warnings are just that warnings that there is some evidence that suggests the model may behave unfairly with respect to a notion of group fairness SLIM In practice we use Super sparse Linear Integer Models or SLIM as the interpretable model SLIM creates a linear perceptron that reduces the magnitudes of the coefficients removes unnecessary coefficients and forces the coefficients to be integers SLIM is a highly interpretable method that is well suited to trading off between model complexity in presentation and accuracy SLIM has been used in sensitive applications such as risk scoring It has hyperparameters C and C controls the marginal accuracy a coefficient must add to stay in the model while does the same except for the magnitude of the coefficients Fairness Warnings Algorithm In order to train we generate some user specified number of perturbed versions of D using mean shifts We generate shifts for numerical features by randomly sampling from a Gaussian distribution with the standard deviation of the feature and mean zero The number sampled is the mean shift across the feature To perform the shift we simply add the number to all the values in the feature We assume categorical features are one-hot encoded and thus only have two binary categorical features in We shift each categorical feature by assuming each feature is drawn from a binomial distribution and use the percentage of features labeled as p We shift the feature vector by drawing a new p from a Gaussian distribution p and randomly sample a new vector according to p If p is less than or greater than we adjust p to or respectively Doing this a user specified number of times we create a set of shifted variations D of the original D For each shifted data set we generate a fairness label using the binary notion of group fairness We create a data set of mean shifted data sets and their group fairness behavior with respect to D Finally we train using D as the features and as the labels Intuitively we train so that it learns to predict what mean shifts may result in unfairness Assuming shi t is some function that computes the mean shifting scheme above the algorithm for generating fairness warnings is given as Algorithm Fair Meta-Learning K-shot Fairness In order to address the problem of learning fairly from minimal data on a new task we introduce the notion of K-shot fairness Given training examples K-shot fairness aims to Require D data set Require fairness notion Require interpretable model Require N number of shifts to perform for i N do D shi tD D D end for Train using D as features and as labels return Algorithm Fairness Warnings Require pT distribution over tasks Require step size hyperparameters randomly initialize while not done do Sample batch of tasks Ti pT for all Ti do Sample datapoints D from Ti Evaluate using D and Compute updated parameters i Sample new datapoints D i x from Ti to be used in the meta-update end for Update i i using each D i end while Algorithm Fair-MAML quickly train a model that is both fair and accurate on a given task Additionally because the relationship between fairness and accuracy is often understood as a trade-off an additional aim is to allow tuning of such a model so that it achieves different balances between accuracy and fairness using training points The language used in this paper surrounding K-shot learning differs slightly from the language used in typical K-shot learning scenarios such as image recognition In K-shot image recognition the goal is to learn how to distinguish between N different image labels using only training examples of each type The training set size is then examples Because we assume all the tasks to be binary labeled all of our tasks are -way In referencing K-shot fairness we will mean that we are using training examples total irrespective of class label with the assumption that all tasks are -way Fair-MAML Framework We expand the meta learning framework from section such that each task includes a fairness regularization term R and fairness hyperparameter Additionally we require that D have a protected feature A such that D X Y A The goal of R is to minimize some notion of group fairness and dictates the trade off between R and L A task is defined as T We adjust equation such that the optimal parameters are now argmin In order to train a fair meta-learning model we adapt Model-Agnostic Meta-Learning or MAML to our fair meta-learning framework and introduce Fair-MAML MAML is trained by optimizing performance of across a variety of tasks after one gradient step MAML is particularly well suited to easy fairness adaption because it works with any model that can be trained with gradient descent The core assumption of MAML is that some internal representations are better suited to transfer learning The loss function used by MAML is effectively the loss across a batch of task losses Thus the MAML learning configuration encourages learning representations that encode more general features than a traditional learning approach The MAML algorithm works by first sampling a batch of tasks computing the updated parameters after one gradient step of training on data points sampled from each task and finally updating based on the performance of on a new sample of points We modify MAML to Fair-MAML by including a fairness regularization term R in the task losses The algorithm for Fair-MAML is given in algorithm By including a regularization term we hope to encourage MAML to learn generalizable internal representations that strike a desirable balance between accuracy and fairness Fairness Regularizers A variety of fairness regularizers have been proposed to handle various definitions of group fairness Because MAML has shown success with the use of deep neural networks we require regularization terms compatible with neural networks Methods that require the model to be linear are clearly not applicable In addition Fair-MAML requires that second derivatives be computed through a Hessian-vector product in order to calculate the meta-loss function which can be computationally intensive and time-consuming Thus it is critical that our fairness regularization term be quick to compute in order to allow for reasonable Fair-MAML training times We propose two simple regularization terms aimed at achieving demographic parity and equal opportunity that are easy to implement and extremely quick to compute denote the protected instances in X and Y The demographic parity regularizer is D PY A x D x This regularizer incurs a penalty if the probability that the protected group receives positive outcomes is low Our value assumption here is that we want to adjust the likelihood of the protected class receiving a positive outcome upwards Namely we do not reduce the rate at which the unprotected class receives positive outcomes and instead adjust upwards the rate at which the protected class receives positive outcomes Additionally we consider a regularizer aimed at improving equal opportunity Let D denote the instances within X that are both protected and have the positive outcome in Y D PY A Y x D We have a similar value assumption using this regularizer as the one for demographic parity We adjust the true positive rate of the protected class upwards and do not decrease the true positive rate of the unprotected class In the case of recidivism prediction our value system could be likened to the belief that it is better not to classify more non-black defendants as high likelihood for recidivism and instead classify black defendants at a lower rate of likelihood to recidivate EXPERIMENTS We first demonstrate the individual utility of both Fairness Warnings and Fair-MAML We then show their usefulness as a combined method Fairness Warnings COMPAS Recidivism Experiment Setup We initially consider applying Fairness Warnings to the COMPAS recidivism data set The COMPAS recidivism data set consists of data from over criminal defendants from Broward County Florida It includes attributes such as the sex age race and priors for the defendants We pre-process the data set as described in Angwin et al We create a binary sensitive column for whether the defendant is African-American We predict the ProPublica collected label of whether the defendant was rearrested within two years We trained a neural network as the model to use with Fairness Warnings We trained two models one regularized for demographic parity and the other equal opportunity using the regularization terms from equations and respectively The demographic parity regularized model scored accuracy and demographic parity on a test set The equal opportunity regularized model scored accuracy and equal opportunity using the same test set For the demographic parity fairness warnings we set the fairness warnings demographic parity threshold at Meaning if the classifier scored demographic parity above it was deemed fair In the equal opportunity setting we set the threshold to We generated perturbed data sets of which were classified unfairly according to demographic parity We set to e and C to e We found that was able to classify whether the shifts applied to the perturbed data sets would result in unfair group fairness behavior with accuracy on a test set Using the same perturbed set the equal opportunity regularized network was found to be unfair in of the perturbed examples Using the same hyperparameters as before was able to classify whether the shifts would result in unfairness with respect to equal opportunity with accuracy The Fairness Warnings for the COMPAS data set is given in figure COMPAS Recidivism Experiment Analysis The COMPAS Fairness Warnings both rely on priors_count and age to determine what mean shifts to the data set may result in unfairness In the demographic parity warning for instance if the mean group age applied to were to increase by years and mean priors were to remain unchanged the fairness warning would predict unfairness because the score total would be However in the equal opportunity case the same shift would not yield unfairness because A case that would result in unfairness in the equal opportunity setting would be a decrease in mean priors count by one charge and for age to remain level ie Overall the SLIM implementation of fairness warnings showed good ability to classify whether certain mean shifts applied to the feature values of the COMPAS data set would result in unfairness Because SLIM is tunable with respect to the importance threshold of features shown in the presentation of the model the classifier only outputs of a possible feature values in both warnings The presentation is simple A practitioner would only have to perform a few arithmetic operations in order to compute the fairness warning outcome Additionally we were able to train a random forest classifier using estimators from the Scikit-learn implementation which scored and accuracy on the demographic parity and equal opportunity fairness warnings tasks respectively This suggests that more robust models could serve as much more accurate fairness warnings than SLIM Presenting a random forest of such size in a digestible way to a user would be difficult However the success of the random forest to perform this task indicates that improved interpretable methods that achieve equal levels of interpretability to SLIM but higher levels of accuracy on the fairness warnings task could serve as more desirable fairness warnings Fair-MAML Synthetic Experiment Setup We illustrate the usefulness of Fair-MAML as opposed to a regularized pre-trained model in fair few-shot classification through a synthetic example based on Zafar et al We generate two Gaussian distributions using the means and covariances from Zafar et al The first distribution is set to N and the second is set to N During training we simulate a variety of tasks by dividing the class labels along a line with y-intercept of and a slope randomly selected on the range All points above the line in terms of their y-coordinate receive a positive outcome while those below are negative Using the formulation from Zafar et al we create a sensitive feature by drawing from a Bernoulli distribution where the probability of the example being in the protected class y y y where x Here controls the correlation between the sensitive attribute and class labels The lower the more correlation and unfairness We randomly select from the range to simulate a variety in fairness between tasks In order to assess the fine-tuning capacity of Fair-MAML and the pre-trained neural network we introduced a more difficult finetuning task During training the two classes were separated clearly by a line For fine-tuning we set each of the binary class labels to a distribution The positive class was set to distribution and the negative class was set to distribution In this scenario a straight line cannot clearly divide the two classes We assigned Predict UNFAIR DEMOGRAPHIC PARITY if SCORE Feature Original Mean Score per unit increase/decrease Total priors_count priors points prior age years points year ADD POINTS FROM ROWS to SCORE Warning accuracy true positive rate true negative rate Predict UNFAIR EQUAL OPPORTUNITY if SCORE Feature Original Mean Score per unit increase/decrease Total priors_count priors points prior age years points year ADD POINTS FROM ROWS to SCORE Warning accuracy true positive rate true negative rate Figure The Fairness Warnings for the COMPAS Recidivism data set for both demographic parity and equal opportunity Warning accuracy indicates SLIM accuracy on the FairnessWarnings prediction task The original model is a neural network regularized for the respective notion of fairness This fairness warning is meant to be read as the expected mean shift away from the original mean of the features presented in a practitioners application For instance if priors count were to decrease prior and age were to decrease years in the demographic parity case the score would be points points point so the warning would predict unfairness Critically the fairness warning only makes a claim surrounding unfairness If the model predicts a score the model does not certify fair behavior sensitive attributes using the same strategy as above and used a of Additionally we only gave positive-outcome examples from the protected class We hoped to simulate a situation where a fair classifier is needed on a new task but there are only a few protected examples in the positive outcome to learn from simulating the situation where the distribution of fine-tuning task data is biased An example of such a scenario could be if a practitioner needed to train a new recidivism tool and had access to only a few examples of African-Americans who had previously been labeled as low risk We randomly generated synthetic tasks that we cached before training We sampled examples from each task during meta-training used a meta-batch size of for Fair-MAML and performed a single epoch of optimization within the internal MAML loop We trained Fair-MAML for meta-iterations For the pre-trained neural network we performed a single epoch of optimization for each task We trained over batches of tasks per batch to match the training set size used by Fair-MAML The loss used is the cross-entropy loss between the prediction x and the true value using the demographic parity regularizer from equation We use a neural network with two hidden layers consisting of nodes and the ReLU activation function We used the softmax activation function on the last layer When training with Fair-MAML we used examples and performed one gradient step update We set the step size to used the Adam optimizer to update the meta-loss with learning rate set to e We pre-trained a baseline neural network on the same architecture as Fair-MAML To one-shot update the pre-trained neural network we experimented with step sizes of and ultimately found that yielded the best trade offs between accuracy and fairness Additionally we tested values during training and finetuning of We present an example task in figure using fine-tuning points from the positive outcome and protected class When Fair-MAML does not incur any fairness regularization so the model is just MAML Synthetic Experiment Analysis In the new task there is an unseen configuration of positively labeled points It was not possible for positively labeled points to fall below y during training Fair-MAML is able to perform well with respect to both fairness and accuracy on the fine-tuning task when only biased fine-tuning data is available The pre-trained neural network fails at performing the new task when the fine-tuning data does not come from the original distribution of data This example suggests that Fair-MAML has learned a more useful internal representation for both fairness and accuracy than the pre-trained neural network Communities and Crime Experiment Next we consider an example using the Communities and Crime data set The Communities and Crime data set includes information relevant to crime eg police per population income as well as demographic information such as race and sex in different communities across the United States The goal is to predict the violent crime rate in the community We convert this data set to a few-shot fairness setting by using each state as a different task Because the violent crime rate is a continuous value we convert it into a binary label based on whether the community is in the top in terms of violent crime rate within a state Additionally we add a binary sensitive column that receives a protected label if African-Americans are the highest or second highest population in a community in terms of percentage racial makeup The Communities and Crime data set has data from states ranging in number of communities from to communities per state We only used states with or more communities leaving Figure Example decision boundaries given random draw of fine-tuning points from the pre-trained neural network MAML and Fair-MAML on the synthetic example note Fair-MAML is MAML with Points that are colored the same as the side of the boundary are correct Only points in the positive outcome and protected class are given for the fine-tuning task The instance where the pre-trained neural network does provide a reasonable solution happens to come from the training distribution otherwise the pre-trained neural network is not able to generalize to the new situation suggesting that Fair-MAML has learned more useful internal representations states We held out randomly selected states for testing and trained using states We set and cached meta-batches of size states for training For testing we randomly selected communities from the hold out task that we used for fine-tuning and evaluated on whatever number of communities were left over The number of evaluation communities is guaranteed to be at least because we only included states with or more communities We trained two Fair-MAML models one with the demographic parity regularizer from equation and another with the equal opportunity regularizer from equation For both models we used a neural network with two hidden layers of nodes We trained the model with one gradient step using a step size of e and a meta-learning rate of e using the Adam optimizer We trained the model for meta-iterations In order to assess Fair-MAML we trained a neural network regularized for fairness using the same architecture and training data We fine-tuned the neural network for each of the assessment tasks We used a learning rate of e for training and assessed learning rates of e e e e for fine-tuning We found the fine-tuning rate of e to perform the best trade offs between accuracy and fairness and present results using this learning rate We varied over incremented by for the demographic parity regularizer We found higher s to work better for the equal opportunity regularizer and varied from incremented by Additionally we trained two LAFTR models on the transfer tasks as comparisons for demographic parity and equal opportunity LAFTR is not intended to be compatible with our proposed K-shot fairness experiments because training on fine-tuning tasks with a minimal number of epochs and training points is not expected However we find that it is the most relevant fair transfer learning method to use as comparison We used the same transfer methodology and hyperparameters as described in Madras et al and used a neural network with a hidden layer of nodes as the encoder We used another neural network with a hidden layer of nodes as the multilayer perception MLP to be trained on the fairly encoded representation We used the demographic parity and equal opportunity adversarial objectives for the first and second LAFTR model respectively We trained each encoder for epochs and swept over a range of s We trained with all the data not held out as one of the testing tasks When training a MLP from the encoder on each of the transfer tasks we found that LAFTR struggled to produce useful results with only training points from the new task over any number of training epochs We found that we were able to get reasonable results from LAFTR using fine-tuning points and epochs of optimization using a minimal number of epochs was unsuccessful It makes sense that a minimal number of training epochs for the new task is unsuccessful because the MLP trained on the fairly encoded data is trained from scratch The results are presented in figure We were able to generate similar results with LAFTR to Fair-MAML using training points from the new task after epochs of optimization These results are given in the appendix We observe that Fair-MAML achieves the best trade off between fairness and accuracy both in terms of demographic parity and equal opportunity In our proposed problem setting LAFTR was not successful at learning with minimal data and a small number of fine-tuning epochs for the new task The pre-trained neural network shows some ability to learn the new task using little data and fine-tuning epochs At low s Fair-MAML is able to achieve higher accuracy than the pre-trained neural network and LAFTR Crucially Fair-MAML is able to learn more accurate representations that are also fairer for a range of s than both of the baselines In order to generalize to new states only communities are needed in order to achieve strong predictive accuracy and fairness using Fair-MAML Fair-MAML with Fairness Warnings Motivation We next consider Fairness Warnings applied to Fair-MAML We argue that Fairness Warnings can serve as a complementary tool to Fair-MAML Because we expect Fair-MAML to be used in situations with minimal data available it is possible that testing data given to a fine-tuned Fair-MAML model is unrepresentative of the true distribution of data for a particular task While in section we empirically demonstrate that Fair-MAML can still achieve good results when training data is available from one value in a sensitive attribute or label it still may be useful for practitioners to have indication surrounding situations in which their model may fail to be fair in testing Communities and Crime Fairness Warning/Fair-MAML Experiment We apply fairness warnings to Fair-MAML on the communities and crimes experimental setup from section using demographic parity as our notion of fairness We randomly chose an evaluation state to apply Fairness Warnings and left the rest for meta-training We trained two Fair-MAML models as in fairness warnings using the demographic parity regularizer for the first model and equal opportunity regularizer for the second model We used for the demographic parity Fair-MAML model and for the equal opportunity Fair-MAML model We trained for meta-iterations in a -step optimization setting with the update learning rate set to e and the meta learning rate set to e The demographic parity Fair-MAML model scored demographic parity on the test set of the fine-tuning task and accuracy of The equal opportunity Fair-MAML model scored accuracy and equal opportunity of To train Fairness Warnings on the fine-tuning task we created shifted data sets of the fine-tuning test data We trained a Fairness Warning for both demographic parity and equal opportunity We used the rule of demographic parity in the demographic parity warning and a equal opportunity threshold in the equal opportunity warning We found that or close to of the shifted data sets were classified fairly according to with respect to demographic parity and that of the shifted data sets were classified fairly according to equal opportunity We trained SLIM using of e and C of e for the demographic parity fairness warning We adjusted C to e for the equal opportunity fairness warning SLIM was able to predict whether the mean shifts across the features in the communities and crime data set would result in demographic parity unfairness with accuracy on a test set A random forest with estimators was able to predict the same task with accuracy In the equal opportunity setting SLIM predicted the task with accuracy A random forest with estimators was able to perform the same task with accuracy The fairness warnings are presented in figure Communities and Crime Fairness Warning/Fair-MAML Analysis The Fairness Warning trained on the fine-tuned Fair-MAML model is able to perform reasonable prediction accuracy and generates informative results Particularly it is interesting to consider that the demographic parity fine-tuned model behaves unfairly when the testing data set changes according to features such as number people living under the poverty line in urban areas and number of police officer A similar result is found in the equal opportunity setting with police operating budget In both the demographic parity and equal opportunity cases the fairness warnings demonstrate that seemingly small and perhaps innocuous differences between states where Fair-MAML is trained and applied could result in unfair behavior For instance the addition of a couple dozen additional police officers across communities in a state Figure The accuracy/fairness trade off for the communities and crimes example sweeping over a range of s The data presented is the mean across three runs on each using randomly selected hold out tasks The fairness numbers presented are the ratio between the protected and unprotected groups Higher accuracy and fairness values closer to indicate more successful outcomes The pre-trained neural network and Fair-MAML received fine-tuning points and were optimized for epoch We did not find useful results using LAFTR with only fine-tuning points or with a minimal number of fine-tuning epochs so the LAFTR example given here is with fine-tuning points and epochs of optimization Fair-MAML is able achieve better levels of accuracy and fairness than both the pre-trained network and LAFTR on the transfer tasks using minimal fine-tuning data in the demographic parity case could lead to the classifier behaving unfairly The same is true for equal opportunity and a slight increase to the mean police operating budget As we see in this example reasonable real world changes to the testing distribution can result in negative changes to the group fairness of the fine-tuned Fair-MAML model Providing Fairness Warnings to accompany the fine-tune meta model could lend additional guidance to a practitioner and help them better understand if their model will not behave fairly in application LIMITATIONS AND CONCLUSIONS In this paper we introduced Fairness Warnings and Fair-MAML Fairness Warnings provides an interpretable model that predicts which changes to the testing distribution will cause a model to behave unfairly Fair-MAML is a method that learns to learn fairly and can be used to train a fair model quickly from minimal data We demonstrate empirically the usefulness of both methods through multiple examples on both synthetic and real data sets In this work we explore Fairness Warnings applied to mean shifts in the testing distribution It is a relatively straight forward extension to apply Fairness Warnings to other distribution shifts such as changes to the standard deviation Though we are able to generate Fairness Warnings that show useful results they ultimately are only applied to summary statistics Meaning changes to the distribution that are not captured by such statistics could affect fairness in unpredictable ways Thus we only propose fairness warnings as boundary conditions under which the model may not be fair In this regard receiving a non-unfair score in fairness warnings does not guarantee that the model will behave fairly in the new domain We emphasize the importance of this directionality to any lawmakers or practitioners who would be interested in using Fairness Warnings and advise that they be used only to decide against the use of certain models instead of verify that models will behave fairly A final limitation to our work is that we assess Fair-MAML when there are many related training tasks to learn from In reality there may only be a few related training tasks available We leave assessing how useful Fair-MAML is on domains with only a few related training tasks to future work