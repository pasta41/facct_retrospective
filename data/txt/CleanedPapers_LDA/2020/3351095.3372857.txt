Fair Classification and Social Welfare Now that machine learning algorithms lie at the center of many important resource allocation pipelines computer scientists have been unwittingly cast as partial social planners Given this state of affairs important questions follow How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare In this paper we present a welfare-based analysis of fair classification regimes Our main endings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs We fully characterize the ranges of perturbations to a fairness parameter in a fair Soft Margin SVM problem that yield better worse and neutral outcomes in utility for individuals and by extension groups Our method of analysis allows for fast and efficient computation of fairness-to-welfare solution paths thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-o Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups More generally always preferring more fair classifiers does not abide by the Pareto Principle a fundamental axiom of social choice theory and welfare economics Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups By showing that these constraints often fail to translate into improved outcomes for these groups we cast doubt on their effectiveness as a means to ensure fairness and justice INTRODUCTION In his Tanner Lectures Amartya Sen noted that since nearly all egalitarian theories are founded on an equality of some sort the heart of the issue rests on clarifying the equality of what problem The eld of fair machine learning has not escaped this essential question Does machine learning have an obligation to assure probabilistic equality of outcomes across various social groups Or does it simply owe an equality of treatment Does fairness demand that individuals or groups be subject to equal mistreatment rates Or does being fair refer only to avoiding some intolerable level of algorithmic error Currently the task of accounting for fair machine learning cashes out in the comparison of myriad metrics probability distributions error likelihoods classification rates sliced up every way possible to reveal the range of inequalities that may arise before during and after the learning process But as shown in work by Chouldechova and Kleinberg et al fundamental statistical incompatibilities rule out any solution that can satisfy all parity metrics Fairness-constrained loss minimization offers little guidance on its own for choosing among the fairness desiderata which appear incommensurable and result in different impacts on different individuals and groups We are thus left with the harsh but unavoidable task of adjudicating between these measures and methods How ought we decide For a given application who actually benefits from the operationalization of a certain fairness constraint This is a basic but critical question that must be answered if we are to understand the impact that fairness constraints have on classification outcomes Much research in fairness has been motivated by the well-documented negative impacts that these systems can have on already structurally disadvantaged groups But do fairness constraints as currently formulated in fact earn their reputation as serving to improve the welfares of marginalized social groups When algorithms are adopted in social environments consider for example the use of predictive systems in the financial services industry classification outcomes directly bear on individuals material well-beings We thus view predictions as resource allocations awarded to individuals and by extension to various social groups In this paper we build out a method of analysis that takes in generic fair learning regimes and analyzes them from a welfare perspective Our main contributions presented in Section are methodological as well as substantive in the eld of algorithmic fairness We show that how fair a classifier is how well it accords with a group parity constraint such as equality of opportunity or balance for false positives does not neatly translate into statements about different groups welfares are affected Drawing on techniques from parametric programming and ending a SVMs regularization path our method of analysis finds the optimal -fair Soft-Margin SVM solution for all values of a fairness tolerance parameter We track the welfares of individuals and groups as a function of and identify those ranges of values that support solutions that are Pareto-dominated by neighboring values Further the algorithmic implementation of our analyses is computationally efficient with a complexity on the same order as current standard SVM solvers that t a single SVM model and is thus practical as a procedure that translates fairness constraints into welfare effects for all Our substantive results show that a classifier that abides by a stricter fairness standard does not necessarily issue improved outcomes for the disadvantaged group In particular we prove two results rst starting at any nonzero -fair optimal SVM solution we express the range of perturbations that tighten the fairness constraint and lead to classifier-output allocations that are weakly Pareto dominated by those issued by the less fair original classifier Second there are nonzero -fair optimal SVM solutions such that there exist perturbations that yield classifications that are strongly Pareto dominated by those issued by the less fair original classifier We demonstrate these endings on the Adult dataset In general our results show that when notions of fairness rest entirely on leading parity-based notions always preferring more fair machine learning classifiers does not accord with the Pareto Principle an axiom typically seen as fundamental in social choice theory and welfare economics The purposes of our paper are twofold The rst is simply to encourage a welfare-centric understanding of algorithmic fairness Whenever machine learning is deployed within important social and economic processes concerns for fairness arise when societal ideals are in tension with a decision-makers interests Most leading methodologies have focused on optimization of utility or welfare to the vendor but have rarely awarded those individuals and groups who are subject to these systems the same kind of attention to welfare effects Our work explicitly focuses its analysis on the latter We also seek to highlight the limits of conceptualizing fairness only in terms of group-based parity measures Our results show that at current making a system more fair as defined by popular metrics can harm the vulnerable social populations that were ostensibly meant to be served by the imposition of such constraints Though the Pareto Principle is not without faults the frequency with which more fair classification outcomes are welfare-wise dominated by less fair ones occurs is troublesome and should lead scholars to reevaluate popular methodologies by which we understand the impact of machine learning on different social populations Related Work Research in fair machine learning has largely centered on computationally defining fairness as a property of a classifier and then showing that techniques can be invented to satisfy such a notion Since most methods are meant to apply to learning problems generally many such notions of fairness center on parity-based metrics about a classifiers behavior on various legally protected social groups rather than on matters of welfare Most of the works that do look toward a welfare-based framework for interpreting appeals to fairness sit at the intersection of computing and economics Mullainathan also makes a comparison between policies as set by machine learning systems and policies as set by a social planner He argues that systems that make explicit their description of a global welfare function are less likely to perpetrate biased outcomes and are more successful at ameliorating social inequities Heidari et al propose using social welfare functions as fairness constraints on loss minimization programs They suggest that a learner ought to optimize her classifier while in Rawls original position As a result their approach to social welfare is closely tied with considerations of risk Rather than integrate social welfare functions into the supervised learning pipeline we claim that the result of an algorithmic classification system can itself be considered a welfare-impacting allocation Thus our work simply takes a generic -fair learning problem as-is and then considers the welfare implications of its full path of outcomes for all on individuals as well as groups Attention to the potential harms of machine learning systems is not new of course Within the fairness literature Corbett-Davies Goel and Liu et al devote most of their analyses to the person-impacting effects of algorithmic systems We agree that these effects are relevant to the question of fairness but our results differ in their methodological focus we introduce a technique that derives the full range of welfare effects achieved by a fair classification algorithm The techniques that we use to translate fair learning outcomes into welfare paths are related to a number of existing works The proxy fairness constraint in our instantiation of the -fair SVM problem original appeared in Zafar et als work on restricting the disparate impact of machine classifiers Their research introduces this particular proxy fairness constrained program and shows that it can be efficiently solved and well approximates target fairness constraints We use the constraint to demonstrate our overall endings about the effect of fairness criteria on individual and group welfares We share some of the preliminary formulations of the fair SVM problem with Donini et al though they focus on the statistical and fairness guarantees of the generalized ERM program Lastly though work on tuning hyperparameters of SVMs and the solution paths that result seem far afield from questions of fairness and welfare our analysis on the effect of fairness perturbations on welfare take advantage of methods in that line of work PROBLEM FORMALIZATION Our framework and results are motivated by those algorithmic use cases in which considerations of fairness and welfare stand alongside those of efficiency Because our paper connects machine classification and notions of algorithmic fairness with conceptions of social welfare we rst provide an overview of the notation and assumptions that feature throughout our work In the empirical loss minimization problem a learner seeks a classifier that issues the most accurate predictions when trained on set of n data points xi i ni Each triple gives an individuals feature vector xi X protected class attribute and true label i A classifier that assigns an incorrect label i incurs a penalty The empirical risk minimizing predictor is given by argmin i where hypothesis X R gives a learners model the loss function R R gives the penalty incurred by a prediction and is the hypothesis class under the learners consideration Binary classification systems issue predictions Notions of fairness have been formalized in a variety of ways in the machine learning literature Though Dwork et als initial conceptualization remains prominent and influential much work Though individuals in a dataset will typically be coded with many protected class attributes in this paper we will consider only a single sensitive attribute of focus has since defined fairness as a parity notion applied across different protected class groups The following definition gives the general form of these types of fairness criteria definition A classifier satisfies a general group-based notion of -fairness if xi i xi i where is some function of classifier performance and and are events that occur with respect to groups z and z respectively Further specifications of the function and the events E instantiate particular group-based fairness notions For example when xi i and refers to the events in which i for each group definition gives an -approximation of equality of opportunity xi i i and refers to all classification events for each group definition gives the notion of -approximation of overall error rate balance Notice that as increases the constraint loosens and the solution is considered less fair As decreases the fairness constraint becomes more strict and the solution is considered more fair Mapping classification outcomes to changes in individuals welfares gives a useful method of analysis for many data-based algorithmic systems that are involved in resource distribution pipelines In particular we consider tools that issue outcomes uniformly ranked or preferred by those individuals who are the subjects of the system That is individuals agree on which outcome is preferred Examples of such systems abound applicants for credit generally want to be found eligible candidates for jobs generally want to be hired or at least ranked highly in their pool These realms are precisely those in which fairness considerations are urgent and where fairness-adjusted learning methods are most likely to be adopted WELFARE IMPACTS OF FAIRNESS CONSTRAINTS The central inquiry of our work asks how fairness constraints as popularized in the algorithmic fairness community relate to welfare-based analyses that are dominant in economics and policy-making circles Do fairness-adjusted optimization problems actually make marginalized groups better-o in terms of welfare In this section we work from an empirical risk minimization ERM program with generic fairness constraints parametrized by a tolerance parameter and trace individuals and groups welfares as a function of We assume that an individual benefits from receiving a positive classification and thus we define group welfare as Wk X i where nk give the number of individuals in group z We note can be defined in ways other than which assumes that positive classification are always and only welfare-enhancing Other work has considered the possibility that positive classifications may in fact make individuals worse-o if they are false positives The definition can be generalized to account for these cases First in Section we present an instantiation of the -fair ERM problem with a fairness constraint proposed in prior work in algorithmic fairness We work from the Soft-Margin SVM program and derive the various dual formulations that will be of use in the following analyses In Section we move on to show how perturbations to the fairness constraint in the -fair ERM problem yield changes in classification outcomes for individuals and by extension how they impact a groups overall welfare Our approach which draws a connection between fairness perturbations and searches for an optimal SVM regularization parameter tracks changes in an individuals classification by taking advantage of the codependence of variables in the dual of the SVM By perturbing the fairness constraint we observe changes in not its own corresponding dual variable but in the corresponding dual of the margin constraints which relay the classification fates of data points Leveraging this technique we plot the solution paths of the dual variable as a function of which in turn allows us to compute group welfares as a function of and draw out substantive results on the dynamics of how classification outcomes change in response to -fair learning We prove that stricter fairness standards do not necessarily support welfare-enhancing outcomes for the disadvantaged group In many such cases the learning goal of ensuring group-based fairness is incompatible with the Pareto Principle definition Pareto Principle Let x be two social alternatives Let i be the preference ordering of individuals i n and P be the preference ordering of a social planner The planner abides by the Pareto Principle if x P whenever x i for all i In welfare economics the Pareto Principle is a standard requirement of social welfare functionals it would appear that the selection of an allocation that is Pareto dominated by an available alternative would be undesirable and even irresponsible Nevertheless we show that applying fairness criteria to loss minimization tasks in some cases do just that We perform our analysis on the Soft-Margin SVM optimization problem and for concreteness work with a well-known fairness formulation in the literature However we note that our methods and results apply to fairness-constrained convex loss minimization programs more generally We also show that this method of analysis can form practical tools In Section we present a computationally efficient algorithmic implementation of our analyses fitting full welfare solution paths for all values in a time complexity that is on the same order as that of a single SVM t We close this section by working from the shadow price of the fairness constraint to derive local and global sensitivities of the optimal solution to perturbations Setting up the -fair ERM program The general fairness-constrained empirical loss minimization program can be written as minimize subject to fh x where gives the empirical loss of a classifier on the dataset X To maximize accuracy the learner ought to minimize loss however because the loss function is non-convex a convex surrogate loss such as hinge loss or log loss log is frequently substituted in its place to ensure that globally optimal solutions may be efficiently found fh x gives a group-based fairness constraint of the type given in definition where is the unfairness tolerance parameter a greater permits a greater group disparity on a metric of interest a smaller more tightly restricts the level of permissible disparity We examine the behavior of fairness-constrained linear SVM classifiers though we note that our techniques generalize to nonlinear kernels SVMs since interpretations of the dual of the SVM and the full SVM regularization path are the same with kernels Our learner minimizes hinge loss with L regularization equivalently she seeks a Soft-Margin SVM that is -fair Both SVM models and fair training approaches are in broad circulation The fair empirical risk minimization program is thus given as minimize b C i i subject to i xi b i -fair Soft-SVM i fb x where the learner seeks SVM parameters b i are non-negative slack variables that violate the margin constraint in the Hard-Margin SVM problem i xi b and C is a hyperparameter tunable by the learner to express the trade-off between preferring a larger margin and penalizing violations of the margin fb x is the group parity-based fairness constraint The abundant literature on algorithmic fairness presents a long menu of options for the various forms that fb could take but generally speaking the constraints are non-convex As such much work has enlisted methods that depart from directly pursuing efficient constraint-based convex programming techniques in order to solve them Researchers have also devised convex proxy alternatives which have been shown to approximate the intended outcomes of original fairness constraints well In particular in this paper we work with the proxy constraint proposed by Zafar et al which constrains disparities in covariance between group membership and the signed distance between individuals feature vectors and the hyperplane decision boundary fb x i z xi b z reflects the bias in the demographic makeup of X z n i Let -fair-SVM-P be the Soft-Margin SVM program with this covariance constraint The corresponding Lagrangian is LP b C i i i i i i xi b i i z xi b -fair-SVM-L i z xi b where Rd b R are primal variables The nonnegative Lagrange multipliers correspond to the n nonnegativity constraints i and the margin-slack constraints i xi b i respectively The multipliers R correspond to the two linearized forms of the absolute value fairness constraint By complementary slackness dual variables reveal information about the satisfaction or violation of their corresponding constraints The analyses in the subsequent two subsections will focus on these interpretations By the Karush-Kuhn-Tucker conditions at the solution of the convex program the gradients of L with respect to b and i are zero Plugging in these conditions the dual Lagrangian is LD i n i i where The dual maximizes this objective subject to the constraints C for all i n and P i We thus arrive at the Wolfe dual problem maximize V i n i i V subject to C i n -fair-SVM-D i V V where we have introduced the variable V to eliminate the absolute value function in the objective Notice that when and neither of the constraints bind we recover the standard dual SVM program Since we are concerned with fair learning that does alter an optimal solution we consider cases where V is strictly positive We introduce additional dual variables and corresponding to the V V constraint and derive the Lagrangian L V i n i i V V Under conditions and nn i kuk where u i geometrically gives some group-sensitive average of x X We can now rewrite -fair-SVM-D as maximize i I i n P i n kuk subject to C i n i -fair SVM-D where I Pu The former is the identity matrix and the latter is the projection matrix onto the vector u As was also observed by Donini et al the version of -fair SVM-D is equivalent to the standard formulation of the dual SVM program with Kernel xi hI I i Since we are interested in the welfare impacts of fair learning when fairness constraints do have an impact on optimal solutions we will assume that the fairness constraint binds For clarity of exposition we assume that the positive covariance constraint binds and thus that and in -fair SVM-D This is without loss of generalization the same analyses apply when the negative covariance constraint binds The dual -fair SVM program becomes minimize i I i n P i n kuk subject to C i n -fair SVM-D i We will work from this formulation of the constrained optimization problem for the remainder of the paper Impact of Fair Learning on Individuals Welfares We now move on to investigate the effects of perturbing a fixed -fair SVM by some on the classification outcomes that are issued We ask How are individuals and groups classifications and thus their welfares impacted when a learner tightens or loosens a fairness constraint The key insight that drives our methods and results is that rather than perform sensitivity analysis directly on the dual variable corresponding to the fairness constraint which as we will see in Section only gives information about the change in the learners objective value we track changes in the classifiers behavior by analyzing the effect of perturbations on another set of dual variables that correspond to the primal margin constraints Each of these n dual variables indicate whether its corresponding vector xi is correctly classified lies in the margin or incorrectly classified Leveraging how these change as a function thereby allows us to track the solution paths of individual points and by extension compute group welfare paths define a function p R R that gives the optimal value of the -fair loss minimizing program in -fair SVM-P which by duality is also the optimal value of -fair SVM-D We begin at a solution p and consider changes in classifications at the solution p where are perturbations can be positive or negative so long as At an optimal solution the classification fate of each data point xi is encoded in the dual variable which is a function of is the -parameterized solution path of such that at any particular solution p the optimal value of the dual variable As a slight abuse of notation we reserve notation for the functional form of the solution path and write i to refer to the value of the dual variable at a given L The dual variable paths for all i n are piecewise linear in Though this lemma seems merely of technical interest it is a workhorse result for both our methodological contributions our analytical results and our computationally efficient algorithm which converts fairness constraints to welfare paths as well as our substantive fairness results about how fairness perturbations impact individual and learner welfares The algorithm we present in Section performs full welfare analysis for all values of in a computationally efficient manner by taking advantage of the piecewise linear form of individual and group welfares Piecewise linearity also sets the stage for the later substantive results about the tension between fairness improvements and the Pareto Principle We thus walk through the longer proof of this key result in the main text of the paper as it provides important exposition definitions and derivations for subsequent results P Let D be the value of the objective function in -fair SVM-D By the dual formulation of the Soft-Margin SVM we can use the value of D to partition the set of indices n in a way that corresponds to the classification fates of individual vectors at the optimal solution D and D C and M D C and E Hence are either correctly classified free vectors vectors in the margin or error vectors We track membership in these sets by letting M E be the index set partition at the -fair solution To analyze the impact that applying a fairness constraint has on individuals or groups welfares we track the behavior of D and observe how vector index membership in sets M and E change under a perturbation to This information will in turn reveal how classifications change or remain stable upon tightening or loosening the fairness constraint Fairness perturbations do not always shuffle data points across the different membership sets M and E It is clear that for E so long as a perturbation of does not cause D to ip signs or to vanish to will belong to the same set and where gives the -fair classification outcome for In these cases an individuals welfare is unaffected by the change in the fairness tolerance level from to In contrast vectors with M are subject to a different condition to ensure that they stay in the margin D D ie perturbing by does not lead to any changes in D D i I I nj kuk bj for all M Let rj be the change in upon perturbing by then we have r recalling that is the value of at the optimal solution p Let r be the vector of sensitivities to perturbations with r as the change in the b For all unshuffled M we can compute rj by taking the finite difference of with respect to a perturbation i r i ij hI I i r nj kuk hu i It is clear that ri for all i that are left unshuffled in the partition E For these stable ranges where no i changes its index set membership we can simplify the previous expression by summing over only those ri where i M X i M r i ij hI I i r nj kuk hu i Thus we can compute ri by inverting the matrix M ij hI I i M R M M where indices are renumbered to only reflects i M This matrix is invertible so long as the margin is not empty and the Kernel xi hI I i forms a positive definite matrix Since the objective function in -fair SVM-D is quadratic a sufficient condition forK to be invertible is that the objective is strictly convex we assume this as a technical condition The sensitivities of for M to perturbations are given by r n kuk v z r where v hu i Plugging this back into we have n kuk v z r Hence for all M that stay in the margin the solution path function is linear in For E that stay in their partition sets so the function is constant We mention the case in which the margin is empty in Section though we refer the interested reader to the Appendix for a full exposition of how are updated when the margin is empty and as a result we cannot compute how i move across index sets via the sensitivities r When perturbations do result in changes in the partition there are four ways that indices could be shuffled across sets E moves moves M moves into M moves into E Since index transitions only occur by way of changes to the margin we need now only confirm that each of these transitions maintains continuous paths for all n in order to conclude the proof that the paths are piecewise-linear The linearity of paths for M gives conditions on the ranges of wherein individuals classification outcomes do not change As such for any given tolerance parameter we can compute the perturbations that yield no changes to individuals welfares The following Proposition gives the analytical form of these stable regions where although fairness appears to be improving or worsening the adjusted learning process has no material effects on the classificatory outcomes that individuals receive P Denote the optimal values at an -fair SVM solution as for n Let r n kuk v with and v as defined in and X i M hI I i rj i i i I I nj kuk bj All perturbations of in the range max min Mj where mj min r r M E E Mj min r r M E E yield no changes to index memberships in the partition M E We defer the interested reader to the Appendix for the full proof of this Proposition though we provide a sketch here The result follows from observing that the sensitivities ri for i M defined in affect the values D for all n and additional conditions must hold to ensure that the vectors that are not on the margin are also unshuffled by the fairness perturbation define i i i I I nj kuk bj d D X i M r i ij hI I i r The condition for stability of vectors for M is given by d Recall the conditions of membership in sets and E as given in and respectively The following observations are critical to computing the bounds of the stable region For perturbations that increase do not threaten exiting the set if decreases then can Inversely for E perturbations that decrease ensure that stays in the same partition ie E Perturbations that increase can cause to shuffle For M to stay in the margin we need C Once hits either endpoint of the interval risks shuffling across to or E Computing these transition inequalities results in a set of conditions that ensure that a partition is stable Since can be either positive or negative we take the maximum of the lower bounds mj and the minimum of the upper bounds Mj to arrive at the range of stable perturbations given in We call the bounds of this interval the breakpoints of the solution paths This Proposition reveals a mismatch between the ostensible changes to the fairness level of an -fair Soft-Margin SVM learning process and the actual felt changes in outcomes by the individuals who are subject to the system This results from the simple fact that the optimization problem captures changes in the learners optimal solution but does not oer such fine-grained information on how individuals outcomes vary as a result of perturbations So long as the fairness constraint is binding and its associated dual variable then tightening or loosening a fairness constraint does alter the loss of the optimal learner classifier the actual SVM solution changes yet analyzed from the perspective of the individual agents xi so long as the perturbation occurs within the range given by classifications issued under this -fair SVM solution are identical to those under the -fair solution Thus despite the apparent more fair signal that a classifier abiding by sends agents are made no better o in terms of welfare This result is summarized in the following Corollary C Let p be a triple expressing the welfares of the learner group z and group z under the -fair SVM solution Then for any max where is defined in p p Once we have demarcated the limits of perturbations that yield no changes to the partition ie M E M E we can move on to consider the welfare effects of perturbations that exceed the stable region outlined in Proposition At each such breakpoint when reaches max or min Mj as defined in the margin set changes M As such rj for M must be recomputed via These sensitivities hold until the next breakpoint when the set M updates again We can associate a group welfare with the classification scheme at each of the breakpoints As already illustrated index partitions are static in the stable regions around each breakpoint so group welfares will also be unchanged in these regions As such we need only compute welfares at breakpoints to characterize the paths for This method of analysis allows practitioners to straightforwardly determine whether the next breakpoint actually translates into better or worse outcomes for the group as a whole Of the four possible events that occur at a breakpoint index transitions between the partitions M and E correspond to changed classifications that affect group utilities The following Proposition characterizes those breakpoint transitions that effect welfares triples p for the learner group z and group z that are strictly Pareto dominated by the welfare triple at a neighboring breakpoint The full proof is left to the Appendix P Consider the welfare triple at the optimal -fair SVM solution given by p Let bL max be the neighboring lower breakpoint where index argmax let min Mj be the neighboring upper breakpoint where index u argmin Mj assuming uniqueness in the argmax and argmin If E and or if M and then p bL bL bL p If u E and u or if u M and u then p p Thus minimizing loss in the presence of stricter fairness constraints need not correspond to monotonic gains or losses in the welfare levels of social groups Fairness perturbations do not have a straightforward effect on classifications Further these results do not only arise as an unfortunate outcome of using the particular proxy fairness constraint suggested by Zafar et al So long as the parameter appears in the linear part of the dual Soft-Margin SVM objective function the paths exhibit a piecewise linear form characterized by stable regions and breakpoints Hence these results apply to many proxy fairness criteria that have so far been proposed in the literature Even when the dual variable paths are not piecewise linear so long as they are non-monotonic fairer classification outcomes do not necessarily confer welfare benefits to the disadvantaged group Monotonicity in welfare space is mathematically distinct from monotonicity in fairness space The preceding analyses show that although fairness constraints are often intended to improve classification outcomes for some disadvantaged group they in general do not abide by the Pareto Principle a common welfare economic axiom for deciding among social alternatives That is asking that an algorithmic procedure abide by a more stringent fairness criteria can lead to enacting classification schemes that actually make every stakeholder group worse-o Here the supposed improved fairness achieved by decreasing the unfairness tolerance parameter fails to translate into any meaningful improvements in the number of desirable outcomes issued to members of either group T Consider two fairness-constrained ERM programs parameterized by and where Then a decision-maker who always prefers the classification outcomes issued under the more fair -fair solution to those under the less fair -fair solution does not abide by the Pareto Principle Algorithm and Complexity We build upon the previous section of translating fairness constraints into individual welfare outcomes by considering the operationalization of our analysis and its practicality The algorithmic procedure presented in this section computes breakpoints and tracks the solution paths of the for all individuals Hence the procedure enables the comparison of different social groups welfares where welfare is determined by the machines allocative outcome by aggregating the classification outcomes of all individuals in a group z Algorithm outputs two useful fairness-relevant constructs that have as yet not been explored in the literature solution paths for n tracking individuals welfares and full parameterized curves tracking groups welfares The analysis of the previous section forms the backbone of the main update rules that construct the paths in Algorithm In particular the values rj and d as defined in and respectively are key to computing the breakpoints which in turn fully determine the piecewise linear form of There is however one corner case that the procedure must check that was not discussed in the preceding section We had previously required that the matrix be invertible which is the case whenever our objective function is strictly convex But if the margin is empty the standard update procedure which computes sensitivities rj and will not suffice The optimality condition i requires that the multiple indices moving in the margin at once must be positive and negative examples For this reason we must refer to a different procedure to compute the breakpoint at which this transition occurs For continuity of the main text of this paper the full exposition of this analysis is given in the Appendix The following complexity result highlights the practicality of implementing the fairness-to-welfare mapping in Algorithm to track the full solution paths of an -fair SVM program We note that standard SVM algorithms such as LibSVM run in O n and thus once the algorithm has been initialized with the unconstrained SVM solution the complexity of computing both the full individual solution paths and the full group welfare curves is on the same order as that of computing a single SVM solution T Each iteration of Algorithm runs n M For breakpoints on the order of n the full run time complexity is O n n M P Each iteration of the fairness-to-welfare algorithm requires the inversion of matrix R M and the computations of rj R M for M and and for E The standard Gauss-Jordan matrix inversion technique runs in O M but we take advantage of partition update rules to lower the number of computations Since at each new breakpoint the ALGORITHM Fairness-to-welfare solution paths as a function of Input set X of n data points xi i Output solutions paths and group welfare curves argmin D of -fair SVM-D n i n i while do for each do update M E according to if C i C i then end end n n if M then mini Mi as given in update M E according to and end compute r d according to mini Mi as given in r i for i M for i E end return partition tends to change because of additions or eliminations of a single index from the set M we can use the Cholesky decomposition rank-one update or downdate to ease the need to recompute the full matrix inverse at every iteration thereby reducing the complexity of the operation toO M Computing the stability region conditions for E requires O n M M steps As such at each breakpoint the total computational cost is O M n The number of breakpoints for each full run of the algorithm depends on the data distribution and how sensitive the solution is to the constraint As a heuristic datasets whose fairness constraints bind for smaller have fewer breakpoints Previous empirical results on the full SVM path for L and L regularization have found that the number of breakpoints tends to be on the order of n Thus after initialization with -fair SVM solution the final complexity for the algorithm is O n n M Impact of Fair Learning on Learners Welfare Having proven the main welfare-relevant sensitivity result for groups we return to more standard analysis of the effect of perturbations on the learners loss In this case we directly solve for the dual variable of the fairness constraint Recall from nn i kuk By complementary slackness one of and is zero and the other is In particular if then then we know that Thus the original fairness constraint that binds is the upper bound on covariance suggesting that the optimal classifier must be constrained to limit its positive covariance with group z If then and and the classifier must be constrained to limit its positive covariance with group z We can interpret the value of the dual variable Lagrange multiplier as the shadow price of the fairness constraint It gives the additional loss in the objective value that the learner would achieve if the fairness constraint were infinitesimally loosened Whenever a fairness constraint binds its shadow price is readily computable and is given by It bears noting that because -fair Soft-SVM is not a linear program can only be interpreted as a measure of local sensitivity valid only in a small neighborhood around an optimal solution But through an alternative lens of sensitivity analysis we can derive a lower bound on global sensitivity due to changes in the fairness tolerance parameter By writing as a perturbation variable we can perform sensitivity analysis on the same -constrained problem Returning to the perturbation function p we have p sup L where L gives the solution to the -fair SVM problem L max i I Pu xi X i The perturbation formulation given in is identical in form to the original program -fair-SVM-P but gives a global bound on p for all Since gives a lower bound the global sensitivity bound yields an asymmetric interpretation P If and p If and for small p and is thus also small in magnitude Proposition shows that tightening the fairness constraint when its shadow price is high leads to a great increase in learner loss but loosening the fairness constraint when its shadow price is small leads only to a small decrease in loss EXPERIMENTS To demonstrate the efficacy of our approach we track the impact of -fairness constrained SVM programs on the classification outcomes of individuals in the Adult dataset The target variable in the dataset is a binary value indicating whether the individual has an annual income of more or less than If such a dataset were used to train a tool to be deployed in consequential resource allocation say for the purpose of determining access to credit then classification decisions directly impact individuals welfares Individual solution paths and relative group welfare changes are given in Figure As increases from left to right the fairness constraint is loosened and outcomes become less fair In the case of the -fair SVM solution to the Adult dataset the fairness constraint ceases to bind at the optimal solution when The top panel shows example individual piecewise linear paths of dual variables providing a visual depiction of how individual points can transition across index sets from i and being correctly labeled to i M being correctly labeled but in margin to i E and being incorrectly labeled Solid paths indicate individuals coded female dashed paths indicate those coded males As the top panel of Figure shows the actual journey of these paths are varied as changes As expected tightening the fairness constraint in the -fair program does tend to lead to improved welfare outcomes for females as a group more female individuals receive a positive classification while males experience a relative decline in group welfare receiving fewer positive classifications However as suggested by our results in Section these welfare changes are not monotonic for either group Tightening the fairness constraint could lead to declines in both groups welfares demonstrating that preferring more fair solutions in this predictive model does not abide by the Pareto Principle We highlight an instance of this result in the bottom panel of Figure where orange dashed lines to the left of black ones mark o solutions where more fair outcomes orange are Pareto-dominated by less fair black ones A practitioner working in a domain in which welfare considerations might override parity-based fairness ones may prefer the outcomes of a fair learning procedure with to one with Additional plots showing absolute changes in group welfare and optimal learner value are given in the Appendix DISCUSSION The question that leads o this paper How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare sets an important agenda to come for the eld of algorithmic fairness It asks that the community look to disciplines that have long considered the problem of allocating goods in accordance with ideals of justice and fairness For example the notion of welfare in this paper draws from work in welfare and public economics The outcomes issued by an optimal classifier can thus be interpreted using welfare economic tools developed for considerations of social efficiency and equity In an effort to situate computer scientists notions of fairness within a broader understanding of distributive justice we also show that loss minimization problems can indeed be mapped onto welfare maximization ones and vice versa For reasons of continuity analyses of this correspondence do not appear in the main text we defer the interested reader to the Appendix though we present an abbreviated overview here We encourage readers to consider the main results of this paper which construct welfare paths out of fair learning algorithms as a part of this larger project of bridging the two approaches Bridging Fair Machine Learning and Social Welfare Maximization To highlight the correspondence between the machine learning and welfare economic approaches to allocation we show that loss minimizing solutions can be understood as welfare-maximizing ones under a particular social welfare function In the Planners Problem a planner maximizes social welfare represented as the weighted sum of utility functions Inverting the Planners Problem gives a question concerning social equity Given a particular allocation what is the presumptive social weight function that would yield it as optimal We show that the set of predictions issued by the optimal classifier of any loss minimization task can be given as the set of allocations in the Planners Problem over the same individuals endowed with a set of welfare weights Analyzing the distribution of Figure Fairness-to-welfare solution paths for individuals top panel and groups bottom panel on the Adult dataset implied weights of individuals and groups offers a welfare economic way of considering the fairness of classifications We also derive a converse result Given a social welfare maximizing allocation what model that can achieve an equivalent classification Our solutions approach records the set of welfares defined by the number of positively labeled individuals achievable for each social group Interpreting Welfare Alongside Fairness Welfare economics can lend particular insights into formalizing notions of distributional fairness and general insights into building a technical eld and methodology that grapples with normative questions The eld is concerned with what public policies ought to be how to improve individuals well-beings and what distribution of outcomes are preferable Answers to these questions appeal to values and judgments that refer to more than just descriptive or predictive facts about the world The success of fair machine will largely hang on how well it can adapt to a similar ambitious task However welfare economics is not the only nor should it even serve as the main academic resource for thinking through how goods ought to be provisioned in a just society In this moment of broad appeal to the prowess of algorithmic systems researchers in computing are called on to advise on matters beyond their specialized expertise and training Many of these matters require explicit normative political and social-scientic reasoning Insights and methods from across the arts humanities social sciences and natural sciences bear fruit in answering these questions This paper does not look to contribute a new fair learning algorithm or a new fairness definition We take a popular classification algorithm the Soft Margin SVM append a parity-based fairness constraint and analyze its implications on welfare The constraint that we center in the paper is just one concretization of a large menu of fairness notions that have been offered up to now The method of analysis developed in the paper applies generally to any convex formulations of these constraints including versions of balance for false positives balance for false negatives and equality of opportunity that have circulated in the literature It is important future work to investigate the welfare implications of state-of-the-art fair classification algorithms that the community continues to develop which can deal with a wider range of models and constraints including non-convex ones This paper asks that researchers in fair machine learning reevaluate not only their lodestars of optimality and efficiency but also their latest metrics of fairness By viewing classification outcomes as allocations of a good we incorporate considerations of individual and group utility in our analysis of classification regimes The concept of utility in evaluations of social policy remains controversial but in many cases of social distribution utility considerations provide a partial but still important perspective on what is at stake within an allocative task Utility-based notions of welfare can capture the relative benefit that a particular good can have on a particular individual If machine learning systems are in effect serving as resource distribution mechanisms then questions about fairness should align with questions of Who benefits Our results show that many parity-based formulations of fairness do not ensure that disadvantaged groups benefit Preferring a classifier that better accords with a fairness measure can lead to selecting allocations that lower the welfare for every group Nevertheless there remain reasons in favor of limiting levels of inequality not reflected in utilitarian calculus In some cases the gap between groups is itself objectionable and minimizing this difference overrides maximizing the absolute utility level of disadvantaged groups But without acknowledging and accounting for these reasons well-intentioned optimization tasks that seek to be fairer can further disadvantage social groups for no reason but to satisfy a given fairness metric