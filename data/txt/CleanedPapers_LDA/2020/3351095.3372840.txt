Algorithmic Realism Expanding the Boundaries of Algorithmic Thought Although computer scientists are eager to help address social problems the eld faces a growing awareness that many well-intentioned applications of algorithms in social contexts have led to cant harm We argue that addressing this gap between the fields desire to do good and the harmful impacts of many of its interventions requires looking to the epistemic and methodological underpinnings of algorithms We diagnose the dominant mode of algorithmic reasoning as algorithmic formalism and describe how formalist orientations lead to harmful algorithmic interventions Addressing these harms requires pursuing a new mode of algorithmic thinking that is attentive to the internal limits of algorithms and to the social concerns that fall beyond the bounds of algorithmic formalism To understand what a methodological evolution beyond formalism looks like and what it may achieve we turn to the twentieth century evolution in American legal thought from legal formalism to legal realism Drawing on the lessons of legal realism we propose a new mode of algorithmic thinking algorithmic realism that provides tools for computer scientists to account for the realities of social life and of algorithmic impacts These realist approaches although not foolproof will better equip computer scientists to reduce algorithmic harms and to reason well about doing good CONCEPTS Theory of computation Design and analysis of algorithms Applied computing Law social and behavioral sciences KEYWORDS algorithms law STS critical algorithm studies epistemology INTRODUCTION Much of the work in computer science labs and technology companies is motivated by a desire to improve society Many computer scientists aim to change the world and contribute to social good leading to the development of algorithms for use in courts city governments hospitals schools and other essential societal institutions Yet alongside computer sciences growing interest in addressing social challenges has come a recognition driven by affected communities and scholarship in science technology and society STS and critical algorithm studies that many well-intentioned applications of algorithms have led to harm Algorithms can be biased discriminatory dehumanizing and violent They can exclude people from receiving social services spread hateful ideas and facilitate government oppression of minorities Computer science thus faces a gap between its desire to do good and the harmful impacts of many of its interventions The challenge for the eld is how to account for social and political concerns in order to more reliably achieve its aims Our goal in this paper is to engage with this challenge and to elaborate a positive vision of how computer science can better contribute to society To engage in this task we consider the relationship between algorithmic thinking how algorithms are canonically taught and understood and algorithmic interventions how algorithms are deployed to address social problems We are specifically interested in interrogating the influence of algorithmic thinking on algorithmic interventions and focus on the application of optimization and machine learning algorithms The divergent meanings of algorithms within critical discourse and computer science reflects the differences between algorithms in theory and the algorithmic systems intricate dynamic arrangements of people and code that exist in practice Understanding the relationship between these two notions of algorithms thus requires approaching algorithms as multiples unstable objects that are enacted through the varied practices that people use to engage with them Algorithmic thinking can be understood as a mode of reasoning it shapes how computer scientists see the world understand problems and develop solutions to those problems Like all methodologies algorithmics relies on de ning the bounds of its analysis Considerations that fall within a methods analytic boundaries are the subject of sharp focus but when aspects of the world fall outside these boundaries a method has no hope of discovering these truths since it has no means of representing them Thus as computer science increasingly engages with social and political contexts the eld has come up against the limits of algorithmic thinking computer science lacks the language and methods to fully recognize reason about and evaluate the social aspects and impacts of algorithmic interventions In turn even well-intentioned algorithmic interventions are at risk of producing social harms Enabling computer science to responsibly navigate its social effects requires several steps diagnosing the attributes of algorithmic thinking and how those attributes lead to harm evaluating the potential and limits of current efforts to reform algorithms describing how the eld can expand its epistemic and methodological boundaries and articulating the tenets of a computer science practice that is evolved based on the concerns raised by a communities and disciplines such as STS This paper takes on each of these tasks in turn First we argue that many of the harms of algorithmic interventions derive from the dominant mode of thinking within computer science which we characterize as algorithmic formalism Algorithmic formalism involves three key orientations objectivity/neutrality internalism and universalism Although often reasonable even valuable within the context of traditional algorithmic work these orientations can lead to algorithmic interventions that entrench existing social conditions narrow the range of possible reforms and impose algorithmic logics at the expense of others Characterizing these concerns which draw heavily on STS and critical algorithm studies under the banner of formalism provides a path to evaluating and pursuing potential remedies Second we evaluate the dominant approaches to reducing algorithmic harms such as efforts to promote algorithmic fairness ethics and various forms of data and model documentation Such efforts provide important mechanisms to mitigate certain algorithmic harms Yet these reforms involve incorporating new processes or metrics into the formal method and thus do not allow practitioners to transcend formalism itself Additions of form most notably algorithmic fairness fail to provide the epistemic and methodological tools necessary to fully identify and act upon the social implications of algorithmic work To solve the chronic failures of algorithmic formalism computer scientists need new modes of reasoning about the social both as a terrain of intervention and as an attribute of their own work This requires an evolution of algorithmic reasoning expanding the bounds of what it means to think algorithmically and do algorithmic interventions Third we consider a possible path forward An epistemic and methodological evolution is a daunting task and it is not obvious how such a shift could occur or that it would be productive With this in mind we draw on our characterization of algorithmic formalism to explore a parallel to formalism in another eld law and to how legal formalism was addressed with a methodological evolution toward legal realism From around through the beginning of the twentieth century American legal thought was characterized by legal formalism a project to systematize law around c and deductive principles Because this mode of thought adhered to objective principles but did not consider those principles actual impacts its application upheld highly unequal social conditions These impacts provoked critiques that led to a methodological evolution toward legal realism Legal realism did not wholly supplant formalism but instead provided lawyers and judges with additional tools to account for the realities of social life and of the laws impacts This shift which expanded the terrain on which law could be evaluated and debated suggests both a path toward reforming computer science and the utility of such a path Fourth drawing on the lessons of legal realism we propose a new mode of computer science thinking algorithmic realism that responds to the concerns raised by STS and related disciplines Compared to algorithmic formalism algorithmic realism provides three alternative orientations a re political consciousness a porousness that recognizes the complexity and of the social world and contextualism As such algorithmic realism provides the epistemic and methodological tools to develop algorithmic interventions that question unjust social conditions expand the range of possible reforms and account for a wide array of values and goals At rst glance the law may seem like an unusual place to look for inspiration regarding computer science With a few exceptions law and computer science are typically seen as in tension or subject to opposing logics technology moves fast while law is slow technology is about innovation while law is about regulation and so on Yet several parallels suggest why this comparison is apt Algorithmic interventions operate in a manner akin to legal ones often taking the place of or more precisely o a particular technical form of legal reforms Like the law algorithms are commonly invoked as neutral mechanisms of formalized decision making Yet in practice both are subject to debates regarding the proper role for discretion ways to combat discrimination and determinations of the legitimate bases for decision making Moreover the recent surge of enthusiasm for public interest technology explicitly follows in the footsteps of and indeed takes its name from a prior movement in legal education Of course our goal is not to claim a neat one-to-one correspondence between computer science and law there certainly are substantial di but to point to how the lessons of law can inform computer science Like computer science the law involves training in a methodological practice that structures how its practitioners create and evaluate social interventions Modes of legal thought influence legal interventions in much the same way that modes of algorithmic thought influence algorithmic interventions Legal scholars have long considered the relationship between the intended and actual impacts of social interventions Thus we see the parallel to legal formalism/realism as a way to identify a bridge between the deconstructive critique of algorithmic formalism from STS and a new mode of computer science practice algorithmic realism that productively engages with these critiques Following the history of law the distinction between algorithmic formalism and realism does not re a rigid dichotomy the evolution toward realism is an expansion of computer science to embrace realist orientations alongside formalist ones not a wholesale rejection of formalism It is precisely the formalism of algorithmic methods that has enabled many of computer sciences most exciting advances Algorithmic realism provides complementary approaches that make sociotechnical considerations legible and commonplace within computer science thinking This expanded epistemic and methodological toolkit can help computer scientists to address existing problems more fully and to see new questions Nor does the distinction between algorithmic formalism and realism fully characterize the behaviors of computer scientists In practice computer scientists are diverse and ambivalent characters who blend formalist and realist methods engaging in nuanced contextualized and re practices as they continuously straddle the competing demands of formal abstraction and empirical contingency Some computer science sub fields such as CSCW have long histories of engaging with sociotechnical practices while others such as FAT are actively developing such methods We aim to highlight examples of realist-aligned work to help shift such work from exception to standard practice Nonetheless computer scientists recognize that the insights of STS and critical algorithm studies fall beyond their own interpretive frames Even within the FAT community critical evaluations of the mathematization of fairness suggest the need for further evolution from formalism towards realism Of course a turn toward algorithmic realism would not remedy or prevent every algorithmic harm Computer scientists are just one set of actors within larger sociotechnical systems that include other people institutions policies and pressures Algorithmic realism may do little directly to remedy the harms of algorithms deployed through discriminatory public policies by authoritarian regimes or under exploitative business models A great deal of algorithmic work is also done by people without formal computer science training Algorithmic thinking presents a potent site for reform however Computer science plays an role in society both directly through the work of developing algorithmic interventions and indirectly as algorithmic thinking shapes how scholars both inside and outside the eld practitioners and public o conceive of social challenges and progress For instance various public policies and business practices draw on algorithmic reasoning as a way to gain legitimacy Following STS scholars such as jasanoff and Winner we aim to trace a middle path between technological determinism and social determinism exploring the ways in which algorithmic artifacts have politics We see algorithmic realism not as distinct from sociotechnical systems but valuable precisely because it situates algorithmic interventions within sociotechnical systems Computer scientists are not the only or the most important actors within sociotechnical systems nor should they be Yet reforming such systems requires that computer scientists recognize their positionality and reason about what roles they do and should have Thus providing computer scientists with the epistemic capacity to navigate the inherent socio-political dimensions of their work is an essential component of sociotechnical reform ALGORITHMIC FORMALISM Formalism implies an adherence to prescribed form and rules The chosen form eg text or numbers is analyzed according to particular rules often with the explicit purpose of constricting the choice of a decisionmaker or analyst In literature for example formalism involves the view that the formal properties of a form structure grammar and so ne its boundaries such that the text stands on its own as a complete entity Similarly formalism in mathematics involves the idea that mathematics is not a body of propositions representing an abstract sector of reality but is much more akin to a game where meaning derives from manipulating symbols according to the rules Formalism is not itself intrinsically bad It is a method one that has many virtues Conceptually formalizing a problem can lead to analytical clarity Practically formalizing a problem can make complex problems tractable No system that decides how to distribute water govern educational resources or predict weather can do so without in some way formalizing the problem at hand Yet formalism also has cant limitations The danger is not in formal methods per se but in the failure to recognize the limits of formal insights and in the uncritical deployment of formal methods in complex social contexts where their assumptions may be invalid Because formal knowledge requires a narrowing of vision writes James Scott the formal order encoded in social-engineering designs inevitably leaves out elements that are essential to their actual functioning This narrowing who or what is left on the epistemic cutting room and systemically excluded or made the focus of ed analytical scrutiny involves political decisions about what is and is not important Formal orders are often sites of political and social struggles with brutal consequences for those being ed Formalism is at the core of algorithms As one canonical algorithms textbook describes an algorithm is a computational procedure for solving a well-specified computational problem The essential attribute of this reasoning is formalism through abstraction algorithms require the explicit mathematical articulation of inputs outputs and goals This process of employing abstraction to formulate a problem to admit a computational solution is deemed the hallmark of computational thinking As another introductory algorithms textbook explains At their most e algorithmic ideas do not just provide solutions to well-posed problems they form the language that lets you cleanly express the underlying questions Done well a clean algorithmic de can formalize a notion that initially seems too fuzzy and nonintuitive to work with mathematically Formalism has been a consistent subject of critique in computing Brian Smith argued that computer scientists should be attentive to the gulf between the abstractions of models and the complexity of the world Philip Leith called for computer science to replace computing formalism with a sociological imagination Philip Agre decried the false precision of formalism as an extremely constricting cognitive style Pat Langley noted that machine learning research has seen an increased emphasis on mathematical formalization and a bias against papers that do not include such treatments More recently scholars have highlighted the limits of abstraction and formalism with regard to algorithmic fairness articulating the need for a sociotechnical frame Objective and Neutral The algorithmic formalist emphasis on objectivity and neutrality occurs on two related levels First algorithms are perceived as neutral tools and are often argued for on the grounds that they are capable of making objective and neutral decisions Second computer scientists are seen by themselves and others as neutral actors following the c principles of algorithm design from positions of objectivity Such an ethos has long been prevalent among scientists for whom objectivity the suppression of some aspect of the self the countering of subjectivity has become a widespread set of ethical and normative practices The orientation of objectivity and neutrality prevents computer scientists from grounding algorithmic interventions in explicit de of desirable social outcomes Social and political concerns are granted little space within algorithmic thinking leaving data science projects to emerge through the negotiation of technical We invoke politics not in the sense of ideologies political parties or elections but in a manner akin to Winner to reference broader debates about the goodie the set of processes and dynamics that shape social outcomes and that distribute power among people and groups considerations such as data availability and model accuracy with explicit normative considerations rarely in mind Even efforts that are motivated as contributions to social good typically lack a clear explanation of what such good entails instead relying on vague and notions Computer scientists engaged in algorithmic interventions have argued Im just an engineer and Our job isnt to take political stances This emphasis on objectivity and neutrality leads to algorithmic interventions that reproduce existing social conditions and policies For objectivity and neutrality do not mean value-free they instead mean acquiescence to dominant c social and political values c standards of objectivity account for certain kinds of individual subjectivity but methods for maximizing objectivism have no way of detecting values interests discursive resources and ways of organizing the production of knowledge As such the supposedly objective c gaze from nowhere is nothing more than an illusion Neutrality similarly represents an illusory and ultimately idolatrous goal that often serves to freeze existing conditions in place Conceptions of algorithms and computer scientists as objective and neutral launder the perspectives of dominant social groups into perspectivelessness reinforcing their status as the only ones entitled to legitimate claims of neutrality Anything that challenges existing social structures is therefore seen as political yet reform efforts are no more political than efforts to resist reform or even the choice simply to not act both of which preserve existing structures Predictive policing systems o er a particularly pointed example of how striving to remain neutral entrenches and legitimizes existing political conditions These algorithms exist on the backdrop of a criminal justice system that is increasingly recognized as de by racial injustice De of crime are the products of racist and classist histories that associated black men with criminality Moreover predictive policing is based on the discredited model of broken windows policing that has been found to be and racially discriminatory In this context algorithms that uphold common de of crime and how to address it are not indeed cannot be removed from politics they merely seem removed from politics Computer scientists are not responsible for this context but they are responsible for choosing how they interact with it When intervening in social contexts steeped in contested histories and politics in other words it is impossible for computer scientists to not take political stances The point is not to argue for a single correct conception of the good to which all computer scientists must adhere It is precisely because a multiplicity of perspectives exists that judgments regarding c practices and normative commitments must be explicitly incorporated into algorithm development and evaluation Yet the epistemic commitments of neutrality and objectivity exclude such considerations from algorithmic reasoning allowing these judgments to pass without deliberation or scrutiny Internalist Another attribute of algorithmic formalism is internalism only considerations that are legible within the language of algorithms eg e and accuracy are recognized as important design and evaluation considerations The analysis of an algorithm primarily emphasizes its run time or e characterizing its behaviors in terms of upper lower and tight bounds all features that can be mathematically de based on the algorithms operations Machine learning algorithms are additionally centered on a corpus of data from which to derive patterns and are evaluated according to accuracy metrics such as area under the curve AUC From predictive policing to healthcare to fake news claims regarding an algorithms e and quality emphasize accuracy along these metrics This approach of de ning and measuring algorithms by their mathematical characteristics provides little internal capacity to reason about the social considerations such as laws policies and social norms that are intertwined with these algorithms performance and impacts The internalist emphasis on the mathematical features of algorithms leads to algorithmic interventions based in a technologically determinist theory of social change Because cant aspects of the social and political world are illegible within algorithmic reasoning these features are held as constants outside of the algorithmic system In turn algorithms are proposed as a sole mechanism of social change with the existing social and political conditions treated as static For instance several papers analyzing recidivism prediction tools explicitly describe crime rates in this manner One describes recidivism prevalence as a constraint one that we have no direct control over while another explains the algorithm cannot alter the risk scores themselves In an immediate sense it is reasonable to see existing recidivism rates as the backdrop against which a risk assessment makes predictions yet the recurring practice in computer science of treating these social conditions as a constraint exempli es the internalist assumption that algorithms operate atop a static world Internalist reasoning leads to algorithmic interventions that optimize social systems according to existing policies and assumptions drastically narrowing the range of possible reforms Algorithmic interventions conceived through the internalist orientation have a tendency to optimize the status quo rather than challenge it The goal becomes to predict static distributions of social outcomes in order to make more informed decisions rather than to shift distributions in order to enable better outcomes In this vein an argument made for risk assessments is that algorithms permit unprecedented clarity because they let us precisely quantify s among societys di goals eg fairness and low crime rates algorithms thereby force us to make more explicit judgments about underlying principles Yet this calculus can be seen to provide clarity only if the contingency and contestability of social conditions are beyond consideration Note the s that fall beyond the purview of risk assessments and are therefore rendered irrelevant for instance the between pretrial detention and due process or the between implementing risk assessments and abolishing pretrial detention Moreover because this internalist orientation emphasizes an algorithms mathematical properties algorithmic interventions are unable to account for the particular ways that people institutions and society will actually interact with algorithms This is one reason why the deployment of algorithms can generate unintended social outcomes Algorithmic interventions are thus indeterminate the deployment of an algorithm provides little guarantee that the social impacts expected according to internalist evaluations will be realized A paradigmatic case involves optimization algorithms which are modeled on the assumption that increasing road capacity will reduce c Such algorithms have informed urban planning interventions over the past century from efforts in the s to manage the in of automobiles to todays visions for self-driving cars Yet because they rarely account for the second-order effects of their own introduction these algorithms drastically overestimate the bene of increasing roadway capacity in response to more e automobile travel motorists change their behavior to take advantage of the new road capacity ultimately leading to more driving and congestion It is impossible for an algorithm to account for every aspect of society or every way that people might respond to it Every method needs to set boundaries Yet the choice of where to set those boundaries shapes what factors are considered or ignored and in turn shapes the impacts of interventions developed through that method Internalism enforces a strict frame of analysis preventing algorithmic interventions from adapting to social considerations that are material to success Computer scientists therefore need to reason more thoroughly about when certain factors can be ignored and when they must be grappled with Universalism Algorithmic formalism emphasizes an orientation of universalism a sense that algorithms can be applied to all situations and problems Popular algorithms textbooks extol the ubiquitous applications of algorithms and the pervasive reach of algorithmic ideas An in computer scientist hails computational thinking as the new literacy of the st century excitedly describing how this mode of thinking has already in the research agenda of all science and engineering disciplines and can readily be applied in daily life While some have recognized that there are contexts in which it is better not to design technology the common practice among computer scientists is to focus on how to design algorithms rather than whether algorithms are actually appropriate in any given context In fact when students in data science ethics classes have questioned whether algorithms should be used to address social challenges they are told that the question is out of scope Ge personal communication April This universalist orientation leads to interventions developed under an assumption that algorithms can provide a solution in every attitude that has been described in recent years as technological solutionism tech goggles and techno-chauvinism Algorithmic interventions have been proposed as a solution for problems ranging from police discrimination to misinformation to depression detection Numerous initiatives strive to develop data science and intelligence for social good across a wide range of domains typically taking for granted with scant cation that algorithms are an e tool for addressing social problems Algorithmic interventions pursued under universalism impose a narrow algorithmic frame that structures how problems are conceived and limits the range of solutions deemed viable Given that the way in which a problem is conceived decides what specific suggestions are entertained and which are dismissed applying algorithmic thinking to social problems imposes algorithmic logics namely accuracy and e these domains at the expense of other values In smart cities for instance algorithms are being deployed to make many aspects of municipal governance more e Yet e is just one of many values that city governments must promote and in fact is often in tension with those other values behaviors such as making small talk with residents can improve a municipalitys ability to provide fair social services and garner public trust More broadly an emphasis on e in urban life can erode vital civic actions such as deliberation dissent and community building Algorithms can of course model a variety of contexts E and accuracy are often important factors But they are typically not the only nor the most important factors Algorithmic interventions require reasoning about what values to prioritize and what bene algorithms can provide However the universalist orientation prevents computer scientists from recognizing the limits of algorithms and thoroughly evaluating whether algorithms are appropriate This uncritical deployment of algorithmic interventions in turn elevates the status of the algorithmic reasoning behind such interventions Algorithmic formalism has in many ways become the hallmark of what it means to conceive of any problem rigorously regardless of the many examples of how such thinking faces serious epistemic defects in various social settings As such a cant risk of algorithmic formalism is that it contributes to formal methods dominating and crowding out other forms of knowledge and inquiry particularly local forms of situated knowledge that may be better equipped to the tasks at hand FORMALIST INCORPORATION One approach to addressing the failures of algorithmic formalism is to incorporate new processes variables or metrics into its logic This process which we call formalist incorporation is particularly appealing to practitioners operating within algorithmic formalism who tend to respond to critiques of formalizations with calls for alternative formalizations For example one paper that describes an algorithmic intervention whose implementation was blocked by community resistance notes the legitimate concerns raised by these families can be modeled as objectives within our general formulation and integrated within our framework We see many of the recent efforts in the algorithm research and policy communities as examples of formalist incorporation Specific interventions of this sort include the methods of algorithmic fairness and approaches to improve data and model documentation Such reforms have cant value and can improve many aspects of algorithms but they are not designed to provide an alternative mode of reasoning about algorithms Similarly although the burgeoning frame of ethics has potential to expand algorithmic reasoning efforts to promote ethics within computer science and the tech industry have tended to follow a narrow logic of technological determinism and technological solutionism Because these reforms operate within the logic of algorithmic formalism they are ultimately as remedies formalist incorporation cannot address the failures of formalism itself When computer scientists raise concerns and engage with social science in this manner broader epistemological ontological and political questions about data science tools are often sidelined We focus here on the methods and research regarding algorithmic fairness which represents arguably the most significant recent change to algorithmic research and practice in response to algorithmic harms As currently conceived algorithmic fairness is ill-equipped to address these concerns because it is itself a manifestation of algorithmic formalism via formalist incorporation First algorithmic fairness is grounded in objectivity and neutrality Fairness is treated as an objective concept one that can be articulated and pursued without explicit normative commitments Approaches to algorithmic fairness often position their goals in painfully neutral terms such as non-discrimination Much of the work on fairness points to bad actors reinforcing the view that algorithms themselves are neutral In turn emphasizing an algorithms fairness often obscures deeper issues such as unjust practices and policies What may appear fair within a narrow computational scope can reinforce historical discrimination Second fairness relies on a narrow internalist approach the mandate within the fair-ML community has been to mathematically de ne aspects of the fundamentally vague notions of fairness in society in order to incorporate fairness ideals into machine learning For example one paper explicitly reformulates algorithmic fairness as constrained optimization The deployment of an algorithm mathematically deemed fair is assumed to increase the fairness of the system in which the algorithm is embedded For example predictive policing algorithms and risk assessments have been hailed as remedying the injustices of the criminal justice system with cant energy spent ensuring that these algorithms satisfy mathematical fairness standards Yet such assessments typically overlook the ways in which these fair algorithms can lead to unfair social impacts whether through biased uses by practitioners entrenching unjust policies distorting deliberative processes or shifting control of governance toward unaccountable private actors Third fairness embodies an attitude of universalism Attempts to de ne and operationalize fairness treat the concept universally with little attention to the normative meaning behind these de or to the social and political context of analysis Much of the algorithmic fairness literature prioritizes portability of de and methods across contexts evaluation tools are designed to t into any machine learning pipeline In turn fairness is applied as the solution wherever algorithmic biases or other harms are exposed For instance when research exposed that face and gender recognition systems are more accurate on light-skinned men than on dark-skinned women the primary response was to strive for less biased systems in one case by targeting homeless people of color for facial images Yet such a pursuit of fair facial recognition does not prevent the systemic harms of this technology instead making facial recognition fair may legitimize its use under the guise of technical validation Because of its formalist underpinnings fair machine learning fails to provide the tools for computer scientists to engage with the critical normative and political considerations at stake when developing and deploying algorithms Addressing the ways in which algorithms reproduce injustice requires pursuing a new mode of algorithmic thinking that is attentive to the social concerns that fall beyond the bounds of algorithmic formalism METHODOLOGICAL REFORM FROM FORMALISM TO REALISM IN THE LAW To understand the nature and impacts of an intervention to remedy the limits of formalist reasoning we turn to the evolution in American legal thought from legal formalism to legal realism Legal Formalism The period from about through the First World War was one of consensus in American legal thought The dominant method called legal formalism was the product of concerted effort by legal scholars and judges to promote formal methods in law Legal formalism provided a both a descriptive and a normative account positing how judicial reasoning does and should occur American legal thought in this period was formal in several senses Most fundamentally law was seen as a science that consists of certain principles or doctrines Legal thought aimed to identify classify and arrange the principles embodied in legal cases as part of a uni ed system Jurists working in this mode tended to see legal authority as separated along sharp analytical boundaries between public and private between law politics and morality and between state and civil society Each entity exercised absolute power within its sphere of authority but was not supposed to consider what lay beyond its internalist bounds Legal formalists favored the application of law along a series of bright-line rules these rigid rules were believed to create a more objective and c application of law because they prevented exceptions or context-specific claims Finally legal formalism aspired to determinism It was assumed that a small number of universal principles derived from natural rights could be applied to reliably deduce the correct application of law in specific instances The height of legal formalism coincided with the period of laissez-faire policies and provided reasoning well-suited to defend these policies from progressive challenge Legal formalism emphasized the autonomy of private citizens and the divide between the authority of the state and that of private actors From these general principles judges deduced that efforts to regulate the economy were unconstitutional In the seminal case Lochner v New York the US Supreme Court concluded that a law limiting the working hours of employees represented unreasonable unnecessary and arbitrary interference with the right and liberty of the individual to contract In his dissent Justice Oliver Wendell Holmes argued that the Court failed to consider the context of the case noting General propositions do not decide concrete cases Legal scholar Roscoe Pound argued that Lochner an ignorance of actual working conditions in the United States which he attributed in part to the blinders imposed on judges by the mechanical style of judicial reasoning Following Lochner it became clear among reform-minded legal scholars that enabling the law to account for the realities of social life necessitated as a rst step methodological critiques of the formal reasoning that judges used to uphold the status quo The term legal formalism was not used by its adherents but was rst introduced by legal realists to describe the dominant mode of reasoning they sought to displace Contemporary legal scholars typically refer to this mode of reasoning and the period in which it was dominant as Classical Legal Thought Legal Realism The consensus around legal formalism was upended by an alternative mode of thought legal realism Motivated by what they saw as the failure of legal reasoning to account for its real-world impacts the legal realists challenged the formalist jurisprudence of forms concepts and rules They believed that the inability of supposedly well-reasoned legal analysis to address social challenges such as poor working conditions and staggering inequality stemmed from the fact that context-specic realities and the social impact of laws had no place in formal legal analysis Achieving social reform therefore required a methodological intervention a shift in the everyday reasoning of lawyers and judges in order to render social concerns legible in legal thought Holmes wrote that the main purpose of legal realist interventions is to emphasize certain oft-neglected matters that may aid in the understanding and in the solution of practical every-day problems of the law This pragmatic approach to reform was deeply rooted in the commitment of the legal realists to create a realistic jurisprudence focused not on the paper rules of black letter doctrine but the real rules that actually described the behavior of courts Realists aimed to enable the law and themselves as practitioners of the law to deal with things with people with tangibles not with words alone Rather than simply point out the failures of legal formalism realist critiques put forward new modes of practical reasoning that overcame the epistemic limitations of formalism and that expanded the commonsense modes of thinking like a lawyer From Universal Principles to Contextual Grounding A primary legal realist insight was that legal outcomes were not and could not be the result of a c process Wesley Hohfeld argued that formal legal thought engaged in deductive error by treating legal principles as universal the tendency and the fallacy has been to treat the specific problem as if it were far less complex than it really is and this has furnished a serious obstacle to the clear understanding the orderly statement and the correct solution of legal problems The issue arose because efforts to deduce rights and duties from universal principles of liberty or autonomy overlooked how the law was indeterminate eg it could protect liberty in multiple competing yet equally plausible ways confronting decision makers with a choice which could not be solved by deduction In conventional examples of legal reasoning legal realists ed instances of legal pluralism the capacity for legal materials eg prior cases statutes rules and principles to render multiple legitimate outcomes due to gaps con ambiguities and circularities within those materials The resulting indeterminacy and pluralism forced legal actors to make judgments based on their interpretations and values rather than mechanical procedures that would structure social dynamics in e making policy Realists argued that this adherence to deduction from general principles played a key role in laws complicity with the social harms of the day Formal legal analysis was evaluated based on whether it correctly ed and applied legal principles This privileged the formally correct application of principles over the often unequal results that such applications created Realists decried this adherence to an academic theory of equality in the face of practical conditions of inequality as methodologically absurd as well as socially harmful Instead realists asserted legal decisions should be evaluated based on their actual impact in their particular context law should be understood as a means to achieving social ends not as an end in itself Unlike philosophy argued Holmes the law was not a project of the ideal but an instrumental means of administering justice in the messy and complex world From Objective Decisions to Political Assessments Because cases could not be solved by applying general principles realists argued that it is impossible to engage in legal decision making without exercising some degree of subjective judgment The act of filling gaps in legal reasoning with policymaking was thus infused with politics the ideological predilections and commitments of the judge The upshot for realists was not that such expressions of politics are inappropriate but that they are inevitable Realist insights enabled legal practitioners to grapple with the policymaking nature of their work For example in cases regarding workers rights following Lochner judges could no longer reason that judicial interference would be impermissible because judicial restraint was as much of a political choice as judicial intervention More broadly realists displaced the dominance of bright-line rules with a shift towards standards meant to structure reasoning regarding laws social context and impacts Moving from rules to standards rendering gaps in deductive legal reasoning more explicit and legible was one way that the law evolved its methods to incorporate social context and impact into legal doctrines From Internalist Boundaries to Porous Analysis The effort to evaluate the law vis-à-vis its social impact opened up legal analysis to the languages and methods of other disciplines Legal realists were enthusiastic about filling normative legal gaps with pragmatist philosophy political science statistics sociology and economics and decried the failure of law to keep up with developments in social economic and philosophical thinking They developed limits to legal reasoning within legal authority carving out spaces where law should defer to these disciplines rather than to a judge This emphasis on social impact affected legal analysis in two important ways First it opened up terrain for the positive program of incorporating considerations of social advantage into legal decision making to resolve ambiguities in legal materials by looking to social realities Robert Hales analysis of industrial workplace conditions typifies this approach Hale argued that judicial decisions relying on broad commitments to freedom and opposition to government coercion to protect freedom of contract from workplace unionization few in the face of Industrial Era workplace conditions where workers faced extreme coercive pressure from private employers Hale showed how legal decisions necessarily distribute freedom and coercion among parties thus necessitating that decisions be made in reference to a broader social objective Second the focus on impact shifted legal thinking toward considering how opinions and laws would play out in practice Holmes Eg If you are on the property of another without consent they are not liable for any injury you may su er under trespass Eg Under certain conditions it may be socially desirable for us to enforce liability even under conditions of trespass for example if the harm came to a child lured onto the property by an attractive nuisance argued that legal inquiry should concern itself with the messy administration of justice among real-world actors Legal scholars and judges should therefore think of law as would a bad man who is not motivated by the vaguer sanctions of conscience but only the material consequences that may befall him if he runs afoul of the law To assess whether a law is good or bad in other words legal thinkers ought to anticipate the behavior of actors looking to take advantage of the law The Realist Evolution of Legal Common Sense Realist critiques and proposals were controversial and spurred intense debate Moreover realist interventions did not provide a silver bullet to the intractable challenge of administering justice through law Nor did legal realism fully supplant legal formalism many formalist orientations remain common in American legal thought and have in recent decades regained prominence in many areas Nonetheless legal realism provided the methodological basis for profound legal reform Realist methods enabled progressive changes in private law provided the intellectual foundations for the administrative state and led to the overturn of Lochner v New York and the subsequent creation of American labor law Perhaps legal realisms most significant contribution was expanding the epistemic and methodological terrain on which legal reasoning and legal debate could occur By the s law students became adept at reasoning about the limitations of law and at making arguments about the policy effects of legal decisions American legal pedagogy deeply absorbed the basic idea that the validity of laws should be measured in part in terms of their social and economic effects Realist methods remain highly in and have provided the intellectual foundation for several subsequent and ongoing efforts to expand legal thinking including critical legal studies critical race theory law and economics and law and political economy ALGORITHMIC REALISM Recognizing the dangers of algorithmic formalism and the lessons of legal realism we turn now to articulating the principles of algorithmic realism These aspirational attributes counter the orientations of algorithmic formalism with particular attention to preventing or at least mitigating the harms it can produce As the case of legal thought demonstrates such a shift can productively enhance a disciplines epistemic and methodological ability to engage with the social While no mode of reasoning can avoid imposing its logic on the world self-conscious modes can expand their internal logic to explicitly reason about their effects on the world Political Rather than strive for unattainable notions of objectivity and neutrality algorithmic realism emphasizes that algorithmic interventions are inherently political This does not entail computer science entirely abandoning objectivity and its practices such as the norm against manipulating data in order to generate desired results Instead it means interrogating the types of subjectivity that typically y under the radar of objective practice choices such as formulating research questions selecting methodologies and evaluation metrics and interpreting results This political orientation enables computer scientists to re on the normative commitments and outcomes of algorithmic interventions Rather than creating paralysis with computer scientists unsure how to be neutral and objective when doing so is impossible algorithmic realism provides a language to reason about political commitments and impacts as part of what it means to do algorithms First freed from the strict imperative to be neutral and objective computer scientists can interrogate the ways in which their assumptions and values influence algorithm design This re turn can help computer scientists regardless of their particular normative commitments better reason about the relationship between their design choices their professional role and their vision of the good Such reflection should occur through open discussion and deliberation forming a central component of the research process Second algorithmic realism shifts the primary focus of algorithmic interventions from the quality of an algorithmic system in an internalist sense to the social outcomes that the intervention produces in practice No matter how technically advanced or impressive a system is its success under an algorithmic realist frame is de by whether that system actually leads to the desired social changes This approach enables interventions that question rather than uphold unjust social conditions and policies Several approaches can inform such development of algorithms The schema of reformist and non-reformist reforms articulated by social philosopher André Gorz provides a way to evaluate interventions based on their political implications While a reformist reform subordinates its objectives to the criteria of rationality and practicability of a given system and policy a non-reformist reform is conceived not in terms of what is possible within the framework of a given system and administration but in view of what should be made possible in terms of human needs and demands Designers Anthony Dunne and Fiona Raby classify design into two categories a normative design which reinforces how things are now and critical design which rejects how things are now as being the only possibility A related framework is anti-oppressive design which orients the choice of a research topic the focus of a new social enterprise or the selection of clients and projects around challenging oppression Similarly the Design Justice Network provides ten design principles that include prioritize designs impact on the community over the intentions of the designer These frameworks show that recognizing algorithmic interventions as political does not prevent computer scientists from doing computer science instead doing so can help them incorporate normative re into the methods and questions that drive their work With this in mind computer scientists can ask a variety of questions to inform their practice Would the implementation of this algorithm represent a reformist or non-reformist reform Is the design of this algorithm a or critical Would providing our project partner with this algorithm entrench or challenge oppression Is the project prioritizing outcomes over my intentions Will this algorithm empower the communities it affects An example of the expanded practical reasoning that a political orientation provides involves burgeoning activism among employees of technology companies against developing algorithmic interventions for use by the United States Departments of Defense and Homeland Security Rather than perceiving themselves as just an engineer these computer scientists recognize their position within larger sociotechnical systems perceive the connection between developing an algorithmic intervention and the political and social outcomes of those interventions and hold themselves and their companies accountable to the impacts of the algorithms they develop Building on this movement in thousands of computer science students from more than a dozen US universities pledged that they would not work for Palantir due to its partnerships with Immigration and Customs Enforcement ICE Porous Recognizing algorithmic formalisms limited ability to characterize sociotechnical systems algorithmic realism is porous expanding the range of considerations deemed relevant to algorithm design and evaluation Factors that were previously beyond the internalist algorithmic frame become central to what it means to have knowledge or make claims about algorithms A porous approach to algorithms means that formalist considerations eg accuracy efficiency and fairness are recognized as necessary but no longer su to de ne the efficacy or quality of an algorithm additional modes of analysis are essential As in law realism entails both an appreciation of the insights of other fields and a willingness where appropriate to carve out spaces of deference to those fields This porous orientation allows for algorithmic interventions that eschew technological determinism and instead recognize the contingency and of the social world It makes legible the potential for social and policy change in addition to or instead of technological change This does not mean adopting a mantra of social determinism believing that social systems will evolve irrespective of technology Instead a porous approach to algorithmic interventions follows an STS understanding of how the realities of human experience emerge as the joint achievements of c technical and social enterprise This porous orientation gives computer scientists the capacity to widen rather than narrow the range of possible reforms Rather than optimizing existing systems under the assumption of a static society computer scientists can develop interventions under the recognition of a society Several projects exemplify this approach For example instead of developing predictive policing or risk assessment algorithms that treat risk levels and policy responses as static computer scientists have developed algorithms to reduce the risk of crime and violence through targeted and non-punitive social services In other contexts computer scientists have subordinated their priorities to broader communities helping to empower groups advocating for change Furthermore by bringing the social world into the algorithmic frame a porous orientation allows for algorithmic interventions that recognize and account for indeterminacy Under algorithmic realism good algorithm design means not simply designing to promote desired outcomes but de ning what outcomes are desirable and undesirable understanding how potential harms could arise and developing anticipatory mechanisms to prevent or mitigate those outcomes By incorporating these considerations as essential to algorithm design algorithmic realism casts practices such as failing to consider how users interact with an algorithm as no less negligent than failing to test a models accuracy Although it is impossible to fully account for indeterminacy or to guarantee that an intervention will have particular impacts scholarship from STS and critical algorithm studies provides valuable starting points for analyzing the relationship between algorithmic interventions and social impacts The Social Construction of Technology SCOT for example argues that new technologies contain numerous potential interpretations and purposes how a technology stabilizes in closure depends on the social groups involved in de ning that technology and the relative resources each has to promote its particular vision Co-production more richly articulates the intertwined nature of technology and social conditions noting identities institutions discourses and representations as particularly salient pathways of social and technological change A great deal of other recent work has documented the particular ways in which the design application and use of algorithms can exacerbate marginalization and inequality Taking these approaches as a guide numerous questions can inform computer scientists understanding of how an algorithm will interact with and impact communities in a given context These include Who are the relevant social actors What are their interests and relative amounts of power Which people need to approve this algorithm What are their goals On whose use of the algorithmic system does success depend What are their interests and capabilities How might this algorithm a existing c social and political discourses or introduce new discourses This approach has particular value in anticipating and preventing harmful social impacts of algorithms Just as Holmes urged legal scholars and judges to evaluate laws in light of how they will be carried out in practice so too should computer scientists evaluate algorithmic interventions through the lens of how people may actually apply them For example recognizing how police use of algorithms can distort interventions toward surveillance and punishment some researchers developing algorithms to identify people at risk of involvement in crime or violence explicitly articulate their commitment to partnering with community groups and social service providers rather than with law enforcement Contextual In contrast to the universalism of algorithmic formalism algorithmic realism is grounded in contextualism emphasizing the need to understand social contexts in order to determine the validity of any algorithmic intervention Rather than question how a situation can be modeled and acted upon algorithmically a contextual approach questions to what extent a situation can be modeled and should be acted upon algorithmically Context is de here not in a positivist sense of data that can be incorporated into algorithms but in a broader sense entailing the social relations activities and histories that shape any particular setting Gleaning context therefore requires a porous approach rather than an internalist focus on data Such context is essential to understanding relationships and behaviors in sociotechnical systems A contextual orientation allows computer scientists to avoid solutionism and instead take an agnostic approach to algorithmic interventions Agnosticism entails approaching algorithms instrumentally recognizing them as just one type of intervention one that cannot provide the solution to every problem In other words an agnostic approach prioritizes the social impacts of reform regardless of the role played by algorithms it is agnostic as to the means but not the ends This approach can help not just to avoid harmful algorithms but also to place algorithms alongside institutional and policy reforms in order to robustly promote well-articulated social ends For even in contexts where algorithms can help to address social challenges they cannot do so in isolation the most impactful algorithmic interventions occur when algorithms are deployed in conjunction with policy and governance reforms This approach also allows algorithmic thinking to be incorporated into social and policy reform efforts without requiring the deployment of an algorithm and the imposition of algorithmic logics Contextualism makes legible questions about whether algorithms can capture the essential aspects of a real-world context and whether algorithms can generate the desired social impacts Computer scientists pursuing interventions through a contextual approach can pose numerous questions What elements of this context does an algorithmic approach capture and overlook What values are important for any solution To what extent can an algorithm account for those values How does an algorithm compare to other reforms in terms of producing better outcomes If the answers to these questions suggest a significant divide between the context and an algorithms ability to model and improve that context then it is likely that an algorithmic intervention is an ill-advised approach to providing the desired social bene To see this in practice consider the experience of one of this papers authors while working as a data scientist with a municipal Emergency Medical Services EMS department The author was asked to improve ambulance response times with data analytics The instinct of an algorithmic formalist following a universalist orientation would be to develop an algorithm that optimizes ambulance dispatch Yet when the author studied the context of the problem it became clear that such a solution would not t into EMSs operations nor would it address the underlying issues generating long response times The authors analysis revealed that cant resources were being deployed to calls for people struggling with homelessness mental illness and drug addiction These individuals did not require the acute medical care that EMS was providing at the expense of providing it for other incidents instead they needed social services that EMS was ill-equipped to provide It became clear that ambulance response e was a limited frame for understanding and thus reforming EMSs operations the e of ambulance responses said nothing about the broader goal of providing services that address peoples needs Although a dispatch optimization algorithm may perform well along formalist metrics of e such an algorithm would have failed to address the underlying issue The author instead worked with EMS to create a new unit of EMTs who would respond to these incidents via bicycle or car and be specially trained to connect people to local social services the parameters of when and where this unit would operate were determined by analyzing EMS incident data Notably the ultimate intervention was not to integrate an algorithm into existing procedures a policy change informed by data was better suited to improve both e and service quality Rather than representing a failure to take advantage of algorithms this effort was recognized as a positive collaboration that integrated data analysis and institutional context to improve social services DISCUSSION The numerous and cant harms of algorithms may appear to be the result of computer scientists failing to follow best practices Yet our articulation of algorithmic formalism describes how these outcomes are due to the logic of algorithmic thinking itself not an imperfect or malevolent application thereof The chronic tunnel vision of algorithmic formalism can lead to harmful outcomes despite good intentions and following current best practices Remedying these failings requires not incorporating new variables or metrics such as fairness into the formal method but instead introducing new epistemic and methodological tools that expand the bounds of what it means to do algorithms Algorithmic realism represents this evolution in algorithmic thought providing new modes of practical reasoning about the relationship between algorithms and the social world The realist orientations described here provide important starting points for computer scientists and others pursuing algorithmic interventions Following the political orientation practitioners should consider what assumptions and values they may be taking for granted and what normative commitments they want their intervention to embody Following the porous orientation practitioners should consider what theory of change motivates their work and how to responsibly account for unexpected impacts Following the contextual orientation practitioners should consider what goals are central to a given context and whether an algorithm actually provides an appropriate intervention In a realist mode of reasoning all of these questions are seen as integral to rigorous algorithmic work rather than as beyond the scope of algorithmic design These realist practices will enable the eld not just to avoid harmful impacts but also to identify new research questions and directions to pursue As in law algorithmic realism is not meant to provide a wholesale rejection of formal methods nor will it provide a wholesale solution to the intractable challenges of designing just algorithmic systems Even to the extent that the turn to algorithmic realism is motivated by a broader program of social reform à la the turn to legal realism new epistemic and methodological tools cannot by themselves achieve a vision of the good let alone determine which vision of the good to work towards Nonetheless algorithmic realism can help computer scientists reflexively approach their work in light of their larger normative commitments and the impacts of algorithmic systems As such algorithmic realism enables computer scientists to reason well about doing good