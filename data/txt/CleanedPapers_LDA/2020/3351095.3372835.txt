The Case for Voter-Centered Audits of Search Engines During Political Elections Search engines by ranking a few links ahead of million others based on opaque rules open themselves up to criticism of bias Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results Insofar that these concerns are related to the principle of fairness this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large Thus we ask the following research question how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public ie voters operate To answer this question we qualitatively explore four datasets about elections and politics in the United States a survey of eligible US voters about their information needs ahead of the US elections a dataset of biased political phrases used in a large-scale Google audit ahead of the US election Googles related searches phrases for two groups of political candidates in the US election one group is composed entirely of women and autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the US state of Virginia in We find that voters have much broader information needs than the search engine audit literature has accounted for in the past and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits CONCEPTS Information systems Web search engines KEYWORDS algorithm audits search engines Google voters elections bias INTRODUCTION A week after the US presidential election a news story about a problematic Google search result page made the media rounds The journalist had searched for the query final vote count and Googles top search result had claimed that Trump won both the popular vote and electoral college President Trump lost the popular vote by million votes This top search result was published by a conspiracy blog one of the dubious sources that contributed to the creation and spread of a conspiracy theory repeated by President Trump that Trump would have won the popular vote if not for voting irregularities unauthorized immigrant voters voter fraud etc How did a false news story from a conspiracy blog end up as the top news story on the Google search results page Googles response through a spokesperson was non-revealing The goal of Search is to provide the most relevant and useful results for our users In this case we clearly didnt get it right but we are continually working to improve our algorithms Other questions that can be asked in this context are how many users searched Google for this topic final vote count How many users saw the conspiracy blog story as the top result What fraction of users clicked to read it These questions aimed at transparency are important when trying to understand how disinformation spreads and affects users in order to mitigate its potential harm For example Twitter notified million users in the United States that they were exposed to content generated by Russias Internet Research Agency during the US Election Although journalists have documented several examples of disinformation or other harmful content being displayed at the top of Google Search results Googles response has been to change the algorithms to fix the particular problem without providing transparency to stakeholders This lack of transparency about the how of such scenarios is often defended as a protective measure malicious third-party actors could benefit from efforts of transparency to increase the success New York Times Trump Repeats Lie about Popular Vote in Meeting with Lawmakers January rate of their attacks However Googles secrecy about the extent of exposure to such highly ranked disinformation is potentially harmful to the public In at least one highly-publicized case one individuals path to a devastating hate crime that of Dylan Roof who killed nine black people in their own church started with the search for black on white crime on Google There is a difference between the two search phrases we have discussed so far final vote count and black on white crime The first one is a seemingly neutral query that anyone can perform innocently out of curiosity The second query is problematic from the start as it was pushed on the Internet discourse by groups with a white supremacist agenda The researchers Michael Golebiewski and danah boyd coined the concept data void to refer to phrases like this one which when searched lead to either low quality or problematic content As they explain once black on white was publicized by Roofs crime high quality websites created content that filled the data void One could argue that these are isolated cases that the majority of queries are not problematic and will not lead to problematic content However Googles general lack of transparency has opened the door for continuous political attacks Most recently President Trump accused Google of manipulating the search results to favor his political opponent Hillary Clinton basing his claim on a white paper of a small-scale Google audit which had hypothesized the possibility of Google swaying the elections If Google is unable to provide transparency researchers can try to use audits as a tool for increasing literacy around the complexity of generating search results The public should be able to search for anything problematic content as well but it should be Googles responsibility to not serve harmful content at the very least at the top of the search result page Google was able to fix the issues of consumer-harming web spam that plagued online shopping results in the early s We should demand the same kind of commitment to high-quality results for political content too This is particularly important because research has found that the public believes that Google shows trustworthy information at the top of its search results However if the US presidential election was any indication efforts to propagate disinformation in novel ways will only increase in frequency and sophistication Continuous and large-scale audits can serve to raise awareness about vulnerabilities in the information ecosystem for which search engines are a central gateway Auditing for Bias Social scientists have been worried about the power of search engines and their potential bias since they came into prominence in the s Thus auditing search engines in general and especially their role in political elections is not a new research problem However the underlying assumptions that have motivated such research over the past ten years have been different and led to specific approaches Concretely we can distinguish three different kinds of efforts to detect political bias in search platforms Third-party manipulation In the early s Google was susceptible to forms of political activism that came to be known as Google bombing According to Democratic party activists used this technique to promote in the Google search results negative stories about Republican candidates in the US Congressional elections However such techniques didnt succeed in due to Googles changes in its algorithms to promote official sources when searching for candidate names A repetition of this study with more search engines in confirmed the previous finding of official websites topping the rankings Ranking Bias Experimental work by Epstein and Robertson in tested the following hypothesis were a search engine like Google to subtly manipulate the ranking of stories about political candidates this would be sufficient to affect the outcome of an election They named this the search engine manipulation effect SEME This hypothesis is connected to the popular theory of filter bubbles by Eli Pariser according to which different search engine users might be exposed to different search results for the same query Various studies to test this theory using political search terms although not during election periods have found little evidence for it Ecosystem bias Recent audit studies have recognized that focusing on the the producers of content or on the ranking algorithms individually is not sufficient to appropriately characterize the complex nature of interactions during search which involve the users who initiate the search the content providers and the ranking algorithms Furthermore the search page itself has evolved over the past years providing more outlets for showcasing content without having to click on any links on the search page This ecosystem approach to bias was articulated first in in the context of Twitter search but can also be found in As we can notice from this chronological literature survey there has been a progressive move toward expanding the focus of where bias and as result manipulation is located For example the work by Kulshrestha et al introduced the concept of the interaction among various kinds of biases input bias and ranking bias in order to produce a more pronounced output bias Meanwhile the work by Hu et al changed how audits are performed by adopting biased searches as starting points for the audit This allowed them to notice a similar effect to that shown by Kulshrestha et al on Twitter namely that when using biased search phrases the text snippets shown by Google are more biased than the web pages from which they are extracted Considering the Voters This progress on auditing search engines in the context of political elections especially recognizing the many sources of bias and their interaction is important but insufficient towards the goal of developing voter-centered audits For one most of the mentioned studies utilized queries chosen by researchers themselves without any input from voters or insights from political scientists knowledge of voters Additionally queries are focused on politicians occasionally rightly so while elections are multi-faceted what else besides the candidates affects participation and decision making in elections Meanwhile what role does the interface of the search engine eg the autocomplete suggestions feature play in shaping voters information seeking behavior Finally do the audits consider if searches are leading to high quality information from reliable sources This last question is especially important because efforts to manipulate elections might not come in the form of political bias which is what most audits have been focusing on Efforts to suppress voters participation in elections can target information such as when does the early voting start Where are the polling locations situated When is the voter registration deadline Ensuring that all voters get reliable information on such questions is important because such information is location dependent In this paper we explore four different datasets that capture various aspects of searches about politics and elections with the explicit goal of identifying important features for the design of search engine audits that are centered on fairness to voters and their information needs Our takeaways explained in more detail in the rest of the paper are the following Information Cues We need to utilize the vast political science theory on voter modeling to choose queries for audits For example we found empirical evidence that many voters behave as predicted by the theory of information cues by Arthur Lupia This theory suggests that voters prefer to take shortcuts to get informed about elections Examples are searches for endorsements or political alignment Women candidates are more likely to be the subject of such searches Biased Searches Recent audits like Hu et al are right about including biased searches in their seed dataset of phrases Our survey with voters found evidence that voters do indeed perform biased searches However comparing the phrases that Hu et al extracted from politician speeches with those used by voters showed almost no overlap Biased searches need to come from voters Beyond Candidate Names While candidates are central to the information seeking process queries about them are more complex than just their names Voters first need to find out who the candidates are eg Colorado candidates and then will ask for more specific information such as what does Beto stand for or Gillum  DeSantis polling Moreover questions about the logistics of voting and the nature of elections are common too Unreliable Localization Voters might prefer short and ambiguous queries eg election or voting near me and rely on the search engine to either provide autocomplete suggestions or display the right information Googles algorithms appear to be sensitive to real-world events such as elections but this behavior seems difficult to predict We hope that these insights will serve to guide the design of future voter-centered audits which by taking an ecosystem view aim to uncover whether voters are receiving reliable information from This is especially true in the United States where each US state has different elections norms and laws trustworthy sources independently of their geographic location or the sophistication of their search phrases RELATED RESEARCH Given our focus on the voters we start this section with theories that try to explain and predict voter behavior with respect to their information needs Then to expand upon the overview of audits in the context of political elections which we presented in the introduction section we summarize aspects of the audit methodology that is relevant to our discussion Informed or Uninformed Voters In the landmark study of The American Voter changed the way the political science research community approached the understanding of electoral behavior by borrowing methods from the field of psychology One of the important insights drawn from this new methodology was the discovery of a stable party identification as the principal factor in voting Further exploration of this studys panel data by Philip Converse exposed the various belief systems of different groups of voters Many interpreted this research as suggesting that voters are dumb and Converse later tried to distance himself from such an interpretation insisting that he had only made the case for a great maldistribution of political information across the public that is a majority of the public knows little and is inconsistent in their positions uninformed or low-information voters while only a fraction of the electorate has sufficient knowledge to be consistent in their positions over time A more sympathetic view of the voters comes from Arthur Lupia who in earlier studies contended that voters could not be expected to have an encyclopedic knowledge about politics He suggested that information cues eg who supports a ballot initiative Ralph Nader or the auto insurance lobby Or is the candidate a Democrat or Republican are a good way for voters to align themselves with the issues or candidates see also A cue according to Lupia is a piece of information that can take the place of other information as the basis of competence at a particular task Cues are not unique to politics individuals use them in many situations for example brand names eg Nike or Volvo are a cue for product quality In the survey data we collected many queries formulate an explicit information request for party identification eg what party is Harry Arora Gavin Newsoms party as well as other information cues such as endorsements and polls We qualitatively code all phrases that request information cues to find out the extent this strategy is used by voters Additionally Lupia and other researchers highlight the role that negative emotions such as fear and anxiety play in information seeking behaviors We noticed this emotional manifestation in searches from our survey participants are the elections rigged or will democrats win the senate and decided to look for more examples of such searches which we broadly categorize as biased searches because they come from an one-sided perspective usually in favor or against a certain political party individual or issue More on Search Engine Audits Search engines as one of the major platforms for satisfying information needs are a particularly frequent target of algorithm audits But how are audits carried out In Sandvig et al formally define four different types of audits code audits scraping audits sock puppet audits and non-invasive user audits Code audits involve researchers examining the code bases that underlie algorithmic systems To date there are no such audits for search engines and most likely the complexity and scale of their code bases might make that task impossible Instead the most common way to perform search engine audits is the Scraping audit during which automated scripts send batches of queries to a search platform and then analyze the results The datasets used in this paper are collected via scraping audits Recently sock puppet audits have increased in popularity as they add a new dimension the process imitates users by constructing fake user profiles before making repeated queries Sandvig et al express concern that these techniques violate platforms Terms of Service and present some legal issues However the auditing community has argued that the importance of understanding potential discrimination and biases in these algorithms outweighs these risks As this method evolves some of the sock puppet audits are now employing real users who have been recruited throw crowdsourcing platforms The non-invasive user audits require users to answer questions about the ways they interacted with the platform This could include but is not limited to search query log collections as well Sandvig et al raise several concerns about this technique including users inability to accurately report their behavior users not searching enough queries that will reveal platform discrimination sampling problems and the difficulty to establish causality due to lack of systematic controls over what users do As a result such audits are almost non-existent in the search engine audits literature However this is a limitation that needs to be overcome through a combination of large-scale representative sampling as the one used in political science surveys with advances in browser plugin technologies such as the ones used in or DATA AND METHODS In this paper we make use of four different datasets An overview of the datasets is provided in Table Each subsection provides information about how we collected and processed the datasets More details about the survey with AMT workers the audit method and the Virginia election are provided online Dataset Voter Searches Knowing what voters search ahead of elections and what results they are shown by a search engine can help us ascertain that search engines are not promoting disinformation harmful to the public or the candidates participating in the electoral process One way to do that would be to recruit participants that are willing to install a plugin on their browser a technique used by which will allow us to collect their interactions with the search engine However implementing plugin based studies is expensive introduces concerns about user privacy and presents new technical challenges in having to discern which queries are election related Therefore we created a survey rather than a plugin to collect election related queries We recruited US citizens through Amazon Mechanical Turk AMT and asked them to share searches that they have performed or will perform on the days ahead of the election Oct -Nov To attract a diverse group of participants we didnt set restrictions for participation except for age over and citizenship United States This led to a large number of incomplete or irrelevant submissions that we manually discarded At the end we had responses from eligible US voters who supplied between to search phrases together with demographic information The respondents generated a total of search phrases with of queries only appearing once in the dataset The average length of a search phrase was words standard deviation words and median words This finding mirrors what studies of search logs across many datasets have found an average query length of  words That is despite the fact that the queries in this dataset were not extracted from search logs they still conform to the usual form of search queries We perform qualitative coding of these search phrases to detect themes identify biased searches as well as requests for information cues Dataset Partisan Queries Hu et al performed an audit of Google search snippets during Oct  ahead of the US Congressional Elections This is the largest audit about elections ever reported in the literature The authors started with a list of politician names and left and right leaning terms and phrases Then they extracted Google autocomplete suggested phrases for all of them and collected the resulting SERPs search engine result pages Differently from previous audits in the literature which mostly rely on candidate names and phrases selected by researchers this time the politically-biased searches came from politicians Concretely the authors created a lexicon of unigrams and bigrams that represent partisan cues These n-grams came from speeches of Democrats and Republican politicians scraped from the website votesmartorg and spanning the time period January August All n-grams that occurred more than times were assigned a partisan bias score which ranged from  ngram used only by Democrats to ngram used only by Republicans Then from this lexicon the authors chose all phrases with an absolute score greater than and manually selected meaningful phrases to search The dataset that they shared with us contains such phrases and their bias scores For example gun lobby has a score of  mostly used by democrats while gun rights has a score of mostly used by republicans In our analysis we search if any of these phrases is contained in the searches formulated by the AMT respondents in order to discover if such phrases are representative of what voters themselves could search ahead of an election Dataset RS-Candidates For Google Search users there are two opportunities to learn what other users are searching the autocomplete suggestion feature and the related searches at the bottom of a SERP Autocomplete searches are sensitive to external shocks events that happen in the real world and are updated more frequently than related searches Refer to materials on our webpage Table An overview of the four datasets used throughout this paper Three datasets were created by this papers authors and one was received by the authors of Hu et al Name Period Description Creator Voter Searches Oct  Nov A list of search phrases collected through a survey of AMTs Authors Partisan Queries Jan  Aug A list of phrases extracted from politicians speeches of democrats and republicans with a partisan bias score Hu et al RS-Candidates Oct  Nov Googles related searches RS in SERPs for two groups of candidates Authors Virginia Election June Autocomplete phrases and SERPs for queries searched simultaneously in Virginia and Massachusetts Authors One way to think about these two groups of searches is that the former correspond to trending searches while the latter to most popular searches about a seed query After collecting SERPs for two groups of candidates for the US midterm elections we extracted the related searches RS from each page The groups of candidates were as follows Women candidates Women running for congress or governor Their names were gathered from a Washington Post article that was tracking the political fortunes of women candidates Senate challengers Challengers running in the primaries and the Senate election for US states The names of candidates were collected from Ballotpedia in July by visiting the electoral information webpage of each US state holding a election An example of related searches for the candidate Alexandria Ocasio-Cortez is shown in Figure For our analysis we are interested in the words or phrases next to the candidate name in this case husband married age etc To have a more complete dataset we collected SERPs for both groups of candidates on different days on the period Oct  Nov and compiled all unique phrases in related searches for each candidate Then we automatically removed the names of the candidates to focus on the additional words and phrases in the queries We found unique phrases for Women and unique phrases for Challengers These phrases were manually inspected to correct some of the inevitable errors of the automatic phrase extraction Figure The related searches for Alexandria Ocasio-Cortez a woman candidate running for the US Congress screenshot taken on Nov We automatically remove the candidate name from the related searches and aggregate the remaining words or phrases In this case these words are husband married age etc Dataset Virginia Election When we analyzed the phrases from the Voter Searches dataset we noticed that many of them were short and ambiguous For example respondents wrote phrases like voting or elections as well as candidates near me We hypothesized that one reason that voters might formulate such queries is that they rely on the search engine to correctly interpret their intentions despite their searches being short and ambiguous Google users are familiar with searches like weather or pizza near me that correctly leverage the users location to personalize the search results Another reason for supplying short queries to the search engine might be that users are accustomed to taking advantage of Googles autocomplete feature to suggest longer and more appropriate queries In order to test such hypotheses we conducted a pilot study the day of the primary election for the State Legislature in the US state of Virginia June We selected query phrases from the Voters Searches dataset mostly short queries such as voting election day democrats as well as syntactic variations of semantically similar queries such as where can i vote where do i vote or where to vote We used two computers one in Virginia and one in Massachusetts to automatically collect the SERPs for these search queries from Google at the same exact time and through the same automated procedure Additionally we collected the autocomplete suggestions for all the queries for each in both locations using the open-source tool introduced in Thus this dataset is composed of SERPs for each location and autocomplete search phrases for each location Qualitative Coding For the Voter Searches dataset we performed multiple rounds of qualitative coding We started with a thematic analysis of the data in order to inductively draw themes from the queries Then we performed deductive coding for the presence of phrases that elicit information cues yes no presence of bias yes no and expression of bias semantic pragmatic For the thematic analysis two coders independently looked for themes in a subset of of the phrases until they reached a saturation point The coders then met to discuss themes and sub-themes and create the code book The rest of the data was thematically coded by a single coder For the deductive coding of information cues bias presence and bias expression two independent coders The list of all queries can be found online Our procedure is described here and the research team defined strategies and rules for applying the codes and then the coders proceeded independently After each round they had the opportunity to check their differences and decide to change or retain their label We report the inter-rater reliability IRR score calculated as the Cohens kappa in the Results section Operationalizing Bias Our definition of what is biased was deliberately broad any explicit or implicit expression of preference toward or against a person group issue or event We recognize that the discussion of bias in literature is often much more narrow and usually with a negative connotation However to the extent that bias seems to be correlated with information cues which when accurate can help voters make good decisions we think that our broad definition is warranted Particularly in psychology many biases eg optimism bias are shown to have positive effects on our lives Inspired by this interpretation we dont regard the presence of bias in search queries the phrases provided by voters negatively Once we had identified the subset of biased phrases we proceeded with a second round of coding this time to identify the expression of the bias using a binary categorization semantically and pragmatically biased queries A query was coded as semantically biased if it included language that denotes bias independently of its context For example the phrases the best candidate and will Beto win are semantically biased because of the individual words best and win Alternatively a query was coded as pragmatically biased if it contains bias as a result of its context within a broader narrative Such examples would include diane feinsteins age or blue wave None of the words in these phrases indicates bias but in the context of the election the phrases can be interpreted as biased Concretely there were calls for California Senator Diane Feinstein to not run again for a senate seat because she was deemed too old leading to accusations of ageism Meanwhile blue wave referred to a theory in which the Democrats whose color is blue were going to regain the power in Congress leading to much excitement and mobilization for Democratic Party voters Our terminology semantic/pragmatic was inspired by the relevance theory of meaning within linguistics and philosophy of language scholarship Developed in the s by Dan Sperber and Deirdre Wilson relevance theory distinguishes between the semantic and pragmatic meaning of a linguistic utterance in order to describe how people interpret context-independent linguistic cues based on speaker intention and contextual situation Operationalizing Information Cues Drawing on Arthur Lupias discussion of information cues in his book we identified queries which likely contained shortcuts to information that voters may not have on recall As a reminder Lupia explains that a cue is a piece of information that can take the place of other information as the basis of competence at a particular task Our dataset doesnt contain the cues themselves instead it contains the query phrases that voters formulate in order to access such cues For example when they search about Gavin Newsoms party they are looking for this candidates partisan alignment that serves as a strong information cue Other queries that serve to access information cues are endorsements polls rankings or position alignments with hot-button issues Limitations of the Collected Datasets All our datasets can be considered as small datasets compared to the ones used in other auditing studies This was by design so that we could inspect the results manually and only small datasets are amenable to such analysis Our goal is not to perform an audit but to find out how to design audits that are centered on voters We believe that the exploration of these small datasets provides important insights that can be tested via large-scale and partially automated audits Nevertheless its worth pointing out the limitations of the datasets in order to mitigate such limitations in future studies Sampling Bias The survey with AMT respondents is not representative of the voter population in the United States However recent research has indicated that AMT is better than or equal in terms of diversity to other survey participant pools Ecological Validity We didnt receive actual search logs from the respondents thus its fair to question how valid the query phrases are As part of the survey we asked participants how likely it was that they have performed or will perform these searches and responded with extremely likely or somewhat likely The average length of phrases was words within the range described in literature It is possible that when searching with a search engine voters will formulate different queries but we believe that this doesnt invalidate that the queries in our dataset are possible too Given that this study only uses the data to inform design for future audits the incompleteness of the dataset might be acceptable Algorithmic Exclusion Two datasets Virginia Election and RS-Candidates are based on data that Googles algorithms for generating related searches and autocomplete suggestions provide It is the nature of these algorithms to exclude certain terms and phrases that are deemed harmful Relying on accounts of researchers who have accessed unrestricted search logs we know that users perform many searches that are not made public in any way While the exclusion of such queries limits what we can learn from the datasets it is also a reminder of why we cannot perform audits relying only on autocomplete searches but instead find ways to collect real queries from users given that access to search engine logs is off limits for most researchers RESULTS One central motivation for this study is to gather information for designing voter-centered audits of search engines during election periods The analysis of the four datasets we have assembled provides us with the opportunity to do so Information Cues Our analysis of information cues phrases that serve as shortcuts to evaluate candidates explores two datasets Voter Searches and RS-Candidates In Voter Searches we manually coded all phrases as eliciting information cues or not The first rater coded phrases and the second rater coded phrases as eliciting such cues Their agreement as calculated by Cohens kappa was substantial agreement By matching the phrases back to the respondents we found that participants formulated at least one query that elicits information cues about candidates or Table Comparing the top most related searches for two datasets Women and Challengers collected during the US Congressional Elections Searches known as information cues such as polls or endorsement are more often directed at women candidates Women Dataset related searches polls facebook wikipedia bio for congress age twitter endorsements husband congress Challengers Dataset related searches senate for senate twitter facebook wikipedia polls bio us senate age net worth issues with participants formulating three or more such queries Thus  of participants engaged in this kind of information gathering strategy as Lupias theory on voters suggests This is important on two counts a it indicates that we can rely on political science theory for voter modeling to extract insights to inform audits b it raises the issue of whether the results that are shown in response to such queries are reliable given that voters are using them to make decisions about who or what to vote for While Voter Searches coverage is restricted to the convenience sample that we recruited through Amazon Mechanical Turk n related searches for candidates on Google Search might correspond to a larger sample accumulated over weeks and months Thus it is informative to analyze what Google users most frequently search about candidates Table summarizes the top phrases that we extracted for the two sets of candidates Women and Challengers The value in percentage indicates the proportion of candidates for which the query was found at least once in their related searches For example searches about polls were found in of Women and in of Challengers From the perspective of the information cues theory such results are interesting because they not only indicate that Google users are looking for them for example polls and endorsements shown in bold but that such requests are much more likely for women candidates While many of the phrases in Table can be regarded as navigational queries based on Broders taxonomy some of the information queries formulated for women candidates age or husband are a good indicator of the personal scrutiny that women running for office face another reason for considering the presence of bias by voters in search phrases Bias in Search Phrases Our analysis of bias was focused on two datasets Voter Searches and Partisan Queries Two independent raters labeled phrases in Voter Searches as biased or not In the first round of labeling However we have no way of knowing this with any certainty since Google doesnt divulge absolute numbers about search volume they achieved an inter-rater agreement Cohens kappa of substantial agreement and in the second round of reconciliation the agreement rose to almost perfect agreement At the end of these two rounds the two raters agreed that phrases or of the whole dataset is composed of biased search phrases Matching these phrases to the participants in the AMT survey we found that out of of the participants have formulated at least one biased search Meanwhile out of of participants formulated three or more biased searches These results indicate that formulating biased queries is a common practice for a majority of voters and that a sizeable portion of them will frequently perform such searches For the set of biased phrases the raters performed another round of coding to establish the expression of bias as semantic or pragmatic The two raters labeled and respectively of phrases as semantically biased with the rest labeled as pragmatic Their agreement according to the Kappa score was substantial agreement The good news is that the majority of the biased searches express this bias semantically which we defined as being located in one of the query words themselves These are words that are found in sentiment analysis and opinion mining lexicons as having a positive or negative valence eg or through NLP approaches This will make it easier for large-scale surveys to identify biased searches in order to scrutinize the search results shown for them However a substantial portion of bias is expressed pragmatically and this will complicate automated analysis Although we didnt explicitly set out to find data voids while performing the labelling into semantic vs pragmatic bias we noticed that several of the phrases fit the description of searches that have the potential to lead to data voids SERPs with low-quality content from adversarial information providers The typology of data voids proposed by Golebiewski boyd would classify some of them as data voids that can be weaponized after breaking news events eg maga bomber or th ammendment and others that may be weaponized due to ongoing discriminatory events in our society eg voter purge or immigration based crime Thus an advantage of collecting such queries from users is to understand the extent to which certain weaponized phrases succeed to spread in the public In the past most researchers have used neutral phrases as queries for the search engine audits Hu et al intentionally used biased phrases and their autocomplete suggestions to perform their audit In light of our above-mentioned findings such a decision is the right one given that voters will either occasionally or frequently perform biased searches However its worth examining more closely the details of how bias is expressed For that we compared the phrases in Voter Searches with those in Partisan Queries We found an exact overlap for only phrases These phrases are occurrence given in parentheses early voting gun control illegal immigration voter turnout voter suppression russia investigation election system That is phrases or of biased phrases in Partisan Queries extracted from politician speeches were found times in Voter Searches This very small overlap is concerning because it indicates that we cannot rely on the speech of politicians or other political elites as a source of knowledge Additionally one reason that Hu et al s data might be showing this low overlap is that it draws from politician speeches over an extended period of ten years while the public Table The major themes that we identified coding the data Many search phrases are coded with multiple themes since they dont fit exclusively in one theme only As a result the relative frequencies dont add up to Theme Examples of search phrases Rel Freq Ballot ballot initiatives in proposition MA question Oregon measure Candidates Black democratic candidates Florida is there a female running in Georgia policies of Alaskan candidates Election election reviews Senate seats up for reelection Ohio election polls voter suppression Issue immigration based crime georgia hope scholarship Abrams gillibrand gun control arizona health care News source AP midterm facts ballotpedia senate forecast recent election news Office NC judicial candidates Virginia Beach mayor who my senate candidates are best candidate for governor Party how can Dems take the house florida democratic party Travis county GOP Republican donation limits People Gillum vs DeSantis polling Ben Jealous policies bernie sanders endorsements Claire McCaskill State overview of stances of arizona politicians california voter guide Colorado poll whos winning florida Voting where can I vote when do polls end register to vote Ohio absentee ballot voting locations central Michigan might be more tuned to phrases that are currently in the news Thus gathering search phrases directly from the public might be a necessary condition for voter-centered audits Beyond Candidate Names Most audits in the literature have relied on candidate names While it is true that voters are interested in candidates their query phrases are not simply candidate names We found only query phrases that explicitly mention candidates Out of these mentioned candidates in context such as Bredeson stance on healthcare or elizabeth warren ancestry Of the other half referred to complete names to last names only eg Gillum and to first names only eg Beto Most importantly though initially only respondents explicitly mentioned a candidate in their phrases With our prompt this number increased to slightly less than half of all participants Meanwhile as the results of the thematic coding shown in Table indicate the largest group of searches is about unnamed candidates with queries such as Black democratic candidates Florida or policies of Alaskan candidates This is indicative of another political science theory that most voters are initially uninformed Thus the role that the search engine will be playing by directing voters to information sources in response to such queries is even more important given the scale that this might be happening The thematic coding in Table contains more insights The voters are interested not only in a wide range of topics but also in a variety of information kinds factual information where can I vote speculative information whos winning Florida and so-called problematic queries that might lead to data voids eg immigration based crime Given such variation to carry out comprehensive voter-centered audits we will need to design mechanisms to elicit such queries from voters in order to examine how the search engines treat these distinct epistemic categories factual speculative and problematic of information needs Ethnographic research by provides additional evidence for such a need Unreliable Localization The Virginia Election dataset contains SERPs and autocomplete suggestions that were collected on two different locations Virginia Table Out of seed queries received autocomplete suggestions for the location in Virginia related to the primary election happening that day in Virginia R Rank Seed Suggestion R candidates candidates in virginia primary election election day virginia election day election day virginia elections elections in virginia elections in virginia elections in va how to register to vote how to register to vote in va how to register to vote in virginia primaries primaries in virginia primaries virginia sample ballot sample ballot virginia sample ballot fairfax county voter registration voter registration virginia voter registration card va voting voting in virginia voting in va voting virginia where do i vote in primaries where to vote where to vote in virginia where to vote in virginia primary who is running who is running for virginia senate and Massachusetts on June the election day in Virginia Comparing the two sets of autocomplete phrases we find an overlap of While the overlap is substantial the fact that of phrases are different indicates that there is a degree of localization happening in both locations Given that our focus was on the election happening that day we further analyzed the seeds and suggested phrases that indicated an awareness about local elections The results for both locations are shown in Table and Table Comparing the results in the two tables shows that there are Table Out of seed queries received autocomplete suggestions for the location in Massachusetts There was no election happening in Massachusetts on the collection date Seed Suggestion Rank election day election day massachusetts how to register how to regiser to vote in ma how to register to vote how to register to vote in ma how to register to vote in mass primaries primaries in massachusetts sample ballot sample ballot brookline ma sample ballot massachusetts voter registration voter registration ma voter registration card ma where do i vote where do i vote ma where do i vote boston where do i vote brookline ma the suggestions generally appear towards the top of the SERP This can lead us to hypothesize that to some extent Googles algorithms are able to pick up the signal about Virginias elections and update the autocomplete suggestions accordingly However most of the seed queries didnt contain any localized suggestions for VA and for MA and it is difficult to predict which queries will display locally relevant results This is especially concerning for queries that are semantically similar but display minor syntactic differences For example the phrases who is running and whos running differ only in the contraction of the verb but all their suggestions but one are different Similarly suggestions for where can I vote where do i vote and where to vote dont overlap much This random variation makes it challenging to consider autocomplete suggestions as a reliable source for directing searchers toward valuable suggestions during an election period On Virginias election day we also collected SERPs in both locations Are SERPs personalized to show results based on location for simple queries such as elections As the screenshot in Figure shows there is evidence for such hypothesis too Nevertheless drawing a clear conclusion from the comparison of the organic links from all SERPs between the two locations was also challenging For of the queries the results are identical eg polls election polls republicans who is running etc but for of them the Jaccard similarity is less than little overlap between links Some of the search phrases with little overlap include voting locations elections poll results sample ballot how to vote etc For these queries many local links local government or local news are shown This is a desirable situation but the unpredictability of which queries lead to locally relevant SERPs and which do not makes this an unreliable feature similarly to autocomplete suggestions How consistently are results for important searches localized Does it depend on ones locality Does it depend on the sample size of users who engage in such searches A similarity measure that is calculated as the ratio of the size of the intersection of the two lists with the size of the union of the two lists It is for two lists with no overlap and for two lists with full overlap Figure Screenshoot of Googles SERP for the query election taken on June from a computer in Virginia Two articles in Top Stories and the second result are about the Virginia election from that location If so that might be unfair to voters who live in remote areas away from densely populated cities or who lack local institutions that will post on the web reliable information Further research is needed to addresss such questions DISCUSSION AND IMPLICATIONS There is surprisingly little research on how voters use search engines in the context of political elections see for a recent multi-national survey supported by Google Concretely the long-running American National Election Study ANES which has been collecting data on voters and elections in the US since has yet to include questions about the use of search engines by voters despite asking questions about Twitter a much less used platform by the population at large The analysis of our datasets despite the stated limitations provides insights that should inform further research both on how voters utilize search engines for staying informed during elections and on the kind of results search engines provide Bias Information Cues and Data Voids By labeling all search phrases in our dataset as either biased or not biased we found that of participants formulated biased searches toward or against a political party ideological issue candidate trait or entire groups of people Often this bias is evident through partisan words eg Democrat or Republican positive and negative emotions or situations eg win lose good bad verbs that pass a judgment eg lie impeach or verbs that express We didnt label the direction of bias support endorse approve etc We refer to all these instances as semantically biased phrases given that the bias is evident in the meaning of the words In the biased phrases dataset close to two thirds of phrases are semantically biased This is a useful finding because when bias is expressed in this way it is possible to identify it using automated approaches based on natural language processing techniques The remaining one third of the phrases were labeled as pragmatically biased indicating that in order to infer the bias we need contextual information that is not present in the words themselves For example a phrase like kavanaugh hearing is pragmatically biased because one needs to know that the way senators voted during the hearings of Supreme Court Justice Brett Kavanaugh became an electoral issue in the November US midterm campaigns Our analysis also illustrates the relationship of biased phrases to two important concepts in political and media communication information cues and data voids In our dataset of queries labeled as biased search phrases were also labeled as eliciting information cues In addition to well-known cues such as the party affiliation of a candidate or their endorsement by a trusted entity there seems to be a shift toward a new and broader set of cues Concretely we noticed many queries asking for the stance of a candidate on what have become ideologically divisive issues such as abortion gun control marijuana inequality climate etc which are not always aligned with ones political affiliation Finding reliable information on the web about such issues might be challenging for many voters Therefore Googles decision to display an On the issues tab as part of the knowledge panel for the presidential candidates in and some senators in may be a positive step toward solving this issue However some researchers have criticized the approach because it relies on biased news sources and it might not be available for all candidates running for office Another type of biased searches that we identified as part of the semantic/pragmatic labeling of bias are rumors or conspiracy theories that have the potential to lead to so-called data voids situations in which the search engine only shows results from low-quality information sources because such rumors are not covered from reliable news sources As discussed by Golebiewski and boyd users are often nudged to search for certain phrases eg caravan immigration based crime voting fraud voter purge etc by trails left on other media Twitter talk show radio YouTube from different political actors with varying agendas Since such rumors often related to current events go viral unexpectedly eg maga bomber or anti trumper shoots up synagogue identifying them when auditing search results is a challenging task However to the extent that they might influence elections it is a topic that we believe needs further attention by the research community Implications for Voter-Centered Audits Here are some important takeaways to consider when designing voter-centered audits in the context of political elections Acknowledge Bias Voters perform biased searches but their expression of bias doesnt match that of established politicians Fortunately most of their biased searches are semantically expressed and can be discovered automatically through NLP techniques Meanwhile pragmatically biased phrases are difficult to recognize and interpret and may occasionally lead to data voids Finding ways to gather/discover such queries and audit their results in a timely fashion should be an important area for future work especially in the context of fighting political disinformation by bad-faith actors Candidates When voters search for specific candidates they dont simply use the candidates names Instead they formulate specific questions that use the names in context For some groups of candidates eg women there are common pieces of information being asked which might reveal greater bias toward them Its thus worth considering auditing for groups of candidates in case they are target of disinformation that might be visible only when results are compared to other groups of candidates Cues Voters are formulating searches to lead them to information cues In addition to well-established cues such as partisanship endorsements and polls a new set of cues stances on specific issues not clearly aligned with partisanship are emerging This need for quick access to such cues raises the issue of the authority and the political interests of sources that are providing the answers on Google Methods for assessing who is a trustworthy source in such contexts need to be established Localization Virginia is the th most populated state in the US with million inhabitants Their election searches might have been easy to pick up by Googles algorithms But how do these algorithms behave in other parts of the country or other countries in the world on the days ahead of the election Large-scale audits that target diverse geographical areas following the method in are needed to ensure that voters in these areas have access to reliable information as well CONCLUSION Search engines are one of the most used platforms for accessing information about elections Our exploratory qualitative analysis of four datasets related to elections in the United States indicated that  of voters formulate queries to elicit information cues about elections shortcuts to information that helps them decide Similarly voters also perform biased searches as well as problematic ones It is thus important that future search engine audits go beyond identifying whether their ranking algorithms are biased but instead take a broader ecosystem approach This means that audits should specifically target the quality of information in response to varied queries tested in different geographical locations in order to detect and measure possible pollution in the informational ecosystem which often is the result of deliberate disinformation by bad-faith actors who are engaged in information warfare