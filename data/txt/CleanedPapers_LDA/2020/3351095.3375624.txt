Explainable Machine Learning in Deployment Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores counterfactual explanations or influential training data Yet there is little understanding of how organizations use these methods in practice This study explores how organizations view and use explainability for stakeholder consumption We find that currently the majority of deployments are not for end users affected by the model but rather for machine learning engineers who use explainability to debug the model itself There is thus a gap between explainability in practice and the goal of transparency since explanations primarily serve internal stakeholders rather than external ones Our study synthesizes the limitations of current explainability techniques that hamper their use for end users To facilitate end user interaction we develop a framework for establishing clear goals for explainability We end by discussing concerns raised regarding explainability CONCEPTS Human-centered computing Computing methodologies Philosophical/theoretical foundations of artificial intelligence Machine learning KEYWORDS machine learning explainability transparency deployed systems qualitative study INTRODUCTION Machine learning ML models are being increasingly embedded into many aspects of daily life such as healthcare finance and social media To build ML models worthy of human trust researchers have proposed a variety of techniques for explaining ML models to stakeholders Deemed explainability this body of previous work attempts to illuminate the reasoning used by ML models Explainability loosely refers to any technique that helps the user or developer of ML models understand why models behave the way they do Explanations can come in many forms from telling patients which symptoms were indicative of a particular diagnosis to helping factory workers analyze inefficiencies in a production pipeline Explainability has been touted as away to enhance transparency of ML models Transparency includes a wide variety of efforts to provide stakeholders particularly end users with relevant information about how a model works One form of this would be to publish an algorithms code though this type of transparency would not provide an intelligible explanation to most users Another form would be to disclose properties of the training procedure and datasets used Users however are generally not equipped to be able to understand how raw data and code translate into benefits or harms that might affect them individually By providing an explanation for how the model made a decision explainability techniques seek to provide transparency directly targeted to human users often aiming to increase trustworthiness The importance of explainability as a concept has been reflected in legal and ethical guidelines for data and ML In cases of automated decision-making Articles of the European General Data Protection Regulation GDPR require that data subjects have access to meaningful information about the logic involved as well as the significance and the envisaged consequences of such processing for the data subject In addition technology companies have released artificial intelligence AI principles that include transparency as a core value including notions of explainability interpretability or intelligibility With growing interest in peering under the hood of ML models and in providing explanations to human users explainability has become an important subfield of ML Despite a burgeoning literature there has been little work characterizing how explanations have been deployed by organizations in the real world In this paper we explore how organizations have deployed local explainability techniques so that we can observe which techniques work best in practice report on the shortcomings of existing techniques and recommend paths for future research We focus specifically on local explainability techniques since these techniques explain individual predictions making them typically the most relevant form of model transparency for end users Our study synthesizes interviews with roughly fifty people from approximately thirty organizations to understand which explainability techniques are used and how We report trends from two sets of interviews and provide recommendations to organizations deploying explainability To the best of our knowledge we are the first to conduct a study of how explainability techniques are used by organizations that deploy ML models in their workflows Our main contributions are threefold We interview around twenty data scientists who are not currently using explainability tools to understand their organizations needs for explainability We interview around thirty different individuals on how their organizations have deployed explainability techniques reporting case studies and takeaways for each technique We suggest a framework for organizations to clarify their goals for deploying explainability The rest of this paper is organized as follows We discuss the methodology of our survey in Section We summarize our overall findings in Section We detail how local explainability techniques are used at various organizations and discuss technique-specific takeaways in Section We develop a framework for establishing clear goals when deploying local explainability in Section and discuss concerns of explainability in Section METHODOLOGY In the spirit of Holstein et al we study how industry practitioners look at and deploy explainable ML Specifically we study how particular organizations deploy explainability algorithms including who consumes the explanation and how it is evaluated for the intended stakeholder We conduct two sets of interviews Group consisted at how data scientists who are not currently using explainable machine learning hope to leverage various explainability tools while Group the crux of this paper consisted at how explainable machine learning has been deployed in practice For Group Fiddler Labs led a set of around twenty interviews to assess explainability needs across various organizations in the technology and financial services sectors We specifically focused on teams that do not currently employ explainability tools These semi-structured hour-long interviews included but were not limited to the following questions What are your ML use cases What is your current model development workflow What are your pain points in deploying ML models Would explainability help address those pain points Group spanned roughly thirty people across approximately twenty different organizations both for-profit and non-profit Most of these organizations are members of the Partnership on AI which is a global multistakeholder non-profit established to study and formulate best practices for AI to benefit society With each individual we held a thirty-minute to two-hour semi-structured interview to understand the state of explainability in their organization their motivation for using explanations and the benefits and shortcomings of the methods used Some organizations asked to stay anonymous not to be referred to explicitly in the prose or not to be included in the acknowledgements Of the people we spoke with in Group around one-third represented non-profit organizations academics civil society organizations and think tanks while the rest worked for for-profit organizations corporations industrial research labs and start-ups Broken down by organization around half were for-profit and half were academic non-profit Around one-third of the interviewees were executives at their organization around half were research scientists or engineers and the remainder were professors at academic institutions who commented on the consulting they had done with industry leaders to commercialize their research The questions we asked Group included but were not limited to the following Does your organization use ML model explanations What type of explanations have you used eg feature-based sample-based counterfactual or natural language Who is the audience for the model explanation eg research scientists product managers domain experts or users In what context have you deployed the explanations eg informing the development process informing human decisionmakers about the model or informing the end user on how actions were taken based on the models output How does your organization decide when and where to use model explanations SUMMARY OF FINDINGS Here we synthesize the results from both interview groups For the sake of clarity we define various terms based on the context in which they appear in the forthcoming prose Trustworthiness refers to the extent to which stakeholders can reasonably trust a models outputs Transparency refers to attempts to provide stakeholders particularly external stakeholders with relevant information about how the model works this includes documentation of the training procedure analysis of training data distribution code releases feature-level explanations etc Explainability refers to attempts to provide insights into a models behavior Stakeholders are the people who either want a model to be explainable will consume the model explanation or are affected by decisions made based on model output Practice refers to the real-world context in which the model has been deployed Local Explainability aims to explain the models behavior for a specific input Global Explainability attempts to understand the high-level concepts and reasoning used by a model Explainability Needs This subsection provides an overview of explainability needs that were uncovered with Group data scientists from organizations that do not currently deploy explainability techniques These data scientists were asked to describe their pain points in building and deploying ML models and how they hope to use explainability Model debugging Most data scientists struggle with debugging poor model performance They wish to identify why the model performs poorly on certain inputs and also to identify regions of the input space with below average performance In addition they seek guidance on how to engineer new features drop redundant features and gather more data to improve model performance For instance one data scientist said If I have features maybe its equally effective if I just have features Dealing with feature interactions was also a concern as the data scientist continued Feature A will impact feature B since feature A might negatively affect feature B how do I attribute importance in the presence of correlations Others mentioned explainability as a debugging solution helping to narrow down where things are broken Model monitoring Several individuals worry about drift in the feature and prediction distributions after deployment Ideally they would like to be alerted when there is a significant drift relative to the training distribution One organization would like explanations for how drift in feature distributions would impact model outcomes and feature contribution to the model We can compute how much each feature is drifting but we want to cross-reference this with which features are impacting the model a lot Model transparency Organizations that deploy models to make decisions that directly affect end users seek explanations for model predictions The explanations are meant to increase model transparency and comply with current or forthcoming regulations In general data scientists believe that explanations can also help communicate predictions to a broader external audience of other business teams and customers One company stressed the need to show your work to provide reasons on underwriting decisions to customers and another company needed explanations to respond to customer complaints Model audit In financial organizations due to regulatory requirements all deployed ML models must go through an internal audit Data scientists building these models need to have them reviewed by internal risk and legal teams One of the goals of the model audit is to conduct various kinds of tests provided by regulations like SR An effective model validation framework should include evaluation of conceptual soundness of the model ongoing monitoring including benchmarking and outcomes analysis including back-testing Explainability is viewed as a tool for evaluating the soundness of the model on various data points Financial institutions would like to conduct sensitivity analyses checking the impact of small changes to inputs on model outputs Unexpectedly large changes in outputs can indicate an unstable model Explainability Usage In Table we aggregate some of the explainability use cases that we received from different organizations in Group For each use case we define the domain of use ie the industry in which the model is deployed the purpose of the model the explainability technique used the stakeholder consuming the explanation and how the explanation is evaluated Evaluation criteria denote how the organization compares the success of various explanation functions for the chosen technique eg after selecting feature importance as the technique an organization can compare LIME and SHAP explanations via the faithfulness criterion In our study feature importance was the most common explainability technique and Shapley values were the most common type of feature importance explanation The most common stakeholders were ML engineers or research scientists followed by domain experts eg loan officers and content moderators Section provides definitions for each technique and further details on how these techniques were used at Group organizations Stakeholders Most organizations in Group deploy explainability atop their existing ML workflow for one of the following stakeholders Executives These individuals deem explainability necessary to achieve an organizations AI principles One research scientist felt that explainability was strongly advised and marketed by higher-ups though sometimes explainability simply became a checkbox ML Engineers These individuals including data scientists and researchers train ML models at their organization and use explainability techniques to understand how the trained model works do the most important features most similar samples and nearest training points in the opposite class make sense Using explainability to debug what the model has learned this group of individuals were the most common explanation consumers in our study End Users This is the most intuitive consumer of an explanation The person consuming the output of an ML model or making a decision based on the model output is the end user Explainability shows the end user why the model behaved the way it did which is important for showing that the model is trustworthy and also providing greater transparency Other Stakeholders There are many other possible stakeholders for explainability One such group is regulators who may mandate that certain algorithmic decision-making systems provide explanations to affected populations or the regulators themselves It is important that this group understands how explanations are deployed based on existing research what techniques are feasible and how the techniques can align with the desired explanation from a model Another group is domain experts who are often tasked with auditing the models behavior and ensuring it aligns with expert intuition For many organizations minimizing the divergence between the experts intuition and the explanation used by the model is key to successfully implementing explainability Overwhelmingly we found that local explainability techniques are mostly consumed by ML engineers and data scientists to audit models before deployment rather than to provide explanations to end users Our interviews reveal factors that prevent organizations from showing explanations to end users or those affected by decisions made from ML model outputs Key Takeaways This subsection summarizes some key takeaways from Group that shed light on the reasons for the limited deployment of explainability techniques and their use primarily as sanity checks for ML engineers Organizations generally still consider the judgments of domain experts to be the implicit ground truth for explanations Since explanations produced by current techniques often Domain Model Purpose Explainability Technique Stakeholders Evaluation Criteria Finance Loan Repayment Feature Importance Loan Officers Completeness Insurance Risk Assessment Feature Importance Risk Analysts Completeness Content Moderation Malicious Reviews Feature Importance Content Moderators Completeness Finance Cash Distribution Feature Importance ML Engineers Sensitivity Facial Recognition Smile Detection Feature Importance ML Engineers Faithfulness Content Moderation Sentiment Analysis Feature Importance QA ML Engineers norm Healthcare Medicare access Counterfactual Explanations ML Engineers normalized norm Content Moderation Object Detection Adversarial Perturbation QA ML Engineers norm Table Summary of select deployed local explainability use cases deviate from the understanding of domain experts some organizations still use human experts to evaluate the explanation before it is presented to users Part of this deviation stems from the potential for ML explanations to reflect spurious correlations which result from models detecting patterns in the data that lack causal underpinnings As a result organizations find explainability techniques useful for helping their ML engineers identify and reconcile inconsistencies between the models explanations and their intuition or that of domain experts rather than for directly providing explanations to end users In addition there are technical limitations that make it difficult for organizations to show end users explanations in real-time The non-convexity of certain models make certain explanations eg providing the most influential datapoints hard to compute quickly Moreover finding plausible counterfactual datapoints that are feasible in the real world and on the input data manifold is nontrivial and many existing techniques currently make crude approximations or return the closest datapoint of the other class in the training set Moreover providing certain explanations can raise privacy concerns due to the risk of model inversion More broadly organizations lack frameworks for deciding why they want an explanation and current research fails to capture the objective of an explanation For example large gradients representing the direction of maximal variation with respect to the output manifold do not necessarily explain anything to stakeholders At best gradient-based explanations provide an interpretation of how the model behaves upon an infinitesimal perturbation not necessarily a feasible one but does not explain if the model captures the underlying causal mechanism from the data DEPLOYING LOCAL EXPLAINABILITY In this section we dive into how local explainability techniques are used at various organizations Group After reviewing technical notation we define local explainability techniques discuss organizations use cases and then report takeaways for each technique Preliminaries A black box model maps an input x X Rd to an output x Y Rd Y When we assume has a parametric form we write Lf denotes the loss function used to train on a dataset D of input-output pairs x Each organization we spoke with has deployed an ML model They hope to explain a data point x using an explanation function Local explainability refers to an explanation for why predicted x for a fixed point x The local explanation methods we discuss come in one of the following forms Which feature xi of x was most important for prediction with Which training datapoint z D was most important to x What is the minimal change to the input x required to change the output x In this paper we deliberately decide to focus on the more popularly deployed local explainability techniques instead of global explainability techniques Global explainability refers to techniques that attempt to explain the model as a whole These techniques attempt to characterize the concepts learned by the model simpler models learned from the representation of complex models prototypical samples from a particular model output or the topology of the data itself None of our interviewees reported deploying global explainability techniques though some studied these techniques in research settings Feature Importance Feature importance was by far the most popular technique we found across our study It is used across many different domains finance healthcare facial recognition and content moderation Also known as feature-level interpretations feature attributions or saliency maps this method is by far the most widely used and most well-studied explainability technique Formulation Feature importance methods define an explanation function Rd R d that takes in a model and a point of interest x and returns importance scores x Rd for all features is the importance of or attribution for feature xi of x These explanation functions roughly fall into two categories perturbation-based techniques and gradient-based techniques Note that gradient-based techniques can be seen as a special case of a perturbation-based technique with an infinitesimal perturbation size Heatmaps are also a type of feature-level explanation that denote the importance of a region or collection of features A prominent class of perturbation based methods is based on Shapley values from cooperative game theory Shapley values are a way to distribute the gains from a cooperative game to its players In applying the method to explaining a model prediction a cooperative game is defined between the features with the model prediction as the gain The highlight of Shapley values is that they enjoy axiomatic uniqueness guarantees Unfortunately calculating the exact Shapley value is exponential in d input dimensionality however the literature has proposed approximate methods using weighted linear regression Monte Carlo approximation centroid aggregation and graph-structured factorization When we refer to Shapley-related methods hereafter we mean such approximate methods Shapley Values in Practice Organization A works with financial institutions and helps explain models for credit risk analysis To integrate into the existing ML workflow of these institutions Organization A proceeds as follows They let data scientists train a model to the desired accuracy Note that Organization A focuses mostly on models trained on tabular data though they are beginning to venture into unstructured data ie language and images During model validation risk analysts conduct stress tests before deploying the model to loan officers and other decisionmakers After decision-makers vet the model outputs as a sanity check and decide whether or not to override the model output Organization A generates Shapley value explanations Before launching the model risk analysts are asked to review the Shapley value explanations to ensure that the model exhibits expected behavior ie the model uses the same features that a human would for the same task Notably the customer support team at these institutions can also use these explanations to provide individuals information about what went into the decision-making process for their loan approval or cash distribution decision They are shown the percentage contribution to the model output the positive norm of the Shapley value explanation along with the sign of contribution This means that the explanation would be along the lines of of the decision was decided by your age which positively correlated with the predicted outcome When comparing Shapley value explanations to other popular feature importance techniques Organization A found that in practice LIME explanations give unexpected explanations that do not align with human intuition Recent work shows that the fragility of LIME explanations can be traced to the sampling variance when explaining a singular data point and to the explanation sensitivity to sample size and sampling proximity Though decision-makers have access to the feature-importance explanations end users are still not shown these explanations as reasoning for model output Organization A aspires to eventually provide this explanation to end users For gradient-based language models Organization A uses Integrated Gradients related to Shapley Values by Sundararajan et al to flag malicious reviews and moderate content at the aforementioned institutions This information can be highlighted to ensure the trustworthiness and transparency of the model to the decision maker the hired content moderator here since they can now see which word was most important to flag the content Going forward Organization A intends to use a global variant of the Shapley value explanations by exposing how Shapley value explanations work on average for datapoints of a particular predicted class eg on average someone who was denied a loan had their age matter most for the prediction This global explanation would help risk analysts get a birds-eye view of how a model behaves and whether it aligns with their expectations Heatmaps in Transportation Organization B looks to detect facial expressions from video feeds of users driving They hope to use explainability to identify the actions a user is performing while the user drives Organization B has tried feature visualization and activation visualization techniques that get attributions by back-propagating gradients to regions of interest Specifically they use these probabilistic Winner-Take-All techniques variants of existing gradient-based feature importance techniques to localize the region of importance in the input space for a particular classification task For example when detecting a smile they expect the mouth of the driver to be important Though none of these desired techniques have been deployed for the end user the driver in this case ML engineers at Organization B found these techniques useful for qualitative review On tiny datasets engineers can figure out which scenarios have false positives videos falsely detected to contain smiles and why They can also identify if true positives are paying attention to the right place or if there is a problem with spurious artifacts However while trying to understand why the model erred by analyzing similarities in false positives they have struggled to scale this local technique across heatmaps in aggregate across multiple videos They are able to qualitatively evaluate a sequence of heatmaps for one video but doing so across M frames simultaneously is far more difficult Paraphrasing the VP of AI at Organization B aggregating saliency maps across videos is moot and contains little information Note that an individual heatmap is an example of a local explainability technique but an aggregate heatmap for M frames would be a global technique Unlike aggregating Shapley values for tabular data as done at Organization A taking an expectation over heatmaps in the statistical sense does not work since aggregating pixel attributions is meaningless One option Organization B discussed would be to clustering low dimensional representations of the heatmaps and then tagging each cluster based on what the model is focusing on unfortunately humans would still have to manually label the clusters of important regions Spurious Correlations Related to model monitoring for feature drift detection discussed in Section Organization B has encountered issues with spurious correlations in their smile detection models Their Vice President of AI noted that ML engineers must know to what extent you want ML to leverage highly correlated data to make classifications Explainability can help identify models that focus on that correlation and can find ways to have models ignore it For example there may be a side effect of a correlated facial expression or co-occurrence cheek raising for example co-occurs with smiling In a cheek-raise detector trained on the same dataset as a smile detector but with different labels the model still focused on the mouth instead of the cheeks Both models were fixated on a prevalent co-occurrence Attending to the mouth was undesirable in the cheek-raise detector but allowed in the smile detector One way Organization B combats this is by using simpler models on top of complex feature engineering For example they use black box deep learning models for building good descriptors that are robust across camera viewpoints and will detect different features that subject matter experts deem important for drowsiness There is one model per important descriptor ie one model for eyes closed one for yawns etc Then they fit a simple model on the extracted descriptors such that the important descriptors are obvious for the final prediction of drowsiness Ideally if Organization B had guarantees about the disentanglement of data generating factors they would be able to understand which factors descriptors play a role in downstream classification Feature Importance Takeaways Shapley values are rigorously motivated and approximate methods are simple to deploy for decision makers to sanity check the models they have built Feature importance is not shown to end users but is used by machine learning engineers as a sanity check Looping other stakeholders who make decisions based on model outputs into the model development process is essential to understanding what type of explanations the model delivers Heatmaps and feature importance scores in general are hard to aggregate which makes it hard to do false positive detection at scale Spurious correlations can be detected with simple gradient-based techniques Counterfactual Explanations Counterfactual explanations are techniques that explain individual predictions by providing a means for recourse Contrastive explanations that highlight contextually relevant information to the model output are most similar to human explanations however in their current form finding the relevant set of plausible counterfactual point is no clear Moreover while some existing open source implementations for counterfactual explanations exist they either work for specific model-types or are not black-box in nature In this section we discuss the formulation for counterfactual explanations and describe one solution for each deployed technique Formulation Counterfactual explanations are points close to the input for which the decision of the classifier changes For example for a person who was rejected for a loan by a ML model a counterfactual explanation would possibly suggest Had your income been greater by the loan would have been granted Given an input x a classifier and a distance metric d we find a counterfactual explanation c by solving the optimization problem min c x c The method can be tailored to allow only certain relevant features to be changed Note that the term counterfactual has a different meaning in the causality literature Counterfactual explanations for ML were introduced by Wachter et al Sharma et al provide details on existing techniques Counterfactual Explanations in Healthcare Organization C uses a faster version of the formulation in Sharma et al to find counterfactual explanations for projects in healthcare When people apply for Medicare Organization C hopes to flag if a users application has errors and to provide explanations on how to correct the errors Moreover ML engineers can use the robustness score to compare different models trained using this data this robustness score is effectively a suitably normalized and averaged distance between the counterfactual and original point in Euclidean space The original formulation makes use of a slower genetic algorithm so they optimized the counterfactual explanation generation process They are currently developing a first-of-its-kind application that can directly take in any black-box model and data and return a robustness score fairness measure and counterfactual explanations all from a single underlying algorithm The use of this approach has several advantages it can be applied to black-box models works for any input data type and generates multiple explanations in a single run of the algorithm However there are some shortcomings that Organization C is addressing One challenge of counterfactual models is that the counterfactual might not be feasible Organization C addresses this by using the training data to guide the counterfactual generation process and by providing a user interface that allows domain experts to specify constraints In addition the flexibility of the counterfactual approach comes with a drawback that is common among explanations for black-box models there is no guarantee of the optimality of the explanation since black-box techniques cannot guarantee optimality Through the creation of a deployed solution for this method the organization realized that clients would ideally want an explainability score along with a measure of fairness and robustness as such they have developed an explainability score that can be used to compare the explainability of different models Counterfactual Explanations Takeaways Organizations are interested in counterfactual explanation solutions since the underlying method is flexible and such explanations are easy for end users to understand It is not clear exactly what should be optimized for when generating a counterfactual or how to do it efficiently Still approximate solutions may suffice in practical applications Adversarial Training In order to ensure the model being deployed is robust to adversaries and behaves as intended many organizations we interviewed use adversarial training to improve performance It has recently been shown that in fact this also can lead to more human interpretable features Formulation Other works have also explored the intersection between adversarial robustness and model interpretations The claim of one of these works is that the closest adversarial example should perturb fragile features enabling the model to fit to robust features indicative of a particular class The setup of feature importance in the adversarial training setting from Singla et al is as follows x max x L x x x x We let be the top-k feature importance scores of the input x This is similar to the adversarial example setup which is usually written in the same manner as the above without the norm to limit the number of features that changed It is also interesting to note that the formulation to find counterfactual explanations above matches the formulation for finding adversarial examples Sharma et al use this connection to generate adversarial examples and define a black-box model robustness score Image Content Moderation Organization D moderates user-generated content UGC on several public platforms Specifically the RD team at Organization D developed several models to detect adult and violent content from users uploaded images Their quality assurance QA team measures model robustness to improve content detection accuracy under the threat of adversarial examples The robustness of a content moderation model is measured by the minimum perturbations required for an image to evade detection Given a gradient-based image classification model Rd and we assume x argmax i Z xi where Z x R is the final logit layer output xi is the prediction score for the i-th class The objective can be formulated as the following optimization problem to find the minimum perturbation argmin x d d is some distance measure that Organization D chooses to be the distance in Euclidean space L is the cross-entropy loss function and c is a balancing factor As is common in the adversarial literature Organization D applies Projected Gradient Descent to search for the minimum perturbation from the set of allowable perturbations S Rd The search process can be formulated as until is misclassified by the detection model ML engineers on the QA team are shown a â„“-norm perturbation distance averaged over n test images randomly sampled from the test dataset The larger the average perturbation the more robust the model is as it takes greater effort for an attacker to evade detection The average perturbation required is also widely used as a metric when comparing different candidate models and different versions of a given model Organization D finds that more robust models have more convincing gradient-based explanations ie the gradient of the output with respect to the input shows that the model is focusing on relevant portions of the images confirming recent research Text Content Moderation Organization E uses text content moderation algorithms on its UGC platforms such as forums Its QA team is responsible for the reliability and robustness of a sentiment analysis model which labels posts as positive or negative trained on UGC The QA team seeks to find the minimum perturbation required to change the classification of a post In particular they want to know how to take misclassified posts eg negative ones classified as positive and change them to the correct class Given a sentiment analysis model X Y which maps from feature space X to a set of classY an adversary aims to generate an adversarial post from the original post x X whose ground truth label is x y Y so that y The QA team tries to minimize for a domain-specific distance function Organization E uses the distance in the embedding space but it is equally valid to use the editing distance Note that perturbation technique changes accordingly In practice to find the minimum distance in embedding space Organization E chooses to iteratively modify the words in the original post starting from the words with the highest importance Here importance is defined as the gradient of the model output with respect to a particular word ML engineers compute the Jacobian matrix of the given posts x x x where xi is the i-th word The Jacobian matrix is as follows Jf x x x x xi i N where represents the number of classes in this case and represents the confidence value of the class The importance of word xi is defined as Jf i y y x xi ie the partial derivative of the confidence value based on the predicted classy regarding to the input word xi This procedure ranks the words by their impact on the sentiment analysis results The QA team then applies a set of transformations/perturbations to the most important words to find the minimum number of important words that must be perturbed in order to flip an sentiment analysis API result Adversarial Training Takeaways There is a relation between model robustness and explainability Model robustness improves the quality of feature importances specifically saliency maps confirming recent research findings Feature importance helps find minimal adversarial perturbations for language models in practice Influential Samples This technique asks the question Which data point in the training dataset x is most influential to the models output x test for a test point x test Statisticians have used measures like Cooks distance which measure the effect of deleting a data point on the model output However such measures require an exhaustive search and hence do not scale well for larger datasets Formulation For over half of the organizations influence functions has been the tool of choice for explaining which training points are influential to the models output for a point x though only one organization actually deployed the technique We let Lf x be the models loss for point x The empirical risk minimizer is given by argmin N i Lf i Note that x is the predicted output at x with the trained risk minimizer and Liang define the most influential data point z to a fixed point x as that which maximizes the following loss This quantity measures the effect of upweighting on datapoint z on the loss The goal of sample importance is to uncover which training examples when perturbed would have the largest effect positive or negative on the loss of a test point Influence Functions in Insurance Organization uses influence functions to explain risk models in the insurance industry They hope to identify which customers might see an increase in their premiums based on their driving history in the past The organization hopes to divulge to the end user how the premiums for drivers similar to them are priced In other words they hope to identify the influential training data points to understand which past drivers had the greatest influence on the prediction for the observed driver Unfortunately Organization has struggled to provide this information to end users since the Hessian computation has made doing so impractical since the latency is high More pressingly even when Organization lets the influence function procedure run they find that many influential data points are simply outliers that are important for all drivers since those anomalous drivers are far out of distribution As a result instead of identifying which drivers are most similar to a given driver the influential sample explanation identifies drivers that are very different from any driver ie outliers While this is could in theory be useful for outlier detection it prevents the explanations from being used in practice Influential Samples Takeaways Influence functions can be intractable for large datasets as such a significant effort is needed to improve these methods to make them easy to deploy in practice Influence functions can be sensitive to outliers in the data such that they might be more useful for outlier detection than for providing end users explanations RECOMMENDATIONS This section provides recommendations for organizations based on the key takeaways in Section and the technique-specific takeaways in Section In order to address the challenges organizations face when striving to provide explanations to end users we recommend a framework for establishing clear desiderata in explainability and then include concerns associated with explainability Establish Clear Desiderata Most organizations we spoke to solely deploy explainability techniques for internal engineers and scientists as a debugging mechanism or as a sanity check At the same time these organizations also affirmed the importance of understanding the stakeholder and hope to be able to explain a model prediction to the end user Once the target population of the explanation is understood organizations can attempt to devise and deploy explainability techniques accordingly We propose the following three steps for establishing clear desiderata and improving decision making around explainability These include clearly identifying the target population understanding their needs and clarifying the intention of the explanation Identify stakeholders Who are your desired explanation consumers Typically this will be those affected by or shown model outputs Preece et al describe how stakeholders have different needs for explainability Distinctions between these groups can help design better explanation techniques Engage with each stakeholder Ask the stakeholder some variant of What would you need the model to explain to you in order to understand trust or contest the model prediction and What type of explanation do you want from your model Doshi-Velez and Kim highlight how the task being modeled dictates what type of explanation the human will need from the model Understand the purpose of the explanation Once the context and helpfulness of the explanation are established understand what the stakeholder wants to do with the explanation Static Consumption Will the explanation be used as a one-off sanity check for some stakeholders or shown to other stakeholders as reasoning for a particular prediction Dynamic Model Updates Will the explanation be used to garner feedback from the stakeholder as to how the model ought to be updated to better align with their intuition That is how does the stakeholder interact with the model after viewing the explanation Ross et al attempt to develop a technique for dynamic explanations wherein the human can guide the model towards learning the correct explanation Once the desiderata are clarified stakeholders should be consulted again Concerns of Explainability While there are many positive reasons to encourage explainability of machine learning models we note come concerns which were raised in our interviews On Causality One chief scientist told us that Figuring out causal factors is the holy grail of explainability However causal explanations are largely lacking in the literature with a few exceptions Though non-causal explanations can still provide valid and useful interpretations of how the model works many organizations said that they would be keen to use causal explanations if they were available On Privacy Three organizations mentioned data privacy in the context of explainability since in some cases explanations can be used to learn about the model or the training data Methods to counter these concerns have been developed For example Harder et al develop a methodology for training a differentially private model that generates local and global explanations using locally linear maps On Improving Performance One purpose of explanations is to improve ML engineers understanding of their models in order to help them refine and improve performance Since machine learning models are dual use we should be aware that in some settings explanations or other tools could enable malicious users to increase capabilities and performance of undesirable systems For example several organizations we talked with use explanation methods to improve their natural language processing and image recognition models for content moderation in ways that may concern some stakeholders Beyond Deep Learning Though deep learning has gained popularity in recent years many organizations still use classical ML techniques eg logistic regression support vector machines likely due to a need for simpler more interpretable models Many in the explainability community have focused on interpreting black-box deep learning models even though practitioners feel that there is a dearth of model-specific techniques to understand traditional ML models For example one research scientist noted that Many financial institutions use kernel-based methods on tabular data As a result there is a desire to translate explainability techniques for kernel support vector machines in genomics to models trained on tabular data Model agnostic techniques like Lundberg and Lee can be used for traditional models but are likely overkill for explaining kernel-based ML models according to one research scientist since model-agnostic methods can be computationally expensive and lead to poorly approximated explanations CONCLUSION In this study we critically examine how explanation techniques are used in practice We are the first to our knowledge to interview various organizations on how they deploy explainability in their ML workflows concluding with salient directions for future research We found that while ML engineers are increasingly using explainability techniques as sanity checks during the development process there are still significant limitations to current techniques that prevent their use to directly inform end users These limitations include the need for domain experts to evaluate explanations the risk of spurious correlations reflected in model explanations the lack of causal intuition and the latency in computing and showing explanations in real-time Future research should seek to address these limitations We also highlighted the need for organizations to establish clear desiderata for their explanation techniques and to be cognizant of the concerns associated with explainability Through this analysis we take a step towards describing explainability deployment and hope that future research builds trustworthy explainability solutions