There has been rapidly growing interest in the use of algorithms in hiring especially as a means to address or mitigate bias Yet to date little is known about how these methods are used in practice How are algorithmic assessments built validated and examined for bias In this work we document and analyze the claims and practices of companies offering algorithms for employment assessment In particular we identify vendors of algorithmic pre-employment assessments ie algorithms to screen candidates document what they have disclosed about their development and validation procedures and evaluate their practices focusing particularly on efforts to detect and mitigate bias Our analysis considers both technical and legal perspectives Technically we consider the various choices vendors make regarding data collection and prediction targets and explore the risks and trade-offs that these choices pose We also discuss how algorithmic de-biasing techniques interface with and create challenges for antidiscrimination law CONCEPTS Social and professional topics Employment issues Computing methodologies Machine learning Applied computing Law KEYWORDS algorithmic hiring discrimination law algorithmic bias INTRODUCTION The study of algorithmic bias and fairness in machine learning has quickly matured into a field of study in its own right delivering a wide range of formal definitions and quantitative metrics As industry takes up these tools and accompanying terminology promises of eliminating algorithmic bias using computational methods have begun to proliferate In some cases however rather than forcing precision and specificity the existence of formal definitions and metrics has had the paradoxical result of giving undue credence to vague claims about de-biasing and fairness In this work we use algorithmic pre-employment assessment as a case study to show how formal definitions of fairness allow us to ask focused questions about the meaning of fair and unbiased models The hiring domain makes for an effective case study because of both its prevalence and its long history of bias We know from decades of audit studies that employers tend to discriminate against women and ethnic minorities and a recent meta-analysis suggests that little has improved over the past years Citing evidence that algorithms may help reduce human biases advocates argue for the adoption of algorithmic techniques in hiring with a variety of computational metrics proposed to identify and prevent unfair behavior But to date little is known about how these methods are used in practice One of the biggest obstacles to empirically characterizing industry practices is the lack of publicly available information Much technical work has focused on using computational notions of equity and fairness to evaluate specific models or datasets Indeed when these models are available we can and should investigate them to identify potential problems But what do we do when we have little or no access to models or the data they produce Certain models may be completely inaccessible to the public whether for practical or legal reasons and attempts to audit these models by examining their training data or outputs might jeopardize users privacy With algorithmic pre-employment assessments we find that this is very much the case models much less the sensitive employee data used to construct them are in general kept private As such the only information we can consistently glean about industry practices is limited to what companies publicly disclose Despite this one of the key findings of our work is that even without access to models or data we can still learn a considerable amount by investigating what corporations disclose about their practices for developing validating and removing bias from these tools Documenting claims and evaluating practices Following a review of firms offering recruitment technologies we identify vendors of pre-employment assessments We document what each company has disclosed about its practices and consider the implications of these claims In so doing we develop an understanding of industry attempts to mitigate bias and what critical issues are unaddressed Prior work has sought to taxonomize the points at which bias can enter machine learning systems noting that the choice of target variable or outcome to predict the training data used and labelling of examples are all potential sources of disparities Following these frameworks we seek to understand how practitioners handle these key decisions in the machine learning pipeline In particular we surface choices and trade-offs vendors face with regard to the collection of data the ability to validate on representative populations and the effects of discrimination law on efforts to prevent bias The heterogeneity we observe in vendors practices indicates evolving industry norms that are sensitive to concerns of bias but lack clear guidance on how to respond to these worries Of course analyzing publicly available information has its limitations We are unable for example to identify issues that any particular model might raise in practice Nor can we be sure that vendors arent doing more behind the scenes to ensure that their models are non-discriminatory And while other publicly accessible information eg news articles and videos from conferences might offer further details about vendors practices for the sake of consistent comparison we limit ourselves to statements on vendors websites As such our analysis should not be viewed as exhaustive however as we will see it is still possible to draw meaningful conclusions and characterize industry trends through our methods One notable limitation we encounter is the lack of information about the validity of these assessments It is of paramount importance to know the extent to which these tools actually work but we cannot do so without additional transparency from vendors We stress that our analysis is not intended as an exposé of industry practices Many of the vendors we study exist precisely because they seek to provide a fairer alternative to traditional hiring practices Our hope is that this work will paint a realistic picture of the landscape of algorithmic techniques in pre-employment assessment and offer recommendations for their effective and appropriate use Organization of the rest of the paper Section contains an overview of pre-employment assessments their history and relevant legal precedents In Section we systematically review vendors of algorithmic screening tools and empirically characterize their practices based on the claims that they make We analyze these practices in detail in Sections and from technical and legal perspectives examining ambiguities and particular causes for concern We provide concluding thoughts and recommendations in Section BACKGROUND Pre-employment assessments in the hiring pipeline Hiring decisions are among the most consequential that individuals face determining key aspects of their lives including where they live and how much they earn These decisions are similarly impactful for employers who face significant financial pressure to make high-quality hires quickly and efficiently As a result many employers seek tools with which to optimize their hiring processes Broadly speaking there are four distinct stages of the hiring pipeline though the boundaries between them are not always rigid sourcing screening interviewing and selection Sourcing consists of building a candidate pool which is then screened to choose a subset to interview Finally after candidates are interviewed selected candidates receive offers We will focus on screening and in particular pre-employment assessments that algorithmically evaluate candidates This includes for example questionnaires and video interviews that are analyzed automatically Prior work has considered the rise of algorithmic tools in the context of hiring highlighting the concerns that they raise for fairness Bogen and Rieke provide an overview of the various ways in which algorithms are being introduced into this pipeline with a focus on their implications for equity Garr surveys a number of platforms designed to promote diversity and inclusion in hiring Sánchez-Monedero et al analyze some of the vendors considered here from the perspective of UK law addressing concerns over both discrimination and data protection Broadly considering the use of data science in HR-related activities Cappelli et al identify several practical challenges to the use of algorithmic systems in hiring and propose a framework to help address them Ajunwa provides a legal framework to consider the problems algorithmic tools introduce and argues against subjective targets like cultural fit Kim also raises legal concerns over the use of algorithms in hiring in both advertising and screening contexts Scholars in the field of Industrial-Organizational IO Psychology have also begun to grapple with the variety of new pre-employment assessment methods and sources of information enabled by algorithms and big data Chamorro-Prezumic et al find that academic research has been unable to keep pace with rapidly evolving technology allowing vendors to push the boundaries of assessments without rigorous independent research A report by the National Research Council summarizes a number of ethical issues that arise in pre-employment assessment including the role of human intervention the provision of feedback to candidates and the goal of hiring for fit especially in light of modern data sources And although proponents argue that pre-employment assessments can push back against human biases assessments especially data-driven algorithmic ones run the risk of codifying inequalities while providing a veneer of objectivity A history of equity concerns in assessment Pre-employment assessments date back to examinations for the Chinese civil service thousands of years ago In the early s the idea that assessments could reveal innate cognitive abilities gained traction in Western industrial and academic circles leading to the formation of Industrial Psychology as an academic discipline During the two World Wars the US government turned to these assessments in an attempt to quantify soldiers abilities paving the way for their widespread adoption in postwar industry Historically these assessments were primarily behavioral or cognitive in nature like the Stanford-Binet IQ test the Myers-Briggs type indicator and the Big Five personality traits IO Psychology remains a prominent component of these modern assessment tools many vendors we examine employ IO psychologists who work with data scientists to create and validate assessments Cognitive assessments have imposed adverse impacts on minority populations since their introduction into mainstream use Critics have long contended that observed group differences in test outcomes indicated flaws in the tests themselves and a growing consensus has formed around the idea that while assessments do have some predictive validity they often disadvantage minorities despite the fact that minority candidates have similar real-world job performance to their white counterparts Disparities in assessment outcomes for minority populations are not limited to preemployment assessments In the education literature the adverse impact of assessments The American Psychological Association APA recognizes these concerns as examples of predictive bias when an assessment systematically over- or under-predicts scores for a particular group in its Principles for the Validation and Use of Personnel Selection Procedures The APA Principles consider several potential definitions of fairness and while they encourage practitioners to identify and mitigate predictive bias they explicitly reject the view that fairness requires equal outcomes As we will see this focus on predictive bias over outcome-based definitions of fairness forms interesting connections and contrasts with US employment discrimination law A brief overview of US employment discrimination law Title VII of the Civil Rights Act of forms the basis of regulatory oversight regarding discrimination in employment It prohibits discrimination with respect to a number of protected attributes race color religion sex and national origin establishing the Equal Employment Opportunity Commission EEOC to ensure compliance The EEOC in turn issued the Uniform Guidelines on Employment Selection Procedures in to set standards for how employers can choose their employees According to the Uniform Guidelines the gold standard for pre-employment assessments is validity the outcome of a test should say something meaningful about a candidates potential as an employee The EEOC accepts three forms of evidence for validity criterion content and construct Criterion validity refers to predictive ability do test scores correlate with meaningful job outcomes eg sales numbers An assessment with content validity tests candidates in similar situations to ones that they will encounter on the job eg a coding interview Finally assessments demonstrate construct validity if they test for some fundamental characteristic eg grit or leadership required for good job performance When is an assessment legally considered discriminatory Based on existing precedent the Uniform Guidelines provide two avenues to challenge an assessment disparate treatment and disparate impact Disparate treatment is relatively straightforward it is illegal to explicitly treat candidates differently based on categories protected under Title VII Disparate impact is more nuanced and while we provide an overview of the process here we refer the reader to for a more complete discussion Under the Uniform Guidelines the rule of thumb to decide when a disparate impact case can be brought against an employer is the  rule if the selection rate for one protected group is less than  of that of the group with the highest selection rate the employer may be at risk If a significant disparity in selection rates is established an employer may defend itself by showing that its selection procedures are both valid and necessary from a business perspective Even when a business necessity has been established an employer can be held liable if the plaintiff can produce an alternative selection procedure with less adverse impact that the employer could have used instead with little business cost Ultimately both the APA Principles and the Uniform Guidelines on minorities is well-documented This has led to a decades-long line of literature seeking to measure and mitigate the observed disparities see for a survey It should be noted that this description is based on a particular although the most common interpretation of Title VII Some legal scholars contend that Title VII offers stronger protections to minorities and there is disagreement on how or whether to operationalize the  rule through statistical tests In this agree that validity is fundamental to a good assessment And while validity can be used as a defense against disparate selection rates we will see that the Uniform Guidelines emphasis on outcome disparities and the  rule significantly impacts vendors practices EMPIRICAL FINDINGS Methodology Identifying companies offering algorithmic pre-employment assessments In order to get a broad overview of the emerging industry surrounding algorithmic pre-employment assessments we conducted a systematic review of assessment vendors with Englishlanguage websites To identify relevant companies we consulted Crunchbases list of the top start-ups by funding amount under its recruiting category Crunchbase offers information on public and private companies providing details on funding and other investment activity While Crunchbase is not an exhaustive list of all companies working in an industry it is an often-used resource for tracking developments in start-up companies Companies can create profiles for themselves subject to validation We supplemented this list with an inventory of relevant companies found in recent reports by Upturn a technology research and advocacy firm focused on civil rights and RedThread Research a research and advisory firm specializing in new technologies for human resource management This resulted in additional companies for a combined total of There was substantial overlap between the three sources considered Thirty-nine of these companies did not have English-language websites so we excluded them Recall that the hiring pipeline has four primary stages sourcing screening interviewing and selection we ruled out vendors that do not provide assessment services at the screening stage leaving us with vendors Note that this excluded companies that merely provide online job boards or marketplaces like Monstercom and Upwork Twenty-two of the remaining vendors did not obviously use any predictive technology eg coding interview platforms that only evaluated correctness or rule-based screening or did not offer explicit assessments eg scraping candidate information from other sources and an additional did not provide enough information for us to make concrete determinations leaving us with vendors in our sample With these vendors in April we recorded administrative information available on Crunchbase approximate number of employees location and total funding and undertook a review of their claims and practices which we explain below Documenting vendors claims and practices Based on prior frameworks intended to interrogate machine learning pipelines for bias we ask the following questions of vendors work we will not consider alternative interpretations of Title VII nor will we get into the specifics of how exactly to detect violations of the  rule Many psychologists disagree with the specific conception of validity endorsed by the Uniform Guidelines however there is broad agreement that some form of validation is necessary Our empirical findings are specific to this moment in time practices and documentation may have changed since then What types of assessments do they provide eg questions video interviews or games Features What is the outcome or quality that these assessments aim to predict eg sales revenue annual review score or grit Target variable What data are used to develop the assessment eg the clients or the vendors own data Training data What information do they provide regarding validation processes eg validation studies or whitepapers Validation What claims or guarantees if any are made regarding bias or fairness When applicable how do they achieve these guarantees Fairness To answer these questions we exhaustively searched the websites of each company This included following all internal links downloading any reports or whitepapers they provided and watching webinars found on their websites Almost all vendors provided an option to request a demo we avoided doing so since our focus is on accessible and public information Sometimes company websites were quite sparse on information and we were unable to conclusively answer all questions for all vendors Findings In our review we found vendors providing algorithmically driven pre-employment assessments Those that had available funding information on Crunchbase out of ranged in funding from around million to million Most vendors had or fewer employees and half were based in the United States vendors were present in Crunchbases Recruiting Startups list the remaining vendors were taken from reports by Upturn and RedThread Research Many vendors were present in all of these sources Table summarizes our findings Table in Appendix A contains administrative information about the vendors we included Assessment types The types of assessments offered varied by vendor The most popular assessment types were questions vendors video interview analysis vendors and gameplay eg puzzles or video games vendors Note that many vendors offered multiple types of assessments Question-based assessments included personality tests situational judgment tests and other formats For video interviews candidates were typically either asked to record answers to particular questions or more free-form video resumes highlighting their strengths These videos are then algorithmically analyzed by vendors Target variables and training data Most of the vendors offer custom or customizable assessments adapting the assessment to the clients particular data or job requirements In practice decisions about target variables and training data are made together based on where the data come from Eight vendors build assessments based on data from the clients past and current employees see Figure Vendors in general leave it up to clients to determine what outcomes they want to predict including for example performance reviews sales numbers and retention time Other vendors who offer customizable assessments without using client data either use human expertise to determine which of a pre-determined set of competencies are most relevant to the particular job the vendors analysis of a job role or a clients knowledge of relevant requirements or dont explicitly specify their prediction targets In such cases the vendor provides an assessment that scores applicants on various competencies which are then combined into a fit score based on a custom formula Thus even among vendors who tailor their assessments to a client they do so in different ways Vendors who only offer pre-built assessments typically either provide assessments designed for a particular job role eg salesperson or provide a sort of competency report with scores on a number of cognitive or behavioral traits eg leadership grit teamwork These assessments are closer in spirit to traditional psychometric assessments like the Myers-Briggs Type Indicator or Big Five Personality Test however unlike traditional assessments that rely on a small number of questions modern assessments may build psychographic profiles using machine learning to analyze rich data sources like a video interview or gameplay Validation Generally vendors websites do not make clear whether vendors validate their models what validation methodologies they use how they select validation data or how validation procedures might be tailored to the particular client Good Co notably provides fairly rigorous validation studies of the psychometric component of their assessment as well as a detailed audit of how the scores differ across demographic groups however they do not provide similar documentation justifying the algorithmic techniques they use to recommend candidates based on culture fit Accounting for bias In total while of the vendors made at least abstract references to bias sometimes in the context of well-established human bias in hiring only vendors explicitly discussed compliance or adverse impact with respect to the assessments they offered Three vendors explicitly mentioned the  rule and an additional advertised compliance or claimed to control adverse impact more generally Several of these vendors claimed to test models for bias fixing it when it appeared HireVue and pymetrics in particular offered a detailed description of their overall approaches to de-biasing which involves removing features correlated with protected attributes when adverse impact is detected Other vendors eg Knockri and PredictiveHire claimed to fix adverse impact when it is found without going into further detail Among those that do make concrete claims all vendors we examined specifically focus on equality of outcomes and compliance with the  rule Roughly speaking there are two ways in which vendors claim to achieve these goals naturally unbiased assessments and active algorithmic de-biasing Typically vendors claiming to provide naturally unbiased assessments seek to measure underlying cognitive or behavioral traits so their assessments output a small number of scores one for each competency being measured In this setting a naturally unbiased assessment in one that produces similar score distributions across demographic groups Koru for instance measures traits eg grit and presence and claims that in all panels since the Pre-Hire assessment does not show bias against women or minority respondents Other vendors actively intervene in their learned models to remove biases One technique that we have observed across multiple vendors eg HireVue pymetrics PredictiveHire is the following Figure Description of the pymetrics process screenshot from the pymetrics website employers/ Vendor name Assessment types Features Custom Target Training data Validation info Validation Adverse impact Fairness and Above phone video S bias mentioned ActiView VR assessment C validation claimed bias mentioned Assessment Innovation games questions bias mentioned GoodCo questions C P multiple studies adverse impact Harver games questions S HireVue games questions video C P  rule impressai questions S Knockri video S bias mentioned Koru questions S some description adverse impact LaunchPad Recruits questions video bias mentioned myInterview video compliance Plumio questions games S validation claimed bias mentioned PredictiveHire questions C  rule pymetrics games C small case study  rule Scoutible games C Teamscope questions S P bias mentioned ThriveMap questions C bias mentioned Yobs video C S adverse impact Table Examining the websites of vendors of algorithmic pre-employment assessments we answer a number of questions regarding their assessments in relation to questions of fairness and bias This involves exhaustively searching their websites downloading whitepapers they provide and watching webinars they make available This table presents our findings The Assessment types column gives the types of assessments each vendor offers In the Custom column we consider the source of data used to build an assessment C denotes custom uses employer data S denotes semi-custom qualitatively tailored to employer without data and P denotes pre-built The Validation column contains information vendors publicly provided about their validation processes In the Adverse impact column we recorded phrases found on vendors websites addressing concerns over bias build a model and test it for adverse impact against various subgroups As Bogen and Rieke also observe if adverse impact is found the model and/or data are modified to try to remove it and then the model is tested again for adverse impact HireVue and pymetrics downweight or remove features found to be highly correlated with the protected attribute in question noting that this can significantly reduce adverse impact with little effect on the predictive accuracy of the assessment This is done prior to the models pymetrics for instance open-sources the tests it uses audit-ai deployment on actual applicants though some vendors claim to periodically test and update models In Section we discuss in depth these efforts to define and remove bias ANALYSIS OF TECHNICAL CONCERNS Our findings in Section raise several technical challenges for the pre-employment assessment process In this section we focus on two areas that are particularly salient in the context of algorithmic hiring data choices where vendors must decide where to draw data from and what outcomes to predict and the use of alternative Vendor Claim about bias HireVue Provide a highly valid bias-mitigated assessment pymetrics the Pre-Hire assessment does not show bias against women or minority respondents PredictiveHire AI bias is testable hence fixable Knockri Knockris AI is unbiased because of its full spectrum database that ensures theres no benchmark of what the ideal candidate looks like Table Examples of claims that vendors make about bias taken from their websites assessment formats like game- or video-based assessments that rely on larger feature sets and more complex machine learning tools than traditional question-based assessments Data Choices Machine learning is often viewed as a process by which we predict a given output from a given set of inputs In reality neither the inputs nor outputs are fixed Where do the data come from What is the right outcome to predict These and others are crucial decisions in the machine learning pipeline and can create opportunities for bias to enter the process Custom assessments Consider a hypothetical practitioner building a custom assessment to identify the best candidates for her client As is the case in many domains translating this to a feasible data-driven task forces our practitioner to make certain compromises It quickly becomes clear that she must operationalize best in some measurable way What does the client value Sales numbers Cultural fit Retention And crucially what data does the client have This is a nontrivial constraint many companies dont maintain comprehensive and accessible data about employee performance and thus a practitioner may be forced to do the best she can with the limited data that she is given Note that relying on the clients data has already forced the practitioner to only learn from the clients existing employees at the outset at least she has data on how those who werent hired would have performed Once a target is identified the practitioner needs a dataset on which to train a model Since she has performance data on previous employees she needs them to take the assessment so she can link their scores to their observed job performance How many employees data does she need in order to get an accurate model What if certain employees dont want to or dont have time to take the assessment Is the set of employees who respond representative of the larger applicant pool who will ultimately be assessed Finally the practitioner is in a position to actually build a model Along the way however she had to make several key choices often based on factors like client data availability outside her control The choice of target variable is particularly salient Proxies like job evaluations for instance can exhibit biases against minorities Moreover predicting the success of future employees based on current employees inherently skews the task toward finding candidates resembling those who have already been hired Some vendors go beyond trying to identify candidates who are generically good or even good for a particular client and explicitly focus on finding candidates who fit with an existing employee or team Both Good Co and Teamscope provide these team-specific tools for employers and Good Co further advertises their assessments as a way to replicate your top performers If models are localized to predict fit with particular teams any role at any company could in principle have its own tailor-made predictive model But when models are customized at such a small scale it can be quite difficult to determine what it means for such a model to be biased or discriminatory Does each team-specific model need to be audited for bias How would a vendor go about doing so And yet while it is easy to criticize vendors for their choices its not clear that there are better alternatives In practice it is impossible to even define let alone collect data on an objective measure of a good employee Nor is it always feasible to get completely representative data Vendors and advocates point out that many of the potentially problematic elements here subjective evaluations biased historical samples emphasis on fit are equally present if not more so in traditional human hiring practices Customizable and pre-built assessments Instead of building a new custom assessment for each client it may be tempting to instead offer a pre-built assessment perhaps specific to a particular job role that has been validated across data from a variety of clients This approach has its advantages it isnt subject to the idiosyncratic data of each client and it can draw from a diverse range of candidates and employees to learn a broad notion of what a good employee looks like Additionally pre-built assessments may be attractive to clients with too few existing employees to build a custom assessment Some vendors offer assessments that are mostly pre-built but somewhat customizable Koru and Plumio for example provide pre-built assessments to evaluate a fixed number of competencies Experts then analyze the job description and role for a particular client and determine which competencies are most important for the clients needs Thus these vendors hope to get the best of both worlds assessments validated on large populations that are still flexible enough to adapt to the specific requirements of each client As shown in Figure the firm and Above profiles over traits based on a video interview but also reports a single Elev score tailored to the particular client Despite these benefits pre-built assessments do have drawbacks Individual competencies like grit or openness are themselves constructs and attempts to measure them must rely on other psychometric assessments as ground truth Given that traits can be measured by multiple tests that dont perfectly correlate with one another it may be difficult to create an objective benchmark against which to compare an algorithmic assessment Furthermore it is generally considered good practice to build and validate assessments on a representative population for a particular job role and both underlying candidate pools and job specifics differ across locations companies and job descriptions Pre-built assessments must by nature be general but as a consequence they may not adapt well to the clients requirements Necessary trade-offs This leads to an inherently challenging technical problem on the one hand more data is usually beneficial in creating and validating an assessment on the other hand drawing upon data from related but somewhat different sources may lead to inaccurate conclusions We can view this as an instance of domain adaptation and the bias-variance tradeoff well studied in the statistics and machine learning literature Pooling data from multiple companies or geographic locations may reduce variance due to small sample sizes at a particular company but comes at the cost of biasing the outcomes away from the clients specific needs There is no obvious answer or clear best practice here and vendors and clients must carefully consider the pros and cons of various assessment types Larger clients may be better positioned for vendors to build custom assessments based solely on their data smaller clients may turn to pre-built assessments making the assumption that the candidate pool and job role on which the assessment was built is sufficiently similar to warrant generalizing its conclusions Alternative Assessment Formats Once an assessment has been built it must be validated to verify that it performs as expected Psychologists have developed extensive standards to guide assessment creators in this process however modern assessment vendors are pushing the boundaries of assessment formats far beyond the pen-and-paper tests of old often with little regulatory oversight Game- and video-based assessments in particular are increasingly common Vendors point to an emerging line of literature showing that features derived from these modern assessment formats correlate with job outcomes and personality traits as evidence that these assessments contain information that can be predictive of job outcomes though they rarely release rigorous validation studies of their own Technical challenges for alternative assessments While there is evidence for the predictive validity of alternative assessments empirical correlation is no substitute for theoretical justification Historically IO psychologists have designed assessments based on their research-driven knowledge that certain traits correlate with desirable outcomes To some extent machine learning attempts to automate this process by discovering relationships eg between actions in a video game and personality traits instead of quantifying known relationships Of course machine learning can be used to unearth meaningful relationships But it may also find relationships that experts dont understand When the expert is unable to explain why for example the cadence of a candidates voice indicates higher job performance or why reaction time predicts employee retention should a vendor rely on these features From a technical perspective correlations that cannot be theoretically justified may fail to generalize well or remain stable over time and in light of such concerns the APA Principles caution that a practitioner should establish a clear rationale for linking the resulting scores to the criterion constructs of interest Yet when an algorithm takes in millions of data points for each candidate as advertised by pymetrics it may not be possible to provide a qualitative justification for the inclusion of each feature Moreover automated discovery of relationships makes it difficult for a critical expert to detect when the model makes indirect use of a proscribed characteristic Rich sources of data can easily encode properties that are illegal to use in the hiring process Facial analysis in particular has been heavily scrutinized recently A wave of studies has shown that several commercially available facial analysis techniques suffer from disparities in error rates across gender and racial lines and more broadly evidence suggests that we may not be able to reliably infer emotions from facial expressions especially cross-culturally Concerns have also been raised over the use of affect and emotion recognition for those with disabilities particularly in the context of employment Because it can be quite expensive and technically challenging to build facial analysis software in-house vendors will often turn to third parties eg Affectiva who provide facial analysis as a service As a result vendors lack the ability or resources to thoroughly audit the software they use With these concerns in mind US Senators Kamala Harris Patty Murray and Elizabeth Warren recently wrote a letter to the EEOC asking for a report on the legality and potential issues with the use of facial analysis in pre-employment assessments Even more recently Illinois passed a law requiring applicants to be notified and provide consent if their video interviews will be analyzed by artificial intelligence though its not clear what happens if an applicant refuses to consent While heightened publicity regarding racial disparities in facial analysis has prompted many third-party vendors of this technology to respond by improving the performance of their tools on minority populations it remains unclear what information facial analysis relies on to draw conclusions about candidates Facial expressions may contain information about a range of sensitive attributes from obvious ones like ethnicity gender and age to more subtle traits like a candidates mental and physical health Given the opacity of the deep learning models used for facial analysis it can be difficult or even impossible to detect if a model inadvertently learns proxies for prohibited features ALGORITHMIC DE-BIASING Under Title VII employers bear ultimate legal responsibility for their hiring decisions Employers then remain strongly motivated to mitigate their potential liability against disparate impact claims Vendors in turn are incentivized to build demonstrably unbiased tools that help employers to avoid such liability As we have described all vendors in our sample who made concrete claims about de-biasing including the two best-funded firms in our sample did so with reference to equality of outcomes and compliance with the  rule In this section we explore the effects of this reliance on the stages of a typical disparate impact lawsuit We then explore technical approaches that have been proposed As a general matter the Americans with Disabilities Act prohibits employers from collecting or considering information about candidates health Figure Part of a sample candidate profile from and Above based on a -second recorded video cover letter screenshot from the and Above website to control outcome disparities and their relationship to the law Finally we describe some important consequences of the de-biasing strategies favored by vendors Algorithmic De-Biasing and Disparate Impact Litigation Recall the three steps in a disparate impact case The plaintiff must first establish that the employers selection procedure generates a disparate impact Once established the employer must then defend itself by justifying the disparate impact by reference to some business necessity In this case an employer would likely do so by establishing the validity of the model driving its hiring decisions Finally the plaintiff may then challenge the proffered justification as faulty or demonstrate that an alternative practice exists that would serve the employers business objective equally well while reducing the disparate impact in its selection rates Note that disparate impact doctrine does not prohibit disparate impact altogether it renders employers liable for an unjustified or avoidable disparate impact Vendors choice to enforce the  rule might therefore seem overly cautious although employers could justify an assessment that has a disparate impact by demonstrating its validity as we discuss in Section vendors take steps to ensure that employers are not placed in this position because assessments are prevented from having a disparate impact in the first place One possible explanation for adopting the  rule is that vendors might be catering to employers aversion to legal risk As to the second step the practical effect of vendors reliance on the  rule is to obviate the need for an employer to demonstrate business necessity through a legally rigorous validation process According to the Uniform Guidelines employers only need to validate their selection procedure if it has a disparate impact Of course clients might still expect and even demand validation studies from vendors given their goal of selecting qualified candidates As a consequence the choice of how to validate seems to become a business decision rather than a legal imperative The final step in a disparate impact case raises yet another possible explanation for vendors decisions to adopt the  rule as a constraint Recall that at this stage employers bear liability if they failed to adopt an alternative practice that could have minimized any business-justified disparity created by their selection procedure provided that such practices were not too costly Employers therefore run significant legal risks if they do not take such steps In turn should vendors have some way to minimize disparity without sacrificing the accuracy of their assessments failing to do so might place their clients in legal jeopardy A plaintiff could assert that this very possibility reveals that any evident disparate impact even if justified by a validation study was avoidable While the burden of identifying this alternative business practice rests with the plaintiff vendors may want to preempt this argument by taking affirmative steps to explore how to minimize disparate impact without imposing unwelcome costs on the employer In the past such exploratory efforts might have been costly and difficult since discovering an alternative business practice that is equally effective for the firm while generating less disparity in selection rates was no easy task Many modern assessments eg those with a large number of features make some degree of exploration almost trivial allowing vendors to find a model that nearly maintains maximum accuracy while reducing disparate impact In this way the ready availability of algorithmic techniques might effectively create a legal imperative to use them If the adverse impact of a business-justified model could be reduced through algorithmic de-biasing without significantly harming predictive ability and at trivial cost de-biasing itself might be considered an alternative business practice and therefore render the employer liable for not adopting it Methods to Control Outcome Disparities Thus for legal reasons a vendor may choose to control outcome disparities in strict adherence to the  rule But this is not the end of the story multiple techniques exist to control outcome differences Here we explore both historical and contemporary approaches in comparison with the de-biasing techniques we observe The most straightforward approach to control outcome differences is known as within-group scoring under which scores are reported as a percentile with respect to the particular group in question Employers could then select candidates above a particular threshold for each group top from Group A top from Group B etc which would naturally result in equal selection rates Recall that in the de-biasing reviewed above vendors achieve approximately equal selection rates by systematically removing features from the model that contribute to a disparate impact In so doing they may lose useful information contained in these features as well undermining their ability to maintain an accurate rank order within each group In contrast within-group scoring may theoretically be the optimal way to equalize selection rates since it preserves rank order In fact within-group scoring was used for the General Aptitude Test Battery GATB a pre-employment assessment developed in the s by the US Employment Service USES due to significant differences in score distributions across ethnic groups In particular the USES reported results as within-group percentile scores by ethnicity black Hispanic and other Commissioned to investigate the justification for such a policy a National Academy of Sciences study recommended the continued use of within-group percentiles because without them minority applicants would suffer from higher false-rejection rates In principle within-group score reporting also known as race-norming would satisfy the  rule so why dont vendors use it In fact within-group reporting would likely be considered illegal today In the Department of Justice challenged its legality in the GATB claiming that it constituted disparate treatment and the practice was prohibited by the Civil Rights Act of This points to a longstanding tension between disparate treatment and disparate impact some techniques to control outcome disparities require the use of protected attributes which may be considered disparate treatment To circumvent this the vendors we observe engaging in algorithmic de-biasing take into account protected attributes when building models but ultimately produce models that do not take protected attributes as input In this way individual decisions do not exhibit disparate treatment and yet outcome disparities can still be mitigated In fact these techniques fit into a broader category of methods known as Disparate Learning Processes DLPs a family of algorithms designed to produce decision rules that approximately equalize outcomes without engaging in disparate treatment at the individual level There are slight differences between DLPs as found in the computer science literature and vendors algorithmic de-biasing efforts DLPs typically work by imposing constraints that prevent outcome disparities on the learning algorithm that produces the model the algorithmic de-biasing we observe on the other hand simply removes features correlated with protected attributes until outcomes are within a tolerable range In spirit however these techniques are ultimately quite related Similar connections exist to fair representation learning where an encoder is built to process data by removing information about protected attributes including proxies and correlations Thus any model built on data processed by the encoder would have approximately equal outcomes since outputs of the encoder contain very little information about protected attributes As in DLPs protected attributes are used only to create the encoder after deployment when the encoder processes any individuals data it does not have access to protected attributes We can think of some vendors practices as analogous to building such an encoder one that processes data by simply discarding features highly correlated with protected attributes Limitations of Outcome-Based De-Biasing Despite the perhaps good reasons vendors have to use the particular form of algorithmic de-biasing discussed above these techniques face important caveats and consequences worth mentioning Outcome-based notions of bias are intimately tied to the datasets on which they are evaluated As both the EEOC Guidelines and APA Principles clearly articulate a representative sample is crucial for validation The same holds true for claims regarding outcome disparities they may depend on whether the assessment is taken by recent college grads in Michigan applying for sales positions or high school dropouts in New York applying for jobs stocking warehouses Thus when evaluating claims regarding outcome disparities it is critical to understand how vendors collect and maintain relevant representative data While outcome disparities are important for vendors to consider especially in light of US regulations discrimination and the  rule should not be conflated Vendors may find it necessary from a legal or business perspective to build models that satisfy the  rule but this is not a substitute for a critical analysis into the mechanisms by which bias and harm manifest in an assessment For example differential validity which occurs when an assessment is better at ranking members of one group than another should be a top-level concern when examining an assessment But because of the legal emphasis placed on adverse impact vendors have little incentive to structure their techniques around it Furthermore it can be challenging to identify and mitigate outcome disparities with respect to protected attributes employers typically dont collect eg sexual orientation In such cases vendors may need to consider alternative approaches to prevent discrimination More broadly bias is not limited to the task of predicting outputs from inputs a critical holistic examination of the entire assessment development pipeline may surface deeper concerns Where do inputs and outputs come from and what justification do they have Are there features that shouldnt be used This isnt to say that some vendors are not already asking these questions however in the interest of forming industry standards surrounding algorithmic assessments the legal operationalization of the  rule as a definition of bias runs the risk of downplaying the importance of examining a system as a whole Both the law and existing techniques focus on assessment outcomes as binary screened in/out however some platforms actually rank candidates explicitly or implicitly by assigning numerical scores While screening decisions can ultimately be viewed as binary a candidate is either interviewed or not there are a number of subtleties induced by ranking a lower-ranked candidate may only be interviewed after higher-ranked candidates and their lower score could unduly bias future decision-makers against them There is no clear analog of the  rule for ranking in practice vendors may choose a cut-off score and test for adverse impact via the  rule In the computer science literature there are ongoing efforts to define technical constraints on rankings in the spirit of equal representation and the  rule and LinkedIn has adopted a similar approach to encourage demographic diversity in its search results However none of these approaches has received any sort of consensus or official endorsement From a policy perspective the EEOC can and should clarify its position on the use of algorithmic de-biasing techniques which to our knowledge has yet to be challenged in court Legal scholars have begun to debate the legality of algorithmic affirmative action in various contexts but the debate is far from settled While existing guidelines can be argued to apply to ML-based assessments the de-biasing techniques described above do present new opportunities and challenges DISCUSSION AND RECOMMENDATIONS In this work we have presented an in-depth analysis into the bias-related practices of vendors of algorithmic pre-employment assessments Our findings have implications not only for hiring pipelines but more broadly for investigations into algorithmic and sociotechnical systems Given the proprietary and sensitive nature of models built for actual clients it is often infeasible for external researchers to perform a traditional audit despite this we are able to glean valuable information by delving into vendors publicly available statements Broadly speaking models result from the application of a vendors practices to a real-world setting Thus by learning about these practices we can draw conclusions and raise relevant questions about the resultant models In doing so we can create a common vocabulary with which we can discuss and compare practices We found it useful to limit the scope of our inquiry in order to be able to ask and answer concrete questions Even just considering algorithms used in the context of hiring we found enough heterogeneity as have previous reports on the subject that it was necessary to further refine our focus to those used in pre-employment assessments While this did lead us to exclude a number of innovative and intriguing hiring technologies see eg Textio or Jopwell it allowed us to make specific Textio analyzes job descriptions for gender bias and makes suggestions for alternative gender-neutral framings Jopwell builds and maintains a network of Black Latinx and Native American students and connects students these with employers and direct comparisons between vendors and get a more detailed understanding of the technical challenges specific to assessments In analyzing models via practices we observe that it is crucial to consider technical systems in conjunction with the context surrounding their use and deployment It would be difficult to understand vendors design decisions without paying attention to the relevant legal historical and social influences Moreover in order to push beyond hypothetical or anecdotal accounts of algorithmic bias we need to incorporate empirical evidence from the field Based on our findings we summarize the following policy recommendations discussed throughout this work Transparency is crucial to further our understanding of these systems While there are some exceptions vendors in general are not particularly forthcoming about their practices Additional transparency is necessary to craft effective policy and enable meaningful oversight Disparate impact is not the only indicator of bias Vendors should also monitor other metrics like differential validity Outcome-based measures of bias including tests for disparate impact and differential validity are limited in their power They require representative datasets for particular applicant pools and fail to critically examine the appropriateness of individual predictors Moreover they depend on access to protected attributes that are not always available We may need to reconsider legal standards of validity under the Uniform Guidelines in light of machine learning Because machine learning may discover relationships that we do not understand a statistically valid assessment may inadvertently leverage ethically problematic correlations Algorithmic de-biasing techniques have significant implications for alternative business practices since they automate the search for less discriminatory alternatives Vendors should explore these techniques to reduce disparate impact and the EEOC should offer clarity about how the law applies Our work leads naturally to a range of questions ranging from those that seem quite technical What is the effect of algorithmic debiasing on model outputs When should data from other sources be incorporated to socio-political What additional regulatory constraints could improve the use of algorithms in assessment How can assessments promote the autonomy and dignity of candidates Because the systems we examine are shaped by technical legal political and social forces we believe that an interdisciplinary approach is necessary to get a broader picture of both the problems they face and the potential avenues for improvement