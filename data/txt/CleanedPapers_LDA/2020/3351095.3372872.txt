Fair Decision Making Using Privacy-Protected Data Data collected about individuals is regularly used to make decisions that impact those same individuals We consider settings where sensitive personal data is used to decide who will receive resources or benefits While it is well known that there is a tradeoff between protecting privacy and the accuracy of decisions we initiate a first-of-its-kind study into the impact of formally private mechanisms based on differential privacy on fair and equitable decision-making We empirically investigate novel tradeoffs on two real-world decisions made using US Census data allocation of federal funds and assignment of voting rights benefits as well as a classic apportionment problem Our results show that if decisions are made using an Ïµ-differentially private version of the data under strict privacy constraints smaller the noise added to achieve privacy may disproportionately impact some groups over others We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities We also explore improved algorithms to remedy the unfairness observed INTRODUCTION Data collected about individuals is regularly used to make decisions that impact those same individuals One of our main motivations is the practice of statistical agencies eg the US Census Bureau which publicly release statistics about groups of individuals that are then used as input to a number of critical civic decision-making procedures The resulting decisions can have significant impacts on individual welfare or political representation For example election materials must be printed in minority languages in specified electoral jurisdictions only if certain conditions are met which are determined by published counts of minority language speakers and their illiteracy rates annual funds to assist disadvantaged children are allocated to school districts determined by published counts of the number of eligible school-age children meeting financial need criteria seats in legislative bodies national and state legislatures and municipal boards are apportioned to regions based on their count of residents For example seats in the Indian parliament are allocated to states in proportion to their population In many cases the statistics used to make these decisions are sensitive and their confidentiality is strictly regulated by law For instance in the US census data is regulated under Title which requires that no individual be identified from any data released by the Census Bureau and data released about students is regulated under FERPA regulated under GDPR and confidentiality requirements by releasing statistics that have passed through a privacy mechanism In the US a handful of critical decisions eg congressional apportionment are made on unprotected true values but the vast majority of decisions are made using privatized releases Our focus is the impact of mechanisms satisfying formal privacy guarantees based on differential privacy on resource allocation decisions The accuracy of the above decisions is clearly important but it conflicts with the need to protect individuals from the potential harms of privacy breaches To achieve formal privacy protection some error must be introduced into the properties of groups ie states voting districts school districts potentially distorting the decisions that are made In the examples above the consequences of error can be serious seats in parliament could be gained or lost impacting the degree of representation of a states citizens funding may not reach eligible children or a district deserving minority voting support may not get it disenfranchising a group of voters The tradeoff between privacy protection and the accuracy of decision making must therefore be carefully considered The right balance is an important social choice and the model of differential privacy allows for a more precise analysis of this choice Maximizing the accuracy achievable under differentially privacy has been a major focus of recent privacy research resulting in many sophisticated algorithmic techniques Yet that effort has considered accuracy almost exclusively through aggregate European Parliament measures of expected error which can hide disparate effects on individuals or groups In this paper we look beyond the classic tradeoff between privacy and error to consider fair treatment in decision problems based on private data If we accept that privacy protection will require some degree of error in decision making does that error impact groups or individuals equally Or are some populations systematically disadvantaged as a result of privacy technology These questions are especially important now the adoption of differential privacy is growing and in particular the US Census Bureau is currently designing differentially private methods planned for use in protecting census data The contributions of our work include the following We present a novel study of the impact of common privacy algorithms on the equitable treatment of individuals In settings where the noise from the privacy algorithm is modest relative to the statistics underlying a decision impacts may be negligible But when stricter privacy ie small values of is adopted or decisions involve small populations significant inequities can arise We demonstrate the importance of these impacts by simulating three real-world decisions made using sensitive public data the assignment of voting rights benefits the allocation of federal funds and parliamentary apportionment We show that even if privacy mechanisms add equivalent noise to independent populations significant disparities in outcomes can nevertheless result For instance in the federal funds allocation use case under strict privacy settings of some districts receive over their proportional share of funds while others receive less than half their proportional share Under weaker privacy settings this disparity is still observed but on a much smaller scale For assigning voting rights benefits to minority language communities we find that noise for privacy can lead to significant disparities in the rates of correct identification of those deserving the benefits especially under stricter privacy settings For the parliamentary apportionment problem surprisingly there are settings of where the apportionment of seats to Indian states based on the noisy data is more equitable ex ante than the standard deterministic apportionment For funds allocation and voting benefits the allocation problems with the greatest disparities we propose methods to remedy inequity which can be implemented without modifying the private release mechanism Our study reveals that the use of privacy algorithms involves complex tradeoffs which can impact social welfare Further these impacts are not easy to predict or control because they may be caused by features of the privacy algorithm the structure of the decision problem and/or properties of the input data We believe these findings call for new standards in the design and evaluation of the privacy algorithms that are starting to be deployed by companies and statistical agencies The organization of the paper is as follows In the next section we describe our problem setting followed by related work in Section In Sections to we investigate fairness in the example problem domains of voting rights funds allocation and apportionment respectively We conclude with open challenges in Section The appendix includes algorithm details to aid reproducibility and proofs but is not essential to the claims of the paper Remark This work uses only public data released by the US Census Bureau and other institutions Our empirical results do not measure the actual impacts of any agency practice currently in use Instead we simulate the use of state-of-the-art privacy algorithms on real use cases in order to understand and quantify potential unfair impacts should these privacy algorithms be adopted PROBLEM SETTING Below we provide a general definition of the assignment problems we consider define differential privacy and assignment based on private inputs as well as our methodology for assessing fairness Assignment Problems We assume a universe of individuals each described by a record in a table I Individuals are divided into disjoint assignee populations each population denoted by a label a A In our example problems assignee populations are characterized by and labeled with the geographic region in which they reside eg state county school district For example we may have a Wyoming and use to denote the set of records for all Wyoming residents An assignment method M A O associates a resource or benefit with each assignee population formalized as an outcome from an outcome set O We are primarily concerned with equitable treatment of assignee populations in terms of the outcomes they receive from an assignment The assignment methods we consider are deterministic in the absence of privacy protection and depend on properties of the assignee populations which are described by statistics These are formalized by one or more statistical queries Q evaluated on the records corresponding to the assignee population For example we may write Q tot where is the query that computes the total population of an assignee a These statistics are stored in a matrix X indexed by elements a A and q Q such that a An assignment method M will typically be defined with respect to this matrix of statistics X and we use the notation MaX to make this dependence clear The vector of outcomes o formed from elements MaX is the ground truth assignment because it is computed on the true unmodified statistics about the assignee populations Tables to in the later sections contain the formal descriptions of the assignment methods for our three example problems including a specification of the assignee population the outcome space the query set and the rule underlying the assignment Differential Privacy Differential privacy is a formal model of privacy that offers each individual a persuasive guarantee any released data computed from the sensitive input would have been almost as likely had the individual opted-out More formally differential privacy is a property of a randomized algorithm that bounds the ratio of output probabilities induced by changes to an individuals data Let be the set of databases differing from I in at most one record Fair Decision Making Using Privacy-Protected Data FAT January Barcelona Spain Definition Differential Privacy A randomized algorithm A is Ïµ-differentially private if for any instance I any I and any outputs O Pr AI O Pr AI O Differentially private algorithms protect individuals and all of their associated properties and in addition every individual enjoys the same bound on privacy loss which is quantified by a function of the privacy parameter Smaller implies greater privacy but greater noise and the parameter is sometimes referred to as the privacy loss budget A useful property of statistics computed in a differentially private manner is that any subsequent computations that use those statistics are also differentially private for the same assuming they do not also use the sensitive data We use two privacy mechanisms in this paper The first is the standard Laplace mechanism While the Laplace Mechanism is a fundamental building block of many differentially private algorithms it can offer sub-optimal error if applied directly to some tasks Therefore we also consider the Data- and Workload-Aware DAWA algorithm It is one of a number of recently-proposed algorithms cf which introduce complex noise that is adapted to the input data These techniques can offer substantially reduced error rates in some settings but may introduce statistical bias in the estimates produced This is in contrast to the Laplace mechanism which produces unbiased estimates and with error that is independent of the input We chose DAWA because it was reported to perform well in benchmarks In each of the sections that follow we describe how these algorithms are adapted to the allocation problems studied We provide further background in the appendix Assignment Using Private Inputs Given an assignment problem we protect the privacy of the members of each assignee population by answering the queries in Q using differentially private mechanism The resulting noisy query answers satisfy differential privacy for a given privacy parameter I X We then assume the private assignments are computed with M using X in place of X Ma X As noted above o inherits the privacy guarantee of X While M is deterministic when M is composed with the randomized private computation of statistics the result is a randomized assignment algorithm inducing a probability distribution over outcome vectors Assessments of fairness must therefore be probabilistic in nature The expected error in the statistics introduced by the privacy mechanism is E X X for a suitable metric which we distinguish from error in the outcome space E o o Note that we assume the private computation of the supporting statistics Q is followed by an assignment method M In this initial work we restrict our attention to this data publishing model because it follows the practice of many statistical agencies they release fixed sets of statistics after invoking disclosure limitation methods which are used for a variety of purposes In experiments we therefore begin by measuring the effects of current practice applying the standard assignment method to the privatized data But we also consider remedy approaches that alter the assignment method to account for the presence of noise introduced by the privacy mechanism Other alternatives namely altering the privacy mechanism itself are noted as future work in Section Methodology The example problems we consider in Sections to assign resources or benefits to populations according to properties of those populations that define their entitlement For example for Title funding Section a school districts entitlement is proportional to the number of students who meet a specific qualification condition Our goal is not to question the fairness of the declared entitlement or the resulting ground truth assignment as these are typically mandated by law Instead we consider the change in outcomes due to the introduction of privacy protection Since different populations have different entitlements we do not seek to treat each population equally but instead to treat equals equally However with a randomized assignment method even identical populations will receive different outcomes over runs of the algorithm so we must evaluate equal treatment in expectation or with high probability We provide problem-specific fairness measures in the following sections RELATEDWORK While fairness and privacy are topics that have been considered by philosophers and theologians for thousands of years it is only recently that these values have begun to be engineered into algorithms Differential privacy provides a formal model for reasoning about and controlling a quantitative measure of privacy loss Fairness has been formalized in economics and more recently in definitions emerging from machine learning Yet relatively little work has considered the direct interaction of privacy and fairness Dwork and Mulligan warn against the expectation that privacy controls and transparency alone can offer resistance to discrimination in the context of large-scale data collection and automated classification And Dwork et al propose a framework for fair classification which they show can be viewed as a generalization of differential privacy Both of these focus on settings distinct from ours Conceptually closest to our work is a recent position paper in which Ekstrand et al raise a number of questions about the equitable provision of privacy protections privacy-fairness and equitable impacts of privacy-protection mechanisms accuracy-fairness In addition very recently Bagdasaryan and Shmatikov have shown the disparate impact of differential privacy on learned models Economics and Social Choice Theory and Schmutte characterize accuracy and privacy protection as competing social goods and invoke an economic framework in which the demand for accuracy is balanced with the demand for privacy They use the model of differential privacy to quantify privacy loss and study Title I funds allocation in detail They measure inaccuracy using total squared error a standard metric in the privacy community and explain that this corresponds to utilitarian social welfare This work inspired ours motivating us to ask whether there are other social welfare functions to consider in the design of privacy algorithms In the literature on social choice fair allocation methods have been widely studied Two of the example problems we consider are instances of fair division problems Funds allocation is a fair division problem for a divisible and homogeneous resource since money can be divided and only the amount matters where agents in our example school districts value the resource equally but have differing rights to the resource eg based on eligible population This is a trivial fair division problem whose solution is a proportional division In our setting the division deviates from proportional because the information about agents rights to the resource is noisy We are not aware of fairness definitions which consider this variant directly although Xue considers a related scenario where agents rights are uncertain and proposes a division that discounts an agents allocation accordingly Apportionment is a fair division problem for an indivisible and homogeneous good since seats cannot be divided and only the number of seats matters where agents in our example states value the resource equally but have differing rights to the resource determined by population Again in our setting we must consider the impact of noisy information about agents rights While the study of apportionment methods and their properties has a long history we are aware of no existing approaches to cope with noisy inputs The closest related work may be that of Grimmett which proposes a randomized apportionment method along with a fairness criterion we consider in Section Fairness in Machine Learning A number of fairness definitions have been proposed recently for assessing the impacts of predictive algorithms primarily focused on algorithms that assign scores or classifications to individuals Fairness criteria measure the degree of disparate treatment for groups of individuals who should be treated equally eg males and females in the context of hiring Our example problem concerning minority language benefits is related since the goal is to classify jurisdictions However rather than studying the impact of a classifier that may display biased performance on unseen examples we have a fixed decision rule mandated by law but error is introduced into outcomes because of noise in the input statistics Although we could certainly compare impacts across groups eg whether Hispanic and Chinese minority language speakers are treated equally we are also concerned with equitable treatment of arbitrary pairs of jurisdictions The metric we use for this problem is related to error rate balance but other metrics could also be considered Statistical Agency Practices Statistical agencies like the US Census Bureau have considered the potential impacts of inaccuracy and bias in their data products for decades Broadly errors may arise from sampling data cleaning or privacy protection Census data products derived from surveys rather than censuses include margins-of-error representing estimates of uncertainty due to sampling Margins-of-error are intended to quantify sampling error but have not historically considered the distortion introduced by the data transformations applied for privacy protection In most cases released data are treated as true by end users assignment and allocation methods are applied directly to released summary statistics without any modification to take into account potential inaccuracies We are not aware of systematic studies of potential bias in the statistics currently released by statistical agencies however Spielman observed that margins-of-error can be correlated with income levels in some Census products leading to greater inaccuracies for low-income persons The Census Bureau will be adopting differential privacy for parts of the Census of Population and Housing This motivates a careful consideration of the implications of differentially private mechanisms on both accuracy and fairness A preliminary version of the planned algorithm was released by the Census Bureau subsequent to this work While the US Census Bureau is required by law to protect individuals privacy it is also obligated to support accurate decision making It therefore makes strategic choices about the accuracy of its released products For some critical assignment problems eg apportionment and redistricting the Census forgoes privacy protection in order to favor accurate allocation and this choice is supported by law In other cases such as minority language benefits special variance reduction methods have been adopted to boost accuracy Ultimately for legacy privacy methods employed by the Census it is not possible for users to evaluate potential biases PROBLEM MINORITY LANGUAGE VOTING RIGHTS The Voting Rights Act is federal legislation passed in which provides a range of protections for racial and language minorities Among its many provisions is Section describing conditions under which local jurisdictions must provide language assistance during elections Each jurisdiction eg a county is evaluated for each of identified minority languages If they meet the conditions they are found to be covered by the provision and must provide all election information including voter registration ballots and instructions in the minority language The coverage determination is made by the Census Bureau every five years using published population statistics Most recently in jurisdictions out of a total of approximately were found to be covered under Section across all language minority groups While a small fraction of all jurisdictions are covered an estimated million voting-age citizens lived in these jurisdictions and were potentially impacted by this benefit Problem Definition Informally a jurisdiction is covered for a language if it i has a large enough population of voting age citizens who speak the language and have limited proficiency in English and ii if the illiteracy rate of those speaking the language is higher than the national average Condition i can be satisfied in either of two ways in percentage terms or absolute terms Table formalizes these criteria defining a binary outcome covered or not-covered for each jurisdiction and for each minority language category Assessing Fairness To evaluate fairness we measure for each jurisdiction the rate of correct classification For a covered jurisdiction ie Mj Covered where l Hispanic we measure Pr Ma X Covered where the probability is over randomness in the privacy algorithm Similarly for a not-covered jurisdiction we measure Pr Ma X Not-covered We evaluate the rates of correct classification across the set of covered and not-covered jurisdictions measuring the disparity in classification accuracy Empirical Findings Experimental Setup We use the public-use data accompanying the Census voting rights determinations treating it as ground Fair Decision Making Using Privacy-Protected Data FAT January Barcelona Spain a The D-Laplace algorithm b The DAWA algorithm c distance to threshold d Repair mechanism for varying p and Figure Minority Language Determinations using D-Laplace and DAWA Table Voting Rights Minority Language Determinations Assignees are all combinations of US voting jurisdictions with each of minority language categories Assignees a l Jurisdictions Languages Outcomes Covered Not-covered Q vac l it where voting age citizens in speaking language l voting age citizens in speaking language l and limited English proficient l it voting age citizens in speaking language l limited English proficient and less than th grade education MaX truth We focus on the Hispanic minority language group and jurisdictions that are counties or minor civil divisions This data provided the values for the variables described in Table namely lit for jurisdictions of which were Covered We consider two algorithms for computing the noisy statistics X The first which we call D-Laplace is an adaptation of the Laplace mechanism in which we decompose the original required queries Q vac lit which together have sensitivity into Q lit which compose in parallel and have sensitivity We use the Laplace mechanism to estimate answers to Q and then derive estimates to Q from them In our experiments this performed consistently better than a standard application of the Laplace mechanism The second algorithm is DAWA as described in Section and with additional background provided in the appendix We run trials of each algorithm for each value Finding M There are significant disparities in the rate of correct classification across jurisdictions Because the failure to correctly classify a true positive is a more costly mistake potentially disenfranchising a group of citizens our results focus on the classification rate for the truly covered jurisdictions For the jurisdictions positively classified for the Hispanic language Figure a shows the correct classification rate for each jurisdiction under the D-Laplace algorithm for four settings of the privacy parameter Jurisdictions are ranked from lowest classification rate to highest For all of the jurisdictions have a correct classification rate greater than For of jurisdictions have a correct classification rate greater than while do for and do for However the plot shows that the lowest correct classification rate is about for and and for The conditions of Section impose thresholds on language minority populations as shown in Table A given covered jurisdiction may be closer to the thresholds making it more likely that perturbation from the privacy mechanism will cause a failure to classify accurately As a particular example consider Maricopa county Arizona and Knox county Texas which are both covered jurisdictions Maricopa county is correctly classified of the time by D-Laplace at while Knox county is correctly classified only of the time Because the D-Laplace algorithm produces unbiased noise of equivalent magnitude to each jurisdiction this difference is fully explained by the distance to the classification threshold Maricopa county is further from the threshold than Knox county so it is more robust to the addition of noise Additionally the distance to the classification threshold is strongly correlated with the population size which is over for Maricopa county but less than for Knox county Thus in this case the significant differences in the rate of successful classification across jurisdictions is a consequence of the decision rule and its interaction with the noise added for privacy Although not shown in Figure there are also significant disparities in classification rates for the negative class uncovered jurisdictions For example the correct negative classification rate for D-Laplace at ranges from to Mistakes on the negative class mean in practice that minority language materials would be required of a jurisdiction which does not truly qualify resulting in an unnecessary administrative and financial burden Finding M While the DAWA algorithm offers equal or lower error on the underlying statistics for small it exacerbates disparities in classification rates Figure b shows a similar plot but for the DAWA algorithm however in this case the disparities are even greater The lowest classification rates are zero for both and implying that a few covered jurisdictions will definitely be not-covered for every run of the algorithm Even with higher values of and the lowest classification rates are below At the high end for of the jurisdictions have a correct classification rate greater than while do for do for and do for It is important to note that the DAWA algorithm offers approximately equivalent error on the statistics X compared to D-Laplace at and in fact offers lower error at This is a critical finding for designers of privacy algorithms optimizing for aggregate error on published statistics does not reliably lead to more accurate or fair outcomes for a downstream decision problem Finding M A jurisdictions distance from the nearest threshold explains classification rates for D-Laplace but not DAWA We plot in Figure c a jurisdictions euclidean distance from the nearest classification threshold against the rate of correct classification for We see that the results for D-Laplace are well-explained correct classification rate increases with distance from the threshold and occurs in a fairly tight band for any given distance measure For the DAWA algorithm however we observe a different result Jurisdictions very far from the threshold have high classification rates as expected presumably because there is simply not enough noise to cause a failure for these cases But for jurisdictions a smaller distance from the threshold there is a wide spread of classification rate and some jurisdictions reasonably far from the threshold have very low classification rates This shows the impact of the bias introduced in by DAWA it sometimes groups together qualified jurisdictions with unqualified ones causing them to be mis-classified Mitigating unfairness We now consider the problem of modifying the allocation mechanism to alleviate some of the fairness concerns identified above To achieve this goal we focus on the Laplace mechanism as the underlying mechanism leaving its privatized counts unchanged Rather than apply the standard allocation rule to the noisy counts we propose an altered allocation method which can account for the noise prioritizing correct positive classification In this context this mechanism allows minimizing disenfranchised voters at the cost of unnecessarily providing voting benefits to some jurisdictions Given noisy counts for jurisdiction a the approach investigated above simply applies the assignment rule returning Ma Instead the principle behind our repair algorithm is to estimate the posterior probability that the jurisdiction is Covered given the observed noisy counts ie we would like to estimate Pr Ma Covered We will then consider the jurisdiction covered if the estimated probability is higher than a supplied parameter p which allows a tradeoff between false negatives and false positives With low values of p most of the jurisdictions that should be covered will be but a larger number of jurisdictions that do not deserve coverage will also be Covered In our implementation we place a uniform prior distribution over the unknown quantities which in this case are the true population counts of a jurisdiction and estimate the probability using Monte Carlo simulation we draw samples in experiments In Fig d we show the results of running the repair algorithm on the public-use data For four settings of the x-axis shows the minimum correct classification rate resulting from a range of settings of p This is a measure of disparity since the maximum correct classification rate is always in the cases considered We are thus able to reduce disparity but must bear the cost of misclassifying jurisdictions which is shown on the y-axis For modest this tradeoff seems appealing while the standard algorithm shown in the plot as a star has a minimum correct classification rate of with an expected false positives we can raise the classification rate to if we are willing to tolerate false positives For smaller values the cost is greater but the repair algorithm can allow for raising the extremely low minimum classification rates borne by some jurisdictions Thus the algorithm could allow policy makers to weigh the risk of disenfranchised voters against the cost of over-supply of voting materials Note that cost here is expressed in terms of the expected number of jurisdictions misclassified Presumably the financial cost results from creating and distributing minority language voting materials in jurisdictions for which it is not legally required A more nuanced evaluation of cost could measure the number of individuals in those jurisdictions since the true cost is likely to have a term that is proportional to the voting age population of the jurisdiction Our approach to repair has the advantage that its mitigating effects are achieved without requiring the data publisher to change their method for producing private counts However it does rely on the fact that the underlying noise added to counts has a known distribution This holds for the Laplace mechanism and some other privacy mechanisms but does not hold for the DAWA algorithm Post-processing noisy outputs is a common technique used to improve the utility of privacy mechanisms Estimating posterior distributions from differentially-private statistics has been studied previously PROBLEM TITLE I FUNDS ALLOCATION We now turn our attention to the important class of funds allocation problems A recent study estimated that the annual distribution of at least billion dollars relies on data released by the Census Bureau This includes funding for educational grants school lunch programs highway construction wildlife restoration among many others As an example of federal funds allocation we consider Title I of the Elementary and Secondary Education Act of This is one of the largest US programs offering educational assistance to disadvantaged children In fiscal year Title I funding amounted to a total of billion of which roughly billion was given out through basic grants which are our focus Problem Definition The federal allocation is divided among qualifying school districts in proportion to a count of children in the district aged to who live in families who fall below the poverty level or receive a form of federal financial aid This proportion is then weighted by a factor that reflects the average per student educational expenditures in the districts state The allocation formula is described formally in Table where the outcome represents the fraction of the total allocation which changes annually the district will receive Table Title I Funding Allocation Assignees are all US school districts outcomes are the fraction of allocated funds for each school district Assignees School Districts Outcome Q eli where average per student expenditures for state containing district a number of eligible students in district a MaX bA Fair Decision Making Using Privacy-Protected Data FAT January Barcelona Spain True Allocation M ul tip at iv e Al lo ca n Er ro r Multiplicative Allocation Error in Michigan with Laplace a Multiplicative Allocation Error True Allocation Ab so te M is al lo ca n pe r m ill io n Misallocation per million dollars in Michigan with Laplace b Misallocation Districts sorted by true allocation Al lo ca n True allocation Allocations true and noise for Michigan With Laplace c Allocations True Allocation M ul tip at iv e Al lo ca n Er ro r Multiplicative Allocation Error in Michigan with DAWA d Multiplicative Allocation Error True Allocation Ab so te M is al lo ca n pe r m ill io n Misallocation per million dollars in Michigan with DAWA e Misallocation Districts sorted by true allocation Al lo ca n True allocation Allocations true and noise for Michigan With Dawa Allocations Figure Fairness in allocation for Michigan using the Laplace Mechanism top and DAWA bottom Assessing Fairness To assess fairness we consider the difference between the allocation vector based on the noisy statistics o and the allocation vector based on true counts o assessing disparities across assignees in this case districts An allocation mechanism is fair if the distance measures do not vary much across districts We can measure fairness ex ante ie before running the randomized allocation mechanism as well as ex post ie on the outcome of the allocation We focus on ex ante measures as they capture disparities due to the randomized allocation mechanism Multiplicative Allocation Error For each district a we compute Differences in this measure across districts can be interpreted as a measure of envy or unequal treatment For instance an example of an unfair allocation would be one where some districts have a ratio much larger than while others have a ratio smaller than In plots we show the entire distribution of the multiplicative allocation error across districts Misallocation per million dollars For each district a we also measure the dollar amount that is under or over-allocated to each district per million dollars allocated in total a A significant difference in this measure between two districts a a would suggest that districts are not treated equally and could be interpreted as a measure of envy Again in plots we show the distribution of across all districts Empirical Findings Experimental Setup The exact counts of Title I eligible students per district are unavailable so as a proxy we used per-district counts of free-lunch eligible students as reported by The National Center for Education Statistics for years For simplicity we treat the average per student expenditures as public following We obtained data for of school districts We use two differentially private algorithms to estimate for each a the Laplace mechanism and DAWA The former adds independent -mean noise to the count in each district The latter adds noise to the total count of groups of districts rather than individual districts The total noisy count of a group of districts is then evenly divided among districts in the group In both algorithms negative counts are rounded to zero The resulting vector of student counts may be fractional but it is non-negative For clarity of presentation we show results on two states Michigan and Florida see Figures and We chose these states because the histograms of the number of eligible students per district show contrasting properties We obtained data for districts in Michigan which included a number of small districts with the smallest containing just eligible students On the other hand Florida has a smaller number of comparatively larger districts we obtained data for the smallest having eligible students Finding T In cases of low there are significant disparities in outcomes over- and under-allocation using private statistics Using the Laplace mechanism the mean allocation for small districts is typically much higher than the true allocation while the mean allocation of larger districts is typically lower This is shown in Figure a which plots the multiplicative allocation error of a district versus its true allocation The districts are shown sorted by true allocation The smallest districts see a increase for a increase for and a increase for The largest districts see their allocations decrease by for for and for The Laplace mechanism adds -mean noise to the data and in expectation the noisy counts should be the same as the true counts However these counts could be negative and since negative counts are rounded to this adds an upward bias to the noisy counts Moreover this bias increases the total number of students thus bringing down the weight of larger districts True Allocation Multiplicative Allocation Error in Florida with Laplace a Multiplicative Allocation Error True Allocation Misallocation per million dollars in Florida with Laplace b Misallocation Algorithm Total Min Max Laplace DAWA Inflationary Laplace DAWA Inflationary c Misallocation per million dollars for Michigan Figure Fairness in allocation for Florida with the Laplace mechanism and misallocation statistics for Michigan Figure b shows the absolute dollars misallocated per million dollars allocated In terms of raw dollar amounts the largest districts see the greatest misallocation and see a drop in funding of about see Figure c On interpretation of this behavior is that larger districts are being taxed to ensure that students in all districts enjoy the same level of privacy protection The results for DAWA Figures d and have more disparity than those of the Laplace mechanism At some districts get about their true allocation while others get only a tenth of their true allocation in expectation whereas under the Laplace mechanism every district gets at least x of their true allocation For districts in Florida see Figure a and Figure b we see almost no difference at At there is very little difference between the true and noisy allocations between districts both additively and multiplicatively At we see the same effect of larger districts being taxed However the effects are less prominent than in Michigan This is because there are fewer small counts in Florida as well as fewer districts overall resulting in a lower variance estimate of the total count used in the denominator of the allocation formula Finding T Populations with small entitlements relative to the privacy parameter will experience significant mis-allocation Detecting small counts or the presence of small effects in data is incompatible with differential privacy This is a fundamental property of any differentially private mechanism and the meaning of small depends on Any Ïµ-differentially private algorithm can not distinguish between counts that differ in log with probability Thus no matter what differentially private algorithm one uses districts with sufficiently small counts will undergo mis-allocation Due to rounding they tend to get higher allocations than they deserve in expectation at the cost of larger districts This phenomenon is evident in Figure c and Figure which show the true and noisy allocations for all districts when Laplace and DAWA are used respectively At for both mechanisms all districts with a true allocation less than end up with an allocation of roughly in expectation This is because in these cases these mechanism can not distinguish between the number of students in those districts and and rounding induces a positive bias On the other hand the noisy allocations at track the truth more closely although even at there is a threshold under which noisy counts cannot reflect their true magnitude and at the true and noisy allocations barely differ Finding T Under some privacy mechanisms districts with a greater entitlement can receive a smaller expected allocation Consider two districts a and b where a has a smaller number of eligible students than b Naturally the true allocation of a will be smaller than the true allocation of b and the inversion of this relationship would violate a commonly held notion of fairness Under the Laplace mechanism in expectation we can show that the allocation for a will be no larger than the allocation for b However this is not true for the DAWA algorithm because of bias in estimated counts In particular for DAWA a smaller district may be grouped with other larger districts while a larger district may be grouped with other smaller districts This results in the smaller district getting a larger expected allocation than the larger district Empirically we find that using DAWA with out of the districts exhibit at least one inversion where a larger district gets a smaller allocation Mitigating unfairness Here we introduce a post-processing step designed to mitigate the inequities present due to the noise introduced for privacy We design the approach with the Laplace mechanism in mind but leave extensions to other mechanisms as future work The goal of this method is to ensure that with high probability each district receives an allocation at least as large as its true allocation More specifically we aim to satisfy the following condition Definition No-penalty allocation Given MB x a no-penalty allocation is any randomized allocation M allocating a new budget B such that for all aM a X MaX with failure probability no greater than We propose a repair mechanism that achieves the above definition but requires inflating the overall allocation in order to guarantee with high probability that no district is disadvantaged In particular our inflationary repair algorithm inflates the counts of each district by a slack variable while deflating the total count of all districts by another slack variable where is the total number of districts The final allocation is M a X a a b A b b Note that both and depend on as they are calibrated to the added noise We prove in that for any given this algorithm provides a no-penalty allocation Fair Decision Making Using Privacy-Protected Data FAT January Barcelona Spain Theorem The repair algorithm satisfies Definition We used this inflationary allocation method in the Title I funds allocation experiment described above setting the acceptable failure probability Compared with the standard allocation applied to the private counts resulting from the Laplace mechanism it removes penalties experienced by some districts at the cost of inflating the overall allocation For in expectation the repair mechanism requires increasing the budget by a relatively modest factor of the original budget ie per million At lower levels of epsilon achieving a no-penalty allocation has a significant cost At in expectation the mechanism allocates the original budget These results are included in Fig c and can be compared with the expected misallocation under the Laplace or DAWA privacy mechanisms when combined with the standard allocation rule This repair algorithm mitigates one aspect of unfairness since districts cannot complain that they were disadvantaged as a result of the noise added to population counts to preserve privacy Of course there may still be inequity in the allocation since some districts allocations can be inflated more than others This approach to mitigation does not alter the underlying privacy mechanism which may be seen as an advantage to a statistical agency publishing privatized counts to be used for many purposes beyond funds allocation However the proposed algorithm relies on an analysis of the noise distribution While feasible for the noise introduced by the Laplace mechanism it would need to be adapted to a mechanism like DAWA whose error distribution is data-dependent possibly requiring additional consumption of the privacy budget to calibrate the slack variables PROBLEM APPORTIONMENT OF LEGISLATIVE REPRESENTATIVES Apportionment is the allocation of representatives to a state or other geographic entity We use parliamentary apportionment as our example domain and consider the particular case of allocating representatives to the Indian Parliaments Lower House in which a fixed number of representatives are apportioned among states Parliamentary apportionment is carried out using population counts obtained from a census While state population counts are aggregates over large groups they nevertheless cannot be released in unmodified form in the standard model of differential privacy ie without an infinite privacy loss parameter as they could reveal presence of individuals in combination with other statistics In experiments we consider values and requisite noise sufficient to impact apportionment methods Whether or not this degree of noise would be used in practice for congressional apportionment the findings apply to apportionment problems over smaller geographies eg allocating seats on a school board to school districts In particular Laplace noise required to provide privacy at on a population of on a smaller population of Problem Definition The principle underlying fair apportionment is equal representation Therefore the ideal allocation of seats to a state is given by the states quota which is its fraction of the population multiplied by the total number of representatives A states quota is typically nonintegral but an integral number of seats must be apportioned Thus any selected apportionment outcome will deviate from the quota values leading to some degree of disparity in representation There are various apportionment methods studied in literature In this paper we do not make a comparison between these algorithms rather we are interested in how adding Laplace noise affects representation of states with different population counts Thus we apply the following simple algorithm Table We compute the quotas for all states and round them to the nearest integer with the constraint that every state receives at least one seat This algorithm is not guaranteed to allocate exactly seats Assessing fairness A desirable fairness property is quota satisfaction ie the number of seats apportioned to a state should be roughly proportional to the population of the state When we add Laplace noise this property may not hold when considering specific random outcomes ie ex-post but could hold in expectation hence we focus on the deviation from the ideal standard of equal representation ie the quota values We consider the following two measures where denotes the quota for state a computed on the true population counts Max-multiplicative this measure considers pairs of states and quantifies the disparity between the ratio of their allocation and their quota E max ab A ob Given a particular outcome o this measure can be interpreted as capturing the maximum incentive for an individual to move from one state to another state in order to increase their representation We consider the expectation of this measure over the randomness in the privacy mechanism Average-Expected-Deviation We also consider the expected absolute deviation from quota on a per-state basis which we then average over the states aA When this measure is small it means that most states will receive on average over the long run an apportionment close to their quota These measures are quite different as our empirical results will show The first is based on an ex-post measure of fairness which can be evaluated on a single apportionment outcome we consider the expected value of this measure The second isolates a particular state evaluating the difference from quota of the expected apportionment for that state and then aggregates over the states It can be Table Apportionment of seats in parliament seats Assignees are all Indian states outcomes are seats in the Lower House of Parliament Assignees States Outcomes Q tot tot total population in state a Calculate quota bA Round to nearest positive integer max Round a Average-expected-deviation b Per-state expected deviation c Max-multiplicative fairness Figure Allocation of seats to the Lower House of the Indian Parliament using population counts with Laplace noise seen as an ex ante measure of fairness if for example two states had equal expected deviation from quota then prior to any execution of the randomized algorithm they may not prefer the other states future outcome We note that an expected deviation from quota of zero was used as a fairness criterion by Grimmett in the context of a randomized but non-private method for apportionment Empirical Findings Experimental Setup We used the state population totals published by the Indian Parliament in the budget of which provides data for states and union territories We evaluate the impact on apportionment outcomes when state population totals are perturbed by the Laplace Mechanism for varying We do not consider more sophisticated privacy mechanisms as we did earlier because for this small set of statistics they do not improve upon the Laplace mechanism Finding A For some noise introduced into population totals can lead to more fair apportionment in expectation Figure a shows the average-expected-deviation measure as it varies with We see that the introduction of noise actually improves over the baseline deviation from quota between approximately and This is because randomization can reduce on average the deviation from quota caused by the integrality constraint A more detailed look is provided by Figure b which shows per state results for a single privacy level For each state the red dot shows the deviation from quota on the true population totals which may be positive or negative The blue bars show the expected deviation from quota for the respective state often substantially lower While this decreased deviation is interesting the expected apportionment is an unattainable outcome in any possible trial so this may be an unsatisfying property in practice Finding decreases apportionment outcomes display a greater multiplicative disparity between most favored and least favored state Figure c shows the max-multiplicative measure as it varies with and here we see the fairness measure worsen as noise increases When considering this ex-post measure noise does not help apportionment outcomes tend to include states receiving substantially more than their quota while others receive substantially less and the disparity increases with the magnitude of the noise CONCLUSION We empirically measure the impact of differentially private algorithms on allocation processes demonstrating with important practical examples that disparities can arise particularly for smaller more protective values of the privacy-loss budget Some practical deployments of differential privacy have been revealed to use high privacy-loss budgets which would diminish impacts however we emphasize that the privacy loss budget must cover all public releases including the supporting statistics of any required allocation problems Thus in practice the privacy loss budget devoted to the statistics for any single allocation problem may be small The disparities in outcomes have multiple causes including bias added by some privacy algorithms threshold conditions inherent to some decisions and divergent treatment of small and large populations Our results show that designers of privacy algorithms must evaluate the fairness of outcomes in addition to conventional aggregate error metrics that have historically been their focus We proposed remedies to the disparities demonstrated for funds allocation and voting benefits but further investigation of mitigating technology is needed One potential approach is to customize privacy mechanisms targeting performance on specific assignment problems While this approach should be pursued it presents agencies like the Census with the difficult prospect of designing an algorithm for each of the thousands of assignment problems that rely on the public data they release Our remedies adapted allocation methods to account for the noise added by a version of the Laplace mechanism But some algorithms including DAWA do not directly support the release of error bounds confounding this approach Furthermore modifying allocation procedures could be inconsistent with the governing regulations We hope to continue to develop and evaluate these approaches in future work