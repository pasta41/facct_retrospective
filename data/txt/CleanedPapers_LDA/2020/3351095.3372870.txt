Explainability Fact Sheets A Framework for Systematic Assessment of Explainable Approaches Explanations in Machine Learning come in many forms but a consensus regarding their desired properties is yet to emerge In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions functional operational usability safety and validation In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature extracting the criteria and desiderata that other authors have proposed or implicitly used in their research The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated as well as papers proposing such criteria from both computer science and social science perspectives This novel framework allows to systematically compare and contrast explainability approaches not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations We developed an operationalisation of the framework in the form of Explainability Fact Sheets which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method When used as a Work Sheet our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions CONCEPTS General and reference Evaluation Computing methodologies Artificial intelligence Machine learning KEYWORDS Explainability Interpretability Transparency Fact Sheet Work Sheet Desiderata Taxonomy AI ML INTRODUCTION With the current surge in eXplainable Artificial Intelligence XAI research it has become a challenge to keep track of analyse and compare many of these approaches A lack of clearly defined properties that explainable systems should be evaluated against hinders progress of this fast moving research field because of undiscovered or undisclosed limitations and properties of explainability approaches and their implementations as well as discrepancies between the two In this work we propose that every explainability method designed for predictive systems should be accompanied by a Fact Sheet that assesses its functional and operational requirements Moreover the quality of explanations should be evaluated against a list of usability criteria to better understand their usefulness from a user perspective Their security privacy and any vulnerabilities that they may introduced to the predictive system should also be discussed on this Fact Sheet Lastly their validation either via user studies or synthetic experiments should be disclosed Therefore a standardised list of explainability properties or to put it otherwise desiderata spanning all of these five dimensions would facilitate a common ground for easy evaluation and comparison of explainability approaches and help their designers consider a range of clearly defined aspects important for this type of techniques Despite theoretical guarantees for selected explainability approaches some of these properties can be lost in implementation due to a particular application domain or data set used As it stands many implementations do not exploit the full potential of the selected explainability technique for example a method based on counterfactuals might not take advantage of their social and interactive aspects A use of guidelines or a systematic evaluation of an approach with a standardised Fact Sheet could help discover these unexpected functionality losses and account for or simply report them for the benefit of the research community and end users To mitigate the current lack of consensus regarding a common set of properties that every explainable method should be evaluated against in this paper we collect review and organise a comprehensive set of characteristics that span both computer science and social sciences insights on that matter Our goal is to provide the community with an Explainability Fact Sheet template that users of explainable systems researchers and practitioners could utilise to systematically discuss evaluate and report properties of their techniques This will not only benefit research but will be of particular importance as a guideline for designing deploying or evaluating explainability methods especially if compliance with best practices or legal regulations is required eg the right to explanation introduced by the European Unions General Data Protection Regulation GDPR Therefore creators of explainable methods who need to understand their specific requirements can use the same template as a Work Sheet We demonstrate the usefulness and practical aspects of these Explainability Fact Sheets by presenting a particular instantiation created for the Local Interpretable Model-agnostic Explanations LIME algorithm see the supplementary material EXPLAINABILITY FACT SHEETS DIMENSIONS Explainability Fact Sheets are intended to evaluate explanatory systems along five dimensions Firstly the functional one which considers algorithmic requirements such as the type of problem it applies to classification regression etc which component of the predictive system it is designed for data models or predictions the scope of the technique eg local vs global explanation and its relation to the predictive system post-hoc vs ante-hoc among many others Secondly the operational one which includes the type of interaction with the end user explainability vs predictive performance trade-off and the users background knowledge required to fully utilise the explainability power of a technique to name a few Thirdly the usability one which takes a user-centred perspective and includes properties of the explanation that make it feel natural and easily comprehensible by the explainee To fulfil the users expectations these include fidelity of the explanation its actionability from the explainees perspective and its brevity to give just a few examples The fourth one is safety which discusses robustness and security of an explainability approach For example these include an analysis of how much information about the predictive model or its training data an explanation leaks and whether an explanation for a fixed data point is consistent throughout different models given the same training data Finally the fact sheet discusses the validation process used to evaluate and prove the effectiveness of an explainability approach by describing a user study or synthetic approach that was carried out Since the Fact Sheet can evolve over time we encourage their creators to systematically version them thereby making their recipients aware of any updates Furthermore indicating whether the whole Fact Sheet or some of its parts are with respect to a theoretical algorithmic approach an actual implementation or a mixture of the two will benefit its clarity To put these explainability dimensions into context an exemplar of the Explainability Fact Sheet for LIME is provided as a supplementary material Functional Requirements These can help to determine whether a particular approach is suitable for a desired application therefore they resemble a taxonomy of Machine Learning ML and explainability approaches The list provided below can be thought of as a check-list of an engineer who was tasked with identifying and deploying the most suitable explainability algorithm for a particular use case All of these properties are well-defined and flexible enough to accommodate any explainability approach  Problem Supervision Level An explainability approach can be applicable to any of the following learning tasks unsupervised semi-supervised supervised and reinforcement A large part of the literature focuses on supervised learning where explanations serve as a justification of a prediction Nevertheless explainability can also benefit unsupervised learning where the user may want to learn about the data insights elicited by a model reinforcement learning where the user is interested in autonomous agents decisions and semi-supervised learning where the user can help the system choose the most informative data points for learning by understanding the systems behaviour  Problem Type We can identify three main problem types in Machine Learning classification binary/multi-class/multi-label and probabilistic/non-probabilistic regression and clustering Additionally we can consider types such as ranking and collaborative filtering By clearly defining the applicable type of a learning task for an explainability method potential users can easily identify ones that are useful to them  Explanation Target The Machine Learning process has three main components data both raw data and features models and predictions Explaining the first one may be difficult or even impossible without any modelling assumptions These are usually summary statistics class ratio feature plots feature correlation and dimensionality reduction techniques Explaining models is concerned with its general functionality and conveying its conceptual behaviour to the explainee Explaining predictions provide a rationale behind the output for any particular data point  Explanation Breadth/Scope This notion varies across data models and predictions It tells the user to what extent an explanation can be generalised See U for a complementary view on this property from the usability perspective The main three explanation generalisability stages are local a single data point or a prediction cohort a subgroup in a data set or a subspace in the models decision space and global a comprehensive model explanation  Computational Complexity Given that some applications may have either time memory or computational power constrains each explainability approach should consider these If for example a given method is capable of explaining both a single prediction and the whole model both of these aspects should be discussed Algorithmic complexity measures such as Big-O or Little-O notations can be used to assess these aspects of an explainable system Alternatively to theoretical performance bounds empirical evaluation can also be discussed eg the average time over iterations that it took to generate an explanation for a single data point with fixed parameters of the explainability algorithm on a single-core CPU with GB of RAM  Applicable Model Class We can identify three main degrees of portability for explainability algorithms model-agnostic working with any model family model class-specific designed for a particular model family eg logical or linear models and model-specific only applicable to a particular model eg decision trees  Relation to the Predictive System We can characterise two main relations between a predictive model and an explainability technique Ante-hoc approaches use the same model for predicting and explaining eg explaining a linear regression with its feature weights It is important to note that some of these techniques may come with caveats and assumptions about the training data or the training process  which need to be satisfied for the explanation to work as intended On the other hand in post-hoc approaches predictions and explanations are made with different models eg a local surrogate explainer One can also name a third type a special case of the post-hoc family a global mimic approach where to explain a complex black-box model a simpler inherently transparent model is built in an attempt to mimic the behaviour of the more complex one eg a global surrogate explainer  Compatible Feature Types Not all models are capable of handling all the feature types for example categorical features are at odds with predictive algorithms that use optimisation as their backbone Furthermore selected model implementations require categorical features to be pre-processed one-hot encoded for example rendering them incompatible with some explainability approaches Therefore every method should have a clear description of compatible feature types numerical ordinal we differentiate numerical and ordinal features as the latter may have a bounded range and categorical In addition to these standard feature types some tasks may come with a hierarchy of features and/or their values in which case the explainability algorithm should clearly state whether these are beneficial for the quality of the explanation and how to utilise this information  Caveats and Assumptions Finally any functional aspects of an explainability approach that do not fit into these categories should be included at the end In particular restrictions with respect to input and output of predictive models and explainability techniques eg support for black-and-white images only validated behaviour on text corpora up to tokens numerical confidence of a prediction or an explanation assumptions such as feature independence the effect of correlated features on the quality of an explanation or simply explainability technique-specific requirements such as feature normalisation when explaining a linear model with its weights Operational Requirements The following properties characterise how users interact with an explainable system and what is expected of them They can be thought of as considerations from a deployment point of view O Explanation Family A great categorisation which we believe is still up to date of explainable approaches accounting for their presence in philosophy psychology and cognitive science was introduced by Johnson and Johnson for expert systems The authors have identified three main types of explanations associations between antecedent and consequent eg model internals such as its parameters features-prediction relations such as explanations based on feature attribution or importance and items-prediction relations such as influential training instances or neighbouring data points contrasts and differences using examples eg prototypes and criticisms similarities and dissimilarities and class-contrastive counterfactual statements and causal mechanisms eg a full causal model O Explanatory Medium An explanation can be delivered as a statistical summarisation visualisation textualisation formal argumentation or mixture of the above Examples of the first one are usually given as numbers for example coefficients of a linear model or a summary statistics of a data set The second one comprises all sort of plots that can be used to help the user comprehend a predictive systems behaviour eg Individual Conditional Expectation or Partial Dependence plots The textualisation is understood as any explanation in form of a natural language description eg a dialogue system that an explainee can query An explanation system based on a formal argumentation framework encompasses approaches that can output logical reasoning behind an explanation hence provide the explainee with an opportunity to argue against it whether in a form of a natural language conversation or highlighting important regions in an image Finally an example of a mixture of these representations can be a plot accompanied by a caption that helps to convey the explanation to the end user Such a mixture may be necessary at times as not all of the media are able to convey the same amount or type of information For example visualisations are confined to three dimensions four when counting time ie animations due to the limitations of the human visual perception system and counterintuitiveness of higher dimensions a phenomenon known as the curse of dimensionality The choice of an explanatory medium is also important as it may limit the expressive power of an explanation O System Interaction The communication protocol that an explainable method employs can be either static or interactive with and without user feedback The first one is a one-size-fits-most approach where the system outputs an explanation based on a predefined protocol specified by the system designer hence may not always satisfy the users expectations for example always outputting the most significant factors in favour and against a particular prediction when the user is interested in a feature not listed therein Alternatively the system can be interactive thereby allowing an explainee to explore all aspects of a particular explanation These include interactive web interfaces and dialogue systems amongst others Furthermore in case of an interactive system its creator should indicate whether the explainer can incorporate any feedback and in what form given by the explainee and how if at all it influences the underlying predictive model eg incremental-learning algorithms O Explanation Domain Explanations are usually expressed in terms of the underlying models parameters or data exemplars and their features original domain However it may be the case that the explanation is presented in a different form transformed domain Consider for example a system explaining image classification results where the data domain is an image and the explanation is a natural language description as opposed to a saliency map overlaid on top of the image Another approach can be an interpretable data representation eg super-pixels instead of raw pixel values introduced by Ribeiro et al as part of the LIME algorithm cf O O Data and Model Transparency An explainable approach should clearly indicate whether the underlying predictive algorithm and/or the training data should be transparent or they can be opaque This requirement is tightly related to  in particular when we are dealing with ante-hoc explainability approaches In case of model explanations does an explainee need to understand the inner workings of a predictive model When data or predictions are being explained do the data features need to be human understandable in the first place This concept in turn is related to Liptons validation approaches discussed in the last paragraph of Section what sort of understanding of the model and/or features is expected of the user For example consider explaining a prediction in terms of a room temperature vs using a squared sum of a room temperature and its height In cases where the input domain is not human comprehensible the system designer may want to give a list of data transformations as a remedy or choose an example-based explanation For example applying a super-pixel segmentation to an image and using its output as higher-level features that are human-comprehensible can help to explain image classification tasks which is done by LIME O Explanation Audience The intended audience of an explainable method may vary from a domain expert through a requirement of a general knowledge about a problem all the way to a lay audience Considering the type of the domain expertise is also important ML and AI knowledge vs domain knowledge Therefore discussing the level and type of background knowledge required to comprehend an explanation is crucial Some techniques may allow to adjust U and U the size U or the complexity level U of an explanation based on the audience U via some of the usability requirements Section Furthermore the transparency of the features and/or the model cf O should be considered with respect to the intended audience For example consider a system that explains its predictions using natural language sentences Given the language skills of the recipient the system can use sentences of varying grammatical and vocabulary complexity to facilitate their easier comprehension Finally given the cognitive capacity of an explainee the system may be able to adjust the granularity of an explanation to suit the recipients needs For example explaining a disease diagnosis to doctors as opposed to patients or their families One of the goals of decreasing the complexity of an explanation which is sometimes at odds with its fidelity cf U and U may be making it easy enough for the explainees to comprehend it in full hence enable them to simulate the decisive process in vivo ie simultability See the validation requirements discussed in the last paragraph of Section for a detailed description of this concept O Function of the Explanation Every explainability approach should be accompanied by a list of its intended applications Most of them are designed for transparency explaining a component of the ML pipeline to an end user whether it is to support decisions compare model elicit knowledge form a predictive model or the data used to build it or extract a causal relation Nevertheless some of them can also be used to assess accountability of the underlying predictive model eg debug and diagnose it to engender trust or demonstrate its fairness eg disparate treatment via counterfactuals It is important to provide the user with the envisaged and validated deployment context to prevent its misuse which may lead to an unintentional harm when deployed in high-risk applications or autonomous system O Causality vs Actionability Most explanations are not of a causal nature If this is the case this property needs to be explicitly communicated to the users so that they can avoid drawing incorrect conclusions Given an actionable and not causal explanation the users should understand that the action suggested by the explanation will result in say a different classification outcome however interpreting it causally no matter how tempting can lead to inaccurate insights Similarly explanations that are derived from a full causal model should be advertised as causal and used to their full potential This concept is closely related to O which specifies the explanation family O Trust vs Performance All of the explainability approaches should be accompanied by a critical discussion of performance explainability trade-offs that the user has to face By large explainability can improve users trust in a predictive system nevertheless sometimes the decrease in predictive performance that is associated with making a particular system more explainable may not be worth it For example consider a case where making a predictive algorithm explainable renders its predictions to be incorrect most of the time Therefore the user of an explainability method needs to decide whether the main objective of a predictive system is to make it more efficient or learn something from data O Provenance Finally an Explainability Fact Sheet should be translucent about the information that contribute to every explanation outputted by an explainability method Most often provenance of an explanation can be attributed to its reliance on a predictive model achieved via interacting with the black-box model or using its internal parameters glass-box a data set introduced by inspecting or comparing data points originating from one or a mixture of the training evaluation and validation data sets or ideally both a predictive model and a data set An example of a purely model-driven explanation is interpreting a k-means model with its centroids A purely data-driven explanation is for example explaining predictions of a k-nearest neighbours model by accounting only for the  closest neighbours to the data point being explained If possible every explanation should be accompanied by an explainability trace indicating which training data points were influential for a prediction and the role that the model and its parameters played In most of the cases a model-specific explainability algorithm  will heavily rely on internal parameters of the underlying predictive model whereas a model-agnostic approach will rely more on data and behaviour of a predictive model Usability Requirements Here we discuss properties of explanations that are important from an explainees point of view Many of these are grounded in social science research hence whenever applicable making algorithmic explanations feel more natural to the end users regardless of their background knowledge and prior experience with this type of systems or technology in general U Soundness This property measures how truthful an explanation is with respect to the underlying predictive model sometimes called concordance Its goal is to quantify local or global adherence of the explanation to the underlying model If the explanation is of ante-hoc type this property does not apply as both the explanation and the prediction are derived using the same algorithm However explanations based on post-hoc or mimic approaches should measure this property to quantify the error introduced by the explainability technique This can be done by calculating a selected performance metric between the outputs of predictive and explanatory models for example average rank correlation between the two A high value of such a metric would ensure the user that an explanation is consistent and aligned with predictions of the underlying model This requirement can also be understood as truthfulness of an explanation Grice et al have noted in their maxim of quality which is one of the rules for cooperative communication that a user should only be presented with claims supported by evidence Soundness is usually one of the two explanation fidelity measures with the other one being completeness U U Completeness For an explanation to be trusted it also needs to generalise well beyond the particular case in which it was produced This mostly applies to local and cohort explanations as the user may want to apply insights learnt from one of these explanations to a similar case pars pro toto Completeness measures how well does an explanation generalise hence to what extent it covers the underlying predictive model It can be quantified by checking correctness of an explanation across similar data points individuals across multiple groups within a data set In particular a support set the number of instances to which the explanation applies divided by the total number of instances can be used to measure completeness Given a context-dependent nature of this metric there is no silver bullet to assess how well an explanation encompasses the model In addition to soundness this is the second explanation fidelity metric U Contextfullness If there are known issues with an explanation completeness a user may not trust it To overcome this and help the user better understand how an explanation can be generalised it can be framed in a context thereby allowing the user to assess its soundness and completeness For example the user will better understand the limitations of an explanation if it is accompanied by all the necessary conditions for it to hold critiques ie explanation oddities and its similarities to other cases Contextfullness can help to make a local explanation either explicitly local allow the explainee to safely generalise it to a cohort-based explanation or even indicate that despite it being derived for a single prediction it can be treated as a global one A specific quantitative case of this property called representativeness aims to measure how many instances in a validation data set does a single explanation cover Another aspect of contextfullness is the degree of importance for each factor contributing to an explanation For example if an explanation is supported by three causes how important are they individually One observation worth noting is that an order in which they are presented rarely ever indicates their relative importance eg a list of conditions in a decision rule This usability requirement can also be compared to the maxim of manner from the rules for cooperative communication that entails being as clear as possible in order to avoid any ambiguity that may lead to a confusion at any point U Interactiveness Given a wide range of explainees experience and background knowledge one explanation cannot satisfy their wide range of expectations Therefore to improve the overall user experience the explanation process should be controllable For example it should be reversible in case the user inputs a wrong answer respect users preferences and feedback be social bidirectional communication is preferred to one-way information offloading allow to adjust the granularity of an explanation and be interactive This means that whenever possible the users should be able to customise the explanation that they get to suit their needs For example if the system explains its decisions with counterfactual statements and a foil used in such a statement does not contain information that the users are interested in they should be able to request an explanation that contains this particular foil if one exists U Actionability When an explanation is provided to help users understand a reason behind a decision then the users prefer explanations that they can treat as guidelines towards the desired outcome For example in a banking context given an explanation based on counterfactual statements it is better from the users perspective to get a statement conditioned on a number of active loans rather than the users age The first one provides the user with an action towards the desired outcome ie pay back one of the loans before reapplying while the latter leaves the user without any options U Chronology Some aspects of an explanation may have inherent time ordering for example loans taken by a borrower In such cases if one of the reasons given in an explanation has a time-line associated with it users prefer explanations that account for more recent events as their cause ie proximal causes For example consider multiple events of the same type contributing equally to a decision an applicant has three current loans and will not be given a new one unless one of the outstanding ones is paid back Paying back any of the three loans is a sufficient explanation however from the users perspective taking the most recent loan is a more natural reason behind the unfavourable loan application outcome then having any of the first two loans U Coherence Many users of explainable systems can have a prior background knowledge and beliefs about the matter that is being predicted and explained their mental model of the domain Therefore any explainable system should be consistent with the explainees prior knowledge which can only be achieved when the explainees mental model is part of the explainability system U as otherwise there is nothing to be coherent with A mental model can either be functional ie shallow in which case the end users know how to interact with something but not how it works in detail or structural ie deep in which case they have a detailed understanding of how and why something works If the prediction of a model is consistent with the users expectations then the reasoning behind it will not matter most of the time unless its logic is fundamentally flawed internal inconsistency or it is at odds with the general knowledge humans tend to ignore information that is inconsistent with their beliefs confirmation bias On the other hand if the output of a predictive model is not what the users expect they will contrast the explanation against their mental model to understand the prediction in which case the explainer should identify and fill in these knowledge gaps Therefore if an explanation uses arguments that are consistent with the users beliefs they will be more likely to accept it This property is highly subjective however a basic coherence with the universal laws for example number ordering should be satisfied U Novelty Providing users with a mundane or expected explanation should be avoided Explanations should contain surprising or abnormal characteristics that have low probability of happening eg a rare feature value to point the users attention in an interesting direction recall U where anomalies prompt the user to request an explanation However this objective requires balancing the trade-off between coherence with the explainees mental model novelty and overall plausibility For example consider an ML system where explanations help to better understand a given phenomenon In this scenario providing the users with explanations that highlight relations that they are already aware of should be avoided The explainees background knowledge should be considered before producing an explanation to make sure that it is novel and surprising at the same time consistency with their mental model should be preserved as long as it is correct Again this usability criterion can only be built on top of the explainees mental model since this knowledge is essential for assessing novelty of causes U Complexity Given a wide spectrum of skills and background knowledge of explainees the complexity of explanations should be tuned to the recipient This can be an operational property of an explainable system O however it is also important to consider it from a user perspective If the system does not allow for explanation complexity to be adjusted by the user it should be as simple as possible by default unless the explainee explicitly asks for a more complex one For example given an explanation of an automated medical diagnosis it should use observable symptoms rather than the underlying biological processes responsible for the condition Choosing the right complexity automatically may only be possible given the availability of the explainees mental model U Personalisation Adjusting an explanation to users requires the explainability technique to model their background knowledge and mental model This is particularly important when attempting to adjust complexity of an explanation U its novelty U and coherence U An explanation can either be personalised on-line via an interaction or off-line by incorporating the necessary information into the model eg parameterisation or data Personalising an explanation is related to another rule of cooperative communication the maxim of relation According to this rule a communication should only relay information that are relevant and necessary at any given point in time Therefore an explainability system has to know what the user knows and expects to determine the content of the explanation U Parsimony Finally explanations should be selective and succinct enough to avoid overwhelming the explainee with unnecessary information ie fill in the most gaps with the fewest arguments This is somewhat connected to the novelty U of an explanation as it can be partially achieved by avoiding premisses that an explainee is already familiar with Furthermore parsimony can be used as a tool to reduce complexity U of an explanation regardless of the explainees background knowledge For example brevity of a counterfactual explanation can be achieved by giving as few reasons number of conditions in the statements foil as possible This requirement is also related to another rule of cooperative communication presented by Grice et al The maxim of quantity states that one should only communicate as much information as necessary and no more a partial explanation Safety Requirements Explainability methods tend to reveal partial information about the data set used to train predictive models these models internal mechanics or parameters and their prediction boundaries Therefore Explainability Fact Sheets aim to consider the effect of explainability on robustness security and privacy aspects of predictive systems which they are built on top of as well as the robustness of explanations themselves S Information Leakage Every explainability approach should be accompanied by a critical evaluation of its privacy and security implications and a discussion about mitigating these factors It is important to consider how much information an explanation reveals about the underlying model and its training data For example consider a counterfactual explanation applied to a logical machine learning model given that this model family applies precise thresholds to data features this type of an explanation is likely to leak them On the other hand explanations for a k-nearest neighbours model can reveal training data points and for a support vector machine these could be data points on the support vectors We could partially mitigate this threat by increasing the parsimony of explanations U producing explanations for aggregated data or obfuscating the exact thresholds or data points for example by k-anonymising the data outputting fuzzy thresholds in the explanations or providing general directions of change eg slightly more to avoid giving out the exact values Another example can be a security trade-off between ante-hoc approaches that reveal information about the predictive model itself and local post-hoc explanations that can only leak behaviour of the decision boundary in the neighbourhood of a selected point S Explanation Misuse With information leakage in mind one can ask how many explanations and of how many different data points does it take to gather enough insight to steal or game the underlying predictive model This can be a major concern especially if the predictive model is a trade secret Furthermore explanations can be used by adversaries to game a model consider a case where a malicious user was able to find a bug in the model by inspecting its explanations hence is now able to take advantage of it This observation indicates a close relation between explanations and adversarial attacks This requirement is closely linked to a consideration of the intended application of an explainability system O a system designed as a certification tool will usually reveal more information than one providing explanations to customers Therefore having clear target audience in mind O is crucial S Explanation Invariance Given a phenomenon modelled by a predictive system data that we gather are just a way to quantify its observed effects The objective of a predictive system should be to elicit insights about the underlying phenomenon and the explanations are a medium to foster their understanding in a human comprehensible context Ideally explanations should be based on a property of the underlying phenomenon rather than an artefact dependent on a predictive model In this setting it is natural to expect an explainability system to be consistent Explanations of similar data points should be similar for a fixed model training data and procedure and explanations of a fixed data point should be comparable across different predictive models or different training runs of the same model trained using the same data stable An explainability approach should provide the same explanation given the same inputs model and/or data point This could be measured by investigating variance of an explanation over multiple executions of an explainability algorithm Ideally explanations produced by one method should be comparable to these produced using another explainability technique given fixed training data If one of these properties is not the case the designer of an explainability algorithm should investigate how model configuration and parameterisation influence explanations that it produces Such inconsistency where the same event is given different often contradictory explanations by different actors explainable algorithms in our case is well documented in social sciences as The Rashmon Effect and should be avoided S Explanation Quality The final safety requirement concerns evaluating the quality and correctness of an explanation with respect to the confidence of the underlying predictive model and the distribution of the training data This requirement is in place as poor predictive performance whether overall or for specific data points usually leads to uninformative explanations After all if a prediction is of poor quality it would be unreasonable to expect the explanation to be sensible We suggest that an explanation be accompanied by a list of uncertainty sources one of which may be the confidence of the predictive model for the instance being explained For example if a method relies on synthetic data as opposed to real data this should be clearly stated as a source of variability hence uncertainty Another example of an explanation that does not convey its quality can be a counterfactual that lies in a sparse region as compared to the distribution of the data set used to train the model Since we have not seen many data points in that region we should not trust the explanation without further investigation Validation Requirements Finally explainable systems should be validated with user studies V or synthetic experiments V in a setting similar to the intended deployment scenario This research area has seen increasing interest in the recent years with Doshi-Velez and Kim providing evaluation criteria and various approaches to validate an explainable system Other researchers highlighted the importance of considering the stakeholders of an explanation before validating it intended application O and audience O Nevertheless there does not appear to be a consensus regarding a validation protocol which hinders the progress of explainable ML research by making explainable methods incomparable A commonly agreed validation protocol could help to eliminate confirmation bias when two explanations are presented side by side mitigate selection bias when a study is carried out via Amazon Mechanical Turk all of the participants are computer-literate and fight a phenomenon called The Illusion of Explanatory Depth to overcome explanation ignorance For example when users are asked to choose the best explainability approach out of all the options presented to them they should not be forced to choose any single one unless they consider at least one of them to be useful When validating interpretability approaches one should also be aware of a phenomenon called Change Blindness humans inability to notice all of the changes in a presented medium This is especially important when the explanatory medium O is an image A good practice in such cases is ensuring that two explanations or the instance to be explained and the explanation are clearly distinguishable by for example highlighting the change U Furthermore a well designed user study could also provide a clear answer to some of the qualitative explanation properties listed in the previous sections For example it can help to evaluate the effectiveness of an explanation for a particular audience O assess the background knowledge necessary to benefit from an explanation O or check the level of technical skills required to use it O as not all explainees may be comfortable with a particular explanatory medium Standardised user studies are not a silver bullet however a protocol such as randomised controlled trails in medical sciences should be in place given that they are the most acceptable approach to validate the explainability powers of a new method Doshi-Velez and Kim identified three types of evaluation approaches application level Validating an explainability approach on a real task with user studies V eg comparing the explanations given by doctors domain experts and an explainability algorithm on X-ray images human level Validating an explainability approach on a simplified task that represents the same domain and a lay audience to avoid using domain experts whose time is often scarce and expensive with user studies V eg an Amazon Mechanical Turk experiment asking the explainees to choose the most appealing explanation given multiple different techniques function level Validating an explainability approach on a proxy task synthetic validation V eg given an already proven explainability method such as explaining decision trees by visualising their structure a proxy can be its measure of complexity tree depth and width A different set of mostly synthetic V validation approaches was proposed by Herman using simulated data with known characteristics to validate correctness of explanations and testing stability and consistency of explanations see the invariance safety requirement S for more details The latter validation approach can be either quantitative V given a well-defined metric or qualitative V given users perceptual evaluation Lipton has come up with three distinct approaches to evaluate how well an explanation is understood based on user studies V simultability Measuring how well a human can recreate or repeat simulate the computational process based on provided explanations of a system for example by asking the explainee a series of counterfactual what-if questions algorithmic transparency Measuring the extent to which a human can fully understand a predictive algorithm its training procedure provenance of its parameters and the process governing its predictions decomposability Quantifying the ability of an explainee to comprehend individual parts and their functionality of a predictive model understanding features of the input parameters of the model eg a monotonic relationship of one of the features and models output EXPLAINABLE SYSTEMS TRADE-OFFS Multiple questions arise when developing an explainability approach for a predictive system and evaluating it based on our list of properties Are they all of equal importance Are they compatible with each other or are some of them at odds In practice many of them cannot be achieved at the same time and their importance often depends on the application area Moreover while selected classes of explainability methods eg counterfactuals may be flexible enough to comply with most of the properties in theory some of them can be lost in implementation due to particular algorithmic choices While both functional and operational requirements are properties of a specific explanation methodology and its implementation the usability desiderata are general properties and any approach should aim to satisfy all of them For example making an explainability technique model-agnostic forces it to be a post-hoc or a mimic approach and prevents it from taking advantage of specifics of a particular model implementation Furthermore such approaches create an extra layer of complexity on top of the predictive model as opposed to ante-hoc techniques that can be detrimental to fidelity of the explanation a trade-off between completeness and soundness that is common to model-agnostic approaches Lombrozo points out that explanations that are simpler U ie with fewer causes more general U and coherent U are usually more appealing to humans however depending on the application O and the target audience O this may not always be desirable When considering coherence of an explanation we may run into difficulties defining the complement of the concept being explained as none-concepts are usually ill-defined eg What is not-a-car Kulesza et al show that both completeness U and soundness U are important however if faced with a trade-off one should choose the first over the latter which as pointed out by Eiband et al is not universal and largely depends on the application domain Similarly Walton argues that users prefer explanations that are more plausible U consistent with multiple outcomes ie explain many things at once U and simple U U While daunting all of these incompatibilities are important to consider as they can help to identify and make informed choices about the trade-offs that every explainability method is facing In particular vanilla counterfactual explanations prioritise completeness over soundness as they are always data point-specific Nevertheless Miller shows that in theory counterfactuals which he considers as the most human-friendly explanations since they are contrastive and answer a Why question can satisfy most of the desiderata and the aforementioned example is an artefact of algorithmic implementations Moreover he shows that based on social sciences research some of the properties of explainable systems should be prioritised necessary causes  are preferred to sufficient ones intentional actions U U form more appealing explanations than those taken without deliberation the fact and the foil of a counterfactual explanation should be clearly distinguishable U short and selective U explanations are preferred to complete ones the social context U U O should drive the content and the nature of an explanation and one explanation covering multiple phenomena U is preferred to multiple unique explanations Some of these properties can be employed to achieve more than one goal For example completeness can be partially achieved by having contextfull explanations If an explainability system is not inherently interactive this requirement can be achieved by deploying it within an interactive platform such as a dialogue system for explanations delivered in natural language or interactive web page for visualisations Actionability and chronology are usually data set-specific and can be achieved by manually annotating features that are actionable and ordering the time-sensitive ones Personalisation along with coherence novelty and complexity that all depend on it is the most difficult criterion to be satisfied On one hand we can argue that the complexity as well as novelty and coherence of an explanation can be adjusted by personalising it either by system design O O O through user interaction O U or with parsimony U Alternatively we can imagine encoding a hierarchy of explanation complexity based on the users mental model and utilising this heuristic to serve explanations of desired complexity DISCUSSION Systematically evaluating properties of explainability techniques can be a useful precursor to user studies eg helping to design them to show their capabilities and compliance with the best practices in the field Furthermore despite theoretical guarantees of selected desiderata for some explainability approaches these guarantees can be lost in implementation For example model-agnostic explainers can render some desiderata difficult to achieve since these approaches cannot take advantage of model-specific aspects of predictive algorithms LIME has recently been subjected to studies aiming to validate its usability which discovered lack of stability for its explanations S and shortcomings of their locality U hence raising questions about the validation methods V used to evaluate this technique in the first place We concur that had a list of requirements such as one presented in this paper been available some of these issues could have been avoided To support this claim we show an example of such a Fact Sheet as a supplementary material distributed with this paper which closely inspects properties of the LIME algorithm along all the five dimensions Target Audience We argue that a comprehensive list of requirements for explainability systems spanning both computational and social aspects of thereof is needed amid lack of a general consensus in this space among researchers and practitioners Some papers discuss a subset of the requirements presented here with many of them being scattered throughout explainability literature hence rendering it difficult to get a coherent and complete view on the matter Having all of the requirements in one place will benefit designers researchers and users of explainability systems by enabling them to use the Fact Sheets to guide their development implementation evaluation and comparison in a systematic and consistent manner identify the gaps between their theoretical capabilities and divergence from those for a given implementation and quickly grasp properties of a given method to choose an appropriate approach for a desired use case given the unified design of the Fact Sheets as with food nutrition labels the users know what to expect and how to navigate them In addition to serving these two communities and associated with them use cases our Fact Sheets can be utilised as a reporting medium aimed at regulators and certification bodies by providing them with the necessary information in a structured and comparable way Given the structure of our Fact Sheets browsing through them should be sufficient to make explainability systems more appealing and transparent to a wider public eg a non-technical audience Delivery Format and Medium We chose to present the explainability requirements in the form of a self-reported fact sheet because we want to empower the designers and users of explainability algorithms to make better choices and consider a wide spectrum of functional operational usability safety and validation aspects when building or using them Such a flexible and comprehensive structure makes them suitable for a wide range of applications and allows their users to take as much or as little as they want from them rather than feel obliged to complete them in their full length and complexity To indicate this flexibility we opted for the use of the fact sheet term as opposed to using standards guidelines or recommendations We suggest that our requirements list can form a future basis of such standards or recommendations however we are not in a position to enforce this hence we leave this task to bodies such as IEEE and their Global Initiative on Ethics of Autonomous and Intelligent Systems which has already produced recommendations for Transparency and Autonomy Furthermore posing our requirements list as a standard reporting tool could undermine its adoption as enforcing standards may impede the pace of explainability research In addition to clear transparent and well defined structure and content framing our requirements list as a fact sheet has one major advantage it can be used as a development and deployment checklist for explainability approaches It has been shown that even experienced practitioners can make obvious mistakes despite their presence of mind especially under stressful conditions or simply due to the repetitive nature of a task Checklists have been shown to eradicate most of such trivial and often dangerous human errors for example a checklist that helps to account for all the tools used during a surgical procedure after it is finished A similar line of reasoning can be applied to designing and deploying explainability algorithms however instead of a checklist the user is provided with a fact sheet to aid critical evaluation of explainability algorithm capabilities and draw attention to its features that may have been overlooked Given the evolving nature of our Fact Sheet requirements list and the Fact Sheets themselves we propose to host them on-line within a single repository eg a web page hosted on GitHub Since the requirements can change over time as new research is published hosting the Fact Sheet template and explainability method-specific Fact Sheets on-line will enable their natural evolution versioning dissemination and revision supported by community contributions which can be peer-reviewed following a process similar to OpenReview All in all we hope that such collection of Fact Sheets and guidelines for their creation will become a go-to resource for learning about transparency of predictive systems Creation Responsibility The effort required to create such a Fact Sheet may seem prohibitively time consuming hence hindering their widespread adoption however their creation can be incremental and is not limited to the creator of an explainability technique Moreover creation of these Fact Sheets is not required during the development of explainability techniques since they can be composed post-hoc Nevertheless we suggest using our Fact Sheet template throughout the development of such approaches as a guideline and a checklist to ensure best practices The process of their creation can be further sped up by allowing the entire explainable AI community to contribute which may even lead to improving the method itself by spotting its straightforward extensions All of this is possible because our Fact Sheets can be retrofitted since they only require familiarity with the approach or its implementation unlike similar solutions for data sets requiring knowledge of the data collection process or AI services usually treated as trade secrets All things considered we argue that researchers designing such methods and software engineers implementing them are best suited and would benefit most from composing Explainability Fact Sheets Dimensions and Requirements Selection Some of the requirements may seem very similar or strongly related to one another at first glance hence their choice may seem arbitrary with some arguing to merge or reorganise them One reason for repetitiveness of selected concepts is the fact that the requirements span five different dimensions with some of them being presented from a social sciences perspective eg users perception while others are rather technical eg deployment performance Another reason for the fine detail is ensuring versatility of our Fact Sheets in their current form they are applicable to both inherently transparent predictive algorithms as well as standalone interpretability techniques One could imagine a Fact Sheet discussing how to interpret linear models with their parameters thereby highlighting caveats such as feature normalisation and independence assumptions Such an elaborate structure will also allow the users to quickly browse through the headings without delving into their details hence make them more appealing and accessible Many requirements presented in this paper originate from a diverse XAI literature academic and on-line articles and have proven to be of value in multiple instances both theoretical and practical The higher-level categorisation dimensions is role-driven eg deployment certification and users perception When composing the list we were as comprehensive as possible to avoid bias and our intervention was limited to grouping together similar concepts presented under different names All things considered we acknowledge that our requirements list by no means should be treated as final and definitive We hope to validate and revise it over time based on the feedback provided by its users who create and deploy explainability solutions and the XAI community Despite a well defined list of requirements preparing an exhaustive Fact Sheet for any single explainability approach is a labour and time-consuming challenge While some of these properties are purely analytical others are empirical We hence identify two approaches to validate them quantitative that should be measured or can be precisely and definitely answered by assertion and qualitative that should be defined in the context of a given explainability approach and either justified by a critical discussion informal argument or validated with user studies given that they may lack a unique answer be subjective and be difficult to measure This lack of a correct answer however should not be held against the Fact Sheets as even a qualitative discussion of fuzzy properties that cannot be operationalised is of paramount importance in advancing transparency of explainability approaches hence clarifying their aspects which cannot be directly measured eg describing and justifying the chosen validation procedure allows the users to assess suitability of a given approach RELATEDWORK The recent surge in interpretability and explainability research in AI may suggest that this is a new research topic but in fact human explainability has been an active research area for much longer in the humanities This observation has encouraged Miller to review interpretable approaches in the AI literature using insights from the social sciences which show how human-centred design can benefit AI interpretability To date a scattered landscape of properties desired of explainable systems has been presented in a wide range of literature that proposes to evaluate explanations by defining their properties and interaction protocol Despite all of these researchers converging towards a coherent list of properties desired of explainable systems none of them collected and organised such a systematic list to serve as a guideline for other people interested in the topic At best some studies discuss a subset of explainable system requirements supported by illustrative examples however their main aim is to familiarise the readers with the concepts and not provide them with an evaluation framework Alternatively organisations such as IEEE attempt to develop standards for transparency of autonomus systems Other studies select a subset of properties and use them to evaluate a selected approach for a given task for example Kulesza et al evaluate interactive visualisations for music recommendations with respect to explanation fidelity soundness and completeness and Kulesza et al examine interactive visualisations for Naïve Bayes classification of emails with respect to fidelity interaction parsimony and actionability Lakkaraju et al mathematically define some of the desiderata eg soundness and completeness to enable quantitative evaluation of their interpretability method User studies are often considered the gold standard in explainability research Doshi-Velez and Kim have come up with guidelines and best practices for evaluating effectiveness of explainability techniques from the perspective of user studies and synthetic validation and others considered validation of explainable approaches by focusing on their audience Nevertheless some research suggests that user studies cannot fully assess the effectiveness of an explanation due to a phenomenon called The Illusion of Explanatory Depth or they can yield unexpectedly positive results as simply presenting an explanation to people makes them believe that it is more likely to be true than not Another approach towards clarifying explainability properties in ML is self-reporting and certification Approaches such as data statements data sheets for data sets and nutrition labels for data sets can help to characterise a data set in a coherent way Kelley et al argued for a similar concept nutrition labels for privacy to assess privacy of systems that handle personal and sensitive information All of these methods revolve around recording details about the data themselves eg the units of features the data collection process and their intended purpose Other researchers argued for a similar approach for predictive models model cards for model reporting nutrition labels for rankings and algorithmic impact assessment forms Finally Arnold et al suggested fact sheets for ML services to communicate their capabilities constrains biases and transparency CONCLUSIONS AND FUTURE WORK In this paper we collated and discussed a list of functional operational and usability characteristics of explainability techniques for predictive systems We also characterised safety security privacy and robustness properties of explanations and discussed their validation methods eg user studies We showed how each property can be used to systematically evaluate explainable ML approaches and discussed their trade-offs Based on these characteristics we propose that explainability approaches are accompanied and assessed by means of Explainability Fact Sheets an example of which is provided as a supplementary material distributed with this paper We are currently working on creating these Fact Sheets for popular explainability techniques such as surrogates and counterfactuals We will provide a review of their implementations to investigate discrepancies between their theoretical and algorithmic capabilities starting with LIME for surrogates see the supplementary material and followed by an optimisation method proposed by Wachter et al for counterfactuals We will publish them on a website where XAI developers and users can also submit their own Explainability Fact Sheets More broadly our future work will aim at evaluating explainability trade-offs in more depth