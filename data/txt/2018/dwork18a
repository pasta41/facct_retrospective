Decoupled Classifiers for Group-Fair and Efficient Machine Learning
Proceedings of Machine Learning Research 81:1â€“15, 2018 Conference on Fairness, Accountability, and Transparency
Cynthia Dwork dwork@seas.harvard.edu
Harvard University
Nicole Immorlica nicimm@microsoft.com
Microsoft Research New England
Adam Tauman Kalai adam.kalai@microsoft.com
Microsoft Research New England
Max Leiserson mdml@cs.umd.edu
University of Maryland
Editors: Sorelle A. Friedler and Christo Wilson
ABSTRACT
When it is ethical and legal to use a sen-
sitive attribute (such as gender or race) in
machine learning systems, the question re-
mains how to do so. We show that the
naÌˆÄ±ve application of machine learning algo-
rithms using sensitive attributes leads to
an inherent tradeoff in accuracy between
groups. We provide a simple and efficient
decoupling technique, which can be added
on top of any black-box machine learning
algorithm, to learn different classifiers for
different groups. Transfer learning is used
to mitigate the problem of having too little
data on any one group.
1 INTRODUCTION
As algorithms are increasingly used to make de-
cisions of social consequence, the social values
encoded in these decision-making procedures are
the subject of increasing study, with fairness
being a chief concern (Pedreschi et al., 2008;
Zliobaite et al., 2011; Kamishima et al., 2011;
Dwork et al., 2011; Friedler et al., 2016; Angwin
et al., 2016; Chouldechova, 2017; Kleinberg et al.,
2016; Hardt et al., 2016; Joseph et al., 2016; Kus-
ner et al., 2017; Berk, 2009). Classification and
regression algorithms are one particular locus of
fairness concerns. Classifiers map individuals to
outcomes: applicants to accept/reject/waitlist;
adults to credit scores; web users to advertise-
ments; felons to estimated recidivism risk. In-
ğ‘¥2
ğ‘¥1
Figure 1: No linear classifiers can achieve greater
than 50% accuracy on both groups.
formally, the concern is whether individuals are
treated â€œfairly,â€ however this is defined. Still
speaking informally, there are many sources of
unfairness, prominent among these being train-
ing the classifier on historically biased data and
a paucity of data for under-represented groups
leading to poor performance on these groups,
which in turn can lead to higher risk for those,
such as lenders, making decisions based on clas-
sification outcomes.
Should ML systems use sensitive attributes,
such as gender or race if available? The legal
and ethical factors behind such a decision vary
by time, country, jurisdiction, culture, and down-
stream application. Still speaking informally, it
is known that â€œignoringâ€ these attributes does
not ensure fairness, both because they may be
closely correlated with other features in the data
and because they provide context for understand-
cÂ© 2018 C. Dwork, N. Immorlica, A.T. Kalai & M. Leiserson.
Decoupled Classifiers for Group-Fair and Efficient Machine Learning
ing the rest of the data, permitting a classifier
to incorporate information about cultural differ-
ences between groups (Dwork et al., 2011). Using
sensitive attributes may increase accuracy for all
groups and may avoid biases where a classifier
favors members of a minority group that meet
criteria optimized for a majority group, as illus-
trated visually in Figure 4 of 8.
In this paper, we consider how to use a sen-
sitive attribute such as gender or race to maxi-
mize fairness and accuracy, assuming that it is
legal and ethical. A data scientist wishing to fit,
say, a simple linear classifier, may use the raw
data, upweight/oversample data from minority
groups, or employ advanced approaches to fit-
ting linear classifiers that aim to be accurate and
fair. No matter what he does and what fairness
criteria he uses, assuming no linear classifier is
perfect, he may be faced with an inherent trade-
off between accuracy on one group and accuracy
on another. As an extreme illustrative example,
consider the two group setting illustrated in Fig-
ure 1, where feature x1 perfectly predicts the bi-
nary outcome y âˆˆ {âˆ’1, 1}. For people in group 1
(where x2 = 1), the majority group, y = sgn(x1),
i.e., y = 1 when x1 > 0 and âˆ’1 otherwise. How-
ever, for the minority group where x2 = 2, ex-
actly the opposite holds: y = âˆ’sgn(x1). Now, if
one performed classification without the sensitive
attribute x2, the most accurate classifier predicts
y = sgn(x1), so the majority group would be per-
fectly classified and the minority group would be
classified as inaccurately as possible. However,
even using the group membership attribute x2,
it is impossible to simultaneously achieve better
than 50% (random) accuracy on both groups.
This is due to limitations of a linear classifier
sgn(w1x1 + w2x2 + b), since the same w1 is used
across groups.
In this paper we define and explore decoupled
classification systems, in which a separate1 classi-
fier is trained on each group. Training a classifier
involves minimizing a loss function that penal-
izes errors; examples include mean squared loss
and absolute loss. In decoupled classification sys-
1. In the case of linear classifiers, training separate clas-
sifiers is equivalent to adding interaction terms be-
tween the sensitive attributes and all other attributes.
More generally, the separate classifiers can equiva-
lently be thought of as a single classifier that branches
on the group attribute. The decoupling technique is a
simple way to add branching to any type of classifier.
tems one first obtains, for each group separately,
a collection of classifiers differing in the numbers
of positive classifications returned for the mem-
bers of the given group. Let this set of outputs
for group k be denoted Ck, k = 1, . . . ,K. The
output of the decoupled training step is an ele-
ment of C1 Ã— . . .Ã—CK , that is, a single classifier
for each group. The output is chosen to minimize
a joint loss function that can penalize differences
in classification statistics between groups. Thus
the loss function can capture group fairness prop-
erties relating the treatment of different groups,
e.g., the false positive (respectively, false nega-
tive) rates are the same across groups; the de-
mographics of the group of individuals receiving
positive (negative) classification are the same as
the demographics of the underlying population;
the positive predictive value is the same across
groups.2 By pinning down a specific objective,
the modeler is forced to make explicit the tradeoff
between accuracy and fairness, since often both
cannot simultaneously be achieved. Finally, a
generalization argument relates fairness proper-
ties, captured by the joint loss on the training set,
to similar fairness properties on the distribution
from which the data were drawn. We broaden our
results so as to enable the use of transfer learning
to mitigate the problems of low data volume for
minority groups.
The following observation provides a property
essential for efficient decoupling. A profile is a
vector specifying, for each group, a number of
positively classified examples from the training
set. For a given profile (p1, . . . , pK), the most
accurate classifier also simultaneously minimizes
the false positives and false negatives. It is the
choice of profile that is determined by the joint
loss criterion. We show that, as long as the joint
loss function satisfies a weak form of monotonic-
ity, one can use off-the-shelf classifiers to find a
decoupled solution that minimizes joint loss.
The monotonicity requirement is that the joint
loss is non-decreasing in error rates, for any fixed
profile. This sheds some light on the thought-
provoking impossibility results of Chouldechova
(2017) and Kleinberg et al. (2016) on the impos-
sibility of simultaneously achieving three specific
2. In contrast individual fairness Dwork et al. (2011) re-
quires that similar people are treated similarly, which
requires a task-specific, culturally-aware, similarity
metric.
notions of group fairness (see Observation 1 in
Section 4.1).
Finally, we present experiments on 47 datasets
downloaded from http://openml.org. The ex-
periments are â€œsemi-syntheticâ€ in the sense that
the first binary feature was used as a substitute
sensitive feature since we did not have access to
sensitive features. We find that on many data
sets our algorithm improves performance while
much less often decreasing performance.
Remark. The question of whether or not to use
decoupled classifiers is orthogonal to our work,
which explores the mathematics of the approach,
and a comprehensive treatment of the pros and
cons is beyond our expertise. Most importantly,
we emphasize that decoupling, together with a
â€œpoorâ€ choice of joint loss, could be used un-
fairly for discriminative purposes. Furthermore,
in some jurisdictions using a different classifica-
tion method, or even using different weights on
attributes for members of demographic groups
differing in a protected attribute, is illegal for
certain classification tasks, e.g. hiring. Even bar-
ring legal restrictions, the assumption that group
membership is an input bit is an oversimplifica-
tion, and in reality the information may be ob-
scured, and the definition of the groups may be
ambiguous at best. Logically pursuing the idea
behind the approach it is not clear which inter-
sectionalities to consider, or how far to subdi-
vide. Nonetheless, we believe decoupling is valu-
able and applicable in certain settings and thus
merits investigation.
The contributions of this work are: (a) show-
ing how, when using sensitive attributes, the
straightforward application of many machine
learning algorithms will face inherent tradeoffs
between accuracy across different groups, (b) in-
troducing an efficient decoupling procedure that
outputs separate classifiers for each class using
transfer learning, (c) modeling fair and accurate
learning as a problem of minimizing a joint loss
function, and (d) presenting experimental results
showing the applicability and potential benefit of
our approach.
1.1. Related Work
Group fairness has a variety of definitions, in-
cluding conditions of statistical parity, class bal-
ance and calibration. In contrast to individual
fairness, these conditions constrain, in various
ways, the dependence of the classifier on the sen-
sitive attributes. The statistical parity condition
requires that the assigned label of an individ-
ual is independent of sensitive attributes. The
condition formalizes the legal doctrine of dis-
parate impact imposed by the Supreme Court
in Griggs v Duke Power Company. Statistical
parity can be approximated by either modifying
the data set or by designing classifiers subject
to fairness regularizers that penalize violations
of statistical parity (see Feldman et al. (2015)
and references therein). Dwork et al. (2011) pro-
pose a â€œfair affirmative actionâ€ methodology that
carefully relaxes between-group individual fair-
ness constraints in order to achieve group fair-
ness. Zemel et al. (2013) introduce a representa-
tional approach that attempts to â€œforgetâ€ group
membership while maintaining enough informa-
tion to classify similar individuals similarly; this
approach also permits generalization to unseen
data points. To our knowledge, the earliest work
on trying to learn fair classifiers from histori-
cally biased data is by Pedreschi et al. (2008);
see also (Zliobaite et al., 2011) and (Kamishima
et al., 2011).
The class-balanced condition (called error-rate
balance by Chouldechova (2017) or equalized odds
by Hardt et al. (2016)), similar to statistical par-
ity, requires that the assigned label is indepen-
dent of sensitive attributes, but only conditional
on the true classification of the individual. For
binary classification tasks, a class-balanced clas-
sifier results in equal false positive and false neg-
ative rates across groups. One can also modify
a given classifier to be class-balanced while min-
imizing loss by adding label noise (Hardt et al.,
2016).
The well-calibrated condition requires that,
conditional on their label, an equal fraction of
individuals from each group have the same true
classification. A well-calibrated classifier labels
individuals from different groups with equal ac-
curacy. Hebert-Johnson et al. (2017) extend cal-
ibration to multi-calibration which requires the
classifier to be well calibrated on a collection of
sets of individuals, eg, all those described by cir-
cuits of a given size. The class-balanced solu-
tion (Hardt et al., 2016) also fails to be well-
calibrated. Chouldechova (2017) and Kleinberg
et al. (2016) independently showed that, except
in cases of perfect predictions or equal base rates
of true classifications across groups, there is no
class-balanced and well-calibrated classifier.
A number of recent works explore causal
approaches to defining and detecting
(un)fairness (Nabi and Shpitser, 2017; Kus-
ner et al., 2017; Bareinboim and Pearl, 2016;
Kilbertus et al., 2017). See the beautiful primer
of Pearl et al. (2016) for an introduction to the
central concepts and machinery.
Finally, we mention that sensitive attributes
are used in various real-world systems. As one
example, Hassidim et al. (2017) describe using
such features in an admissions matching system
for masters students in Israel.
2 PRELIMINARIES
Let X = X1 âˆª X2 âˆª . . . âˆª XK be the set of pos-
sible examples partitioned by group. The set of
possible labels is Y and the set of possible classifi-
cations is Z. A classifier is a function c : X â†’ Z.
We assume that there is a fixed family C of clas-
sifiers. For simplicity, we restrict our analysis the
case of binary classification Y = Z = {0, 1}, but
many of the results extended directly to regres-
sion or randomized classification Y,Z âŠ† R.
We suppose that there is a joint distribu-
tion D over labeled examples x, y âˆˆ X Ã— Y
and we have access to n training examples
(x1, y1), . . . , (xn, yn) âˆˆ X Ã— Y drawn indepen-
dently from D. We denote by g(x) the group
number to which x belongs and gi = g(xi), so
xi âˆˆ Xgi .
Finally, as is common, we consider the loss
`D(c) = Ex,yâˆ¼D[`(y, c(x))] for an application-
specific loss function ` : Y Ã—Z â†’ R where `(y, z)
accounts for the cost of classifying as z an exam-
ple whose true label is y. The group-k loss for
D, c is defined to be `Dk(c) = ED[`(y, c(x))|x âˆˆ
Xk] or 0 if D assigns 0 probability to Xk. The
standard approach in ML is to minimize `D(c)
over c âˆˆ C. Common loss functions include the
L1 loss `(y, z) = |y âˆ’ z| and L2 loss `(y, z) =
(yâˆ’ z)2. In Section 4, we provide a methodology
for incorporating a range of fairness notions into
loss.
3. Decoupling and the cost of
coupling
For a vector of K classifiers, c = (c1, c2, . . . , cK),
the decoupled classifier Î³c : X â†’ Z is defined to
be Î³c(x) = cg(x)(x). The set of decoupled clas-
sifiers is denoted Î³(C) = {Î³c | c âˆˆ CK}. Some
classifiers, such as decision trees of unbounded
size over X = {0, 1}d, are already decoupled, i.e.,
Î³(C) = C. As we shall see, however, in high di-
mensions common families of classifiers in use are
coupled to avoid the curse of dimensionality.
The cost of coupling of a family C of classifiers
(with respect to `) is defined to be the worst-case
maximum of the difference between the loss of the
most accurate coupled and decoupled classifiers
over distributions D.
cost-of-coupling(C, `) =
max
Dâˆˆâˆ†(XÃ—Y)
[
min
câˆˆC
`D(c)âˆ’ min
Î³câˆˆÎ³(C)
`D(Î³c)
]
.
Here âˆ†(S) denotes the set of probability dis-
tributions over set S. To circumvent measure-
theoretic nuisances, we require C,X ,Y to be fi-
nite sets. Note that numbers on digital com-
puters are all represented using a fixed-precision
(bounded number of bits) representation, and
hence all these sets may be assumed to be of finite
(but possibly exponentially large) size.
We now show that the cost of coupling is re-
lated to fairness across groups.
Lemma 1 Suppose cost-of-coupling(C, `) = /c.
Then there is a distribution D such that no mat-
ter which classifier c âˆˆ C is used, there will al-
ways be a group k and a classifier câ€² âˆˆ C whose
group-k loss is at least /c smaller than that of c,
i.e., `Dk(câ€²) â‰¤ `Dk(c)âˆ’ /c.
Proof Let Î³câ€² be a decoupled classifier with min-
imal loss where câ€² = (câ€²1, . . . , c
â€²
K). This loss is a
weighted average (weighted by demography) of
the average loss on each group. Hence, for any c,
there must be some group k on which the loss of
câ€²k is /c less than that of c.
Hence, if the cost of coupling is positive, then the
learning algorithm that selects a classifier faces
an inherent tradeoff in accuracy across groups.
The following theorem shows that the cost of cou-
pling is large (a constant) for linear classifiers and
decision trees; similar arguments exist for other
common classifiers. All remaining proofs are de-
ferred to the full version.
Theorem 2 Fix X = {0, 1}d, Y = {0, 1}, and
K = 2 groups (encoded by the last bit of x). Then
the cost of coupling is at least 1/4 for:
1. Linear regression: Z = R, C = {w Â· x +
b | w âˆˆ Rd, b âˆˆ R}, and `(y, z) = (y âˆ’ z)2
2. Linear separators: Z = {0, 1}, C = {I[w Â·
x + b â‰¥ 0] | w âˆˆ Rd, b âˆˆ R}, and `(y, z) =
|y âˆ’ z|
3. Bounded-size decision trees: For Z =
{0, 1}, C being the set of binary decision trees
of size â‰¤ 2s leaves, and `(y, z) = |y âˆ’ z|
We note that it is straightforward to extend the
above theorem to generalized linear models, i.e.,
functions c(x) = u(w Â·x) for monotonic functions
u : R â†’ R, which includes logistic regression
as one common special case. It is also possible,
though more complex, to provide a lower bound
on the cost of coupling of neural networks, regres-
sion forests, or other complex families of func-
tions of bounded representation size s. In order
to do so, one needs to simply show that the size-
s functions are sufficiently rich in that there are
two different size-s classifiers c = (c1, c2) such
that Î³c has 0 loss (say over the uniform distribu-
tion on X ) but that every single size-s classifier
has significant loss.
4. Joint loss and monotonicity
As discussed, the classifications output by an
ML classifier are often evaluated by their empir-
ical loss 1
n
âˆ‘
i `(yi, zi). To account for fairness,
we generalize loss to joint classifications across
groups. In particular, we consider an application-
specific joint loss LÌ‚ : ([K] Ã— Y Ã— Z)âˆ— â†’ R that
assigns a cost to a set of classifications, where
[K] = {1, 2, . . . ,K} indicates the group number
for each example. A joint loss might be, for pa-
rameter Î» âˆˆ [0, 1]:
LÌ‚
(
ã€ˆgi, yi, ziã€‰ni=1
)
=
Î»
n
nâˆ‘
i=1
|yi âˆ’ zi|+
1âˆ’ Î»
n
Kâˆ‘
k=1
âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£
âˆ‘
i:gi=k
zi âˆ’
âˆ‘
i
zi
âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£ .
The above LÌ‚ trades off accuracy for differences in
number of positive classifications across groups.
For Î» = 1, this is simply L1 loss, while for Î» = 0,
the best classifications would have an equal num-
ber of positives in each group. Joint loss differs
from a standard ML loss function in two ways.
First, joint loss is aware of the sensitive group
membership. Second, it depends on the complete
labelings and is not simply a sum over labels.
Even with only K = 1 group, this captures
situations beyond what is representable by the
sum
âˆ‘
`(yi, zi). A simple example is when one
seeks exactly P positive examples:
LÌ‚
(
ã€ˆgi, yi, ziã€‰ni=1
)
=
{
âˆ‘
|yi âˆ’ zi| if
âˆ‘
zi = P
1 otherwise.
Since 1
n
âˆ‘
|yiâˆ’zi| â‰¤ 1, the 1 ensures that the loss
minimizer will have exactly P positives, if such a
classifier exists in C for the data.
In this section, we denote joint loss LÌ‚ with the
hat notation indicating that it is an empirical ap-
proximation. In the next section we will define
joint loss L for distributions. We denote classifi-
cations by zi rather than the standard notation
yÌ‚i which suggests predictions, because, as in the
above loss, one may choose classifications z 6= y
even with perfect knowledge of the true labels.
For the remainder of our analysis, we hence-
forth consider binary labels and classifications,
Y = Z = {0, 1}. Our approach is general, how-
ever, and our experiments include regression. For
a given ã€ˆxi, yi, ziã€‰ni=1, and for any group k â‰¤ K
and all (y, z) âˆˆ {0, 1}2, recall that the groups are
gi = g(xi) and define:
counts: nk =
âˆ£âˆ£{i | gi = k}
âˆ£âˆ£ âˆˆ {1, 2, . . . , n}
profile: pÌ‚k =
âˆ‘
i:gi=k
zi âˆˆ [0, nk/n]
group losses: Ë†Ì€
k =
âˆ‘
i:gi=k
|zi âˆ’ yi| âˆˆ [0, 1]
Note that the normalization is such that the stan-
dard 0-1 loss is
âˆ‘
k
nk
n
Ë†Ì€
k and the fraction of pos-
itives within any class is n
nk
pÌ‚k.
We note many studied fairness notions, includ-
ing numerical parity, demographic parity, and
false-negative-rate parity can be represented in
a joint loss function. For example, demographic
parity is:
Î»LÌ‚1 + (1âˆ’ Î»)
âˆ‘
k
âˆ£âˆ£âˆ£âˆ£âˆ£pÌ‚k nnk âˆ’ 1
K
âˆ‘
kâ€²
pÌ‚kâ€²
n
nkâ€²
âˆ£âˆ£âˆ£âˆ£âˆ£ .
In many applications there is a different cost
for false positives where (y, z) = (0, 1) and false
negatives where (y, z) = (1, 0). The fractions of
false positives and negatives are defined, below,
for each group k. They can be computed based
on the fraction of positive labels in each group
Ï€k:
Ï€k =
âˆ‘
i:gi=k
yi
FPk =
âˆ‘
i:gi=k
zi(1âˆ’ yi) =
Ë†Ì€
k + pÌ‚k
n
nk
âˆ’ Ï€k
FNk =
âˆ‘
i:gi=k
(1âˆ’ zi)yi =
Ë†Ì€
k + Ï€k âˆ’ pÌ‚k n
nk
(2)
While minimizing group loss Ë†Ì€
k = FPk + FNk in
general does not minimize false positives or false
negatives on their own, the above implies that for
a fixed profile pÌ‚k, the most accurate classifier on
group k simultaneously minimizes false positives
and false negatives. The above can be derived by
adding or subtracting the equations Ë†Ì€
k = FPk +
FNk (since every error is a false positive or a
false negative) and n
nk
pÌ‚k = FPk + (Ï€k âˆ’ FNk)
(since every positive classification is either a false
positive or true positive, and the fraction of true
positives from group k are Ï€k âˆ’ FNk). We also
define the false negative rate FNRk = FNk/Ï€k.
False positive rates can be defined similarly.
Equations (1) and (2) imply that, if one de-
sires fewer false positives and false negatives (all
other things being fixed), then greater accuracy
is better. That is, for a fixed profile, the most
accurate classifier simultaneously minimizes false
positives and false negatives. This motivates the
following monotonicity notion.
Definition 3 (Monotinicity) Joint loss LÌ‚ is
monotonic if, for any fixed ã€ˆgi, yiã€‰ni=1 âˆˆ ([K] Ã—
Y)âˆ—, LÌ‚ can be written as c(ã€ˆË†Ì€k, pÌ‚kã€‰Kk=1) where c :
[0, 1]2K â†’ R is a function that is nondecreasing
in each Ë†Ì€
k fixing all other inputs to c.
That is, for a fixed profile, increasing Ë†Ì€
k can only
increase joint loss. To give further intuition be-
hind monotonicity, we give two other equivalent
definitions.
Definition 4 (Monotonicity) Joint loss LÌ‚ is
monotonic if, for any ã€ˆgi, yi, ziã€‰ni=1 âˆˆ ([K]Ã—Y Ã—
Z)âˆ—, and any i, j where gi = gj, yi â‰¤ yj and
zi â‰¤ zj: swapping zi and zj can only increase
loss, i.e.,
LÌ‚(ã€ˆgi, yi, ziã€‰ni=1) â‰¤ LÌ‚(ã€ˆgi, yi, zâ€²iã€‰ni=1),
where zâ€² is the same as z except zâ€²i = zj and
zâ€²j = zi.
We can see that if yi = yj then swapping zi and zj
does not change the loss (because the condition
can be used in either order). This means that
the loss is â€œsemi-anonymousâ€ in the sense that
it only depends on the numbers of true and false
positives and negatives for each group. The more
interesting case is when (yi, yj) = (0, 1) where
it states that the loss when (zi, zj) = (0, 1) is
no greater than the loss when (zi, zj) = (1, 0).
Finally, monotonicity can also be defined in terms
of false positives and false negatives.
Definition 5 (Monotonicity) Joint loss LÌ‚ is
monotonic if, for any ã€ˆgi, yi, ziã€‰ni=1 âˆˆ ([K]Ã—Y Ã—
Z)âˆ—, and any alternative classifications zâ€²1, . . . , z
â€²
n
such that, in each group k, the same profile as z
but all smaller or equal false positive rates and
all smaller or equal false negative rates, the loss
of classifications zâ€²i is no greater than that of zi.
Lemma 6 Definitions 3, 4, and 5 of Mono-
tonicity are equivalent.
One may be tempted to consider a simpler no-
tion of monotonicity, such as requiring the loss
with zi = yi to be no greater than that of
zi = 1 âˆ’ yi, fixing everything else. However,
this would rule out many natural monotonic joint
losses LÌ‚, such as demographic parity.
4.1. Discussion: fairness metrics versus
objectives
The monotonicity requirement admits a range of
different fairness criteria, but not all. We do not
mean to imply that monotonicity is necessary for
fairness, but rather to discuss the implications of
minimizing a non-monotonic loss objective. The
following example helps illustrate the boundary
between monotonic and non-monotonic.
Observation 1 Fix K = 2. The following joint
loss is monotonic if and only if Î» â‰¤ 1/2:
(1âˆ’ Î»)(Ë†Ì€
1 + Ë†Ì€
2) + Î»|Ë†Ì€1 âˆ’ Ë†Ì€
2|.
The loss in the above lemma trades off accuracy
for differences in loss rates between groups. What
we see is that monotonic losses can account, to
a limited extent, for differences across groups in
fractions of errors, and related statements can be
made for combinations of rates of false positive
and false negative, inspired by â€œequal oddsâ€ def-
initions of fairness. However, when the weight Î»
on the fairness term exceeds 1/2, then the loss
is non-monotonic and one encounters situations
where one group is punished with lower accuracy
in the name of fairness. This may still be desir-
able in a context where equal odds is a primary
requirement, and one would rather have random
classifications (e.g., a lottery) than introduce any
inequity.
What is the goal of an objective function? We
argue that a good objective function is one whose
optimization leads to favorable outcomes, and
should not be confused with a fairness metric
whose goal is quantify unfairness. Often, a differ-
ent function is appropriate for quantifying unfair-
ness than for optimizing it. For example, the dif-
ference in classroom performance across groups
may serve as a good metric of unfairness, but it
may not be a good objective on its own. The root
cause of the unfairness may have begun long be-
fore the class. Now, suppose that the objective
from the above observation was used by a teacher
to design a semester-long curriculum with the
best intention of increasing the minority groupâ€™s
performance to the level of the majority. If there
is no curriculum that in one semester increases
one groupâ€™s performance to the level of another
groupâ€™s performance, then optimizing the above
loss for Î» > 1/2 leads to an undesirable outcome:
the curriculum would be chosen so as to inten-
tionally misteach students the higher-performing
group of students so that their loss increases to
match that of the other group. This can be see
by rewriting the loss as follows:
(1âˆ’ Î»)(Ë†Ì€
1 + Ë†Ì€
2) + Î»|Ë†Ì€1 âˆ’ Ë†Ì€
2|
= 2Î»max{Ë†Ì€1, Ë†Ì€
2}+ (1âˆ’ 2Î»)(Ë†Ì€
1 + Ë†Ì€
2).
This rewriting illuminates why Î» â‰¤ 1/2 is neces-
sary for monotonicity, otherwise there is a nega-
tive weight on the total loss. Î» = 1/2 corresponds
to maximizing the minimum performance across
groups while Î» = 0 means teaching to the aver-
age, and Î» in between allows interpolation. How-
ever, putting too much weight on fairness leads
to undesirable punishing behavior.
5. Minimizing joint loss on training
data
Here, we show how to use learning algorithm to
find a decoupled classifier in Î³(C) that is opti-
mal on the training data. In the next section, we
show how to generalize this to imperfect random-
ized classifiers that generalize to examples drawn
from the same distribution, potentially using an
arbitrary transfer learning algorithm.
Our approach to decoupling uses a learning
algorithm for C as a black box. A C-learning
algorithm A : (X Ã— Y)âˆ— â†’ 2C returns one or
more classifiers from C with differing numbers
of positive classifications on the training data,
i.e., for any two distinct c, câ€² âˆˆ A
(
ã€ˆxi, yiã€‰ni=1),âˆ‘
i c(xi) 6=
âˆ‘
i c
â€²(xi). In ML, it is common
to simultaneously output classifiers with varying
number of positive classifications, e.g., in com-
puting ROC or precision-recall curves (Davis and
Goadrich, 2006). Also note that a classifier that
purely minimizes errors can be massaged into one
that outputs different fractions of positive and
negative examples by reweighting (or subsam-
pling) the positive- and negative-labeled exam-
ples with different weights.
Our analysis will be based on the assumption
that the classifier is in some sense optimal, but
importantly note that it makes sense to apply the
reduction to any off-the-shelf learner. Formally,
we say A is optimal if for every achievable num-
ber of positives P âˆˆ
{âˆ‘
i c(xi)
âˆ£âˆ£ c âˆˆ C}, it out-
puts exactly one classifier that classifies exactly
P positives, and this classifier has minimal er-
ror among all classifiers which classify exactly P
positives. Theorem 7 shows that an optimal clas-
sifier can be used to minimize any (monotonic)
joint loss
Theorem 7 For any monotonic joint loss func-
tion LÌ‚, any C, and any optimal learner A for C,
Algorithm 1: Decouple (A, LÌ‚, {ã€ˆxi, yiã€‰}, {Xi})
Minimize training loss LÌ‚ using learner A
1. For k = 1 to K, Ck â† A
(
ã€ˆxi, yiã€‰i:xiâˆˆXk
)
. Learner outputs a set of classifiers.
2. return Î³c that minimizes
mincâˆˆC1Ã—...Ã—CK
LÌ‚
(
ã€ˆgi, yi, Î³c(xi)ã€‰ni=1
)
. Î³c(xi) = cgi(x))
The simple decoupling algorithm partitions data
by group and runs the learner on each group.
Within each group, the learner outputs one or
more classifiers of differing numbers of positives.
the Decouple procedure from Algorithm 1 re-
turns a classifier in Î³(C) of minimal joint loss
LÌ‚. For constant K, Decouple runs in time lin-
ear in the time to run A and polynomial in the
number of examples n and time to evaluate LÌ‚ and
classifiers c âˆˆ C.
Implementation notes. Note that if the pro-
file is fixed, as in LÌ‚pâˆ— , then one can simply run
the learning algorithm once for each group, tar-
geted at pâˆ—k positives in each group. Otherwise,
also note that to perform the slowest step which
involves searching over O(nK) losses of combina-
tions of classifiers, one can pre-compute the error
rates and profiles of each classifier. In the â€œbig
dataâ€ regime of very large n, the O(nK) evalua-
tions of a simple numeric function of profile and
losses will not be the rate limiting step.
6. Generalization and transfer
learning
We now turn to the more general randomized
classifier model in which Z = [0, 1] but still with
Y = {0, 1}, and we also consider generalization
loss as opposed to simply training loss. We will
define loss in terms of the underlying joint dis-
tribution D over X Ã— Y from which training ex-
amples are drawn independently. We define the
true error, true profile, and true probability:
Î½k = Pr[x âˆˆ Xk] = E[nk/n]
pk = E
[
zI[x âˆˆ Xk]
]
= E[pÌ‚k]
`k = E
[
|y âˆ’ z| | x âˆˆ Xk
]
= E[Ë†Ì€k|nk > 0]
Joint loss L is defined on the joint distribution
Âµ on g, y, z âˆˆ [K] Ã— Y Ã— Z induced by D and
a classifier c : X â†’ Z. A distributional joint
loss L is related to empirical joint loss LÌ‚ in that
L = limnâ†’âˆ E[LÌ‚], i.e., the limit of the empirical
joint loss as the number of training data grows
without bound (if it exists).
Fixing the marginal distribution over [K]Ã—Y,
joint loss L : [0, 1]2K â†’ R can be viewed as
a function of `1, p1, . . . , `K , pK (in addition to
group probabilities Pr[g(x) = k] which are in-
dependent of the classification). In addition to
requiring monotonicity, namely L being nonde-
creasing in `k fixing all other parameters, we will
assume that L is continuous with a bound on the
rate of change of the form:
|L(`1, p1, . . . , `K , pK)âˆ’ L(`â€²1, p
â€²
1, . . . , `
â€²
K , p
â€²
K)| â‰¤
R
âˆ‘
k
(
Î½k|`k âˆ’ `â€²k|+ |pk âˆ’ pâ€²k|
)
, (3)
for parameter R â‰¥ 0 and all `k, `
â€²
k, pk, p
â€²
k âˆˆ [0, 1].
Note that the Î½k in the above bound is neces-
sary for our analysis because a loss that depends
on `k without Î½k may require exponentially large
quantities of data to estimate and optimize over
if Î½k is exponentially small. Of course, alterna-
tively Î½k could be removed from this assumption
by imposing a lower bound on all Î½k.
Many losses, such as L1 and LNPÎ» above, can
be shown to satisfy this continuity requirement
for R = 1 and R = 2, respectively. We also note
that the reduction we present can be modified
to address certain discontinuous loss functions.
For instance, for a given target allocation (i.e., a
fixed fraction of positive classifications for each
group), one simply finds the classifier of minimal
empirical error for each group which achieves the
desired fraction of positives as closely as possible.
A transfer learning algorithm for C is A :
(X Ã—{0, 1})âˆ—Ã—(X Ã—{0, 1})âˆ— â†’ 2C , where A takes
in-group examples ã€ˆxi, yiã€‰ni=1 and out-group ex-
amples ã€ˆxâ€²i, yâ€²iã€‰n
â€²
i=1, both from X Ã— {0, 1}. This is
also called supervised domain adaptation. The
distribution of out-group examples is different
from (but related to) the distribution of in-group
samples. The motivation for using the out-group
examples is that if one is trying to learn a classi-
fier on a small dataset, accuracy may be increased
using related data.
Algorithm 2: G.D. (T, LÌ‚, {ã€ˆxi, yiã€‰}, {Xi})
1. For k = 1 to K,
â€¢ nk â† |{i â‰¤ n | xi âˆˆ Xk}|
â€¢ Ck â† T
(
ã€ˆxi, yiã€‰i:xiâˆˆXk
, ã€ˆxi, yiã€‰i:xi 6âˆˆXk
)
. Run transfer learner, output is a set
2. For all c âˆˆ Ck,
â€¢ pÌ‚k[c]â† 1
n
âˆ‘
i:xiâˆˆXk
c(xi)
. Estimate profile
â€¢ Ë†Ì€
k[c]â† 1
nk
âˆ‘
i:xiâˆˆXk
|yi âˆ’ c(xi)|
. Estimate error rates
3. return Î³c for c âˆˆ
arg minC1Ã—...Ã—CK
LÌ‚
(
ã€ˆË†Ì€i[ci], pÌ‚i[ci]ã€‰Ki=1
)
The general decoupling algorithm uses a transfer
learning algorithm T .
In the next section, we describe and analyze
a simple transfer learning algorithm that down-
weights samples from the out-group. For that
algorithm, we show:
Theorem 8 Suppose that, for any two groups
j, k â‰¤ K and any classifiers c, câ€² âˆˆ C,
|(`j(c)âˆ’ `j(câ€²))âˆ’ (`k(c)âˆ’ `k(câ€²))| â‰¤ âˆ† (4)
For algorithm 2 with the transfer learning algo-
rithm described in Section 6.1, with probability
â‰¥ 1 âˆ’ Î´ over the n iid training data, the algo-
rithm outputs cÌ‚ with L(cÌ‚) at most
min
câˆˆC
L(c) + 5RKÏ„ +R
âˆ‘
k
min
(
Ï„
âˆš
,âˆ†
)
,
where Ï„ =
âˆš
K, the run-time of the algorithm is polynomial
in n and the runtime of the optimizer over C.
The assumption in (4) states that the perfor-
mance difference between classifiers is similar
across different groups and is weaker than an as-
sumption of similar classifier performance across
groups. Note that it would follow from a simpler
but stronger requirement that |`j(c) âˆ’ `k(c)| â‰¤
âˆ†/2 by the triangle inequality.
Parameter settings (see Lemma 10) and tighter
bounds can be found in the next section. How-
ever, we can still see qualitatively that, as n
grows, the bound decreases roughly like O(nâˆ’1/2)
as expected. We also note that for groups with
large Î½k, as we will see in the next section, the
transfer learning algorithm places weight 0 on
(and hences ignores) the out-group data. For
small3 Î½k, the algorithm will place significant
weight on the out-group data.
6.1. A transfer learning algorithm T
In this section, we describe analyze a simple
transfer learning algorithm that down-weights4
out-group examples by parameter Î¸ âˆˆ [0, 1]. To
choose Î¸, we can either use cross-validation on
an independent held-out set, or Î¸ can be chosen
to minimize a bound as we now describe. The
cross-validation, which we do in our experiments,
is appropriate when one does not have bounds at
hand on the size of set of classifiers or the differ-
ence between groups, as we shall assume, or when
one simply has a black-box learner that does not
perfectly optimize over C. We now proceed to
derive a bound on the error that will yield a pa-
rameter choice Î¸.
Consider k to be fixed. For convenience, we
write nâˆ’k = nâˆ’nk as the number of samples from
other groups. Define Ë†Ì€âˆ’k and `âˆ’k analogously to
Ë†Ì€
k and `k for out-of-group data xi 6âˆˆ Xk.
Instead of outputting a set of classifiers, one for
each different number of positives within group
k, it will be simpler to think of the group-k pro-
file pÌ‚k = P as being specified in advance, and we
hence focus our attention on the subset of classi-
fiers,
CkP =
{
c âˆˆ C
âˆ£âˆ£âˆ£âˆ£ 1
n
âˆ‘
i:xiâˆˆXk
c(xi) = P
}
,
which depends on the training data. The bounds
in this section will be uninteresting, of course,
when CkP is empty (e.g., in the unlikely event
3. For very small Î½k < Ï„ , the term Î½k âˆ’ Ï„ is negative
(making the left side of the above min imaginary), in
which case we define the min to be the real term on
the right.
4. If the learning algorithm doesnâ€™t support weighting,
subsampling can be used instead.
that x1 = x2 = . . . = xn, the only realizable pÌ‚k
of interest are 0 and 1). The general algorithm
will simply run the subroutine described in this
section nk+1 â‰¤ n+1 times, once for each possible
value of pÌ‚k.5 Of course, |CkP | â‰¤ |C|.
As before, we will assume that the underlying
learner is optimal, meaning that given a weighted
set of examples (w1, x1, y1), . . . , (wn, xn, yn) with
total weight W =
âˆ‘
wi, it returns a classifier c âˆˆ
CkP that has minimal weighted error
âˆ‘ wi
W |yi âˆ’
c(xi)| among all classifiers in CkP .
In Appendix A, we derive a closed-form solu-
tion for Î¸, the (approximately) optimal down-
weighting of out-group data for our transfer
learning algorithm. This solution depends on
a bound on the difference in classifier ranking
across different groups. For small âˆ†, the dif-
ference in error rates of each pair of classifiers
is approximately the same for in-group and out-
group data. In this case, we expect generalization
to work well and hence Î¸ â‰ˆ 1. For large âˆ†, out-
group data doesnâ€™t provide much guidance for the
optimal in-group classifier, and we expect Î¸ â‰ˆ 0.
For a fixed k and Î¸ âˆˆ [0, 1], let cÌ‚ be a classifier
that minimizes the empirical loss when out-of-
group samples are down-weighted by Î¸, i.e.,
cÌ‚ âˆˆ arg min
câˆˆCkP
nk Ë†Ì€
k(c) + Î¸nâˆ’k Ë†Ì€âˆ’k(c),
and câˆ— be an optimal classifier that minimizes the
true loss, i.e.,
câˆ— âˆˆ arg min
câˆˆCkP
`k(c).
We would like to choose Î¸ such that `k(cÌ‚) is
close to `k(câˆ—). In order to derive a closed-form
solution for Î¸ in terms of âˆ†, we use concentration
bounds to bound the expected error rates of cÌ‚
and câˆ— in terms of âˆ† and Î¸, and then choose Î¸ to
minimize this expression.
We find that, as long as nk <
Î´ the
optimal choice of Î¸ will be strictly in between 0
and 1.
7. Experiment
For this experiment, we used data that is â€œsemi-
syntheticâ€ in that the 47 datasets are â€œrealâ€
5. In practice, classification learning algorithms gener-
ally learn a single real-valued score and consider dif-
ferent score thresholds.
0.2 0.4 0.6 0.8 1.0
decoupled loss/blind loss
0.5
0.6
0.7
0.8
0.9
1.0
co
u
p
le
d
 l
o
ss
/b
lin
d
 l
o
ss
Comparison of joint loss across datasets
Figure 2: Comparing the joint loss of our de-
coupled algorithm with the coupled
and blind baselines. Each point is a
dataset. A ratio less than 1 means
that the loss was smaller for the de-
coupled or coupled algorithm than the
blind baseline, i.e, that using the sensi-
tive feature resulted in decreased error.
Points above the diagonal represent
datasets in which the decoupled algo-
rithm outperformed the coupled one.
(downloaded from openml.org) but an arbitrary
binary attribute was used to represent a sensi-
tive attribute, so K = 2. The base classifier was
chosen to be least-squares linear regression for
its simplicity (no parameters), speed, and repro-
ducibility.
In particular, each dataset was a univari-
ate regression problem with balanced loss for
squared error, i.e., LÌ‚B = 1
2 (Ë†Ì€
1 + Ë†Ì€
2) where Ë†Ì€
k =âˆ‘
i:gi=k
(yiâˆ’ zi)2/nk. To gather the datasets, we
first selected the problems with twenty or fewer
dimensions. Classification problems were con-
verted to regression problems by assigning y = 1
to the most common class and y = 0 to all other
classes. Regression problems were normalized so
that y âˆˆ [0, 1]. Categorical attributes were simi-
larly converted to binary features by assigning 1
to the most frequent category and 0 to others.
The sensitive attribute was chosen to be the
first binary feature such that there were at least
100 examples in both groups (both 0 and 1 val-
ues). Further, large datasets were truncated so
that there were at most 10,000 examples in each
group. If there was no appropriate sensitive at-
0.2 0.4 0.6 0.8 1.0
decoupled loss/blind loss
0.2
0.4
0.6
0.8
1.0
1.2
d
e
co
u
p
le
d
 l
o
ss
/b
lin
d
 l
o
ss
 (
w
it
h
o
u
t 
tr
a
n
sf
e
r)
Comparison of joint loss across datasets
Figure 3: Comparing the joint loss of our de-
coupled algorithm with the decoupled
algorithm with and without transfer
learning. Each point is a dataset. A
ratio less than 1 means that the loss
was smaller for the decoupled algo-
rithm than the blind baseline. Points
above the diagonal represent datasets
in which transfer learning improved
performance compared to decoupling
without transfer learning.
tribute, then the dataset was discarded. We also
discarded a small number of â€œtrivialâ€ datasets
in which the data could be perfectly classified
(less than 0.001 error) with linear regression. The
openml idâ€™s and detailed error rates of the 45 re-
maining datasets are listed in the appendix.
All experiments were done with five-fold cross-
validation to provide an unbiased estimate of gen-
eralization error on each dataset. Algorithm 2
was implemented, where we further used five-fold
cross validation (within each of the outer folds)
to choose the best down-weighting parameter
Î¸ âˆˆ {0, 2âˆ’10, 2âˆ’9, . . . , 1} for each group. Hence,
least-squares regression was run 5 âˆ— 5 âˆ— 11 = 275
times on each dataset to implement our algo-
rithm.
The baselines were considered: the blind base-
line is least-squares linear regression that has no
access to the sensitive attribute, the coupled base-
line is least-squares linear regression that can
take into account the sensitive attribute.
Figure 2 compares the loss of the coupled base-
line (x-axis) and our decoupled algorithm (y-
axis) to that of the blind baseline. In particular,
the log ratio of the squared errors is plotted, as
this quantity is immune to scaling of the y val-
ues. Each point is a dataset. Points to the left of
1 (x < 1) represent datasets where the coupled
classifier outperformed the blind one. Points be-
low the horizontal line y < 1 represent points
in which the decoupled algorithm outperformed
the indiscriminate baseline. Finally, points above
the diagonal line x = y represent datasets where
the decoupled classifier outperformed the coupled
classifier.
Figure 3 compares transfer learning to decou-
pling without any transfer learning (i.e., just
learning on the in-group data or setting Î¸ = 0).
As one can see, on a number of datasets, transfer
learning significantly improves performance. In
fact, without transfer learning the coupled clas-
sifiers significantly outperform decoupled classi-
fiers on a number datasets.
8. Image retrieval experiment
In this section, we describe an anecdotal example
that illustrates the type of effect the theory pre-
dicts, where a classifier biases towards minority
data that which is typical of the majority group.
We hypothesized that standard image classifiers
for two groups of images would display bias to-
wards the majority group, and that a decoupled
classifier could reduce this bias. More specifically,
consider the case where we have a setX = X1âˆªX2
of images, and want to learn a binary classifier
c : X â†’ {0, 1}. We hypothesized that a coupled
classifier would display a specific form of bias we
call majority feature bias, such that images in the
minority group would rank higher if they had fea-
tures of images in the majority group.
We tested this hypothesis by training classifiers
to label images as â€œsuitâ€ or â€œno suitâ€. We con-
structed an image dataset by downloading the
â€œsuit, suit of clothesâ€ synset as a set of posi-
tives, and â€œmale personâ€ and â€œfemale personâ€
synsets as the negatives, from ImageNet Deng
et al. (2009). We manually removed images in the
negatives that included suits or were otherwise
outliers, and manually classified suits as â€œmaleâ€
or â€œfemaleâ€, removing suit images that were nei-
ther. We used the pre-trained BVLC CaffeNet
model â€“ which is similar to the AlexNet mode
from Krizhevsky et al. (2012) â€“ to generate fea-
tures for the images and clean the dataset. We
Linear
classifier
Decoupled 
linear
classifiers
Figure 4: Differences between image classifica-
tions of â€œsuitâ€ using standard lin-
ear classifiers and decoupled classifiers
(trained using standard neural network
image features). The females selected
by the linear classifier are wearing a
tuxedo and blazer more typical of the
majority male group.
used the last fully connected of layer (â€œfc7â€) of
the CaffeNet model as features, and removed im-
ages where the most likely label according to the
CaffeNet model was â€œenvelopeâ€ (indicating that
the image was missing), or â€œsuit, suit of clothesâ€
or â€œbow tie, bow-tie, bowtieâ€ from the negatives.
The dataset included 506 suit images (462 male,
44 female) and 1295 no suit images (633 male,
662 female).
We then trained a coupled and decoupled stan-
dard linear support vector classifier (SVC) on this
dataset, and provide anecdotal evidence that the
decoupled classifier displays less majority feature
bias than the coupled classifier. We trained the
coupled SVC on all images, and then ranked im-
ages according to the predicted class. We trained
decoupled SVCs, with one SVC trained on the
male positives and all negatives, and the other
on female positives and all negatives. Both clas-
sifiers agreed on eight of the top ten â€œfemalesâ€
predicted as â€œsuitâ€, and Fig. 4 shows the four im-
ages (two per classifier) that differed. One of the
images found by the coupled classifier is a woman
in a tuxedo (typically worn by men), which may
be an indication of majority feature bias; adding
a binary gender attribute to the coupled classifier
did not change the top ten predictions for â€œfemale
suit.â€ We further note that we also tested both
the coupled and decoupled classifier on out-of-
sample predictions using 5-fold cross-validation,
and that both were highly accurate (both had
94.5% accuracy, with the coupled classifier pre-
dicting one additional true positive).
We emphasize that we present this experiment
to provide an anecdotal example of the potential
advantages of a decoupled classifier, and we do
not make any claims on generalizability or effect
size on this or other real world datasets because
of the small sample size and the several manual
decisions we made.
9. Conclusions
In this paper, we give a simple technical approach
for a practitioner using ML to incorporate sensi-
tive attributes. Our approach avoids unneces-
sary accuracy tradeoffs between groups and can
accommodate an application-specific objective,
generalizing the standard ML notion of loss. For
a certain family of â€œweakly monotonicâ€ fairness
objectives, we give a black-box reduction that can
use any off-the-shelf classifier to efficiently opti-
mize the objective. In contrast to much prior
work on ML which first requires complete fair-
ness, this work requires the application designer
to pin down a specific loss function that trades
off accuracy for fairness.
Experiments demonstrate that decoupling can
reduce the loss on some datasets for some poten-
tially sensitive features.
References**

Julia Angwin, Jeff Larson, Surya Mattu, and
Lauren Kirchner. Machine bias: Theres soft-
ware used across the country to predict future
criminals. and its biased against blacks. ProP-
ublica, May, 23, 2016.**

Elias Bareinboim and Judea Pearl. Causal infer-
ence and the data-fusion problem. Proceedings
of the National Academy of Sciences, 113(27):
7345â€“7352, 2016.**

Richard Berk. The role of race in forecasts of
violent crime. Race and social problems, 1(4):
231, 2009.**

Alexandra Chouldechova. Fair prediction with
disparate impact: A study of bias in recidivism
prediction instruments. arXiv, 2017.**

Jesse Davis and Mark Goadrich. The relation-
ship between precision-recall and roc curves.
In Proceedings of the 23rd international con-
ference on Machine learning, pages 233â€“240.
ACM, 2006.**

J Deng, W Dong, R Socher, L-J Li, K Li, and
L Fei-Fei. ImageNet: A Large-Scale Hierarchi-
cal Image Database. In CVPR09, 2009.**

Cynthia Dwork, Moritz Hardt, Toniann Pitassi,
Omer Reingold, and Richard Zemel. Fairness
through awareness. ITCS, 2011.**

Michael Feldman, Sorelle A Friedler, John
Moeller, Carlos Scheidegger, and Suresh
Venkatasubramanian. Certifying and removing
disparate impact. Proceedings of the 21th ACM
SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, 2015.**

Sorelle A Friedler, Carlos Scheidegger, and
Suresh Venkatasubramanian. On the
(im)possibility of fairness. arXiv preprint
arXiv:1609.07236, 2016.**

Moritz Hardt, Eric Price, and Nathan Srebro.
Equality of opportunity in supervised learning.
NIPS, 2016.**

Avinatan Hassidim, Assaf Romm, and Ran I
Shorrer. Redesigning the israeli psychology
masterâ€™s match. American Economic Review,
107(5):205â€“09, May 2017. doi: 10.1257/aer.
p20171048. URL http://www.aeaweb.org/
articles?id=10.1257/aer.p20171048.**

U Hebert-Johnson, M Kim, O Reingold,
and G Rothblum. Calibration for the
(computationally-identifiable) masses. 2017.
arXiv:1711.08513v1.**

Matthew Joseph, Michael Kearns, Jamie H Mor-
genstern, and Aaron Roth. Fairness in learn-
ing: Classic and contextual bandits. In Ad-
vances in Neural Information Processing Sys-
tems, pages 325â€“333, 2016.**

Toshihiro Kamishima, Shotaro Akaho, and Jun
Sakuma. Fairness-aware learning through reg-
ularization approach. In Proceedings of the
2011 IEEE 11th International Conference on
Data Mining Workshops, ICDMW â€™11, pages
643â€“650, Washington, DC, USA, 2011. IEEE
Computer Society. ISBN 978-0-7695-4409-0.
doi: 10.1109/ICDMW.2011.83. URL http:
//dx.doi.org/10.1109/ICDMW.2011.83.**

Niki Kilbertus, Mateo Rojas-Carulla, Giambat-
tista Parascandolo, Moritz Hardt, Dominik
Janzing, and Bernhard SchoÌˆlkopf. Avoiding
discrimination through causal reasoning. arXiv
preprint arXiv:1706.02744, 2017.**

Jon Kleinberg, Sendil Mullainathan, and Man-
ish Raghavan. Inherent trade-offs in the fair
determination of risk scores. arXiv, 2016.**

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E
Hinton. Imagenet classification with deep con-
volutional neural networks. In F. Pereira,
C. J. C. Burges, L. Bottou, and K. Q. Wein-
berger, editors, Advances in Neural Informa-
tion Processing Systems 25, pages 1097â€“1105.
Curran Associates, Inc., 2012.**

M J Kusner, J R Loftus, C Russell, and
R Silva. Counterfactual Fairness. ArXiv e-
prints, March 2017.**

Matt J Kusner, Joshua R Loftus, Chris Russell,
and Ricardo Silva. Counterfactual fairness.
arXiv preprint arXiv:1703.06856, 2017.**

Razieh Nabi and Ilya Shpitser. Fair inference on
outcomes. arXiv preprint arXiv:1705.10378,
2017.**

Judea Pearl, Madelyn Glymour, and Nicholas P
Jewell. Causal inference in statistics: a primer.
John Wiley & Sons, 2016.**

Dino Pedreschi, Salvatore Ruggieri, and Franco
Turini. Discrimination-aware data mining. In
Proceedings of the 14th ACM SIGKDD In-
ternational Conference on Knowledge Discov-
ery and Data Mining, KDD â€™08, pages 560â€“
568, New York, NY, USA, 2008. ACM. ISBN
978-1-60558-193-4. doi: 10.1145/1401890.
1401959. URL http://doi.acm.org/10.
1145/1401890.1401959.**

R Zemel, Y Wu, K Swersky, T Pitassi, and
C Dwork. Learning fair representations. Proc.
of Intl. Conf. on Machine Learning, 2013.**

Indre Zliobaite, Faisal Kamiran, and Toon
Calders. Handling conditional discrimination.
In Proceedings of the 2011 IEEE 11th Inter-
national Conference on Data Mining, ICDM
â€™11, pages 992â€“1001, Washington, DC, USA,
2011. IEEE Computer Society. ISBN 978-
0-7695-4408-3. doi: 10.1109/ICDM.2011.
72. URL http://dx.doi.org/10.1109/ICDM.
2011.72.**

Appendix A. Transfer Learning
Bounds
We derive a closed-form solution for Î¸, the
(approximately) optimal down-weighting of out-
group data for our transfer learning algorithm.
This solution depends on a bound âˆ† (defined in
Theorem 8) on the difference in classifier rank-
ing across different groups. For small âˆ†, the dif-
ference in error rates of each pair of classifiers
is approximately the same for in-group and out-
group data. In this case, we expect generalization
to work well and hence Î¸ â‰ˆ 1. For large âˆ†, out-
group data doesnâ€™t provide much guidance for the
optimal in-group classifier, and we expect Î¸ â‰ˆ 0.
Finally, for a fixed k and Î¸ âˆˆ [0, 1], let cÌ‚ be a
classifier that minimizes the empirical loss when
out-of-group samples are down-weighted by Î¸,
i.e.,
cÌ‚ âˆˆ arg min
câˆˆCkP
nk Ë†Ì€
k(c) + Î¸nâˆ’k Ë†Ì€âˆ’k(c),
and câˆ— be an optimal classifier that minimizes the
true loss, i.e.,
câˆ— âˆˆ arg min
câˆˆCkP
`k(c).
We would like to choose Î¸ such that `k(cÌ‚) is
close to `k(câˆ—). In order to derive a closed-form
solution for Î¸ in terms of âˆ†, we use concentration
bounds to bound the expected error rates of cÌ‚
and câˆ— in terms of âˆ† and Î¸, and then choose Î¸ to
minimize this expression.
Lemma 9 Fix any k â‰¤ K,P, nk, nâˆ’k â‰¥ 0
and âˆ†, Î¸ â‰¥ 0. Let ã€ˆxi, yiã€‰ni=1 be n = nk +
nâˆ’k training examples drawn from D conditioned
on exactly nk belonging to group k. Let cÌ‚ âˆˆ
arg mincâˆˆCkP
nk Ë†Ì€
k(c) + Î¸nâˆ’k Ë†Ì€âˆ’k(c) be any min-
imizer of empirical error when the non-group-k
examples have been down-weighted by Î¸. Then,
Pr
[
`k(cÌ‚) â‰¤ min
câˆˆCkP
`k(c) + f(Î¸, nk, nâˆ’k,âˆ†, Î´)
]
â‰¥ 1âˆ’Î´,
where the probability is taken over the n = nk +
nâˆ’k training iid samples, and f(Î¸, nk, nâˆ’k,âˆ†, Î´)
is defined as:
(âˆš
2(nk + Î¸2nâˆ’k) log
2|C|
Î´
+ Î¸nâˆ’kâˆ†
)
.
(5)
Unfortunately, the minimum value of f is a
complicated algebraic quantity that is easy to
compute but not easy to directly interpret. In-
stead, we can see that:
Lemma 10 For f from Equation (5),
g(nk, nâˆ’k,âˆ†, Î´) = minÎ¸âˆˆ[0,1] f(Î¸, nk, nâˆ’k,âˆ†, Î´)
is at most
min
ï£«ï£­âˆš 2
nk
log
2|C|
Î´
,
âˆš
log
2|C|
Î´
+
nâˆ’k
n
âˆ†
ï£¶ï£¸ ,
(6)
with equality if and only if nk â‰¥ 2
âˆ†2 log 2|C|
Î´ in
which case the minimum occurs at Î¸ = 0 where
g(nk, nâˆ’k,âˆ†) =
âˆš
log 2|C|
Î´ . Otherwise the
minimum occurs at,
Î¸âˆ— =
âˆš
Î²2
nâˆ’k
nk
(1âˆ’ Î²)âˆ’ Î²
for Î² = âˆ†2 2
nk
log(2|C|/Î´).
Appendix B. Dataset ids
For reproducibility, the idâ€™s and feature names
for the 47 open ml datasets were as follows:
(21, â€™buyingâ€™), (23, â€™Wifes educationâ€™), (26,
â€™parentsâ€™), (31, â€™checking statusâ€™), (50, â€™top-
left-squareâ€™), (151, â€™dayâ€™), (155, â€™s1â€™), (183,
â€™Sexâ€™), (184, â€™white king rowâ€™), (292, â€™Yâ€™),
(333, â€™classâ€™), (334, â€™classâ€™), (335, â€™classâ€™),
(351, â€™Yâ€™), (354, â€™Yâ€™), (375, â€™speakerâ€™), (469,
â€™DMFT.Beginâ€™), (475, â€™Time of surveyâ€™), (679,
â€™sleep stateâ€™), (720, â€™Sexâ€™), (741, â€™sleep stateâ€™),
(825, â€™RADâ€™), (826, â€™Occasionâ€™), (872, â€™RADâ€™),
(881, â€™x3â€™), (915, â€™SMOKSTATâ€™), (923, â€™isnsâ€™),
(934, â€™family structureâ€™), (959, â€™parentsâ€™), (983,
â€™Wifes educationâ€™), (991, â€™buyingâ€™), (1014,
â€™DMFT.Beginâ€™), (1169, â€™Airlineâ€™), (1216, â€™clickâ€™),
(1217, â€™clickâ€™), (1218, â€™clickâ€™), (1235, â€™elevelâ€™),
(1236, â€™sizeâ€™), (1237, â€™sizeâ€™), (1470, â€™V2â€™), (1481,
â€™V3â€™), (1483, â€™V1â€™), (1498, â€™V5â€™), (1557, â€™V1â€™),
(1568, â€™V1â€™), (4135, â€™RESOURCEâ€™), (4552, â€™V1â€™)
