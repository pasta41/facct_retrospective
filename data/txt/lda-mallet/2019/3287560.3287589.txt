A comparative study of fairness-enhancing interventions in machine learning Computers are increasingly used to make decisions that have significant impact on peoples lives Often these predictions can affect different population subgroups disproportionately As a result the issue of fairness has received much recent interest and a number of fairness-enhanced classifiers have appeared in the literature This paper seeks to study the following questions how do these different techniques fundamentally compare to one another and what accounts for the differences Specifically we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets We find that although different algorithms tend to prefer specific formulations of fairness preservations many of these measures strongly correlate with one another In addition we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition simulated in our benchmark by varying training-test splits and to different forms of preprocessing indicating that fairness interventions might be more brittle than previously thought CONCEPTS Computing methodologies Machine learning Software and its engineering Software libraries and repositories KEYWORDS Source code including instructions for adding your own algorithm or dataset can be found at and installed via pip install fairness INTRODUCTION As the use of machine learning to make decisions about people has increased so has the drive to make fairness-aware machine learning algorithms A considerable body of research over the past ten years has produced algorithms for accurate yet fair decisions under varying definitions of fair for goals such as non-discriminatory hiring risk assessment for sentencing guidance and loan allocation And yet we have not yet seen extensive deployment of these algorithms in the pertinent domains The primary technical obstacle appears to be our ability to compare methods effectively across different evaluation measures and different data sets with consistent data preprocessing and testing methodologies Such comparisons would not just reveal best-in-class methods they would also suggest which measures are robust and how different algorithms are sensitive to different kinds of preprocessing As pointed out by Lehr and Ohm such considerations of the data processing pipeline are not just important for efficient implementation but also have legal ramifications for the resulting automated decision-making process In this paper we present a test-bed to facilitate direct comparisons of algorithms with respect to measures on a variety of datasets Our open-source framework allows for the easy addition of new methods measures and data for the purpose of evaluation We show how to use our test-bed for determining not only which specific algorithm has the best performance under a fairness or accuracy measure but what types of algorithmic interventions tend to be the most effective In addition to the impact of these algorithmic choices we examine the impact of different preprocessing techniques and different measures for accuracy and fairness that have an important and previously obscured impact on the results of these algorithms Our goal is to provide a comprehensive comparative analysis of existing approaches that is currently lacking in the literature In this paper we use the term intervention to refer to how the choice of algorithm used impacts the fairness of the overall system We are not studying causal definitions of fairness Our results Our evaluation yields the following major findings Fairness-accuracy tradeoffs depend on preprocessing Section Different algorithms tend to have slightly different requirements in terms of input how are sensitive attributes encoded Are multiple sensitive attributes supported Does the algorithm directly support categorical attributes or are attribute transformations required Choices for these requirements directly affect the accuracy and fairness of a fairness-aware classifier This is significant because prior formal studies of fairness-accuracy tradeoffs typically focused on hyperparameter tuning rather than preprocessing Measures of discrimination correlate with each other Section Even though there has been a proliferation of measures designed to highlight discrimination instances by machine learning algorithms we find that a large number of these measures tend to strongly correlate with one another As a result techniques optimizing for one measure could perform well for a different measure and similarly for poor performance Algorithms make significantly different fairness-accuracy tradeoffs Section The specific mechanisms that different algorithms employ to increase fairness are quite varied but surprisingly the actual predictions made by these algorithms tend to vary significantly as well As a result no algorithms performance as of the latest state of our benchmark appears to dominate either in accuracy or fairness measures Algorithms are fragile they are sensitive to variations in the input Section We find surprising variability in fairness measures arising from variations in training-test splits this appears to not have been previously mentioned in the literature BACKGROUND Fairness-aware machine learning algorithms seek to provide methods under which the predicted outcome of a classifier operating on data about people is fair or non-discriminatory for people based on their protected class status such as race sex religion etc also known as a sensitive attribute Broadly fairness-aware machine learning algorithms have been categorized as those preprocessing techniques designed to modify the input data so that the outcome of any machine learning algorithm applied to that data will be fair those algorithm modification techniques that modify an existing algorithm or create a new one that will be fair under any inputs and those postprocessing techniques that take the output of any model and modify that output to be fair Many associated metrics for measuring fairness in algorithms have also been explored These are detailed further in Section and are also surveyed in This description of fairness-aware machine learning methods is limited to batch-learning-based interventions We do not consider interventions that focus on sequential or reinforcement learning such as Preprocessing algorithms The motivation behind preprocessing algorithms is the idea that training data is the cause of the discrimination that a machine learning algorithm might learn and so modifying it can keep a learning algorithm trained on it from discriminating This could be because the training data itself captures historical discrimination or because there are more subtle patterns in the data such as an under-representation of a minority group that makes errors on that group both more likely and less costly under certain accuracy measures One such algorithm that we will analyze in this paper is that of Feldman et al that modifies each attribute so that the marginal distributions based on the subsets of that attribute with a given sensitive value are all equal it does not modify the training labels Additional preprocessing approaches include Algorithm modifications Modifications to specific learning algorithms eg in the form of additional constraints have been by far the most common approach We study three such methods in this paper Kamishima et al introduce a fairness focused regularization term and apply it to a logistic regression classifier Zafar et al observe that standard fairness constraints are nonconvex and hard to satisfy directly and introduce a convex relaxation for purpose of optimization Calders and Verwer build separate models for each value of a sensitive attribute and use the appropriate model for inputs with the corresponding value of the attribute Another method that combines preprocessing and algorithm modification is the work by Zemel et al Their approach is to learn a modified representation of the data that is most effective at classification while still being free of signals pertaining to the sensitive attribute Postprocessing techniques A third approach to building fairness into algorithm design is by modifying the results of a previously trained classifier to achieve the desired results on different groups Kamiran et al designed a strategy to modify the labels of leaves in a decision tree after training in order to satisfy fairness constraints Recent work by Hardt et al and Woodworth et al explored the use of post-processing as a way to ensure fairness with respect to error profiles see Section for more on this In this paper we focus on group fairness approaches that aim to ensure non-discrimination across protected groups where the goal is to optimize metrics such as disparate impact Another line of thought known as individual fairness is detailed in In this work we do not study algorithms that seek to optimize individual fairness our goal is to focus on methods that explicitly deal with group-based discrimination and there are to the best of our knowledge no publicly available implemented algorithms that optimize solely for individual fairness although does use individual fairness approximately as a distortion constraint in its pre-processing Related Work Three prior efforts are relevant to our work FairTest provides a general methodology to explore potential biases or feature associations in a data set as well as a way to identify regions of the input space where an algorithm might incur unusually high errors THEMIS takes a blackbox decision-making procedure and designs test cases automatically to explore where the procedure might be exhibiting group-based or causal discrimination Fairness Measures occupies a different point in the design space Given a particular algorithm that one wishes to evaluate they provide a framework to test the algorithm on a variety of datasets and fairness measures This approach on the one hand is more general than our framework because it works with any algorithm On the other hand it is less effective for a comparative evaluation of different algorithms especially if they have different preprocessing and training methods There are other software packages that audit black box software to determine the influence of individual variables We omit a detailed description of these approaches as they are out of the scope of the investigation presented here For more information the reader is referred to the excellent new survey on explainability by Guidotti et al BENCHMARK STRUCTURE In order to provide a platform for clear comparison of results across fairness-aware machine learning algorithms we separate each stage of the learning and analysis process see Figure and ensure that each algorithm is compared using the same dataset including the same preprocessing the same set of training test splits and all desired fairness and accuracy measures Much previous work has combined the preprocessing for a specific dataset with the code for the fairness-aware algorithm which makes comparisons with other algorithms and other datasets difficult Similarly algorithms have often been analyzed only under one or two measures Here we distinguish preprocessing algorithms and measures and create a pipeline in which all algorithms are analyzed under a standard preprocessing of datasets and a large set of measures In order to encourage easy adoption of this codebase as a platform for future algorithmic analysis each of these choices is modularized so that adding new datasets measures and/or algorithms to the pipeline is as easy as creating a new object The pipeline will then ensure that all existing algorithms are evaluated under the new dataset and measure More details and instructions for adding to the code base can be found at the repository DATA We perform all experiments based on five real-world data sets that have been previously considered in the fairness-aware machine learning literature and preprocess each consistently depending on the needs of the algorithm The real-world datasets come from some of the domains impacted by questions of fairness in machine learning hiring and promotion credit-worthiness and loans and recidivism prediction Ricci The Ricci dataset comes from the case of Ricci v DeStefano a case before the US Supreme Court in which the question at issue was an exam given to determine if firefighters would receive a promotion The dataset has entries and five attributes including the sensitive attribute Race The original promotion decision was made by a threshold of achieving at least a score of on the combined exam outcome The goal in a fair learning context is to predict this original promotion decision while achieving fairness with respect to the sensitive attribute Race All raw datasets preprocessing code and resulting processed datasets are available in the repository Preprocessing described here can be reproduced by running python preprocesspy Adult Income The Adult Income dataset contains information about individuals from the US census It is pre-split into a training and test set we use only the training data and re-split it There are instances and attributes including sensitive attributes race and sex instances with missing data are removed during the preprocessing step The prediction task is predicting whether an individual makes more or less than per year German The German Credit dataset contains instances and attributes describing individuals along with a classification of each individual as a good or bad credit risk Sensitive attribute sex is not directly included in the data but can be derived from the given information Sensitive attribute age is included and is discretized into values adult age at least years old and youth based on an analysis by showing this discretization provided for the most discriminatory possibilities ProPublica recidivism The ProPublica data includes data collected about the use of the COMPAS risk assessment tool in Broward County Florida It includes information such as the number of juvenile felonies and the charge degree of the current arrest for individuals along with sensitive attributes race and sex Data is preprocessed according to the filters given in the original analysis Each individual has a binary recidivism outcome that is the prediction task indicating whether they were rearrested within two years after the charge given in the data ProPublica violent recidivism The violent recidivism version of the ProPublica data describes the same scenario as the recidivism data described above but where the predicted outcome is a rearrest for a violent crime within two years individuals are included after preprocessing is applied including instances of rearrest and the sensitive attributes are race and sex Note that while the individuals in this data set are a subset of the overall recividism set from above their labels might be different ie the same individual might have different recidivism labels in the two data sets PREPROCESSING Each algorithm we will analyze has certain requirements for the type of data it will operate over and these necessitate different preprocessing techniques However in order to provide a consistent comparison across algorithms its important that each algorithm receive the same input We reconcile these needs by creating types of inputs that multiple algorithms can handle Algorithms that handle the same input can be directly compared to each other Algorithms can also be compared across different preprocessing strategies for the same dataset even though in this setting conclusions are less clear since the two sources of variability might interfere with one another Our first preprocessing step is to modify the input data according to any data-specific needs removing features that should not be used for classification removing or imputing any missing data and potentially removing items or adding derived features In order to allow the analysis of fairness based on multiple sensitive attributes eg not just ensuring fairness based on race or sex alone but based on both someones race and sex we also add a combined Figure The stages of the fairness-aware benchmarking program data input preprocessing benchmarking and analysis Intermediate files are saved at each stage of the pipeline to ensure reproducibility sensitive attribute eg attribute race-sex with values like White-Woman to each dataset that contains multiple sensitive attributes All algorithms will receive versions of the dataset with this same preprocessing applied While some algorithms are able to handle the datasets for training with only the described initial preprocessing well call this version of the processed data original most algorithms considered here have additional constraints For algorithms that can only handle numerical training data as input we modify the data to include one-hot encoded versions of each categorical variable and call this version of the data numerical Some algorithms additionally require that the sensitive attributes be binary eg White and not White instead of handling multiple racial categorizations for this version of the data numerical-binary we modify the given privileged group to be and all other values to be The three data tags should be interpreted as indicating constraints on the algorithms that use them Analysis With these preprocessed versions of each data set in place we can compare how a single algorithm performs relative to all versions of the dataset on which it can run The most common form of input for the algorithms we consider here is numerical and all these algorithms can additionally handle the numerical-binary version of the dataset This gives an opportunity to determine the effect per algorithm and per dataset of allowing an algorithm access to full information about sensitive attribute categorization or only a binary summary Figure illustrates this analysis on the impact of the numerical binary version of the preprocessed data on the algorithm proposed by Feldman et al In the left figure we examine the relation between the accuracy on numerical preprocessing versus numerical-binary binary-encoded sensitive attributes Each algorithm was run over ten random splits and the result on each split is shown as a single point on the figure As discussed in Section Feldman et al use a generic classifier after running a preprocessing fairness-enhancing filter on the data and the different algorithms reflect the different classifiers used We also automate For example scikit-learn classifiers only handle numerical data even for classifiers like decision trees where this is not inherently a requirement As a consequence some of the tested algorithms that would otherwise handle original data require numerical data since these algorithms internally call scikit-learn procedures the parameter tuning for the fairness-accuracy tradeoff parameter for this algorithm more about parameter tuning specifics can be found in Section for both accuracy and the disparate impact value As we can see for most variants of the algorithm the resulting accuracy is higher when using the numerical-binary representation than when using the numerical representation We speculate that this is because the Feldman et al algorithm conditions on the sensitive value in its preprocessing on the data and this step better preserves accuracy when a larger number of people are in each sensitive group as is the case when the unprivileged groups are grouped together in the binary preprocessing variant We can do a similar analysis on the fairness achieved by the methods as seen in the right side of Figure Again we compare the fairness measure in this case DI see Section achieved for different data representations First we see that the fairness achieved varies across runs an issue we will return to when we discuss measure stability Second we notice that the algorithm variants achieve greater fairness when using the numerical preprocessing of the data likely because each groups fairness is separately ensured while in the numerical-binary variant all unprivileged classes are grouped together Note that this indicates the presence a fairness-accuracy tradeoff which arises not from hyperparameters but from choice of preprocessing MEASURES There are many ways to evaluate the accuracy and fairness of a model Rather than be exhaustive we will focus on representative measures for each aspect Let D X be a dataset where X is the data subset that can be used for training whether categorical or numerical S is the sensitive attribute where is the privileged class and Y is the binary classification label where is the positive outcome and is the negative outcome Let Y be the predicted outcomes of some algorithm We can define accuracy and fairness measures in terms of conditional probabilities of outcome variables Y Y with respect to variables like Y and S Accuracy measures We consider the standard accuracy measures the uniform accuracy PY Y the true positive rate TPR PY Y and the true negative rate TNR PY Y We also consider A recent tutorial puts the number of fairness measures at Figure Examining the results of the Feldman et al algorithm under different preprocessing choices numerical versus numerical-binary Each dot plots the result of a single split of the data in terms of the labeled metric under both preprocessing choices The gray line shows equality between the preprocessing choices The model used within the Feldman algorithm is listed and some variants of the algorithm had the tradeoff parameter optimized for either accuracy or disparate impact value the balanced classification rate BCR a version of accuracy that is unweighted per class Definition BCR PY Y PY Y Fairness measures Fairness measures can be divided into three broad categories in all cases conditioned on values of the sensitive attribute S In what follows we normalize measures to make comparisons easier In all cases the measures lie in the range or where in both cases perfect fairness is achieved at We note that some of these measures have appeared in the literature not as something to be optimized to be close to but as a constraint to be satisfied ie that the appropriate value must equal Measures based on base rates Definition Disparate Impact DI PY S PY S This measure is inspired by one of the two tests for disparate impact in the legal literature in the United States In the cases where there are more than two values for a given sensitive attribute we consider two variants of DI which are equivalent in the case when there are only two sensitive values binary and average In the binary case all unprivileged classes are grouped together into a single value S eg non White that is compared as a group to the privileged class S eg White In the average case pairwise DI calculations are done against the privileged class eg White compared to Black White compared to Asian etc and the average of these calculations is taken This is analogous to the one-vs-all and all-vs-all methodology in multi-class classification Definition CV PY S PY S This measure is the same as DI but where the difference is taken instead of the ratio such a measure has been used for example to measure discrimination in the United Kingdom A binary grouping strategy described above for DI is used in the case where there is more than one sensitive value and the averaging method can also be used Note that we do not take the absolute value of the difference so that skew in favor of one group versus another can be detected We note that requiring CV is sometimes called a demographic parity constraint Measures based on group-conditioned accuracy In general we can think of fairness measures based on group-conditioned accuracy as asking whether the error rates for each group are similar This yields the following definitions Definition Group-conditioned accuracy measures s-Accuracy PY y Y y S s s-TPR PY Y S s s-TNR PY Y S s s-BCR PY Y S s PY Y S s names For example error rate balance is the aim of achieving equal s-TPR and s-TNR values across sensitive groups and equivalently equalized odds is the aim of achieving equal s-TPR and s-TNR across sensitive groups CV comparative-sensitive-TPR accuracy -accuracy -accuracy sensitive-accuracy TNR sensitive-TNR BCR sensitive-calibration comparative-sensitive-accuracy comparative-sensitive-TNR TPR sensitive-TPR CV comparative-sensitive-TPR accuracy -accuracy -accuracy sensitive-accuracy TNR sensitive-TNR BCR sensitive-calibration comparative-sensitive-accuracy comparative-sensitive-TNR TPR sensitive-TPR sensitive-calibration Figure Examining the relationships between different measures of accuracy and fairness when considered across all examined datasets and algorithms including both baseline and fairness-aware algorithms under numerical-binary preprocessing A simple sample correlation statistic is then computed for each set of pairs of measurements Strongly positively correlated metric pairs are shown in blue and strongly negatively correlated pairs are shown in red The top figure shows the results when run on the original datasets and the bottom when the datasets are downsampled to be balanced by class and sensitive attribute Letting any of the above measures be denoted Y Y s the values can then be aggregated for comparison by taking the mean directly s S Y Y or by taking the mean over comparisons analogous to DI and CV Y Y Y Y or Y Y Y Y s In each of these cases as we saw above the unprivileged sensitive values could be grouped together or handled separately in the ratio or difference For example consider a dataset where race is the sensitive attribute and which has been preprocessed so that the sensitive attribute takes binary values In this case the accuracy conditioned on having a sensitive value of eg White is denoted as the accuracy We will denote the average of the accuracy and accuracy in this case as the race-accuracy or in general as the sensitive-accuracy and the mean of the per-race differences ie s S -accuracy s-accuracy/S as the comparative-race-accuracy or in general as the comparativesensitive-accuracy Well use the same naming scheme for other accuracy measures and other sensitive attributes Worst-case notions are not considered in this analysis and paper but can be easily added to the code repository Future work will consider these measures as well Figure A comparison between the CV and comparativesensitive-TPR left and comparative-sensitive-TNR right metrics across all datasets and fairness-aware algorithms considered Each dot represents one out of random train/test splits Dots are colored by algorithm The color legend is the same as that of Figure Figure An illustration of the tradeoff between sensitive-calibration- and sensitive-TPR for all algorithms on the Adult dataset with sensitive value sex Each dot represents one run out of random train-test splits Measures based on group-conditioned calibration A predictor that outputs a probability Y for an event is said to be well-calibrated if PY Y p p Traditionally calibration measures are used for measuring the consistency of confidence scores For instance a well-calibrated predictor should be accurate on all predictions it issued with confidence at least Motivated by this we define fairness measures by conditioning the calibration function p PY Y p on a group Definition s-Calibration PY Y S s Definition s-Calibration PY Y S s Calibration has been introduced previously with the goal of equalizing across sensitive value Note that we define s Calibration- so that good values are close to consistent with the other measures in this paper Analysis Most of the algorithms considered here discussed in more detail in Section were analyzed with respect to the single fairness measure being introduced in the paper or with respect to a subset of the Figure The performance of all algorithms on each dataset with the goal of removing discrimination on a specific attribute From top to bottom the algorithms and sensitive attributes considered are Adult Income on race German Credit on sex Ricci on race ProPublica recidivism on race and ProPublica violent recidivism on race Each point is the result of a single algorithm running on a single training test split each algorithm is shown for ten such splits measures Incorporating all of the above accuracy and fairness measure variations into our software framework allows us to examine measure trends across multiple measures and multiple algorithms While these measures are often presented as opposing here we are interested in analyzing the extent to which this is true in practice There are many variations on these and other measures but we find many of these are correlated on these algorithms and datasets This is not entirely surprising as these measures are definitionally related For example DI takes the ratio of two probabilities while CV takes the difference However by analyzing resulting measures across many algorithms we find correlations that are less obvious In fact it appears that there are a few main clusters of measures In Figure we show the correlations between measures across all algorithms both baseline and fairness-aware and datasets when each fairness-aware algorithm is run for each sensitive attribute including the combined sensitive attributes such as race-sex The top of the figure shows the results when run on the full datasets There appear to be four main clusters DI and CV accuracy-related measures TPR-related measures and sensitive-calibration To determine the extent how this clustering was impacted by the skew in the data in terms of both class and sensitive attribute the datasets were downsampled uniformly with replacement to contain items that were balanced so that have the positive classification and the negative and within each of those have the privileged sensitive attribute and have an unprivileged value The bottom of Figure shows the correlations between metrics on this balanced sample This clarifies the clusterings and five are found DI-related measures accuracy-related measures comparative-accuracy measures TPR measures and sensitive-calibration- On this balanced sample there also appear to be two weaker but larger clusterings DI-related measures versus all accuracy comparative-accuracy and calibration-related measures Within this clustering there are some clear patterns Pairs of accuracy measures and their sensitive counterparts eg accuracy and sensitive-accuracy TPR and sensitive-TPR and TNR and sensitive TNR are always clustered together Recall that sensitive-accuracy is the average of the accuracy on the privileged group the -accuracy and the accuracy on the unprivileged groups the -accuracy so it makes sense that improving the overall accuracy would improve this average as well Perhaps more surprisingly the -accuracy and -accuracy are also strongly positively correlated ie improving the accuracy on the privileged group also improves the accuracy on the unprivileged groups on these algorithms and datasets A caveat to the strength of these clusterings is that these results only consider the measures when assessed on these algorithms Algorithms might exist or be created that focus on optimizing one specific measure that change these clusterings especially in cases where the rationale for the clusterings is less obvious eg the clustering of accuracy and TNR together But this experiment does allow us to assess in practice how optimizing for one fairness measure affects other fairness measures The Calders Feldman Kamishima and Zafar algorithms were all designed to optimize DI CV or similarly motivated measures Since DI and CV are analytically closely related to each other optimizing for one can be reasonably expected to optimize for the other But does optimizing for these base rate focused fairness measures also optimize for the group-conditioned accuracy focused fairness measures When considering only these fairness-aware algorithms the clusterings presented in Figure still hold ie optimizing for DI and CV does not tend to increase accuracy and the other measures in that cluster Interestingly though DI and CV do have a strong positive correlation with comparative-sensitive-TPR and a strong negative correlation with comparative-sensitive-TNR Figure demonstrates these correlations empirically for CV with comparative-sensitive TPR correlation of and comparative-sensitive-TNR correlation of Similar results correlations of and respectively are found on the datasets when sampled to be balanced Note that these numbers are not completely reflective of the correlation due to outliers that have a fixed value of CV We retained them in spite of this in the interest of transparency Additionally in some cases we expect would be tradeoffs between measures Assuming unequal base rates across populations impossibility results show it is impossible to achieve both calibration and error rate balance both the same false positive rate and the same false negative rates across groups In Figure we empirically examine this tradeoff As before each colored point represents one instance of train-test split for an algorithm As Figure shows there is a clear tradeoff between sensitive-calibration- and sensitive-TPR for each dataset Interestingly different algorithms situate themselves in different parts of the tradeoff line ALGORITHMS We choose a selection of existing fairness-aware algorithms to assess based on availability of source code and diversity of fairness interventions eg preprocessing versus algorithm modification Each algorithm is run on each dataset and each metric is calculated on the predicted results Synthesis statistics such as stability are then calculated and comparison graphs are produced We analyze the following algorithms along with non-fairness-aware algorithms chosen for a baseline comparison SVM decision trees Gaussian naive Bayes and logistic regression LR Calders and Verwer Calders and Verwer introduce a fairnessaware algorithm modification called Two Naive Bayes Their approach trains separate models for the values and iteratively assesses the fairness of the combined model under the CV measure makes small changes to the observed probabilities in the direction of reducing the measure and retrains their two models This algorithm can handle both categorical and numerical input data but requires that the given sensitive attribute be binary We use the Kamishima et al implementation of this algorithm The algorithm has a hyperparameter specifying a prior probability for the features We follow the original implementation and use a default of Feldman et al Feldman et al give a preprocessing approach that modifies each attribute so that the marginal distributions based on the subsets of that attribute with a given sensitive value are all equal it does not modify the training labels Any algorithm can then be trained on the resulting repaired data The algorithm can handle both categorical and numerical input data but since we train scikit-learn classifiers based on this preprocessed data our implementation can only handle numerical input Both binary and non-binary sensitive attributes can be handled A tuning parameter is provided to tradeoff between fairness and accuracy where gives the fairness of a regular non-fairness aware classifier and maximizes fairness is used as the default and all All algorithm implementations can be found in the repository algofairness/fairness-comparison along with all resulting metric calculations The full set of results can be reproduced by running python benchmarkpy Algorithm analysis code can be found in the repository and can be reproduced by running python analysispy Figure The stability of algorithms on Adult Each algorithm is tested on ten random train test splits and a rectangle centered on the mean and with a width and height equal to the standard deviation along that measure is plotted On the left the algorithms attempt to remove race discrimination and on the right sex discrimination values of at increments of in are included when the algorithm is optimized using a grid search over the parameters The implementation comes from Feldman et al and Kamishima et al Kamishima et al introduce a fairness-focused regularization term and apply it to a logistic regression classifier Their approach requires numerical input and a binary sensitive attribute A tuning parameter is provided to tradeoff between fairness and accuracy where is the default When optimizing the parameter we use values between and with a finer grid used for the lower values of that range these parameter choices are based on the experimental exploration of this parameter given in As above we use Kamishima et als implementation Zafar et al Zafar et al re-express fairness constraints which can be nonconvex via a convex relaxation This allows them to define efficient versions of fairness-aware logistic regression and support vector machines They propose two related optimization problems one that maximizes accuracy subject to fairness constraints in our experiments we call this Zafar-Accuracy and another to maximize fairness subject to accuracy constraints we call this Zafar-Fairness They use two parameters c is a parameter that controls the degree of independence of the outcome and the sensitive attribute via a covariance calculation setting c forces complete independence and therefore fairness The second parameter fixes the degree of approximation they are willing to tolerate the algorithm is only required to find an answer that is within a factor of the optimal solution In their experiments they set and vary c as a linear function of the corresponding covariance estimate for an unconstrained classifier When optimizing we use values between and in logarithmic steps Figure shows a basic summary of the performance of each algorithm considered on each data set Since each algorithm focuses on creating a fair outcome with respect to a specific attribute in the data we have chosen a single sensitive attribute to consider per For propublica-violent-recidivism and propublica-recidivism the results for GaussianNB and Feldman-GaussianNB are drawn behind Feldman-GaussianNB-accuracy In addition propublica-violent-recidivism is unbalanced with only of the data representing rearrests Figure The results of the Zafar et al algorithm on Ricci top and the Feldman et al algorithm on Adult Income bottom when the provided parameter to tradeoff between fairness and accuracy is used The parameter is varied and each split and each new parameter value is shown dataset in these overall results It is clear that there is no one winner no algorithm that is both more fair and more accurate than the others on all datasets It is also clear that there is tremendous variation even within a single algorithm over the random splits it receives We examine this point in more detail next Stability When analyzing algorithms we are additionally concerned with stability will the algorithm still perform well if the training data is slightly different To assess this we considered the standard deviation of each metric over random splits where for each split the algorithm is trained and evaluated on a different train/test partition The results are shown in Figure for Adult Income for all algorithms when focusing on non-discrimination in terms of race left and sex right using numerical-binary preprocessing These results give perhaps the clearest indication of the quality of an algorithm on a given data set It is also easy to see that each algorithm occupies a slightly different place on the trade-off between fairness measured here by CV when taken over binary sensitive attributes and accuracy For example when focusing on non-discrimination in either sex or race on the Adult dataset Zafar et als algorithm is potentially the best choice in terms of a balance between fairness and accuracy but the large standard deviation over CV may make it a less desirable option Parameters Many fairness-aware learning algorithms provide a parameter to allow manually trading off fairness and accuracy We automate the search for this balance and present results for all algorithms optimizing accuracy or fairness This provides an additional means of testing the algorithm as well as the possibility for further optimizing the tradeoff between the two Figure shows different results based on parameter tuning for the Zafar et al algorithm on Ricci left and the Feldman et al algorithm on Adult Income A clear tradeoff between fairness and accuracy in these algorithms can be seen the parameters are appropriately allowing exploration of the possible solution space Multiple sensitive attributes Figure Four algorithms making predictions while accounting for different protected attributes race sex and a composite attribute These not only behave quite differently from one another but their performance varies significantly depending on which specific attribute is being considered While there are still few fairness-aware algorithms that can explicitly handle multiple sensitive attributes all algorithms discussed can handle them if preprocessed as described earlier so that they are combined into a single sensitive attribute eg racesex However we might expect combining the attributes in this way to degrade performance under some metrics especially when such algorithms can only handle binary sensitive attributes or when too many combinations cause imbalance issues for some of the new combined sensitive values Looking at the Adult dataset when fairness-aware algorithms are run focusing on non-discrimination in terms of race sex and both we find varying results for each of the algorithms in Figure Sex is especially predictive on the Adult Income data set so the CV value for sex is low even on these fairness-aware algorithms Race generally receives a higher CV value from these algorithms When correcting for both at once most of the algorithms find that the CV value is somewhere in between that for race and that for sex but the Zafar et al algorithm has a much larger variance over race and sex than over either individually While it might have been the case that looking at a combined sensitive value would cause these algorithms to drastically lower in accuracy and/or fairness encouragingly this does not appear to be the case DISCUSSION Besides providing a central point of access to existing fairness-enhancing interventions and classification algorithms our benchmark also highlights a number of gaps in the current practice and reporting of fairness issues in machine learning We conclude with the following recommendations for future contributions to the area Emphasize preprocessing requirements If there are multiple plausible ways in which a dataset can be processed to generate training data for an algorithm provide performance metrics for more than one of the possible choices If algorithms are being compared to each other ensure they are compared based on the same preprocessing Avoid proliferation of measures New fairness measures should only be introduced if they behave fundamentally differently from existing metrics Our study indicates that a combination of group-conditioned accuracy and either DI or CV is a good minimal set Account for training instability Showing the performance of an algorithm in a single training-test split appears to be insufficient We recommend reporting algorithm success and stability based on a moderate number of randomized training-test splits One limitation of our benchmark is the number of methods it currently provides implementation for We hope other researchers will contribute their implementations to the repository It would be particularly interesting to see how our conclusions above evolve as the number and variety of methods increases Additionally while we frame some of the differences in algorithm performance as fairness versus accuracy tradeoffs this can be misleading since it makes many assumptions about the data and social context including eg that the labels represent desired outcomes We leave the examination of how the algorithmic choices interact with the social context for other work