Fairness and Abstraction in Sociotechnical Systems A key goal of the fair-ML community is to develop machine-learning based systems that once introduced into a social context can achieve social and legal outcomes such as fairness justice and due process Bedrock concepts in computer science such as abstraction and modular design are used to define notions of fairness and discrimination to produce fairness-aware learning algorithms and to intervene at different stages of a decision-making pipeline to produce fair outcomes In this paper however we contend that these concepts render technical interventions ineffective inaccurate and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems We outline this mismatch with five traps that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them Finally we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions and by drawing abstraction boundaries to include social actors rather than purely technical ones CONCEPTS Applied computing Law social and behavioral sciences Computing methodologies Machine learning KEYWORDS Fairness-aware Machine Learning Sociotechnical Systems Interdisciplinary INTRODUCTION On the typical first day of an introductory computer science course the notion of abstraction is explained Students learn that systems can be described as black boxes defined precisely by their inputs outputs and the relationship between them Desirable properties of a system can then be described in terms of inputs and outputs alone the internals of the system and the provenance of the inputs and outputs have been abstracted away Machine learning systems are designed and built to achieve specific goals and performance metrics eg AUC precision recall Thus far the field of fairness-aware machine learning fair-ML has been focused on trying to engineer fairer and more just machine learning algorithms and models by using fairness itself as a property of the black box system Many papers have been written proposing definitions of fairness and then based on those generating best approximations or fairness guarantees based on hard constraints or fairness metrics Almost all of these papers bound the system of interest narrowly They consider the machine learning model the inputs and the outputs and abstract away any context that surrounds this system We contend that by abstracting away the social context in which these systems will be deployed fair-ML researchers miss the broader context including information necessary to create fairer outcomes or even to understand fairness as a concept Ultimately this is because while performance metrics are properties of systems in total technical systems are subsystems Fairness and justice are properties of social and legal systems like employment and criminal justice not properties of the technical tools within To treat fairness and justice as terms that have meaningful application to technology separate from a social context is therefore to make a category error or as we posit here an abstraction error In this paper we identify five failure modes of this abstraction error We call these the Framing Trap Portability Trap Formalism Trap Ripple Effect Trap and Solutionism Trap Each of these traps arises from failing to consider how social context is interlaced with technology in different forms and thus the remedies also require a deeper understanding of the social to resolve problems After explaining each of these traps and their consequences we draw on the concept of sociotechnical systems to ground our observations in theory and to provide a path forward The field of Science and Technology Studies STS describes systems that consist of a combination of technical and social components as sociotechnical systems Both humans and machines are necessary in order to make any technology work as intended By understanding fair-ML systems as sociotechnical systems and drawing on analytical approaches from STS we can begin to resolve the pitfalls that we identify Concretely we analyze the five traps using methodology from STS and present mitigation strategies that can help avoid the traps or at the very least help designers understand the limitations of the technology we produce Key to this resolution is shifting away from a solutions-oriented approach to a process-oriented one one that draws the boundary of abstraction to include social actors institutions and interactions This paper should not be read as a critique of individual contributions within the field of fair-ML most of which are excellent on their own terms Rather as scholars who have been invested in doing this work we take aim at fair-MLs general methodology limited as it is by the bounds of our primarily technical worldview We echo themes of STS scholars such as Lucy Suchman Harry Collins Phil Agre and Diana Forsythe who in the s critiqued the last wave of artificial intelligence research along similar lines highlighting that the culture of and modes of knowledge production in computer science thwarted the social goals that the field was trying to achieve An emerging wave of similar critiques has been directed at the field of fair-ML We hope to offer a constructive reform in our nascent field by highlighting how technical work can be reoriented away from solutions to process and identifying ways in which technical practitioners and researchers can meaningfully engage social contexts in order to more effectively achieve the goals of fair-ML work THE