Actionable Recourse in Linear Classification models are often used to make decisions that humans whether to approve a loan application extend a job oer or provide insurance In such applications individuals should have the ability to change the decision of the model When a person is denied a loan by a credit scoring model for example they should be able to change the input variables of the model in a way that will guarantee approval Otherwise this person will be denied the loan so long as the model is deployed and more importantly will lack agency over a decision that their livelihood In this paper we propose to evaluate a linear model in terms of recourse which we as the ability of a person to change the decision of the model through actionable input variables eg income vs age or marital status We present an integer programming toolkit to i measure the feasibility and of recourse in a target population and ii generate a list of actionable changes for a person to obtain a desired outcome We discuss how our tools can inform stakeholders by using them to audit recourse for credit scoring models built with real-world datasets Our results illustrate how recourse can be by common modeling practices and motivate the need to evaluate recourse in algorithmic decision-making CONCEPTS Human-centered computing Social recommendation Theory of computation Integer programming foundations of intelligence Machine learning KEYWORDS recourse classification accountability integer programming audit credit scoring INTRODUCTION In the context of machine learning we recourse as the ability of a person to obtain a desired outcome from a prediction model Consider for example a classification model used for loan approval If the model provides recourse to someone who is denied a loan then this person can change the input variables of the model in a way that guarantees approval Otherwise this person will be denied the loan so long as the model is deployed and will lack agency in its decision-making process Recourse is not formally studied in machine learning In this paper we argue that it should be In particular a model should provide all individuals with recourse when it is used to allocate a good that should be universally accessible such as credit employment and public services Given that a lack of human agency is often perceived as a source of injustice in algorithmic decision-making recourse should be evaluated whenever humans are subject to the predictions of a machine learning model The potential lack of recourse is often used to motivate calls for increased transparency and explainability in algorithmic decisionmaking see eg However transparency and explainability do not guarantee recourse In fact even a simple transparent model such as a linear can fail to provide recourse due to seemingly innocuous modeling practices These include Choice of Features A could use features that are immutable eg age conditionally immutable eg which can only change from FALSE TRUE or should not be considered actionable eg married Choice of Operating Point A probabilistic classifier that provides recourse at a given threshold eg i if predicted risk of default may fail to provide recourse at a more conservative threshold eg i if predicted risk of default Out-of-Sample Deployment A feature that must be altered to obtain a desired outcome could be missing immutable or adversely distributed in the population on which the model is deployed ie its target population In most real-world applications an audit provides a practical mechanism to evaluate recourse This is because an audit can examine the recourse of a model without the way a model is developed and reveal that are tailored to its target population Even when an audit suggests that a model will provide recourse within its target population however a model could require drastic changes that preclude certain individuals from obtaining a desired outcome Ideally an audit should therefore evaluate both the feasibility and of recourse within the target population In this paper we present a practical toolkit to evaluate recourse for linear classification models eg logistic regression models linear and rule-based models that can be expressed as linear models such as rule sets and decision lists Our tools allow a range of stakeholders eg practitioners regulators and decision-subjects to answer questions such as Does a model provide recourse to all individuals who are subject to its predictions How does the difficulty of recourse vary across individuals in a target population Are there disparities in recourse between subgroups in the target population What changes can an individual make to obtain a desired prediction from the model Our toolkit is based on a optimization problem that given a linear classifier will identify changes that a person can make to ip their predicted outcome Our problem is formulated changes that are actionable meaning that they will not immutable features nor alter mutable features in an infeasible way eg from or or from TRUE FALSE Since actionable changes for discrete features eg binary ordinal or categorical features can only be enforced through discrete constraints the problem is computationally challenging In order to allow an auditor to state that does not provide a person with recourse we solve our problem directly by expressing it as an integer program IP and handing it to an IP solver eg or We use this procedure to develop two tools to evaluate recourse A procedure to audit the recourse of a classifier in a target population eg for model development procurement or impact assessments Given a sample of points ie feature vectors from the target population we solve our problem for each point receives an undesirable prediction Our procedure outputs an estimate of the feasibility and difficulty of recourse A method to generate a list of actionable changes for a person to obtain a desired outcome We refer to this list as a and provide an example in Figure In the United States for example the Fair Credit Reporting Act requires that individuals who are denied credit be sent an adverse action notice to explain the principal reason for the denial It is well-known that an adverse action notice can fail to provide actionable information see eg for a discussion By including a in an adverse action notice a person would know exact changes that they can make to guarantee approval in the future C C VR V FALSE TRUE FALSE TRUE Figure Illustrative for a is denied credit by a classification model Each item row shows an actionable change to a subset of features that will ip the prediction from to The changes guarantee that the person will be approved for credit so long as the model remains deployed and other features do not change We describe how to build in Section and discuss their limitations in Section Related Work Recourse is broadly related to several topics in machine learning These include inverse classification which aims to determine how the inputs to a prediction model can be manipulated to obtain a desired outcome strategic classification which considers the converse problem of training that are robust to malicious manipulation which studies the robustness of predictions with respect to small changes in input and which are subsets of features that fully determine the prediction of a model Our tools are also broadly related to methods that explain the predictions of a machine learning model at an individual level see eg Although existing methods can produce valuable explanations of how a model outputs a prediction these explanations do not necessarily correspond to actionable changes that guarantee a desired outcome More importantly they do not provide a principled mechanism to evaluate the feasibility and difficulty of recourse in a target population One notable exception are methods to generate counterfactual explanations see eg Our work is related to counterfactual explanations in that we can assess the feasibility of recourse through the existence of an actionable counterfactual explanation In a seminal paper et al present a general method to recover counterfactual explanations from black-box models This method cannot be used or adapted to evaluate recourse as i it cannot constrain changes to be actionable and ii it assumes that all feasible changes are in the training data ie a feasible change is as a x x where x x are points in the training data We overcome these issues by developing a fundamentally approach to evaluate recourse Our approach uses integer programming which allows users to precisely characterize the set of feasible actions certify that a model does not provide recourse and evaluate the difficulty of recourse using a rich class of cost functions and Workshop Paper We provide a software implementation of our tools and scripts to reproduce our experimental results at This paper extends a short workshop paper that was presented at PROBLEM STATEMENT In this section we define the optimization problem that we solve to evaluate recourse and discuss formal guarantees related to the cost and feasibility of recourse We include proofs for all technical results in Appendix A Optimization Framework We consider a standard classification task where each individual is characterized by a vector of features x x X X Rd and a binary label In other words we can claim that a classifier provides recourse to a person if we can nd an actionable counterfactual explanation for this person In order to claim that a classifier does not provide recourse however we must prove that any actionable change will fail to ip this prediction for this person To illustrate the practical consequences of i and ii the method in could output an explanation stating that a person can ip their prediction by changing an immutable feature due to i If so we could not conclude that the model did not provide this person with recourse as there may exist a different way to ip the prediction that was not in the training data due to ii We wish to audit a linear classifier x where wd Rd is a vector of is the intercept We denote the desired outcome as and assume that sign so that Given an individual whose predicted outcome is x we aim to determine if there exists an action a such that x a To this end we solve an optimization problem of the form min st x a a Ax where Ax is a set of feasible actions from x Each action is a vector a a ad where x R x We let x if feature is immutable and say feature is conditionally immutable if x for some x X cost x Ax R is a cost function that encodes preferences between actions or measures quantities of interest for an audit see Section Users can specify any cost function that two properties i no action no cost ii costa x larger actions higher cost Solving the optimization problem in for an individual with features x has different implications with respect to recourse If is infeasible then no action can achieve a desired outcome from x Thus we have that the model does not provide actionable recourse for an individual with features x If is feasible then its optimal solution is the action to ip the prediction of x In this case we can use the solution to create an item in a see Section Assumptions Notation and Terminology We denote the index sets for all features as d for actionable features as Ax x and immutable features as N x x We drop the dependence of index sets on x when it is clear from the context Given a linear classifier x we express its vector as where wA and contain the for features that are actionable and immutable respectively We assume that the classifier is deployed on a target population where features are bounded ie for all x X B for a large B and define the follow of the feature space based on the values of x and x X x x X x D x X D x X Feasibility Guarantees We start with a simple condition for a linear classifier to provide recourse to all individuals in any target population R A linear classifier provides recourse to all individuals if it only uses actionable features and does not trivially predict a single class Remark may be used to guide regulations in applications where a classifier must provide recourse or to design screening questions for algorithmic impact assessments eg can a person change all of the inputs to the classifier We observe that the converse of Remark is also true a classifier fails to provide recourse if all features are immutable or if it trivially predicts a single class In what follows we therefore restrict our attention to linear classifiers with non-zero coefficients that do not trivially predict a single class in the target population Our next set of show that the feasibility of recourse depends on the of the feature space R If all features are unbounded then a linear classifier with one or more actionable features will provide recourse to all individuals in any target population R If all features are bounded then a linear classifier with one or more immutable features may not provide recourse to some individuals in a target population and imply that when a classifier contains a combination of actionable and immutable features then any claim regarding the feasibility of recourse depends on how we specify bounds for actionable features and in particular actionable features with real values Given that a lack of recourse could have important real-valued features should be bounded judiciously In practice we would advise setting loose bounds on real-valued features so that the auditor does not report infeasibility due to overly restrictive bounds This approach has a potential drawback in that loose bounds allow the classifier to provide recourse through potentially drastic changes Although this may appear to reduce the of assessing feasibility such changes will be in an audit through large values of a suitable cost function eg the maximum percentile shift function in Recourse is not guaranteed when a classifier uses features that are immutable or conditionally immutable eg age or As shown in Example a classifier with an immutable feature could achieve perfect predictive accuracy without providing a universal recourse guarantee Example Consider training a linear classifier using a dataset of n examples xi i ni where xi d and the labels i are sampled from the distribution Pr x x so that the Bayes optimal classifier has the form x x i If d then will not provide recourse to any individual where x for an immutable feature N In practice such features may be desirable to include in a model because they can improve its predictive performance or its robustness to strategic manipulation In practice an auditor would determine a set of feasible actions by specifying upper and lower bounds for each actionable feature which must exist given that the target population is Since binary ordinal and categorical features are bounded by an auditor would only have to specify bounds for real-valued features such as age or income In our experiments in Section we set bounds for these features as the maximum observed value in the dataset Cost Guarantees In Theorem we present a bound on the expected cost of recourse in a target population The expected cost of recourse of a classifier X is as where a is an optimal solution to the optimization problem in Our guarantee is expressed in terms of a general cost function of the form where c X is a positive scaling function for actions starting from x X and X is a closed convex set T The expected cost of recourse of a linear classifier over a target population p A p A max A where p is the false omission rate of p is the negative predictive value of A wA is the expected unit cost of actionable changes for false negatives A wA is the expected unit cost of actionable changes for true negatives max A wA is the maximum unit cost of actionable changes for negative predictions p p is the internal risk of actionable features Theorem implies that we can reduce the expected cost of recourse by reducing the maximum unit cost of actionable changes max A or the internal risk of actionable features Here captures the calibration between true outcomes and the actionable component of the for points such that x If then the actionable component of the scores is perfectly aligned with true outcomes which produces a tighter bound on the expected cost of recourse INTEGER PROGRAMMING TOOLKIT In this section we describe how we can solve the optimization problem in using integer programming and then discuss how we use this procedure to audit recourse and to IP Formulation We consider a version of the optimization problem in which can be expressed as an integer program IP and solved with an IP solver see for a list This approach has several i it can directly constrain actions for binary ordinal and categorical features ii it can optimize non-linear and non-convex cost functions iii it allows users to customize the set of feasible actions and iv it can quickly recover a globally optimal solution or certify that actionable recourse does not exist We express the optimization problem in as an IP of the form min cost st cost jA mj c a jA d b mj A c mj A d R A A mj A Here constraint a determines the cost of a feasible action from cost parameters c x Constraint b requires any feasible action to ip the prediction of a linear classifier with Constraints c and d restrict to a grid feasible values via the indicator variables and Note that the variables and constraints only depend on actions for actionable features A since when a feature is immutable Modern IP can quickly solve instances of to optimality ie in s with In practice we can further reduce solution time eg for auditing procedures where we solve multiple times by i dropping for actions that do not agree in sign and ii declaring as a special ordered set of type I which allows the solver to use a more algorithm Users can easily customize the set of feasible actions by adding logical constraints to Many constraints can be expressed using the without having to introduce new variables To limit actions to change r features we can add the constraint r To ensure actions change only one feature in a subset of features S we can add the S Such constraints are required for example when a linear classifier contains a subset of dummy variables to encode a categorical attribute ie a one-hot encoding Discretization The IP formulation in requires users to the actions for real-valued features over a suitable grid In Appendix B we discuss how to the actions for features so that discretization does not affect the feasibility or cost of recourse In particular we show that i discretization does not affect the feasibility of recourse if we restrict the actions for real-valued features to a grid with matching upper and lower bounds and ii the maximum discretization error in the cost of recourse can be bounded by the grid One can avoid discretization entirely by formulating an IP that uses continuous variables to represent the actions for real-valued features see Appendix B In light of our guarantees in Appendix B we do not consider this approach because it unnecessarily users to use linear cost functions Cost Functions Cost functions should be used to encode preferences between feasible actions or to measure quantities of interest in the target population However they should not be used to penalize infeasible actions as infeasibility can be directly modeled by adding hard constraints to the IP formulation The IP in can optimize a large class of cost functions because it these values and encodes them in the c parameters in constraint a Although IP requires costs to be by the values of actions in each dimension cost functions do not need to be strictly separable since the IP can handle some kinds of by introducing additional constraints see eg the cost function for auditing in We present two cost functions for auditing and building in and These functions can be adapted by practitioners who wish to design cost functions Our functions measure costs in terms of the percentiles of x and x Q x and Q x where Q is the of x in the target population Unlike standard distance metrics cost functions based on percentiles do not depend on the scale of features and account for the distribution of features in the target population Our functions assign the same cost for a unit percentile change for each feature which implicitly assumes that percentile changes along different features are equally This assumption can be relaxed by having a domain expert specify the relative difficulty of changing features relative to a baseline feature Auditing Recourse We can use IP to audit the recourse of a linear classifier on a target population The auditing procedure requires i the of the linear classifier and ii a sample of feature vectors from the target population xi ni where xi It solves the IP for each point in the sample to output an estimate of the feasibility of recourse ie the of points for which the IP is feasible an estimate of distribution of the cost of recourse ie values of xi where ai is the action As our cost function we propose the maximum percentile shift ax max A Q x Q x The of auditing with this cost function lies in the meaning of the optimal cost If the optimal cost is for example then any feasible action must change a feature by at least percentiles That is no action can ip the prediction without changing a feature by less than percentiles Using requires replacing constraint a in IP with A constraints of the form cost c for A Minimizing the cost function in is also useful when we wish to evaluate how bounds on changes affect the feasibility of recourse Say that we wanted to measure how many individuals have recourse when actions are bounded to changes of at most percentiles or changes of at most percentiles Instead of running two separate audits with action sets that restrict feasible changes to a percentile shift and a percentile shift we can run a single audit that minimizes the maximum cost of an loosely bounded action set and compare the number of individuals where the optimal cost exceeds and Building a such as the one in Figure by using an enumeration procedure that repeatedly solves the IP in In order to provide an individual with recourse should ideally include multiple actions that will ip the predicted outcome This is because each action can be infeasible in a way that is only known by the individual see eg for an example In Algorithm we present a procedure to enumerate T minimal-cost actions with distinct combinations of features Each iteration solves the IP to obtain the optimal action a then adds a constraint to the IP to eliminate actions that use the same subset of features as a The procedure these steps until it has recovered T actions or the IP becomes infeasible which means that it has a minimal-cost action for all subsets of features that can ip the prediction for x Each action a A produced by Algorithm can be used to create an item in by listing the current feature values x alongside the desired feature values x a for S Algorithm Enumerate T Minimal Cost Actions for Input IP instance of for features x and actions Ax T number of items A actions shown repeat a optimal solution to IP A A a add a to set of optimal actions S indices of features altered by a add constraint to IP to remove actions that alter features S d until A T or IP is infeasible Output A actions shown Algorithm can be adapted to produce different kinds by changing the constraint in Step to enumerate other kinds of successive optima For example one can create a containing mutually exclusive actions by adding the constraint for S to remove all features used in a at each iteration As our cost function we propose the total shift ax A log Q x Q x This function aims to produce where items easy changes with respect to the target population In particular it ensures that cost of increases exponentially as Q x This aims to capture the notion that changes become harder when starting o from a higher percentile value eg changing income from percentiles is harder than DEMONSTRATIONS In this section we illustrate how our tools can be used to audit recourse in three credit scoring problems We have two goals i to show how an audit can provide useful information for different stakeholders eg individuals practitioners and ii to show how the feasibility and difficulty of recourse can be by common modeling practices We provide a software implementation of our tools and scripts to reproduce our analyses at We trained all classifiers as implemented in and used a standard -fold cross-validation setup to tune free parameters and to estimate their predictive performance We solved all for auditing and generation with the solver on a laptop with with GB RAM Model Selection We start with a simple experiment to show how our tools can be used to inform different stakeholders in credit scoring applications Setup We consider a processed version of credit dataset Here i if person i will default on their credit card payment The dataset contains n individuals and d features related to spending and payment patterns education credit history age and marital status We assume that spending and payment patterns and education are actionable and assume that all other variables are immutable We train logistic regression models for penalties in the set We audit the recourse of each model on the training dataset by solving for each individual i such that i Our IP includes the following constraints to ensure changes are actionable i changes for discrete features must be discrete eg ii can only increase iii no changes to immutable features Results We summarize the results of our audit in Figure and present a for a person who is denied credit in Figure As shown in Figure tuning the has a minor on test error but a major on recourse classifiers with small provide all individuals with recourse As the increases however the of individuals with recourse decreases as the coefficients for actionable features are more heavily penalized compared to the coefficients for immutable features The cost of recourse provides a communicable measure of the difficulty of a desired outcome among individuals who have recourse Since we audit using the maximum percentile shift cost function in a cost of q implies a person must change a feature by at least q percentiles to attain a desired outcome Here we see that increasing the -penalty nearly doubles the median cost of recourse from to Thus at a small -penalty the median person with recourse can only attain a desired outcome by changing a feature by at least percentiles At a large -penalty however the median person with recourse can only attain a desired outcome by changing a feature by at least percentiles Discussion Our aim is not to suggest a relationship between recourse and but to show how common practices such as parameter tuning can impact the feasibility and difficulty of recourse Here a practitioner who is primarily interested in performance could deploy a classifier that precludes individuals from achieving a desired outcome eg the one that minimizes mean test error even as there exists a classifier that attains similar performance but provides all individuals with recourse eg a classifier with a slightly lower -penalty Our tools provide the necessary information for a practitioner to choose between such classifiers and incorporate the feasibility and cost of actionable recourse in their model development pipeline Our tools can also identify mechanisms that affect recourse in a target population by comparing the cost and feasibility of recourse for different action For example one can evaluate how the mutability of feature recourse by running audits using i an action set where feature is immutable x for all x X and ii an action set where feature is actionable x for all x X Here such an analysis reveals that the lack of recourse is tied to an immutable feature related to credit history ie a binary feature set to if a person has ever on a loan Given this information a practitioner could replace this feature with a mutable variant ie a binary feature set to if a person has recently on a loan and thereby deploy that provides recourse Such changes are sometimes mandated by regulations see eg policies on forgetfulness in Our tools can support such regulations by providing policymakers with an estimate of their impact on the feasibility and cost of recourse in a population of interest Figure Overview of model performance and recourse over the training sample for logistic We show the mean test error top left of coefficients top right of individuals with recourse bottom left and the distribution of the cost of recourse bottom right for all classifiers C VR V Figure for a person who is denied credit by the most accurate classifier built for credit Each item describes minimal-cost changes for the individual to attain the desired outcome We all items in second using the cost function in and Algorithm Out-of-Sample Deployment We now discuss an experiment that shows how recourse is affected by out-of-sample deployment We consider a setting where a classifier is deployed on individuals who are underrepresented in the training population Our setup is inspired by a real-world feedback loop with credit scoring in the United States namely credit scoring models are built using training datasets that young adults since young adults lack the credit history to apply for loans and produce labeled data thus making it harder for young adults to be approved see eg for a discussion Setup We consider a processed version of the dataset if person i will experience distress over the next two years The dataset contains n individuals and d features related to their age number of dependents and recent history We assume that all features are actionable except for Age and Our audit compares the cost of recourse for individuals in the target population for two -penalized logistic regression models Baseline classifier This is a baseline model that we use for the sake of comparison It is trained using n individuals in the processed dataset which represents our target population Biased classifier This is the model that we wish to audit We train this model on a sample of n individuals from the baseline classifier dataset except individuals with Age individuals are excluded We present both models in Appendix C We set the threshold for approval for each model to approve of individuals in the target population ie i if predicted probability of repaying the loan is We compute the cost of recourse using percentile distributions of features in the target population Results We present the results of our audit in Figure As shown the cost of recourse can change due to dataset shift Here the median cost of recourse of the biased model among young adults in the target population is which means that they can only ip their predictions by a percentile shift in a given feature In comparison the median cost of recourse for the baseline model among young adults is which is significantly observe that the in the cost of recourse are far less pronounced for other age brackets as the median cost for individuals that are represented in both populations does not appear to change To illustrate the of out-of-sample deployment from an individual perspective we choose a young adult from the target population who is denied credit by classifiers and show the minimal that will attain the desired outcome from each classifier in Figure BC BC False Negatives True Negatives Figure Distribution of the cost of recourse in the target population for each classifier split on the basis of the true outcome We show the cost of recourse for the biased classifier left and the baseline classifier right For each classifier we show the cost distribution for false negative predictions top and true negative predictions bottom The baseline classifier is trained using a representative dataset from the target population while the biased classifier is trained using an dataset that young adults Age The cost of recourse for young adults is significantly higher for the biased classifier regardless of their true outcome Discussion Our aim is not to suggest that out-of-sample deployment increases the cost of recourse but that out-of-sample deployment can simply produce a change in the cost of recourse Without theoretical guarantees on how recourse can change due to distributional in the training data and target population such can only be measured by an audit using a sample of features from the target population In practice this procedure could be used for model procurement where classifiers are trained using datasets that are significantly different from the target population on which they will be deployed There are other mechanisms by which out-of-sample deployment can affect recourse that are now shown here In particular models that do not allow users to adjust the threshold to a target population may result in infeasibility or higher costs for that population Moreover the set of feasible actions can significantly between populations Both of these differences were controlled for BC FC VR V BC FC VR V Figure Full for an individual in the baseline Under the this individual has Pr i Under the biased model the individual scores Pr i We show actions that will result in approval from the biased classifier top and the baseline classifier bottom The full for the biased classifier contains only a single item whereas the for the baseline classifier contains multiple items in this experiment we the same action set the same costs and adjusted the threshold so the observed effects of out-of-sample deployment only depend on distributional differences in the feature Evaluating Disparities in Recourse We consider an experiment to demonstrate how our tools could be used to evaluate disparities in recourse across demographic groups In particular we wish to measure the disparity in recourse between males and females in a target population while controlling for potential confounders Here a disparity in recourse between males and females occurs if given comparable individuals who are denied a loan in the target population males can obtain credit by making easier changes or Setup We consider a processed version of the german dataset Here i if an individual is a bad customer which we assume means they have defaulted The dataset contains n individuals and d features related to their loan status and demographics The dataset includes a feature gender which purposely drop from the training dataset We trained a classifier using -penalized logistic regression We set the approval threshold for our classifier to approve individuals with a predicted probability of We ran an audit over all individuals who were denied the loan on the training dataset and evaluated potential disparities by matching individuals with the same true label and similar levels of predicted risk Pr Results As shown in Figure the cost of recourse can vary between males and females in the target population The plot in the left shows the cost for females and males among individuals with a true label of while the plot in the right shows the cost for females and males among individuals with a true label of These disparities in recourse can also be studied by comparing as in Figure show feasible actions for individuals in each subgroup with the same outcome and similar levels of predicted risk Figure Overview of recourse disparities between males and females in the target population On the top row we plot the distribution of the cost of recourse for males and females based on their predicted risk and true label we plot the cost for individuals where left and right with i and FC VR V months months FALSE TRUE FALSE TRUE FALSE TRUE months months FALSE TRUE FALSE TRUE months months FALSE TRUE M with i and FC VR V months months FALSE TRUE TRUE FALSE FALSE TRUE Unemployed TRUE FALSE months months FALSE TRUE TRUE FALSE FALSE TRUE months months FALSE TRUE FALSE TRUE months months FALSE TRUE Figure for a matched pair of individuals from each subgroup Individuals on the basis of their true label i and their predicted risk Pr i CONCLUDING REMARKS We have presented new tools to evaluate the recourse of a linear classifier in a population of interest and shown how they can inform various stakeholders including practitioners who may affect recourse through seemingly harmless modeling decisions regulators who may be interested in certifying that a model provides recourse over a target population and who may wish to learn changes that let them attain a desired predicted outcome Extensions Non-Linear Classifiers We are currently extending our tools to evaluate recourse for non-linear classifiers One could immediately apply our tools to this setting albeit heuristically by solving our IP with a local linear classifier that the local decision boundary in actionable space see eg the technique used to estimate the local decision boundary in This approach could be useful to build but would not provide a proof of infeasibility required to assess the feasibility and difficulty of recourse Evaluating Strategic Incentives Our tools can price incentives of a model in a target population by comparing the cost of recourse for different action sets see eg Consider a case where a credit score contains features that are causally related to creditworthiness eg income as well as ancillary features that have predictive value but are prone to manipulation eg social media presence In this case we could evaluate incentive structures in a target population by comparing the cost of recourse using actions on i only the causal features or ii using causal features and at least one ancillary feature If actions using i are less costly than actions using ii then individuals in the target population may not be incentivized to manipulate the model Measuring Flexibility An interesting extension of our work is to run an audit where for each individual in our target sample we enumerate all distinct minimal-cost actions that will attain a desired outcome ie by running the enumeration procedure in Algorithm until the IP becomes infeasible This produces a collection of minimal-cost actions that fully characterizes all of the ways in which an individual can attain a desired outcome The size of this collection the of recourse for an individual which could be used to quantitatively evaluate other properties of the recourse set eg if a classifier provides types of changes that provide recourse of which are legally contestable then may be deemed contestable This audit would be computationally intensive but not necessarily intractable given that all actions can still be achieved relatively quickly based on the dimensions of the action set ie seconds Limitations Misleading do not necessarily reveal the principle reasons for a decision and may not present legally contestable information when it exists Since the in this work only show features that must be altered a person may fail to ip their prediction after making the suggested changes if they were to unintentionally change other features used by the classifier In practice this limitation can be overcome by providing users with clear guidelines eg a complete list of features or the signs of their coefficients Alternatively one could produce a of actions that would allow a person to ip their prediction while providing a for potential changes on omitted features Cost Functions Our cost functions depend on percentile distributions which may not correctly the difficulty of recourse eg if there are not enough samples from the target population or the sample does not the target population In practice we would expect an auditor to choose cost functions carefully using our guidelines in Section Manipulation and Model The Providing individuals has the drawback in that it could lead individuals to attain a desired outcome by making changes This kind of manipulation may be avoided by releasing with actions pertaining to features that are causally related to the outcome or by training a model that only uses such features in the place see eg Releasing could also lead to model theft see eg and to the credit score in Germany by crowdsourcing In light of potential model theft it would be interesting to study how many actions must be collected to faithfully reconstruct a proprietary model and whether model theft could be mitigated by producing with actions that have weaker guarantees Discussion Machine Learning At glance the goal of building a model that provides recourse may seem antithetical to the goal of building a model that is robust to manipulation However this is not the case if the model uses features that are causally related to the outcome A model could be built so that a person can attain a desired outcome by only making constructive changes eg a person who is denied credit can only be approved by changing features that improve their creditworthiness such as income A model could also be designed so that a person could attain a desired outcome by making antagonistic changes but is incentivized to make constructive changes see Section for a discussion on how our tools can evaluate such incentives As shown in Section recourse and predictive accuracy are not necessarily incompatible it may be possible to train a model that provides recourse which is just as accurate as a model that fails to provides recourse As illustrated in Example however there may exist classification problems in which there is a between recourse and accuracy For example a credit score could include immutable features that improve accuracy but reduce recourse Such a lead to a decision regarding deployment should we deploy a credit score with perfect predictive accuracy but that precludes some individuals from receiving loans Or should we deploy a model that provides all individuals with recourse but that may allocate loans Policy Implications Individual rights with respect to algorithmic decision-making are often motivated by the need for agency over machine-made decisions eg argues that autonomy is one of the core for data protection laws Recourse a precise notion of agency namely the ability to meaningfully a decision-making process A lack of recourse may be contestable in applications where we would expect individuals to have agency over their predicted outcome eg loan approval or hiring It is not clear however if a right to recourse would extend to other areas where machine learning is used In an application such as recidivism prediction for example one could argue that a model should provide defendants who are predicted to recidivate with the ability to change this prediction by altering certain kinds of attributes For example a defendant who is predicted to recidivate due to their age and prior criminal history should be able to alter this prediction by clearing their criminal history While regulations for algorithmic decision-making are still in their infancy the vast majority of existing have sought to ensure this kind of agency indirectly through laws that focus on transparency and explanation see eg regulations for credit scores in the United States such as In light of past efforts we argue that recourse should be treated as a policy objective in applications where it is desirable This is because recourse not only represents a and measurable notion explainability but also because there exist multiple ways in which it can be regulated For example one could mandate that a classification model that makes predictions on individuals must be paired with a recourse audit for its target population or mandate that individuals who attain an undesirable prediction are provided with a set of actions that guarantee a desired outcome