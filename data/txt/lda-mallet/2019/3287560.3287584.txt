A Moral Framework for Understanding Fair ML through Economic Models of Equality of Opportunity We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity EOPan extensively studied ideal of fairness in political philosophy We formally show that through our conceptual mapping many existing definition of algorithmic fairness such as predictive value parity and equality of odds can be interpreted as special cases of EOP In this respect our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness Most importantly this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness and interpret recent fairness impossibility results in a new light Last but not least and inspired by luck egalitarian models of EOP we propose a new family of measures for algorithmic fairness We illustrate our proposal empirically and show that employing a measure of algorithmic unfairness when its underlying moral assumptions are not satisfied can have devastating consequences for the disadvantaged groups welfare CONCEPTS Computing methodologies Supervised learning Batch learning Applied computing Economics Sociology KEYWORDS Equality of Opportunity EOP Fairness for Machine Learning Rawlsian and Luck Egalitarian EOP Statistical Parity Equality of Odds Predictive Value Parity INTRODUCTION Equality of opportunity EOP is a widely supported ideal of fairness and it has been extensively studied in political philosophy over the past years The concept assumes the existence of a broad range of positions some of which are more desirable than others In contrast to equality of outcomes or positions an equal opportunity policy seeks to create a level playing field among individuals after which they are free to compete for different positions The positions that individuals earn under the condition of equality of opportunity reflect their merit or deservingness and for that reason inequality in outcomes is considered ethically acceptable Equality of opportunity emphasizes the importance of personal or native qualifications and seeks to minimize the impact of circumstances and arbitrary factors on individual outcomes For instance within the context of employment one narrow interpretation of EOP requires that desirable jobs are given to those persons most likely to perform well in those with the necessary education and experience and not according to arbitrary factors such as race or family background According to Rawlss broader interpretation of EOP native talent and ambition can justify inequality in social positions whereas circumstances of birth and upbringing such as sex race and social background can not Many consider the distinction between morally acceptable and unacceptable inequality the most significant contribution of the egalitarian doctrine Prior work in economics has sought to formally characterize conditions of equality of opportunity to allow for its precise measurement in practical domains see eg At a high level in these models an individuals outcome/position is assumed to be affected by two main factors his/her circumstance c and effort e Circumstance c is meant to capture all factors that are deemed irrelevant or for which the individual should not be held morally accountable for instance c could specify the socio-economic status he/she is born into Effort e captures all accountability factors those that can morally justify inequality Prior work in economics refers to e as effort for the sake of concreteness but e summarizes all factors for which the individual can be held morally accountable the term effort should not be interpreted in its ordinary sense here For any circumstance c and any effort level e a policy induces a distribution of utility among people of circumstance c and effort e Formally an EOP policy will ensure that an individuals final utility will be to the extent possible only a function of their effort and not their circumstances While EOP has been traditionally discussed in the context of employment practices its scope has been expanded over time to other areas including lending housing college admissions and beyond Decisions made in such domains are increasingly automated and made through Algorithmic Data Driven DecisionMaking systems ADMs We argue therefore that it is only natural to study fairness for ADMs through the lens of EOP In this work we draw a formal connection between the recently proposed notions of fairness for supervised learning and economic models of EOP We observe that in practice predictive models inevitably make errors eg the model may mistakenly predict that a credit-worthy applicant wont pay back their loan in time Sometimes these errors are beneficial to the subject and sometimes they cause harm We posit that in this context EOP would require similar individuals in terms of what they can be held accountable for to have the same prospect of receiving this benefit/harm irrespective of their irrelevant characteristics More precisely we assume that a persons features can be partitioned into two sets those for which we consider it morally acceptable to hold him/her accountable and those for which it is not so We will broadly refer to the former set of attributes as the individuals accountability features and the latter as their arbitrary or irrelevant features Note that there is considerable disagreement on the criteria to determine what factors should belong to each category Roemer for instance proposes that societies decide this democratically We take a neutral stance on this issue and leave it to domain experts and stake-holders to reach a resolution Throughout we assume this partition has been identified and is given We distinguish between an individuals actual and effort-based utility when subjected to algorithmic decision making We assume an individuals advantage or total utility as the result of being subject to ADMs is the difference between their actual and effortbased utility Section Our main conceptual contribution is to map the supervised learning setting to economic models of EOP by treating predictive models as policies irrelevant features as individual circumstance and effort-based utilities as effort Figure We show that using this mapping many existing notions of fairness for classification such as predictive value parity and equality of odds can be interpreted as special cases of EOP In particular equality of odds is equivalent to Rawlsian EOP if we assume all individuals with the same true label are equally accountable for their labels and have the same effort-based utility Section Similarly predictive value parity is equivalent to luck egalitarian EOP if the predicted label/risk is assumed to reflect an individuals effort-based utility Section In this respect our work serves as a unifying framework for understanding existing notions of algorithmic fairness as special cases of EOP Importantly this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness and interpret recent fairness impossibility results in a new light Last but not least inspired by Roemers model of egalitarian EOP we present a new family of measures for algorithmic unfairness applicable to supervised learning tasks beyond binary classification We illustrate our proposal on a real-world regression dataset and compare it with existing notions of fairness for regression We Policy Predictive model Effort e Effort-based utility d Circumstance c Irrelevant features z Utility u Actual effort-based utility a d Economic Models of EOP Fair Machine Learning Figure Our proposed conceptual mapping between Fair ML and economic literature on EOP empirically show that employing the wrong measure of algorithmic fairness when the moral assumptions underlying it are not satisfied can have devastating consequences on the welfare of the disadvantaged group We emphasize that our work is not meant to advocate for any particular notion of algorithmic fairness rather our goal is to establish both formally and via real-world examples that implicit in each notion of fairness is a distinct set of moral assumptions about decision subjects therefore each notion of fairness is suitable only in certain application domains and not others By making these assumptions explicit our framework presents practitioners with a normative guideline to choose the most suitable notion of fairness specifically for every real-world context in which ADMs are to be deployed Equality of Opportunity An Overview Equality of opportunity has been extensively debated among political philosophers Philosophers such as Rawls Dworkin Arneson and Cohen contributed to the egalitarian school of thought by proposing different criteria for making the cut between arbitrary and accountability factors The detailed discussion of their influential ideas is outside the scope of this work and the interested reader is referred to excellent surveys by Arneson and Roemer and Trannoy In this section we briefly mention several prominent interpretations of EOP and discuss their relevance to ADMs Following Arneson we recount three main conceptions of equality of opportunity Libertarian EOP A person is morally at liberty to do what she pleases with what she legitimately owns eg self business etc as long as it does not infringe upon other peoples moral rights eg the use of force fraud theft or damage on persons or property of another individual is considered a violation of their rights Other than these restrictions any outcome that occurs as the result of peoples free choices on their legitimate possessions is considered just In the context of ADMs and assuming no gross violations of individuals data privacy rights this interpretation of EOP leaves the enterprise at total liberty to implement any algorithm it wishes through for decision making The algorithm can utilize all available information including individuals sensitive features such as race or gender to make statistically accurate predictions Formal EOP Also known as careers open to talents formal EOP require desirable social positions to be open to all who possess the attributes relevant for the performance of the duties of the position eg anyone who meets the formal requirements of the job and wish to apply for them The applications must be assessed only based on relevant attributes/qualifications that advances the morally innocent goals of the enterprise Direct discrimination based on factors deemed arbitrary eg race or gender is therefore prohibited under this interpretation of EOP Formal EOP would permit differences in peoples circumstances eg their gender to have indirect but nonetheless deep impact on their prospects For instance if women are less likely to receive higher education due to prejudice against female students as long as a hiring algorithm is blind to gender and applies the same educational requirement to male and female job applicants formal equality of opportunity is maintained In context of ADMs Formal EOP is equivalent to the removal of the sensitive feature information from the learning pipeline In the fair ML community this is sometimes referred to as fairness through blindness Substantive EOP Substantive EOP moves the starting point of the competition for desirable positions further back in time and requires not only open competition for desirable positions but also fair access to the necessary qualifications for the position This implies access to qualifications eg formal requirements for a job should not to be affected by arbitrary factors such as race gender or social class The concept is closely related to indirect discrimination if the ADM indirectly discriminates against people with a certain irrelevant feature eg women or African Americans this may be an indication that the irrelevant/arbitrary feature has played a role in the acquisition of the requirements When there are no alternative morally acceptable explanations for it indirect discrimination is often considered in violation of substantive EOP Our focus in this work is on substantive EOP and in particular on two of its refinements called Rawlsian EOP and Luck Egalitarian EOP Rawlsian EOP According to Rawls those who have the same level of talent or ability and are equally willing to use them must have the same prospect of obtaining desirable social positions regardless of arbitrary factors such as socio-economic background This Rawlsian conception of EOP has been translated into precise mathematical terms as follows let c denote circumstance capturing factors that are not considered legitimate sources of inequality among individuals Let scalar e summarize factors that are viewed as legitimate sources of inequality For the sake of brevity the economic literature refer to e as effort but e is meant to summarize all factors an individual can be held morally accountable for Let u specify individual utility which is a consequence of effort circumstance and policy Formally let c e specify the cumulative distribution of utility under policy at a fixed effort level e and circumstance c Rawlsian/Fair EOP requires that for individuals with similar effort e the distribution of utility should be the same regardless of their circumstances Definition Rawlsian of Opportunity R-EOP A policy satisfies Rawlsian EOP if for all circumstances c c and all effort levels e c e c e Note that this conception of EOP takes an absolutist view of effort it assumes e is a scalar whose absolute value is meaningful and can be compared across individuals This view requires effort e to be inherent to individuals and not itself impacted by the circumstance c or the policy Luck Egalitarian EOP Unlike fair EOP luck egalitarian EOP offers a relative view of effort and allows for the possibility of circumstance c and implemented policy impacting the distribution of effort e In this setting Roemer argues that in comparing efforts of individuals in different types circumstances we should somehow adjust for the fact that those efforts are drawn from distributions which are different As the solution he goes on to propose measuring a persons effort by his rank in the effort distribution of his type/circumstance rather than by the absolute level of effort he expends Formally let be the effort distribution of type c under policy Roemer argues that this distribution is a characteristic of the type c not of any individual belonging to the type Therefore an inter-type comparable measure of effort must factor out the goodness or badness of this distribution Roemer declares two individuals as having exercised the same level of effort if they sit at the same quantile or rank of the effort distribution for their corresponding types More precisely let the indirect utility distribution function specify the distribution of utility for individuals of type c at the th quantile of Equalizing opportunities means choosing the policy to equalize utility distributions across types at fixed levels of Definition Luck Egalitarian of Opportunity e-EOP A satisfies Luck Egalitarian EOP if for all and any two circumstances c c c To better understand the subtle difference between Rawlsian EOP and luck egalitarian EOP consider the following example suppose in the context of employment decisions we consider years of education as effort and gender as circumstance Suppose Alice Note that in Rawlss formulation of EOP talent and ambition are treated as a legitimate source of inequality even when they are independent of a persons effort and responsibility The mathematical formulation proposed here includes talent ability and ambition all in the scalar e Whether natural talent should be treated as a legitimate source of inequality is a subject of controversy As stated earlier throughout this work we assume such questions have been already answered through a democratic process and/or deliberation among stakeholders and domain experts Note that in Roemers original work utility is assumed to be a deterministic function of c e Here we changed the definition slightly to allow for the possibility of non-deterministic dependence and Bob both have years of education whereas Anna and Ben have and years of education respectively Rawlsian EOP would require Alice and Bob to have the same employment prospects so it would ensure that factors such as sexism wouldnt affect Alices employment chances negatively compared to Bob Luck egalitarian EOP goes a step further and calculates everyones rank in terms of years of education among all applicants of their gender In our example Alice is ranked st and Anna is ranked nd Similarly Bob is ranked nd and Ben is ranked st A luck egalitarian EOP policy would ensure that Alice and Ben have the same employment prospects and may indeed assign Bob to a less desirable position than Alice even though they have similar years of education Next we will discuss the above two refinements of substantive EOP in the context of supervised learning SETTING As a running example in this section we consider a business owner who uses ADM to make salary decisions so as to improve business productivity/revenue We assume a higher salary is considered to be more desirable by all employees An ADM is designed to predict the salary that would improve the employees performance at the job using historical data This target variable as we will shortly formalize does not always coincide with the salary the employee is morally accountable/qualified for We consider the standard supervised learning setting A learning algorithm receives a training data set T xi ni consisting of n instances where xi X specifies the feature vector for individual i and Y the true label for him/her the salary that would improve his/her performance Unless otherwise specified we assume Y and X Rk Individuals are assumed to be sampled iid from a distribution The goal of a learning algorithm is to use the training data T to fit a model or pick a hypothesis X Y that accurately predicts the label for new instances Let be the hypothesis class consisting of all the models the learning algorithm can choose from A learning algorithm receives T as the input then utilizes the data to select a model that minimizes some empirical loss L T We denote the predicted label for an individual with feature vector x by y ie y Consider an individual who is subject to algorithmic decision making in this context To discuss EOP we begin by assuming that his/her observable attributes x can be partitioned into two disjoint sets x where z Z denotes the individuals observable characteristics for which he/she is considered morally not accountable this could include sensitive attributes such as race or gender as well as less obvious attributes such as zip code We refer to z as morally arbitrary or irrelevant features Let denote observable attributes that are deemed morally acceptable to hold the individual accountable for in the running example this could include the level of job-related education and experience We refer to as accountability or relevant features We emphasize once again that determining what factors should belong to each category is entirely outside the scope of this work We assume throughout that a resolution has been previously reached in this regard through the appropriate process and is given to us Let d specify the individuals effort-based utility the utility he/she should receive solely based on their accountability factors eg the salary an employee should receive based on his/her years of education and job-related experience Note that this may be different from their actual salary Effort-based utility d is not directly observable but we assume it is estimated via a function X Y R such that d links the observable information to the effortbased utility d Let a be the actual utility the individual receives subsequent to receiving prediction y eg the utility they get as the result of their predicted salary We assume there exists a function X Y R that estimates a a Throughout for simplicity we assume higher values of a and d correspond to more desirable conditions Let u be the advantage or overall utility the individual earns as the result of being subject to predictive model For simplicity and unless otherwise specified we assume u has the following simple form u a d That is u captures the discrepancy between an individuals actual utility a and their effort-based utility d With this formulation an individuals utility is when their actual and effort-based utilities coincide ie u if a d We consider the predictive advantage u to be the currency of equality of opportunity for supervised learning That is u is what we hope to equalize across similar individuals similar in terms of what they can be held accountable for Our moral argument for this choice is as follows the predictive model inevitably makes errors in assigning individuals to their effort-based utilities this could be due to the target variable not properly reflecting effort-based utility the prediction being used improperly or simply a consequence of generalization Sometimes these errors are beneficial to the subject and sometimes they cause harm Advantage u precisely captures this benefit/harm EOP in this setting requires that all individuals who do not differ in ways for which they can be held morally accountable have the same prospect of earning the advantage u regardless of their irrelevant attributes As an example lets assume the true labels in the training data reflects individuals effort-based utilities as we will shortly argue this assumption is not always morally acceptable but for now lets ignore this issue In this case a perfect predictor one that correctly predicts the true label for every individual will distribute no predictive advantage but such predictor almost never exists in real world applications The deployed predictive model almost always distributes some utility among decision-subjects through the errors it makes A fair model with EOP rationale would give all individuals with similar true labels the same prospect of earning this advantage regardless of their irrelevant attributes Our main conceptual contribution is to map the above setting to that of economic models of EOP Section We treat the predictive model as a policy arbitrary features z as circumstance and the effort-based utilities d as effort Figure In the next Section we show that through our proposed mapping most existing statistical notions of fairness can be interpreted as special cases of EOP through EOP FOR SUPERVISED LEARNING In this Section we show that many existing notions of algorithmic fairness such as statistical parity equality of odds equality of accuracy and predictive value parity can be cast as special cases of EOP The summary of our results in this Section can be found in Table To avoid any confusion with the notation we define random variables to specify the feature vector and true label for an individual drawn iid from distribution Similarly given a predictive model random variables Y U specify the predicted label actual utility the effortbased utility and advantage respectively for an individual drawn iid from When the predictive model in reference is clear from the context we drop the superscript for brevity Before we formally establish a connection between algorithmic fairness and EOP we shall briefly overview the Fair ML literature and remind the reader of the precise definition of previously-proposed notions of fairness Existing notions of algorithmic fairness can be divided into two distinct categories individual- and group-level fairness Much of the existing work on algorithmic fairness has been devoted to the study of group unfairness also called statistical unfairness or discrimination Statistical notions of fairness require that given a classifier a certain fairness metric is equal across all protected or socially salient groups More precisely assuming z Z specifies the group each individual belongs to statistical parity seeks to equalize the percentage of people receiving a particular outcome across different groups Definition Statistical Parity A predictive model satisfies statistical parity if z z Y y Z z y Z z Equality of odds requires the equality of false positive and false negative rates across different groups Definition of Odds A predictive model satisfies equality of odds if z z y Y Y y Z y Y y Z y Equality of accuracy requires the classifier to make equally accurate predictions across different groups Definition of Accuracy A predictive model satisfies equality of accuracy if z z Z Y Y Z z Y Y Z z Predictive value parity which can be thought of as a weaker version of calibration requires the equality of positive and negative predictive values across different group Definition Predictive Value Parity A predictive model satisfies predictive value parity if z z y Y Y y Z z Y y Y y Z z Y y Statistical Parity Equality of Odds and Accuracy as Rawlsian EOP We begin by translating Rawlsian EOP into the supervised learning setting using the mapping proposed in Figure Recall that we proposed replacing e with effort-based utility d and circumstance c with vector of irrelevant features z In order for the definition of Rawlsian EOP to be morally acceptable we need d to not be affected by z and the model In other words it can only be a function of and y Let Fh specify the distribution of utility across individuals under predictive model We define Rawlsian EOP for supervised learning as follows Definition R-EOP for supervised learning Suppose d Predictive model satisfies Rawlsian EOP if for all z z Z and all d Fh Z zD d Fh Z zD d In the binary classification setting if we assume the true label Y reflects an individuals effort-based utility D Rawlsian EOP translates into equality of odds across protected groups Proposition of Odds as R-EOP Consider the binary classification task where Y Suppose U A D A Y ie the actual utility is equal to the predicted label and D where Y ie effort-based utility of an individual is assumed to be the same as their true label Then the conditions of R-EOP are equivalent to those of equality of odds Proof Recall that R-EOP requires that z z Zd D and for all possible utility levels u PU u Z zD d PU u Z zD d with ADD withY Y the above is equivalent to z z u PY Y u Z y PY Y u Z y z z u PY u y Z y PY u y Z y z z y PY y Z y PY y Z y where the last line is identical to the conditions of equality of odds for binary classification The important role of the above proposition is to explicitly spell out the moral assumption underlying equality of odds as a measure of fairness by measuring fairness through equality of odds we implicitly assert that all individuals with the same true label have the same effort-based utility This can clearly be problematic in practice true labels dont always reflect/summarize accountability factors At best they are only a reflection of the current state of affairs which itself might be tainted by past injustices For these reasons we argue that equality of odds can only be used as a valid measure of algorithmic fairness with an EOP rationale once the validity of the above moral equivalency assumption has been carefully investigated and its implications are well understood in the specific context it is utilized in Other statistical definitions of algorithmic fairness namely statistical parity and equality of accuracy can similarly be thought of as special instances of R-EOP See Table For example statistical Note that Hardt et al referred to a weaker measure of algorithmic fairness ie equality of true positive rates as equality of opportunity Notion of fairness Effort-based utility D Actual utility A Notion of EOP Accuracy Parity constant eg Y Y Rawlsian Statistical Parity constant eg Y Rawlsian Equality of Odds Y Y Rawlsian Predictive Value Parity Y Y egalitarian Table Interpretation of existing notions of algorithmic fairness for binary classification as special instances of EOP parity can be interpreted as R-EOP if we assume all individuals have the same effort-based utility Proposition Statistical Parity as R-EOP Consider the binary classification task where Y Suppose U A D A Y and D where is a constant function ie effort-based utility of all individuals is assumed to be the same Then the conditions of R-EOP is equivalent to statistical parity Proof Without loss of generality suppose ie all individuals effort-based utility Recall that R-EOP requires that z z Zd Du R PU u Z zD d PU u Z zD d Replacing U with A D D with and A with Y the above is equivalent to z z Zd u PY D u Z zD PY D u Z zD z z Zu PY u Z z PY u Z z z z PY y Z z PY y Z z where the last line is identical to the conditions of statistical parity for binary classification As a real-world example where statistical parity can be applied consider the following suppose the society considers all patients to have the same effort-based utility which can be enjoyed by access to proper clinical examinations Now suppose that undergoing an invasive clinical examination has utility if one has the suspected diseases and otherwise whereas avoiding the same clinical investigation has utility if one does not have the suspected disease and otherwise For all subjects the effort-based utility is the same the maximum utility let us suppose In other words all people with a disease deserve the invasive clinical investigation and all people without the disease deserve to avoid it Consider a policy of giving clinical investigation to all the people without the disease and to no people without the disease This would achieve an equal distribution of effort-based utility D and distribute no advantage U Such policy however could only be achieved with a perfect accuracy predictor For an imperfect accuracy predictor R-EOP would require the distribution of negative in this case utility U to give the same chance to African Americans and white patients with without the disease to receive avoid an invasive clinical exam Statistical parity can be understood as equality of outcomes as well if we assume Y reflects the outcome Proposition of Accuracy as R-EOP Consider the binary classification task where Y A D A Y Y and D where ie effort-based utility of all individuals are assumed to be the same and equal to Then the conditions of R-EOP is equivalent to equality of accuracy Proof Recall that R-EOP requires z Zd Du R PU u Z zD d PU u Z zD d with AD D with Y Y the above is equivalent to z z Zd u D u Z zD d D u Z zD d We can then write z zu PY Y u Z z PY Y u Z z z z Z Y Z z Y Z z where the last line is identical to the conditions of equality of accuracy for binary classification The critical moral assumption underlying equality of accuracy as a measure of fairness with EOP rationale is that errors reflect the advantage distributed by the predictive model among decision subjects This exposes the fundamental ethical problem with adopting equality of accuracy as a measure of algorithmic fairness it fails to distinguish between errors that are beneficial to the subject and those that are harmful For example in the salary prediction example equality of accuracy would make no distinction between an individual who earns a salary higher than what they deserve and someone who earns lower than their effort-based/deserved salary Max-min distribution vs strict equality At a high level R-EOP prescribes equalizing advantage distribution across persons with the same effort-based utility Some egalitarian philosophers have argued that we can remain faithful to the spirit though not the letter of EOP by delivering a max-min distribution of advantage instead of a strict egalitarian one The max-min distribution deviates from equality only when this makes the worst off group better off Even though this distribution permits inequalities that do not reflect accountability factors it is considered a morally superior alternative to equality if it improves the utility of least fortunate The max-min distribution addresses the leveling down objection to equality the disadvantaged group may be more interested in The idea that inequalities are justifiable only when they result from a scheme arranged to maximally benefit the worst off position is expressed through the Difference Principle by John Rawls in his theory of justice as fairness through maximizing their absolute level of utility as opposed to their relative utility compared to that of the advantaged group Predictive Value Parity as Egalitarian EOP Note that predictive value parity equality of positive and negative predictive values across different groups can not be thought of as an instance of R-EOP as it requires the effort-based utility of an individual to be a function of the predictive model as we will shortly show it assumes D This is in violation of the absolutist view of Rawlsian EOP In this Section we show that predictive value parity can be cast as an instance of luck egalitarian EOP We first specialize Roemers model of Egalitarian EOP to the supervised learning setting Recall that egalitarian EOP allows the effort-based utility to be a function of the predictive model that is D When this is the case following the argument put forward by Roemer we posit that the distribution of effort-based utility for a given type z denoted by is a characteristic of the type z not something for which any individual belonging to the type can be held accountable Therefore an inter-type comparable measure of effort-based utility must factor out the goodness or badness of this distribution We consider two individuals as being equally deserving if they sit at the same quantile or rank of the distribution of D for their corresponding type More formally let the indirect utility distribution function denoted by Fh specify the distribution of utility for individuals of type z at the th quantile of effort-based utility distribution Equalizing opportunities means choosing the predictive model to equalize the indirect utility distribution across types at fixed levels of Definition e-EOP for supervised learning Suppose d Predictive model satisfies egalitarian EOP if for all and z z Z Fh Z Fh Z Next we show that predictive value parity can be thought of as a special case of e-EOP where the predicted label/risk is assumed to reflect the individuals effort-based utility and the true label Y reflects his/her actual utility Proposition predictive value parity as e-EOP Consider the binary classification task where Y A D A Y and D where Y ie effortbased utility of an individual under is assumed to be the same as their predicted label Then the conditions of e-EOP are equivalent to those of predictive value parity Proof Recall that e-EOP requires that z z and u R PU u Z PU u Z Note that since D Y and in the binary classification Y can only take on two values there are only two ranks/quantiles possible in terms of the effort-based utilitycorresponding to Y and Y So the above condition is equivalent to z z u PU u Z z Y y PU u Z z Y y with ADD with Y the above is equivalent to z z u PY Y u Z z Y y PY Y u Z z Y y z z u PY u y Z z Y y PY u y Z z Y y z z y PY y Z z Y y PY y Z z Y y where the last line is identical to predictive value parity Note that there are two assumptions needed to cast predictive value parity as an instance of e-EOP the predicted label/risk reflects the individuals effort-based utility and the true label Y reflects his/her actual utility The plausibility of such moral assumptions must be critically evaluated in a given context before predictive parity can e employed as a valid measure of fairness Next we discuss the plausibility of these assumptions through several real-world examples Plausibility of assumption The choice of predicted label as the indicator of effort-based utility may sound odd at first However there are several real-world settings in which this assumption is considered appropriate Consider the case of driving under influence DUI the law considers all drivers equally at risk of causing an accident due to the consumption of alcohol or drugs equally accountable for their risk and punishes them similarly even though only some of them will end up in an actual accident and the rest wont In this context the potential/risk of causing an accident as opposed to the actual outcome justifies unequal treatment because we believe differences in actual outcomes among equally risky individuals is mainly driven by arbitrary factors such as brute luck Arguably such factors should never specify accountability Can assumptions and hold simultaneously The following is an example in which assumptions and hold simultaneously in particular the true label Y specifies the actual utility an individual receives subsequent to being subject to automated decision making Consider the students of a course offered online and open to students from all over the world The final assessment of students enrolled in the course includes an essential oral exam The oral exam is very challenging and extremely competitive The instructors hold an exam session every month Every student is allowed to take the oral exam but since resources for oral examinations are limited to discourage participation without preparation the rule is that if a student fails the exam he/she has to wait one year before taking the exam again Suppose that students belong to one of the two groups African Americans and Asians African American and Asian students study in different ways with different cognitive strategies As a result an African American student with passing score may correspond to a very different feature vector compared to an Asian student with a passing score Suppose a predictive model is used to predict the outcome of the oral exam for individual students based on the students behavioral data The online learning platform records data on how students interact with the course materials Students are given a simple pass/fail prediction to help them make an informed choice about when to take the exam In this example we argue that both assumptions underlying predictive value parity are satisfied A Y Passing the exam is a net utility not passing the exam is a net disutility due to the one year delay Also being predicted to pass per se has no utility associated with it D Y It is plausible to consider students morally responsible for their chances of success because the predictions are calculated based on how they have studied the course material In this example a fair predictor with EOP rationale should satisfies predictive value parity That means students who are predicted to pass should be equally likely to pass the exam irrespective of their race On Recent Fairness Impossibility Results Several papers have recently shown that group-level notions of fairness such as predictive value parity and equality of odds are generally incompatible with one another and cannot hold simultaneously Our approach confers a moral meaning to these impossibility results they can be interpreted as contradictions between fairness desiderata reflecting different and irreconcilable moral assumptions For example predictive value parity and equality of odds make very different assumptions about the effort-based utility d Equality of odds assumes all persons with similar true labels are equally accountable for their labels whereas predictive value parity assumes all persons with the same predicted label/risk are equally accountable for their predictions Note that depending on the context usually only one if any of these assumptions is morally acceptable We argue therefore that unless we are in the highly special case where Y it is often unnecessary from a moral standpoint to ask for both of these fairness criteria to be satisfied simultaneously EGALITARIAN MEASURES OF FAIRNESS In this section inspired by Roemers model of egalitarian EOP we present a new family of measures for algorithmic fairness Our proposal is applicable to supervised learning tasks beyond binary classification and to utility functions beyond the simple linear form specified in Equation We illustrate our proposal empirically and compare it with existing notions of unfairness for regression Our empirical findings suggest that employing a measure of algorithmic unfairness when its underlying assumptions are not met can have devastating consequences on the welfare of decision subjects A New Family of Measures For supervised learning tasks beyond binary classification eg multiclass classification or regression the requirement of equation becomes too stringent as there will be infinitely many quantiles to equalize utilities over The problem persists even if we relax the requirement of equal utility distributions to maximizing the minimum expected utility at each quantile More formally let specify the expected utility of individuals of type z at the th quantile of the effort-based utility distribution For we say that a predictive model satisfies egalitarian EOP at the -slice of the population if arg max min Assuming we are concerned only with the -slice then would be the equal-opportunity predictive model Unfortunately when we move beyond binary classification we generally cannot find a model that is simultaneously optimal for all ranks Therefore we need to find a compromise Following Roemer we define the e-EOP predictive model as follows arg max min That is we consider to be an e-EOP predictive model if it maximizes the expected utility of the worst off group ie Replacing the expectation with its in-sample analogue our proposed family of e-EOP measures can be evaluated on the data set T as follows min i T u xi where u xi is the utility an individual with feature vector xi and true label receives when predictive model is deployed and is the number of individuals in T whose arbitrary features value is z Z The arbitrary features value z specifies the intersectional group each individual belongs to We to denote the number of such intersectional groups For simplicity in our illustration we refer to these groups as G To guarantee fairness we propose the following in-processing method maximize the expected utility of the worst off group subject to error being upper bounded by max st L T Note that if the loss is convex and is concave in model parameters Optimization is convex and can be solved efficiently We remark that our notion of fairness does not require us to explicitly specify the effort-based utility D since it only compares the overall expected utility of different groups with one another without the need to explicitly compare the utility obtained by individuals at a particular rank of D across different groups Furthermore the utility function u does not have to be restricted to take the simple linear form specified in Equation Illustration Next we illustrate our proposal on the Crime and Communities data set The data consists of observations each corresponding to a community/neighborhood in the United States Each community is described by features specifying its socio-economic law enforcement and crime statistics extracted from the FBI UCR Community type eg urban vs rural average family income and Roemer in fact proposes two further alternatives in the first solution the objective function for each -slice of the population is assumed to v z is then weighted by the size of the slice In the second solution he declares the equal opportunity policy to be the average of the policies Roemer expresses no strong preference for any of these alternatives other than the fact that computational simplicity sometimes suggests one over the others This is in fact the reasoning behind our choice of Equation through a b c Figure NRD PRD and average utility of the disadvantaged group as a function of the upperbound squared error The notion of fairness enforced on algorithmic decisions can have a devastating impact on the welfare of the disadvantaged group the per capita number of police officers in the community are a few examples of the explanatory variables included in the dataset The target variable Y is the per capita number of violent crimes We train a linear regression model Rk on this dataset to predict the per capita number of violent crimes for a new community We hypothesize that crime predictions can affect the law enforcement resources assigned to the community the value of properties located in the neighborhood and business investments drawn to it We preprocess the original dataset as follows we remove the instances for which target value is unknown Also we remove features whose values are missing for more than of instances We standardize the data so that each feature has mean and variance We divide all target values by a constant so that labels range from to Furthermore we flip all labels y y so that higher y values correspond to more desirable outcomes We assume a neighborhood belongs to the protected group G if the majority of its residents are non-Caucasian that is the percentage of African American Hispanic and Asian residents of the neighborhood combined is above This divides the training instances into two groups GG We include this group membership information as the sensitive feature z in the training data i G For simplicity we assume the utility has the following functional dependence on x and u y that dependence on x are through z and y x respectively For communities belonging we y y is respectively defined as follows For a majority-Caucasian neighborhood u y y y For a minority-Caucasian neighborhood u y y y y At a high level neighborhoods in both groups enjoy a high utility if their predicted and actual crime rates are low simultaneously note that the absolute value of utility derived from this case is higher for the minority The minority-Caucasian group further benefits from low crime predictions regardless of actual crime rates We assume the effort-based utility for the minority group is one minus the actual crime rate y and for the majority group it is proportional to one minus the predicted crime rate y Note that these utility functions are made up for illustration purposes only and do not reflect any deep knowledge of how crime and law enforcement affect the well-being of a neighborhoods residents To illustrate our proposal we solve the following convex optimization problem for different values of max st i G xi xi i G xi xi n i xi We choose the value of by running a -fold cross validation on the data set For each value of Mean Squared Error we measure the following quantities via -fold cross validation Positive residual difference is the equivalent of false positive rate in regression and is computed by taking the absolute difference of mean positive residuals across the two groups i G max i G max In the above is the number of individuals in group who get a positive residual ie Negative residual difference is the equivalent of false negative rate in regression and is computed by taking the absolute difference of mean negative residuals across the two groups Average utility of the disadvantaged group is computed by taking the average utility of all individuals in the test data set min i G u xi i G u xi Figure shows the results of our simulations Blue curves correspond to our proposal Optimization As evident in Figures a and b positive and negative residual difference increase with while the average utility increases see Figure c To compare our proposal with existing measures of unfairness for regression we utilize the in-processing method of Heidari et al The method enforces an upperbound on i and has been shown to control the positive and negative residual difference across the two groups More precisely we solve the following optimization problem for different values of max i T xi st n i xi Red curves in Figure correspond to this baseline As evident in Figures a and b by enforcing a lower bound on i positive and negative residual difference go to very quickly as expected However the trained model performs very poorly in terms of average utility of the disadvantaged group CONCLUSION Our work makes an important contribution to the rapidly growing line of research on algorithmic fairness by providing a unifying moral framework for understanding existing notions of fairness through philosophical interpretations and economic models of EOP We showed that the choice between statistical parity equality of odds and predictive value parity can be mapped systematically to specific moral assumptions about what decision subjects morally deserve Determining accountability features and effort-based utility is arguably outside the expertise of computer scientists and has to be resolved through the appropriate process with input from stakeholders and domain experts In any given application domain reasonable people may disagree on what constitutes factors that people should be considered morally accountable for and there will rarely be a consensus on the most suitable notion of fairness This however does not imply that in a given context all existing notions of algorithmic fairness are equally acceptable from a moral standpoint