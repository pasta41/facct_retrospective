Racial categories in machine learning Controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness These debates rarely engage with how racial identity is embedded in our social experience making for sociological and psychological complexity This complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes Racial identity is not simply a personal subjective quality For people labeled Black it is an ascribed political category that has consequences for social differentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation In the United States racial classification can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the Negro/black category as stigmatized Social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state corporate and civic institutions and practices This creates a dilemma for society and designers be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality or be conscious of racial categories in a way that itself reifies race We propose a third option By preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation machine learning systems can mitigate the root cause of social disparities social segregation and stratification without further anchoring status categories of disadvantage CONCEPTS Social and professional topics Race and ethnicity Systems analysis and design Applied computing Sociology Computing methodologies Dimensionality reduction and manifold learning KEYWORDS fairness machine learning racial classification segregation A growing community of researchers and practitioners now studies fairness in applications of machine learning in such sensitive areas as credit reporting employment education criminal justice and advertising This scholarship has been motivated by pragmatic concerns about machine-learning-produced group biases and compliance with nondiscrimination law as well as a general concern about social fairness While many of the controversies that have inspired this research have been about discriminatory impact on particular groups such as Blacks or women computer scientists have tended to treat group fairness abstractly in terms of generic protected classes rather than in terms of specific status groups This leads analysts to treat ranked racial and gender status categories simply as nominal categories of personal identity a characteristic of the individual in computational analysis rather than understanding that male/female or Negro black/white are each systems of hierarchical social statuses The typical literature in this field addresses problems in a supervised machine learning paradigm wherein a predictor is trained on a set of personal data X One or more features or columns of the personal data A are protected demographic categories Each person is labeled with the desired outcome value Y and a classifier or predictor Y is trained on the labeled data set The data is assumed to be accurate Fairness is then defined as a formal property of the predictor or prediction algorithm defined in terms of the training data Several different formal definitions of fairness have been proposed and their relationships with each other are well studied Some proposed definitions of fairness are Definition Fairness through unawareness FTU An algorithm is fair so long as protected attributes A are not explicitly used in the decision-making Definition Demographic parity DP A predictor Y satisfies demographic parity if P Y A P Y A Definition Equality of Opportunity A predictor Y satisfies equality of opportunity if if P Y A Y P Y A Y Comparatively little attention is given to how the protected class labels A are assigned why they are being protected and by whom and what that means for the normative presumptions typical of fair machine learning design This paper addresses these questions with focus on the particular but also paradigmatic case in which the protected class is a specific racial category African-American Black We argue using this case that rather than being an abstract nominal category race classification is embedded in state institutions and reinforced in civil society in ways that are relevant to the design of machine learning systems Research demonstrates that race categories have been socially constructed as unequal categories in numerous Latin American nations and in the United States Race provokes discussions of fairness because racial classification signifies social economic and political inequities anchored in state and civic institutional practices Racial categories are also unstable social constructions as a brief history of race since late nineteenth-century America will reveal We will show how race categories have been subject to constant political contestation in meaning and as a consequence racial identity itself is in fact not stable Race is ascribed onto individual bodies at different times and places in society based on many variables including specific ocular-corporeal characteristics social class perceived ancestral origins and state policy As a consequence of the social and historical facts about racial classification many machine learning applications that perform statistical profiling and especially those that use racial statistics are both technically and politically problematic Because race is not an inherent property of a person but a social fact about their political place and social location in society racial statistics do not reflect a stable ground truth Moreover racial statistics by their very nature mark a status inequality a way of sorting peoples life chances and so are by necessity correlated with social outcomes There is nothing fair about racial categories Scholars of fairness in machine learning using racial categories should be reflexive about this paradox The social facts about race present a dilemma for system designers Systems that learn from broad population data sets without considering racial categories will reflect the systemic racial inequality of society Through their effects on resource allocation these systems will reify these categories by disparately impacting racially identified groups On the other hand systems that explicitly take racial classification into account must rely on individual identification and/or social ascription of racial categories that are by definition unequal Even when these classifications are used in a fair way they reify the categories themselves We present a third option as a potential solution to this dilemma The history of racial formation shows that the social fact of racial categorization is reinforced through policies and practices of segregation and stratification in housing education employment and civic life Racial categories are ascribed onto individual bodies those bodies are then sorted socially and in space the segregated bodies are then subject to disparate opportunities and outcomes these unequal social groups then become the empirical basis for racial categorization It is this vicious cycle that is the mechanism of systemic inequality Rather than considering fairness to be a formal property of a specific machine-learnt system we propose that systems can be designed with the objective of combating this cycle directly and without reference to racial category Systems designed with the objective of integration of different kinds of bodies can discover segregated groups in an unsupervised way before using fairness modifiers Section outlines two controversies about race and machine learning that have motivated research in this area We present these cases so that in subsequent sections we can refer to them to illustrate our theoretical claims Section traces the history of race in the United States from its roots in institutional slavery and scientific racism through to changing demographic patterns today This history reveals how race has always primarily been a system for stigmatization which has only recently become the site of ongoing political contest The racial categories continue to reproduce inequality Section discusses how racial categories get ascribed onto individual bodies through identification and classification Seeing ascription as an event we identify several different causes for racial identity including phenotype class and ancestral origin Section outlines the implications of the history and sociology of race for system design We provide a heuristic for analyzing the software and data of machine learning systems for racial impact by categorizing them as either colorblind as explicit racial projects or as facilitators of users engaging in racial projects which may be racist anti-racist or neither We argue that designers have a dilemma using racial statistics reifies race perpetuating categories that are intrinsically unfair Not using them risks the systematic failures of color-blind analysis unwittingly reinforcing racial hegemony Section offers a third alternative to design systems to be sensitive to segregation in society across dimensions of phenotype class and ancestral origin detected through unsupervised learning We sketch techniques for empirically identifying race-like dimensions of segregation in both spatial distributions and social networks These dimensions can then be used to group individuals for fairness interventions in machine learning RACE AND DATA CONTROVERSIES AND CONTEXT In this section we summarize two emblematic controversies involving race and machine learning and some of the ensuing scholarly debate Recidivism prediction One area where racial bias and automated decision-making has been widely studied is criminal sentencing The COMPAS recidivism prediction algorithm developed by Northpointe was determined to have higher false positive rates for black defendants than for white defendants and charged with being racially biased even though explicitly racial information was not used by the predictive algorithm This analysis has been contested on a variety of scientific grounds and the methodological controversy has launched a more general interest in fairness in statistical classification Studies about the statistics of fair classification have discovered that there is a necessary trade-off between classifier accuracy and group based false positive and negative rates under realistic distributions In light of the difficulties of interpreting and applying antidiscrimination law to these cases a wide variety of statistical and algorithmic solutions to the tension between predictive performance and fairness have been proposed Reconsidering the problem as one of causal inference and the predicted outcomes of intervention especially in light of the purposes to which prediction and intervention are intended is a promising path forward The COMPAS algorithm did not explicitly use racial information as an input It used other forms of personal information that were correlated with race The fact that the results of an algorithm that did not take race explicitly into account were correlated with racial classifications is an illustration of the general fact that group-based disparate impact cannot be prevented by ignoring the group memberships statistic fairness must be accomplished through awareness of the sensitive variable Computer scientists have responded by identifying methods for detecting the statistical proxies for a sensitive attribute within a machine learnt model and removing the effects of those proxies from the results In this paper we argue for a different understanding of the role of racial categorization in the analysis of algorithms We argue that because of the socially constructed nature of race racial categories are not simple properties of individual persons but rather are complex results of social processes that are rarely captured within the paradigm of machine learning For example in the analysis of the algorithm that determined alleged racial bias the race of defendants was collected not from the prediction software but rather from the Broward County Sheriffs Office To determine race we used the race classifications used by the Broward County Sheriffs Office which identifies defendants as black white Hispanic Asian and Native American In cases the race was marked as Other A focus on the potential biases of the recidivism prediction algorithm has largely ignored the question of how the Broward County Sheriffs Office developed its racial statistics about defendants We argue that rather than taking racial statistics like these at face value the process that generates them and the process through which they are interpreted should be analyzed with the same rigor and skepticism as the recidivism prediction algorithm Thematically we argue that racial bias is far more likely to come from human judgments in data generation and interpretation than from an algorithmic model and that this has broad implications for fairness in machine learning Ethnic affinity detection Facebook introduced a feature to its advertising platform that allowed the targeting of people in the United States based on racial distinctions which the company called ethnic affinity African American Hispanic or Asian American ProPublica discovered that this feature could be used to racially discriminate when advertising for housing which is illegal under the Fair Housing Act of The report stated that Facebook provided realtors with ad-targeting options that allowed them to narrow their ads to exclude non-white groups like Blacks Asian and Hispanics Facebook in fact drew the attention of the Department of Urban Development HUD Secretary Ben Carson who ordered an investigation of Facebooks compliance with fair housing law Facebook decided to pull the feature while also increasing its certification of advertisers nondiscriminatory practices During the controversy Facebook representatives explicitly made the point that multicultural affinity was not the same thing as race It was not for example based on a users self-identification with a race Facebook does not collect racial identity information directly Rather multicultural affinity was based on data about users activity such as the pages and posts they engaged with on the platform Indeed the fact that racial groups could be profiled by race despite not having users individual racial identity data suggest that race is much more than a characteristic of individual identity but rather is a socially reproduced form of categorical difference The feature did in fact give advertisers a tool to intentionally or unintentionally engage in disparate racial treatment However pulling the feature did not make discrimination using Facebooks platform impossible Speicher et al have investigated Facebooks advertising platforms and discovered that even without the feature there are ways to use the platform to discriminate intentionally and unintentionally and propose that discrimination should be measured by its effects or its disparate impact Related work has been done on Googles advertising platform Discriminating ads have been delivered based on racialized search results and gendered user profiles Studies about user perception and legal liability have explored these issues in depth Noble argues that digital media monopolies like Google have engaged in algorithmic oppression that privilege white people and has led to both the commercial co-optation of black racial identities as well as a kind of digital redlining against racial minorities and women especially Asian Black and Latino women But the question remains is this algorithmic oppression simply the result of a white-dominated industry believing that it was truly color blind leading it to ultimately ignore how race inequality might be reproduced digitally and algorithmically or are these instances of algorithmic and digital racism systemic because searches and algorithms mirror the racial beliefs of users THE POLITICAL ORIGIN OF RACIAL CATEGORIES Race is a social and cultural hierarchical system of categories that stigmatizes differences in human bodies but is not those body differences themselves Race differences are created by ascribing race classifications onto formerly racially unspecified individuals and linking them to stereotyped and stigmatized beliefs about nonwhite groups We use Link and Phelan s definition of stigma as the co-occurrence of labeling stereotyping separation segregation status debasement and discrimination For stigmatization to occur power must be exercised Somebody is racially white not just because they have less melanin in their skin but because of the way society has defined the societal rules for determining racial membership and social status Folk conceptions of racial difference emerged in western societies during the fifteenth century but took on scientific legitimacy during th and th centuries Lamarkian notions of natural and historic races gave way to a more modern conception of race that emphasized the immutability of the Blumenbach-inspired color-coded racial groupings with which most of us have become familiar White Black Red Yellow Eighteenth century notions that linked racial differences to environment and national origin gave way to a more static conceptualizations of racial difference now rooted biological deterministic arguments rooted in appearance and Darwinism Then ideologically supported by what is now debunked scientific theory the white racial category has been nominally defined since the first US Census in named six demographic categories Free White males of years and upward Free White males under years Free White females All other free persons Slaves Known as The Naturalization Act on March the Senate and House of Representatives of the United States of America enacted An act to establish an uniform Rule of Naturalization and extended the possibility of citizenship to any Alien being a free white person who shall have resided within the limits and under the jurisdiction of the United States for the term of two years while naturalizing the children of citizens of the United States that may be born beyond Sea or out of the limits of the United States The Naturalization Act of insured that Free white persons would remain an officially protected category for the next years The construction of an Asian-American social category occurred between the s and s The State of Nevada was first to pass anti-Asian legislation beginning in a precursor to antimiscegenation laws as as well congressional legislation and judicial rulings that contributed to their social isolation and social stigmatization In Takao Ozawa v United States in a Japanese man who had studied at the University of California and lived in the United States was denied his request for citizenship because he was clearly of a race which is not Caucasian In United States v Bhagat Sing Thind a high-caste Hindu of full Indian blood born in Amritsar Punjab India was denied citizenship because though Caucasian he was not white The Immigration and Nationality Act of known as The McCarran-Walter Act removed racial restrictions in Asian naturalization while it also created an Asian quotas system based on race rather than on nationality In the case of Negro Black Americans those once categorized by the US Census as either Slave or Other free persons their racial classification varied The and Censuses recognized mixed-race persons as mulatto defined as someone who is Negro and at least one other race while the Census added another mixed category quadroon to refer to persons who were one-fourth black blood Plessy v Ferguson established the legality of racial segregation that was separate but equal as well as confirmed the quantification of race by law And while the Census dropped all but the Negro category the and Census briefly brought back the mulatto category only to drop it one final time in the Census which finally solidified the Negro category once and for all along the lines of the one drop rule meaning that a single drop of African blood was sufficient to make a person Negro This rule was called the hypodescent rule by anthropologists and the traceable amount rule by the US courts It was policed by an array of government agencies market practices and social norms and was ultimately internalized by individuals of mixed European and African lineage In fact the one drop rule treated blackness as a contaminant of whiteness thus granting rights to those deemed white and by definition privileged After World War II the political trajectory of race in the United States evolved President Truman abolished discrimination based on race color religion or national origin in the armed forces with Executive Order Scientific racism fell into disfavor among scientists and scholars after the war as it was strongly associated with the defeated German Nazis A new social theory of ethnicity that attempted to reduce racial difference to cultural difference and emphasized the possibility of assimilation and equality became increasingly popular But while this accounted for the assimilation of many new immigrant groups real segregation and stratification along race lines ensured that racial categories remained ingrained in societal consciousness including the anti-racist consciousness that mobilized for equal rights Racial categories that had once been a political myth had solidified into social fact through the mechanisms of segregation The Civil Rights Movement in the s and s lead to Brown v Board of Education which ended de jure racial segregation and the passing of anti-discrimination laws such as the Voting Rights Act of and the Civil Rights Act of which prohibited discrimination based on race Thus anti-discrimination law reified the same racial categories that had been defined as a tool for subjugation and segregation Census data would then track racial statistics partly in order to enforce civil rights laws Anti-discrimination policies have in the years since they have passed provoked racial reaction as whites have rearticulated their political interests in new ways Racist and anti-racist political currents have been dialectically battling over racial policy since the rise of anti-racist consciousness Omi and Winant characterize the changes to racial categories through political contest as racial formation Key to their theory of racial formation is the racial project The co-constitutive ways that racial meanings are translated into social structures and become racially signified Definition Racial project A racial project is simultaneously an interpretation representation or explanation of racial identities and meanings and an effort to organize and distribute resources economic political cultural along particular racial lines Racial projects can be racist anti-racist or neither depending on how they align with structures of domination based on racial significance and identities Racial categories in the st century are the result of an ongoing contest of racial projects that connect how social structures are racially signified and the ways that racial meanings are embedded in social structures thereby steering state policy and social practice These policy changes have had lasting changes on the demographics and spatial and social distribution of the population of the United States This has allowed racial categories to change albeit slowly as each generation experiences race differently For example the United States Supreme Court struck down laws banning interracial marriage in with Loving v Virginia When interracial parents desired for their children mixed-race identity they put political pressure on institutions to recognize their children as such In the option to mark one or more racial categories was adopted by the Census in CAUSES OF RACIAL IDENTIFICATION AND ASCRIPTION Racial categories fill societal imagination and are solidified by law But racial categories have their effect by being ascribed to individual bodies Because it is not an intrinsic property of persons but a political category the acquisition of a race by a person depends on several different factors including biometric properties socioeconomic class and ancestral geographic and national origin Biometric properties Though the relationship between phenotype and race is not straightforward Omi and Winant argue that there is an irreducible ocular-corporeal component to race race is the assigning of social meanings to these visible features of the body Indeed the connection between phenotype and race has been assumed in research on fairness in machine learning In their work on intersectional accuracy disparities in gender classification based on photographs of faces Buolamwini and Gebru use the dermatologist approved Fitzpatrick Skin Type classification system to identify faces with lighter and darker skin While they draw the connection between phenotype and race they note that racial categories are unstable and that phenotype can vary widely within a racial or ethnic category Indeed it is neither the case that race can be reduced to phenotype nor that phenotype can be reduced to race there is broad empirical evidence that shows that intra-racially among people identified as black the lighter skinned are treated favorably by schools and the criminal justice system compared to those with darker skin Phenotype is a complex consequence of genotype which is in turn a consequence of biological ancestry With commercially available genetic testing genotype data is far more available than it has been historically It has also exposed the fact that many people have ancestry that is much more mixed in terms of politically constructed racial categories than they would have otherwise assumed this has had irregular consequences for peoples racial identification Both phenotype and genotype may be considered biometric properties under the law and hence these data categories would be protected in many jurisdictions However despite these protections this data is perhaps more available than ever Personal photographs which can reveal phenotype are widely used in public or privately collected digital user profiles Socioeconomic class While racial categories have always been tied to social status and economic class the connection and causal relationship between race and class has been controversial Wilson argued that in the postwar period the rise of the black elite and middle class made race an issue of declining significance despite the continued existence of a black underclass Omi and Winant are critical of this view noting the fragility of the black middle class and its connection to the vicissitudes of available public sector jobs Recent work by Chetty et al on racial effects of intergenerational class mobility confirm that black children have lower rates of upward mobility and higher rates of downward mobility compared to white children even when controlling for those that grow up in twoparent families with comparable incomes education and wealth live on the same city block and attend the same school This is due entirely to differences in outcomes for men not women Massey accounts for the continued stratification along racial lines as a result of ingrained intrinsic patterns or prejudice which is consistent with Genealogy Genotype Inheritance Nationality Phenotype Class Categories Race Figure A model of how individual biological properties genealogy genotype and phenotype are racialized through national political categories and associations with socioeconomic class Here inheritance refers to all forms of capital including economic and social passed from parents to children Broadly speaking genealogy is a strong determiner of race but importantly as a common cause of phenotype class and nationally recognized racial categories which are separate components of racial classification Bordalo et al In the controversial work of Saperstein and Penner racial self-identification and classification was found to be fluid over time in reaction to changes in social position as signaled by concrete events like receiving welfare or being incarcerated This effect has been challenged as a misinterpretation of measurement error though similar results have surfaced outside of the US as when Hungarians of mixed descent are more likely to identify as Roma if under economic hardship Confounding the relationship between individual race and class is the fact that socioeconomic class is largely inherited in other words there is always some class immobility This is acknowledged both in economics in discussions of inherited wealth eg and more broadly in sociology with the transfer of social capital via the family and institutions that bring similar people together with the function of exchange The ways that racial social and spatial segregation lead to the monopolistic group closure of social advantage on racial lines is discussed in Haynes and Hernandez Ancestral national and geographic origins The definitions of races used in the US census are rife with inconsistencies and lack parallel construction but have nevertheless become a de facto standard of racial and ethnic classification Blacks are defined as those with total or partial ancestry from any of the black racial groups of Africa Asian Americans are those which have ancestral origins in East Asia Southeast Asia or South Asia Hispanic Americans are descendants of people from countries of Latin America and the Iberian Peninsula and is considered by the census as an ethnicity and not a race Though phenotype and class may be social markers of race beliefs about race as a true intrinsic property are anchored in perceptions and facts about personal ancestry Ancestry is one of the main conduits of citizenship which determines which legal jurisdiction one is subject to These jurisdictions can influence what categories a person individually identifies with It is not only in the United States that racial categories are anchored in ancestry even though racial categories are constructed differently elsewhere Loveman notes that Latin American nations with a history of slavery commonly use the Black racial category whereas those with without that history are socially more organized around Indigeneity Racial categorization anywhere will depend on those categories available by legal jurisdiction this can be striking to those who migrate and find themselves ascribed to something unfamiliar Consider a person of Latin American of European ancestry who upon moving to the United States becomes a Hispanic HEURISTICS FOR ANALYSIS AND DESIGN OF SYSTEMS We now address how the history and social theory of race discussed above applies to the design of machine learning and other computer systems Responding to the provocation raised by Noble we argue that there is a substantive difference between systems that result in a controversial or unfair outcomes due to the racial bias of their designers and those that do so because they are reflecting a society that is organized by racial categories Using the concept of a racial project introduced in Section we propose a heuristic for detecting racism in machine learning systems We draw a distinction between the software used by a machine system and its input and output data Further we distinguish between three categories of systems that are not mutually exclusive those that are themselves racial projects those that allow their users to engage in racial projects and those that attempt to be blind to race Racial projects may be racist anti-racist or neither Machines that attempt to correct unfairness through explicit use of racial classification do so at the risk of reifying racial categories that are inherently unfair Machine learning systems that allocate resources in ways that are blind to race will reproduce racial inequality in society We propose a new design in Section that avoids both these pitfalls How has the software been designed A first step to evaluating the racial status of a machine system is to evaluate whether the software it uses has been designed for the purpose of achieving a racial outcome or representation Using the language of Omi and Winant the question is whether or not the software has been designed as a racial project If software has been designed as a racial project then it is appropriate to ask whether or not the racial project is racist anti-racist or neither A racial project is racist according to Omi and Winant if it creates or reproduces structures of domination based on racial significance and identities and anti-racist if it undoes or resists structures of domination based on racial significations and identities Example In the case of Facebooks ethnic affiliation feature Facebook engaged in a racial project to discover and represent the racial affiliations of its users Doing so was neither a racist nor an anti-racist project That it passed these representations on to How has the software designed Blind to race A As a racial project B Enabling users racial projects C Are the input data racialized Not explicitly A Explicitly by ascription B Explicitly by self-identification C Is the system output racialized Not at all A System ascribes race B By user interpretation C Figure Heuristics for analysis and design of algorithmic systems Systems of type A are blind to race and therefore risk learning and reproducing the racial inequality inherent in society Systems of type B explicitly use ascribed racial labels and so risk reifying racial categories by treating race as an intrinsic property of a person These systems are racial projects in the sense that they represent racial categories in a way that is relevant to resource allocation Systems of type be considered racial projects but have the distinction that they enable the system users to engage in their own racial projects Racial projects whether in type B or type C systems may be racist anti-racist or neither depending on how they align with structures of domination in society Categories A B and C are not mutually exclusive they are distinguished here as analytic heuristics only the users of its advertising platform gave advertisers the ability to engage in broad range of racial projects These possible racial projects included the potential for illegal racist discrimination in housing advertising The criterion for system software engaging in a racial project is that it engages racial categories through the words concepts or social structures that abstractly represent racial differences Racial projects are efforts to change these categories in one way or another Many systems that use machine learning are also by design platforms for their users political expression These platforms perhaps inevitably become fora for their users diversely racist anti-racist and other racial projects We have also seen that not all institutional outcomes with disparate racial impact are due to racist racial projects even colorblind institutions can have disparate outcomes for groups of people that identify with or are ascribed race based on racial categories Software that has not been designed with any intentional reference to race may still treat people who identify as black relatively poorly These systems which correspond roughly with institutions of racial hegemony critiqued by Omi and Winant reflect a status quo of racial inequality without engaging in it Are the input or output data racialized Beyond the mechanics of the systems software we can also evaluate a systems input data and output Is the input or output being racialized If so how Input data to a machine learning system especially if its personal information may have explicit racial labels These may be generated from individual self-identification institutional ascription or both As discussed above both self-identification and institutional classification are socially embedded and changeable based on circumstance These labels by definition place individuals within a political schema of racial categorization As such it is a mistake to consider such labels a ground truth about the quality of a person as opposed to a particular event at a time place and context Every instance of racial classification in input data should therefore as a matter of sound machine learning practice be annotated with information about who made the ascription when and under what circumstance To do otherwise risks reifying race treating a persons ascribed race as an intrinsic feature which unfairly places them within a system of inequality It does this even if the ultimate use of the data is an anti-racist racial project indeed the potential for racist use of this data is always available as an exposure threat In addition this annotation may give analysts clues as to the political motivations of the system designers and data providers Political context should be seen as part of the generative process that must be modeled to best understand data sources For example the degree to which somebody has culturally assimilated or the degree to which a one drop rule of racial classification or recognition of multi-racial identity is in effect may be an important factor in determining the distribution of racial labels We propose that as a heuristic for analyzing a system for its racial impact an analyst attend to whether the inputs and outputs of the system are racialized either a explicitly through the ascription of racial categories b explicitly through either the self-identification or subjective interpretation of the user or c not at all Those systems designed for ascription are likely to be themselves racial projects in that they use racial categories by design Systems whose inputs allow for racial self-identification may also be racial projects but also crucially allow for their users to engage in racial projects using the system based on how they represent themselves as a member of a race Systems whose outputs are racialized by user interpretation may not be racial projects themselves however users can engage in racial projects based on how the system represents other people Because race is an ascribed category users of a system can ascribe race to people represented by a system based on ocular cues dress and other contextual information Especially if these representations accord with racial stereotypes there may be the perception that the system is reproducing racial disparities If the outputs of a system are racialized by interpretation but not explicitly that interpretive discourse can itself be a racial project In other words the outputs of a system such as a search engine can be the subject of a conversation about race and resource allocation more generally However it may be an error to attribute the content of a racialized interpretation of a system to the system itself A thorough analysis of the system inputs software and outputs is necessary to determine where racial intent or social racial categories caused the output or ascription Some systems whose input and output data represent people may not be explicitly racialized at all However since racial categories structure inequality pervasively throughout society these systems Ascription Formation Disparity Figure Schematic of vicious cycle of racial formation Bodies are ascribed into racial categories then sorted socially and in space based on those ascriptions These sorted bodies are then exposed to disparate outcomes Racial categories are then formed on the basis of those unequal outcomes and their distribution across people based on phenotype ancestry and class indicators Those categories are then ascribed to bodies repeating the cycle will likely reproduce racial inequality anyway The difficulty of designing a system that neither reproduces racial social inequality nor reifies racial categories which are inherently unfair motivates an alternative design discussed in the next section AN ALTERNATIVE DESIGNING FOR SOCIAL INTEGRATION System designers of machine learning systems that determine resource allocation to people face a dilemma They can ignore racial inequality in society and risk having the system learn from and reproduce systemic social inequality due to racial categorization They can also use racial statistics to try to mitigate unfairness in outcomes but in doing so they will reify racial categorization We present a third option as a potential solution to this dilemma This proposal rests on two theoretical assumptions First recall that in our outline of the formation of race segregation and stratification of populations play a key systemic role The social fact of racial categorization is reinforced through policies and practices of segregation and stratification Racial categories are ascribed onto individual bodies those bodies are then sorted socially and in space the segregated bodies are then subject to disparate opportunities and outcomes these unequal social groups then become the empirical basis for racial categorization see Figure Rather than consider fairness to be a formal property of a specific machine-learnt system we propose that systems can be designed to disrupt this vicious cycle This requires treating groups that have been segregated socially and in space similarly so that disparate impacts do not apply The second assumption addresses the problem that ascribing politically constructed racial categories reifies them which contributes to status inequalities We ask how can systems designed with the objective of integration of different kinds of bodies especially those bodies that have been sorted racially but without reference to racial categories themselves Our alternative design also draws on our conclusion from Section The facts about people that cause ascription and self-identification with politically constructed status categories are facts about phenotype social class including events that signal social position and ancestry We propose that categories reflecting past racial segregation can be inferred through unsupervised machine learning based on these facts These inferred categories can then be used in fairness modifiers for other learning algorithms By using inferred race-like categories that are adaptive to real patterns of demographic segregation this proposal aims to address historic racial segregation without reproducing the political construction of racial categories A system designed in this way learns based on real demographics of the populations for which they are used and so will not result in applying national categories in a context where they are inappropriate It is also adaptive to demographic changes in the same place or social network over time Detecting spatial segregation Spatial segregation into different neighborhoods is one of the main vehicles of disparate impact on people of different races In part because of the its long history the question of how to best measure spatial segregation is its own subfield of quantitative sociology whose full breadth is beyond the scope of this paper The most basic measure which is both widely used and widely criticized is the dissimilarity measure D Definition Dissimilarity Black and white D i wi bi B where i ranges over the index of spatial tracts wi and bi are the white and black populations in those tracts and B are the total white and black populations in all tracts While defined above with respect to only two racial groups generalized versions of the metric have been proposed for multiple groups Most criticism of this metric is directed at the fact that it is aspatial obscuring true spatial relationship through the division of land into parcels which may be done in a way that invalidates the result For the purposes of this article we will assume that the spatial tracts are selected adequately in order to focus on a different criticism that this metric assumes that the population has been ascribed to racial categories thereby reifying them We propose a modification of this metric for identifying race-like categories of segregation between land tracts Consider the following sketch of method of detecting spatial segregation Let i range over the indices of land tracts Let range over the indices of individuals Let x be a vector of available personal data about each individual including information relevant to phenotype perhaps derived from photographs class and national origin For simplicity consider the vector to be of binary features Let i be the index of the tract where person resides Let Xi be the aggregation by summation of all x such that i i as a normalized vector Let X be all Xi combined into a matrix The first principle component of X will be a feature vector in the same space as the parcel data vectors Xi that reflects the dimension of greatest variance between parcels Because the parcel data vectors aggregate information about the components of race phenotype class and nationality this would reflect racial segregation without depending on any particular historical or political racial categorization Other principle components would likewise reflect other elements of racial segregation Persons could then be racially classified by transforming their personal data vector through the principle component and thresholding the result This classification could then be used as an input A to fairness interventions in machine learning Detecting social segregation Social segregation by race may be operationalized using network representations of society Homophily the phenomenon that similar people are more likely to be socially connected is a robustly studied and confirmed result and the problem of bridging between isolated niches has been posed as a general social problem beyond the context of race Several metrics for measuring social segregation of all kinds have been proposed These metrics have in common that they assume that nodes in the network have already been accurately assigned to different groups One widely known segregation measure for discrete properties is the assortativity coefficient defined as Definition Assortativity coefficient r i i i where is the fraction of edges in the network that connect nodes of group i to nodes of group ai and bj i As r approaches the network gets more assortatively mixed meaning that edges are within group If the groups in question are racial classifications an assortatively mixed network is a segregated network To adapt to the case where racial classification is not given but component racial features such as phenotype class and nationality are available as vector xi again of binary features for simplicity consider a method similar to that proposed in Section For each edge between i and aggregate xi and x into Xi by summing them then combine these into a matrix X and use the principle components to determine the dimensions of greatest variation between the aggregated properties of each connected pair As before transforming the individual feature vectors by the components and applying a threshold then assigns each person to the race-like groups of greatest social segregation Measuring the assortativity coefficient for these groups will provide another measure of the segregation of the population along race-like lines These classifications can then be used as protected groups A in fair machine learning Threats to validity and future work We have proposed that as a normative goal systems can be designed to promote similar treatment of bodies that are otherwise segregated socially or in space This proposal is motivated by social theory of how segregated perpetuates racial categories as a system of status difference We have not implemented or tested this design and here consider threats to its validity An empirical threat to its validity is if the principal components of the aggregated feature matrices do not reflect what are recognizable as racial categories This could be tested straightforwardly by collecting both ascribed or self-identified racial labels and other features for a population and computing how well the principle component vectors capture the ascribed racial differences If the categories were not matched then it could be argued that the system does not address racial inequality On the other hand if there are ever race-like dimensions of segregation that have not been politically recognized as racial categories then that is an interesting empirical result in its own right It suggests at the very least that there are active forms of discrimination in society based on properties of people that are not currently recognized politically We see the discovery of potentially unrecognized forms of discrimination as a benefit of this technique Another threat to validity of our analysis is the known fact that the schematic vicious cycle of racial formation presented in Figure is an over-simplification We have drawn this theory from a survey of sociology literature on the formation of race However we now only have a hypothesis about the actual effects of such a system designed as proposed here on the politics of race over time Confirming that hypothesis will require implementation and extensive user testing perhaps through a longitudinal study Our discussion of strategies and metrics for reducing segregation along race-like lines has been brief due to the scope of this paper We see refinement of these techniques as a task for future work An example open problem raised by the preceding discussion is which social network segregation measures are best at capturing the effects of racial inequality DISCUSSION Controversies surrounding machine learnings treatment of race have inspired a growing field of research about fairness in machine learning This field often treats fairness as formal property of computational systems where fairness is evaluated in terms of a set of protected groups A in our notation The system is considered fair if outcomes are in some sense balanced with respect to the groups Group membership is considered a simple fact about natural persons In this paper we have scrutinized what it means for racial identity to be a protected group in machine learning We trace the history of racial categories in US law and policy to show how racial categories became ingrained in society through policies of segregation and exclusion The recent manifestation of them in civil rights law is still based on their role as political status categories ascribed based on differences in body class nationality and ancestral origin Because they are intrinsically categories of disadvantage and inequality there is nothing fair about racial identity System designers are caught in a dilemma ignore race and reproduce the inequality of race by accident or explicitly consider racial statistics in order to mitigate inequality in favor of fairness Through its use of racial classification the latter systems put themselves in a paradoxical position of making the unequal equal and invite political opposition and cooption We propose a third way based on the insight that racial categories are perpetuated by real patterns of segregation in space and society We argue that rather than promote fairness as a system property systems should be designed with the objective of promoting social integration based on similar treatment of segregated populations To perform this function systems need a way to determine which which populations are racially segregated without reifying existing racial categories by dependence on racial statistics We propose unsupervised learning methods for finding latent dimensions of racial segregation in race and society These dimensions can be used to dynamically classify people into situationally sensitive racial categories that can then be entered into fairness computations Racial categories and the disadvantage associated with them are solidified through segregation in housing education employment and civic life which can happen through legislation and institutional mechanisms It is the segregation and the disparate advantages of being in segregated groups that is the cause of unfairness We are proposing that fairness in machine learning should be designed to detect segregation in an unsupervised way that does not reify the historical categories of reification while nevertheless being sensitive to the ongoing effects of those categories This design is adaptive to social change the emergence of new segregated and discriminated-against groups and also the emergence of new norms of equality