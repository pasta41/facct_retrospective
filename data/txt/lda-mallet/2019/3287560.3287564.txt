Fairness through Causal Awareness Learning Causal Latent-Variable Models for Biased Data How do we learn from biased data Historical datasets often reflect historical prejudices sensitive or protected attributes may affect the observed treatments and outcomes Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases We advocate a causal modeling approach to learning from biased data exploring the relationship between fair classification and intervention We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome Building on prior work in deep learning and generative modeling we describe how to learn the parameters of this causal model from observational data alone even in the presence of unobserved confounders We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute the treatment and the outcome We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair when presented with a historically biased dataset CONCEPTS Mathematics of computing Causal networks Computing methodologies Latent variable models Neural networks KEYWORDS causal inference variational inference fairness in machine learning INTRODUCTION In this work we consider the problem of fair decision-making from biased datasets Much work has been done recently on the problem of fair classification yielding an abundant supply of definitions models and algorithms for the purposes of learning classifiers whose outputs satisfy distributional constraints Some of the canonical problems for which these algorithms have been proposed are loan assignment criminal risk assessment and school admissions However none of these problems are fully specified by the classification paradigm Rather they are decisionmaking problems each problem requires an action or treatment to be taken in the world which in turn yields an outcome In other words the central question is how to intervene in an ongoing and evolving process rather than predict outcomes alone Decision-making ie learning to intervene requires a fundamentally different approach from learning to classify historical training data are the product of past interventions and thus provide an incomplete view of all possible outcomes Only actions which were previously chosen yield observable outcomes in the training data while the implicit counterfactual outcomes the outcome that would have occurred had another action been taken are never observed The incompleteness of this data can have great impact on learning and inference It has been widely argued that biased data yields unfair machine learning systems In this work we examine dataset bias through the lens of causal inference To understand how past decisions may bias a dataset we first must understand how sensitive attributes may have affected the generative process which created the dataset including the historical decisionmakers actions treatments and results outcomes Causal inference is well suited to this task since we are interested in decision-making rather than classification we should be interested in the causal effects of actions rather than correlations Causal inference has the added benefit of answering counterfactual queries What would this outcome have been under another treatment How would the outcome change if the sensitive attribute were changed all else being equal These questions are core to the mission of learning fair systems which aim to inform decision-making While there is much that causal inference can offer to the field of fair machine learning it also poses several significant challenges For example the presence of hidden confounders unobserved factors that effect both the historical choice of treatment and the outcome often prohibits the exact inference of causal effects Additionally understanding effects at the individual level can be especially complex particularly if the outcome is non-linear in the data and treatments These technical difficulties are often amplified by the problem scope of modern machine learning where large and high-dimensional datasets are commonplace To address these challenges we propose a model for fairly estimating individual-level causal effects from biased data which combines causal modeling with approximate inference in deep latent variable models Our focus on individual-level causal effects and counterfactuals provides a natural fit for application areas requiring fair policies and treatments for individuals such as X Y T a Pearl no hidden confounders X Z Y T b Louizos et al Causal Effect VAE Z Y T c with sensitive attribute X ZA Y T d Proposed model Fair Causal VAE Figure Various approaches causally modeling data features X treatment T and outcome Y a assumes no hidden confounders b models hidden confounders via a latent variable Z to be inferred by an inference neural network not pictured c naively extends b to include a sensitive attribute A as an additional observation not as a confounder d the Fair Causal VAE ours explicitly models the sensitive attribute as confounding the treatment T and the label Y in historical data finance medicine and law Specifically we incorporate the sensitive attribute into our model as a confounding factor which can possibly influence both the treatment and the outcome This is a first step towards achieving fairness through awareness in the interventional setting Our model also leverages recent advances in deep latent-variable modeling to model potential hidden confounders as well as complex non-linear functions between variables which greatly increases the class of relationships which it can represent Through experimental analysis we show that our model can outperform non-causal models as well as causal models which do not consider the sensitive attribute as a confounder We further explore the performance of this model showing that fair-aware causal modeling can lead to more accurate fairer policies in decision-making systems BACKGROUND Causal Inference We employ Structural Causal Models which provide a general theory for modeling causal relationships between variables An is defined by a directed graph containing vertices and edges which respectively represent variables in the world and their pairwise causal relationships There are two types of vertices exogenous variables U and endogenous variables V Exogenous variables are unspecified by the model we model them as unexplained noise distributions and they have no parents Endogenous variables are the objects we wish to understand they are descendants of endogenous variables The value of each endogenous variable is fully determined by its ancestors Each V V has some function fV which maps the values of its immediate parents to its own This function fV is deterministic any randomness in an is due to its exogenous variables In this paper we are primarily concerned with three endogenous variables in particular X the observable features or covariates of some example T a treatment which is applied to an example and Y the outcome of a treatment Our decision problem is given an example with particular values for its features X x what value should we assign to treatment T in order to produce the best outcome Y This is fundamentally different from a classification problem since typically we observe the result of only one treatment per example To answer this decision problem we need to understand the value Y will take if we intervene on T and set it to value t Our first instinct may be to estimate PY T t X x However this is unsatisfactory in general If we are estimating these probabilities from observational data then the fact that x received treatment t in the past may have some correlation with the historical outcome Y This confounding effect the fact that X has an effect on both T and Y is depicted in Figure a by the arrows pointing out of X into T and Y For instance in an observational medical trial it is possible that young people are more likely to choose a treatment and also that young people are more likely to recover A supervised learning model given this data may then overestimate the average effectiveness of the treatment on a test population Broadly to understand the effect of assigning treatment t supervised learning is not enough we need to model the functions fV of the Once we have a fully defined we can use the do operation to simulate the distribution over Y given that we assign some treatment denote this as PY doT x We do the do through graph surgery we assign the value t to T by removing all arrows going into T from the and setting the corresponding structural equation output to the desired value regardless of its input t We then set X x and continue with inference of Y as we normally would A common assumption in causal modelling is the no hidden confounders assumption which states that there are no unobserved variables affecting both the treatment and outcome We follow Louizos et al and use variational inference to model confounders that are not directly observed but can be abstracted from proxies In Sec we consider the implications of this approach and discuss alternative assumptions Approximate Inference Individual and population-level causal effects can be estimated via the do operation when the values of all confounding variables are observed which motivates the common no-hidden-confounders made/action taken and its result respectively These terms are associated with an alternative theory of causal inference which can also be used to describe the methods we propose but which we will not discuss in this paper assumption in causal inference However this assumption is rather strong and precludes classical causal inference in many situations relevant to fair machine learning eg where ill-quantified and hard-to-observe factors such socio-economic status SES may significantly confound the observable data Therefore we follow Louizos et al in modeling unobserved confounders using a high dimensional latent variable Z to be inferred for each observation X T Y They prove that if the full joint distribution is successfully recovered individual treatment effects are identifiable even in the presence of hidden confounders In other words causal effects are identifiable insofar as exact inference can be carried out and the observed covariates are sufficiently informative Because exact inference of Z is intractable for many interesting models we approximately infer Z by variational inference specifying X T Y using a parametric family of distributions and learning the parameters that best approximate the true posterior X T Y by maximizing the evidence lower bound ELBO of the marginal data likelihood In particular we amortize inference by training a neural network whose functional form is specified separately from the causal model to predict the parameters of q given X T Y Amortized inference is much faster but less optimal than local inference alternate inference strategies could be explored for applications where the importance of accuracy in individual estimation justifies the additional computational cost TARNets TARNets are a class of neural network architectures for estimating outcomes of a binary treatment The network comprises two separate armseach predicts the outcomes associated with a separate treatment that share parameters in the lower layers The entire network is trained end to end using gradient-based optimization but with only one arm the one with the treatment which was actually given receiving error signal for any given example The TARNet prediction of result R and input variables V and potential intervention I is expressed by combining the shared representation function with the two functions corresponding to the separate prediction arms This yields two composed functions V I V I with realized as neural networks Shalit et al explore a group-wise penalty on the outputs of we do not use this FAIR CAUSAL INFERENCE As stated in Sec we are interested in modeling the causal effects of treatments on outcomes However when attempting to learn fairly from a biased dataset this problem takes on an extra dimension In this context we become concerned with understanding causal effects in the presence of a sensitive attribute or protected attribute Examples include race gender age or SES When learning from a historical data we may believe that one of these attributes affected the observable treatments and outcomes resulting in a biased dataset Lum and Isaac give an example in the domain of predictive policing of how a dataset of drug crimes may become biased with respect to race through unfair policing practices They note that it is impossible to collect a dataset of all drug crimes in some area rather these datasets are really tracking drug arrests Due to a higher level of police presence in heavily Black than heavily White communities recorded drug arrests will by nature over-represent Black communities Therefore a predictive policing algorithm which attempts to fit this data will continue the pattern of over-policing Black communities Lum and Isaac provide experimental validation of this hypothesis through simulation contrasting the output of a common predictive policing algorithm with independent demographic-based estimates of drug use by neighborhood Their work shows that wrongly specifying a learning problem as one of supervised classification can lead to replicating past biases In order to account for this in the learning process we should be aware of the biases which shaped the data which may include sensitive attributes that historically affected the treatment and/or outcome Using the above example for concreteness we specify the variables at play The decision-making problem is should police be sent to neighborhood X at a given time The variables are A a sensitive attribute For example the majority race of a neighborhood T a treatment For example the presence or absence of police in a certain neighborhood on a particular day Y R an outcome For example the number of arrests recorded in a given neighborhood on a particular day X RD D-dimensional observed features For example statistics about the neighborhood which may change We will represent sensitive attributes and treatments as binary throughout this paper we recognize this is not always an optimal modeling choice in practice Note that the choice of treatment will causally alter the outcome an arrest cannot occur if there are no police in the area Furthermore the sensitive attribute can causally effect the outcome as well research has shown that policing can disparately effect various races even controlling for police presence the treatment in this case We note that in various domains there may be more variables of interest than the ones we list here and more appropriate causal models than those shown in Fig However we believe that the setup we describe is widely applicable and contains the minimal set of variables to be useful for fairness-aware causal analysis We are interested in calculating causal effects between the above variables In particular we seek answers to the following three questions What is the effect of the treatment on the outcome This will help us to understand which T is likely to produce a favorable outcome for a Let us x a doT x A a as the expected conditional outcome under T t that is the ground truth value taken by Y when the treatment T is assigned the value t and conditioning on the values x a for the features and sensitive attribute respectively Then we can express the individual effect of T on Y as x a a a What is the effect of the sensitive attribute on the treatment This allows us to understand how the treatment assignment was biased in the data Similarly we can define x Et doA aX x which is the expected conditional treatment in the historical data when the value a is assigned to the sensitive attribute Then the individual effect of A on T can be expressed as x tAx tAx What is the effect of the sensitive attribute on the outcome This allows us to understand what bias is introduced into the historically observed outcome We can also define x doA aX x T x as the expected conditional outcome under A a the ground truth value of Y conditioned on the features being x if the sensitive attribute were assigned the value a and the treatment T were assigned the ground truth value x Then we can express the individual effect of A on Y as x Intervening on Sensitive Attributes There has been some disagreement around the notion of intervening on an immutable or effectively immutable sensitive attribute Holland argue that there is no causation without manipulation ie an attribute can never be a cause only an experience undergone can be Briefly stated they argue that if the factual and counterfactual versions cannot be defined in principle it is impossible to define the causal effect In a counterargument Marini and Singer claim that a synthesis of intrinsic and extrinsic determination provides a more adequate picture of causal relations meaning that both externally imposed experiences extrinsic and internally defined attributes intrinsic are valid conceptual components of a theory of causation We agree with this view that the notion of a causal effect of an immutable attribute is valid and believe that it is particularly useful in a fairness context Specifically pertaining to race some argue it is possible to understand the causal effect of an immutable attribute in terms of the effects of more manipulable attributes proxies VanderWeele and Robinson argue that rather than interpreting a causal effect estimate of A as a hypothetical randomized intervention on A one can interpret it as a particular type of intervention on some other set of manipulable variables related to A under certain graphical and distributional assumptions on those variables Sen and Wasow take a constructivist approach and consider race to be composed of constituent parts some of which can be theoretically manipulated They describe several experimental designs which could estimate the effects of immutable attributes Another issue with intervening on sensitive attributes is that since many are assigned at conception all observed covariates X are post-treatment as reflected in the design of our in Fig d In statistical analysis a frequent approach is to ignore all post-treatment variables to avoid introducing collider biases However in our model the purpose of the covariates is to deduce the true unobserved values of the latent Z for that individual Therefore when conditioning on the observed covariates correlation of A and Z is the objective rather than an undesired side effect This is the first step Abduction of computing counterfactuals according to Pearl we can think of this as adjusting for bias of the sensitive attribute in the X -generating process PROPOSED METHOD In this section we first conceptualize and describe our proposed causal model depicted in Fig discuss the parameterization of the corresponding and learning procedure A common causal modelling approach is to define a new for each problem Pearl taking advantage of domain specific knowledge for that particular problem This stands in contrast to a classic machine learning ML approach which aims to process data and draw conclusions as generally as possible by automatically discovering patterns of correlation in the data While the causal modelling approach is capable of detecting effects the ML approach cannot the ML approach is attractive since it provides modularity generality and a more automated data processing pipeline In this work we aim to interpolate between the two approaches by considering a single general causal model for observational data Our model contains what we argue are a minimal set of fairly general causal variables for discovering treatment effects and biases in the data-generation process allowing us to interface causally with arbitrary data that fits the proposed structure Two features of our causal model are noteworthy First is the explicit consideration of the sensitive potential source of dataset a confounder which causally affects both the treatment T and the outcome Y This contrasts with approaches from outside the fairness literature eg Fig b which in a fairness setting Fig c would treat potential sensitive attributes as equivalent to other observed features Our model accounts for the possibility that a sensitive attribute may have causal influence on the observed features treatments and outcomes and the historical process which generated them It makes the sensitive attribute distinct from the other attributes of X which we understand not as confounders but observed proxies We can think of this as a causal modeling analogue of fairness through awareness By actively adjusting for causal confounding effects of sensitive attributes we can build a model which accounts for the interplay between the treatment and outcome for both values of the sensitive attribute The other noteworthy aspect of our model is the latent variable Z Together Z and A make up all the confounding variables We note two important points about these confounders Firstly we clarify that the model class we propose a latent Gaussian and a deep neural network is not necessarily the definitive model of the confounders ofT however it is a flexible one with numerous applications in machine learning Secondly we note that causal inference and machine learning have different conventions around unobserved ie latent variables in causal inference these variables are generally considered to be nameable objects in the world eg SES historical predjudice whereas in machine learning they represent some unspecified and perhaps abstract structure in the data Our Z follows the machine learning convention As in Louizos et al Z represents all the unobserved confounding variables which effect the outcomes or treatments other than A The features X can be seen as proxies noisy observations for the confounders Z A Altogether the endogenous variables in our model are X A Z T and Y We also have exogenous variables not shown each the immediate parent of only their respective endogenous variable The structural equations are Z A X Z A T Z A Y Z AT V Z AX T Y does not necessarily refer to tangible objects in the world it is reasonable that Z A in our model This does not prevent a characteristic such as SES which may be correlated with A from being a confounder rather Z could represent the component of SES which is not based on A Since both confounders are inputs to all other variables in the the model can learn to represent variables which are based on A eg SES as a joint distribution of Z and A With this in hand we can estimate various interventional outcomes if we know the values of fV V Z AX T Y For instance we might estimate E Y Z zA PY za E Y Z doT PY z E Y Z PY PT z z which are the expected values over outcomes of interventions on T T and A and just A respectively However the problem with the calculations in is that Z is unobserved so we cannot simply condition on its value Rather we observe some proxies X Since the structural equations go the other direction X is a function of Z not the other way around inferring Z from a given X is a non-trivial matter In summary we need to learn two things a generative model which can approximate the structural functions and an inference model which can approximate the distribution of Z given X Following the lead of Louizos et al we use variational inference parametrized by deep neural networks to learn the parameters of both of these models jointly In variational inference we aim to learn an approximate distribution over the joint variables AX T Y by maximizing a variational lower bound on the log-probability of the observed data As demonstrated in Louizos et al the causal effects in the model become identifiable if we can learn this joint distribution We extend their proof in Appendix B to show identifiability holds when including the sensitive attribute in the model as in Fig d We discuss here the identifiability condition from Louizos et al Given some treatment T and outcome Y the classic no hidden confounders assumption asserts that the set of observed variables O blocks all backdoor paths from T to Y Louizos et al weaken this they assume that there is a set of confounding variables Z such that Z blocks all backdoor paths to Y are observed are unobserved They claim that if we recover the full joint distribtuion of X T Y then we can identify the causal effect T Y However this is only possible if we have sufficiently informative proxies X While recovering the full joint distribution does not mean we have to measure every confounder we do have to at least measure some proxy for each confounder This is a weaker assumption but not fully general There may be confounding factors which cannot be inferred from the proxies X in this case our model will be unable to learn the joint distribution and the causal effect will be unidentifiable In this case we are back to square one our causal estimates may be inaccurate Determining the exact fairness implications of this remains an open problem it would depend on which confounders were missing and which proxies were already collected A complicating factor is that testing for unconfoundedness is difficult and usually requires making further assumptions Therefore we might unintentionally make unfair inferences if we are unaware that we cannot infer all confounders If we think this is the case one solution is to collect more proxies This provides an alternative motivation for the idea of increasing fairness by measuring additional variables To learn a generative model of the data which is faithful to the structural model defined in we define distributions p which will approximate various conditional probabilities in our model We model the joint probability assuming the following factorization AX T Y Z ApT Z Z AT Each of these p corresponds to an in formally v fV for an endogenous variable V and subset of endogenous where V Z AX T Y For simplicity we choose computationally tractable probability distributions for each conditional probability in pA Z A Z X Z A pT Z A Z A are the dimensionalities respectively and is the empirical marginal probability of A across the dataset if this is unknown we could use a Beta prior over that distribution in this paper we assume A is observed for every example For pY Z AT we use either a Bernoulli or a Gaussian distribution depending on if Y is binary or continuous Y Z AT Z AT Y Z AT NY Z AT Y Z AT To flexibly model the potentially complex and non-linear relationships in the true generative process we specify several of the distribution parameters from and as the output of a function which is realized by a neural network or TARNet with parameters We parametrize the model of X with neural networks X X Z A X Z A X Z A Z A We use TARNets see Sec to parameterize the distributions over T and Y In our model A acts as the treatment for the TARNet that outputs T Likewise A and T are joint treatments affecting Y our Y model can be seen as a hierarchical TARNet with one TARNet for each value of A where each TARNet has an arm for each value ofT In all this yields the following parametrization pT Z A Z A Z A pY Z AT T Z AT T Z AT T Z AT Z AT and the same for Z AT Y Z AT is the sigmoid function x and I R V I are defined as in Sec We further define an inference model q to determine the values of the latent variables Z given observed X A This takes the form X A X Z X A where the normal distribution is reparametrized analogously to with networks Z Z Since A is always observed we do not need to infer it even though it is a confounder We note that this is a different inference network from the one in Louizos et al we do not use the given treatments and outcomes in the inference model We found it to be a simpler solution no auxiliary networks necessary and did not see a large change in performance This is similar to the approach taken in Parbhoo et al To learn the parameters of this model we can maximize the expected lower bound on the log probability of the data the ELBO which takes the form below which we note is also a valid ELBO to optimize for lower-bounding the conditional log-probability of the treatments and outcomes given the data L n i xi ai ai ai ai ti xi ai Our work most closely relates to the Causal Effect Variational Autoencoder Some follow-up work is done by Parbhoo et al who suggest a purely discriminative approach using the information bottleneck Our model differs from this work in that they did not include a sensitive attribute in their model and their model does not contain a reconstruction fidelity term in this case ai Previous papers which learn causal effects using deep learning with all confounders observed include Shalit et al and Johansson et al who propose TARNets as well as some form of balancing penalty The intersection of fairness and causality has been explored recently Counterfactual fairness the idea that a fair classifier is one which doesnt change its prediction under the counterfactual value of X when A is flipped is a major theme Criteria for fairness in treatments are proposed in Nabi and Shpitser and fair interventions are further explored in Kusner et al Zhang and Bareinboim present a decomposition which provides a different way of understanding of unfairness in a causal inference model Other work focuses on the causal relationship between sensitive attributes and proxies in fair classification Kallus and Zhou explore the idea of learning from biased data making the point that a fair predictor learned on biased data may not be fair under certain forms of distributional shift while not touching on causal ideas Some conceptually similar work has looked at the selective labels problem where only a biased selection of the data has labels available There has also been related work on feedback loops in fairness and the idea that past decisions can affect future ones in the predictive policing and recommender systems contexts for example Barabas et al advocate for understanding many problems of fair prediction as ones of intervention instead Another variational autoencoder-based fairness model is proposed in Louizos et al but with the goal of fair representation learning rather than causal modelling Dwork et al originated the term fairness through awareness and argued that the sensitive attribute needed to be given a place of privilege in modelling in order to reduce unfairness of outcomes EXPERIMENTS In this section we compare various methods for causal effect estimation The three effects we are interested in are A T the causal effect of A on T ET doA X ET doA X A Y the causal effect of A on Y doA X doA X T Y the causal effect of T on Y doT X A doT X A Note that all three effects are individual-level that is they are conditioned on some observed X and possibly A and then averaged across the dataset Data We evaluate our model using semi-synthetic data The evaluation of causal models using non-synthetic data is challenging since a random control trial on the intervention variable is required to validate correctness this is doubly true in our case where we are concerned with two different possible interventions Additionally while data from random control trials for treatment variables exists albeit uncommon conducting a random control trial for a sensitive attribute is usually impossible We have adapted the dataset a standard semisynthetic causal inference benchmark for use in the setting of causal effect estimation under a sensitive attribute The dataset is from a randomized experiment run by the Infant Health and Development Program in the US which targeted low-birth-weight premature infants and provided the treatment group with both intensive high-quality child care and home visits from a trained provider Pre-treatment variables were collected from both child eg birth weight sex and the mother at time of birth eg age marital status and behaviors engaged in during the pregnancy eg smoked cigarettes drank alcohol as well as the site of the intervention where the family resided We choose our sensitive attribute to be mothers race binarized as White and non-White We follow a similar method for generating outcomes to the Response B surface proposed in Hill However our setup differs since we are interested in additionally modelling a sensitive attribute and hidden confounders so there are three more steps which must be taken First we need to generate outcomes Y for each example for T under the counterfactual sensitive attribute A Second we need to generate a treatment assignment for each example for the counterfactual value of the sensitive attribute Finally we need to remove some data from the observable measurements to act as a hidden confounder as in Louizos et al We detail our full data generation method in Appendix A We denote the outcome Y under interventions doT a as The subroutines in Algorithms and generate all factual and counterfactual outcomes and treatments for each example one for each possible setting of A Values of the constants that we use for data generation can be found in Appendix A We choose our hidden confounding feature Z to be birth weight In the second optional step of data generation we choose to remove or other features Especially if we choose features which are highly correlated with the hidden confounder this has the effect of making the estimation problem more difficult When removing features we do nothing When removing feature we remove the feature which is most highly correlated with Z head size When removing features we remove two features most highly correlated with Z head size weeks born preterm Experimental Setup We run four different models for comparison including the one we propose Since we are interested in estimating three different causal effects simultaneously A T A Y T Y we cannot compare against most standard causal inference benchmark models for treatment effect estimation The models we test are the following Counterfactual MLP a multilayer perception MLP which takes the treatment and sensitive attribute as input concatenated and aims to predict outcome Counterfactual outcomes are calculated by simply flipping the relevant attributes and re-inputting the modified vector to the MLP A similar auxiliary network learns to predict the treatment from a vector of X concatenated to A Counterfactual Multiple MLP a set of four MLPs one for each combination of AT Examples are inputted into the appropriate MLP for the factual outcome and simply inputted into another MLP for the appropriate counterfactual outcome A similar pair of auxiliary networks predict treatment Causal Effect Variational Autoencoder with Sensitive Attribute Fig c a model similar to Louizos et al but with the simpler inference model we propose We incorporate a sensitive by concatenating X to A as input counterfactuals along A are taken by flipping A and re-inputting the modified vector Counterfactuals along T are taken as in Louizos et al Fig d our proposed fair-aware causal model with A concatenated to Z as confounders We run two versions one where A is used to help with reconstructing X and inferring Z from X and one where it is not Formally the inference model and generative model of X in A T T Y A Y Table for each model on data no extra features removed Mean and standard errors shown as calculated over random seedings are X A and Z A and in are X and Z respectively In both versions A is a confounder of both the treatment and the outcome The is purely a classification baseline It learns a mapping from input to output estimating the conditional distribution PY X AT The shares this goal but has a more complex architecture it learns a disjoint set of parameters for each setting of interventions allowing it to model completely separate generative processes However it is still ultimately concerned with supervised prediction Furthermore neither of these models is built to consider the impact of hidden confounders The is a model for causal inference of outcomes from treatments Therefore we should expect it to perform well in Y It is also created to model these effects under hidden confounders Therefore the difference between and the MLPs will tell us the improvement which comes from appropriate causal modelling rather than classification However the does not consider the sensitive attribute A as a confounder rather it treats it simply as another covariate of X So in comparing the to the we observe the improvement that comes from causally modelling the dataset unfairness stemming from a sensitive attribute In comparing the to the MLPs we observe the full impact of the joint causal modelling of treatments outcomes sensitive attributes and hidden confounders See Appendix C for experimental details Results Estimating Causal Effects In this section we evaluate how well the models from Sec can estimate the three causal effects A T A Y T Y To avoid confusion with the words treatment and outcome in each of these three causal interactions we will refer to to the causing variable as the intervention variable and the affected variable as the result variable To evaluate how well our model can estimate causal effects we use Precision in Estimation of Heterogeneous Effects This is calculated as Er r r r where ri is the ground truth value of result from the intervention i and ri is our models estimate of that quantity measures our ability to model both the factual ground truth and the counterfactual results In Tables we show the for each of the models described in Sec for each causal effect of interest Each table shows results for a version of the dataset with of the most informative features removed as measured by correlation with the hidden confounder Model A T T Y A Y Table for each model on data most informative feature removed Mean and standard errors shown as calculated over random seedings Model A T T Y A Y Table for each model on data most informative features removed Mean and standard errors shown as calculated over random seedings Lower is better Therefore the easiest problem is with zero features removed the hardest is with two Note that in Y R Generally as expected we observe that the causal models achieve lower for most estimation problems Also as expected we observe that that the for the more complex estimation problems A Y T Y increases as the most useful proxies are removed from the data We suspect there is less variation in the results for A T since it is a simpler problem there are no extra confounders other than Z or mediating factors to consider We find that our model the compares favorably to the other models in this experiment We see that in general the fair-aware models and have lower than all other models when estimating the causal effects relating to the sensitive attribute A Y A T Furthermore the also performs similarly to the at T Y estimation as well demonstrating a slight improvement at least in the more difficult features removed cases One interesting note is that where A is used in reconstruction of X and in inference of Z and seem to perform similarly with being slightly better if anything This may seem surprising at first since one might imagine that allow the model to learn better representations of X particularly for the purpose of doing counterfactual inference across A To explore this further we examine in table the latent learned by each model in terms of their encoder mutual information which is calculated X the KL-divergence from the encoder posterior to the prior This quantity is roughly the same for both versions of the implying that the inference network does not leverage the additional information provided by A in its latent code Z This is in fact sensible because has access to A as an observed confounder in modeling the structural equations We also noticed that contains about one bit of extra information in its latent Model KL Table KL divergence from the encoder posterior to prior after training on equivalent to encoder mutual information and use X A as input to encoder while uses X only Mean and standard errors shown as calculated over random seedings code implying some degree of success in capturing relevant information about A in Z But if models all confounders during inference why does it underperform relative to estimating the downstream causal effects especially A Y By making explicit the role of A as confounder we hypothesize that can learn the interventional distributions with respect to A eg pY T doA rather than the conditional distributions of eg pY T Z A we suspect that the gating mechanism of the TARNet implementation of the structural equations to be important in this regard Learning a Treatment Policy The next natural question is how does estimating these causal effects contribute to a fair decisionmaking policy We examine two dimensions of this We define a policy X A T as a function which maps inputs features and sensitive attribute to treatments We suppose the goal is to assign using a policy T X A that maximizes its expected defined here as the expected it achieves over the data ie V Y doT x aA aX x For example we could imagine the treatments to be various medications and the outcome to be some health indicator eg number of months survived post-treatment We can derive a policy from an outcome prediction model simply by outputting the predicted argmax value over treatments ie x a T Y doT tA aX x where Y is the models prediction of the true outcome Y The optimal policy a T Y doT tA aX x takes the argmax over ground truth outcomes every time First we look at the mean regret of the policy which is the difference between its achieved value and the the value of the optimal policy V V We note that in general a policys regret is not easy to compute or bound without assumptions on the outcome distribution in the data In Table we display the expected regret values for the learned policies We observe that the fair-aware model achieves lower regret than the unaware causal model and much lower regret than the non-causal models for both the easier and more difficult settings of the data Next we attempt to measure the policys fairness Most fairness metrics are designed for evaluating classification not for intervention However Chen et al explore an idea which is easily adjusted to the interventional setting that an algorithm is unfair if it is much less accurate on one subgroup Here we adapt this notion to evaluate treatment policy fairness For any x let us say the policy is accurate if it chooses the treatment which in fact yields the best outcome for that individual Model removed removed removed Table Regret for each models policy on data with or of the most useful covariates removed Mean and standard errors shown as calculated over random seedings Lower regret is better Model removed removed removed Table Accuracy gap for each models policy on data with or of the most useful covariates removed Mean and standard errors shown as calculated over random seedings Lower gap is more fair ie if x a a We can define the accuracy of the policy x a a where is an indicator function We can define the subgroup accuracy as accuracy calculated while conditioning not intervening on a particular value of A Ex x We condition rather than intervene since we are interested in measuring the impact of the policy on real existing populations rather than hypothetical ones Finally to evaluate the fairness of the policy we can look at the accuracy gap If this is high the model is more unfair since the policy has been more successful at modelling one group than the other and is much more consistently choosing the correct treatment for individuals in that group In Table we display the accuracy gaps for our models and baselines on the dataset We observe that the achieves a smaller accuracy gap than those which do not consider the effect of the sensitive attribute This is an encouraging sign that by understanding the confounding influence of sensitive attributes in biasing historical datasets we can learn treatment policies which are more accurate for all subgroups of the data DISCUSSION In this paper we proposed a causally-motivated model for learning from potentially biased data We emphasize the importance of modeling the potential confounders of historical datasets we model the sensitive attribute as an observed confounder contributing to dataset bias and leverage deep latent variable models to approximately infer other hidden confounders In Sec we demonstrated how to use our model to learn a simple treatment policy from data which assigns treatments more accurately and fairly than several causal and non-causal baselines Looking forward the estimation of sensitive attribute causal effects suggests several compelling new research directions which we non-exhaustively discuss here Counterfactual Fairness Our model learns outcomes for counterfactual values of both T and A This means we could choose to implement a policy where we assess everyone under the same value a by assigning treatments to all individuals no matter their original value a of A based on the inferred outcome distribution PY doA aX T Such a policy respects the definition of counterfactual fairness proposed by Kusner et al which requires invariance to counterfactuals in A at the individual level Path-Specific Effects Our model allows us to decompose A Y into direct and indirect effects through mediation analysis of T By estimating this decomposition we could learn a policy which respects path-specific fairness as proposed by Nabi and Shpitser Analyzing Historical Bias Estimating causal effects permits for the analysis and comparison of bias in historical datasets For instance the effect A T is a measure of bias in a historical policy and the Y is a measure of bias in whatever system historically generated the outcome This could serve as the basis of a bias auditing technique for data scientists absence of data especially not-at-random has strong implications for downstream modeling in both fairness and causal inference Our model outputs counterfactual outcomes for both A and T which could be used for fair missing data imputation This could in turn enable the application of simpler methods like supervised learning to interventional problems Fair Policies Under Constraints In this paper we consider an approach to fairness where understanding dataset bias is paramount rather than the more common fairness-accuracy constraint-based tradeoff However in some domains we may be interested in policies which satisfy a fairness constraint eg the same distribution of treatments are given to each group Estimating the underlying causal effects would be useful for constrained policy learning Incorporating Prior Knowledge Graphical models both probabilistic and permit the specification of prior knowledge when modeling data and provide a framework for inference that balances these beliefs with evidence from the data This is a powerful fairness idea we may believe a priori that a dataset should look a certain way if not for some bias In the context of a fair machine learning pipeline that considers many datasets this relates to the AutoML task of learning distributions over datasets that share global parameters In automated decision making the focus on intervention over classification suggests the more equitable deployment of machine learning when only biased data are available but also raises significant technical challenges We believe causal modeling to be an invaluable tool in addressing these challenges and hope that this paper contributes to the discussion around how best to understand and make predictions from existing datasets without replicating existing biases