A Taxonomy of Ethical Tensions in Inferring Mental Health States from Social Media Powered by machine learning techniques social media provides an unobtrusive lens into individual behaviors emotions and psychological states Recent research has successfully employed social media data to predict mental health states of individuals ranging from the presence and severity of mental disorders like depression to the risk of suicide These algorithmic inferences hold great potential in supporting early detection and treatment of mental disorders and in the design of interventions At the same time the outcomes of this research can pose great risks to individuals such as issues of incorrect opaque algorithmic predictions involvement of bad or unaccountable actors and potential biases from intentional or inadvertent misuse of insights Amplifying these tensions there are also divergent and sometimes inconsistent methodological gaps and under-explored ethics and privacy dimensions This paper presents a taxonomy of these concerns and ethical challenges drawing from existing literature and poses questions to be resolved as this research gains traction We identify three areas of tension ethics committees and the gap of social media research questions of validity data and machine learning and implications of this research for key stakeholders We conclude with calls to action to begin resolving these interdisciplinary dilemmas CONCEPTS Human-centered computing Collaborative and social computing Social media Applied computing Psychology KEYWORDS mental health ethics machine learning algorithms social media INTRODUCTION Last year Facebook unveiled automated tools to identify individuals contemplating suicide or self-injury The company claims that they use pattern recognition technology to help identify posts and live streams as likely to be expressing thoughts of suicide which then can deploy resources to assist the person in crisis Reactions to Facebooks suicide prevention artificial intelligence AI are mixed with some concerned about the use of AI to detect suicidal ideation as well as potential privacy violations Other suicide prevention AIs however have been met with stronger public backlash Samaritans Radar an app that scanned a persons friends for concerning Twitter posts was pulled from production citing concerns for data collection without user permission as well as enabling harassers to intervene when someone was vulnerable Since a new area of research has incorporated techniques from machine learning natural language processing and clinical psychology to categorize individuals moods and expressed well-being from social media data These algorithms are powerful enough to infer with high accuracy whether an individual might be suffering from disorders such as major depression postpartum depression post-traumatic stress schizophrenia and suicidality These algorithms can also reveal symptomatology linked to psychiatric challenges such as self-harm severity of distress or cognitive distortions Together we use the term predicting mental health status to describe these mental disorders and related symptomatology Computer Science  researchers and clinicians are now poised to learn more about the earliest manifestations of psychiatric disorders through social media data New insights could prevent the development of latent conditions mitigate the impact of emerging disorders or as exemplified by Facebooks new suicide AI new opportunities to intervene with life-saving assistance With the rising prevalence of mental disorders many researchers see the benefits of better screening identification and intervention assisting to promote better health and well-being worldwide However the examples of suicide prevention AIs demonstrate major concerns for algorithmic development and their implications This includes new concerns about consent into monitoring or intervention systems and privacy and data management questions Ethics boards do not have standards for managing social media research and the prediction of mental health status raises new questions about consent vulnerable populations and online communities There are also methodological concerns of data collection and bias validity of these results for clinical assessment and the application of machine learning methods to predicting mental health status Furthermore the lack of consistency with methods across this research space makes this problem more troubling For implications actors with many motivations can misuse data and predictions and amplify the harms of algorithms in reproducing unfair stereotypes and discrimination of individuals with mental disorders Caused in part by the interdisciplinary intersection of data science machine learning psychology and human-centered computing unanswered questions emerge around the role of the individual in predictions and managing implications of this research As these technologies are developed to detect mental health status these concerns will grow unless we rectify these problems We stand to gain much from this research in better understanding and making interventions in mental health Addressing these concerns will resolve questions around rigorous science in the area benefit clinical research and safeguard well-being for individuals and society Many of these concerns are not limited to just mental health and social media and apply to other application domains of these technologies that touch on sensitive issues In answering these questions we offer insight into questions on how to ethically and rigorously apply machine learning and AI to sensitive domains such as mental health and we provide this analysis as a case study for ethics in applied and fair AI This work presents a first taxonomy of issues in algorithmic prediction of mental health status on social media data First we discuss the gap between ethics committees and participants in such research on what can be sensitive and sometimes stigmatizing data Second we identify tensions in methods and analysis such as construct validity and bias interpretability of algorithmic output and privacy Finally we examine implications of this research in benefiting mental health research challenges faced by key stakeholders and the risks of designing interventions We contextualize these three areas by drawing from prior work in this domain ethics research around these technological advances and our experiences conducting this research In our analysis of each of our three areas we look to prior work and standards across fields machine learning ML natural language processing NLP human computer interaction HCI clinical psychiatry and data science for guidance We conclude with calls for interdisciplinary action to resolve these dilemmas STATE OF THE ART IN THE FIELD The origins of predictive work come from either population-level analyses or studies of generalized and subjective well-being and affect assessment Borrowing from advances in natural language processing and psychology to represent text as cues of well-being these studies described mood shifts around political events geographic differences in expressed well-being and the seasonality and temporality of mood variation In addition to studying generalized well-being researchers also assessed population happiness both on Twitter and Facebook Besides establishing that psychological and health states can be inferred from this data these findings show that people use social media to discuss their personal mood and activities honestly and candidly instead of their idealized versions Complementary to this research were studies in public health measurement with online data termed infodemiology This famously includes the use of human-generated data to predict influenza outbreaks through search engines Researchers also used social media data to track the spread of disease and to analyze other ailments on population-scale user bases from Twitter Soon after these studies were the first predictive works on the mental health states of individuals beginning with depression In De Choudhury et al used clinically validated depression measures to find Twitter users who tested for major depressive disorder They developed a model that could predict if someone had depression with accuracy Around the same time Park et al developed a mixed methods approach to understand how Facebook use corresponded to clinical scales for depression In Coppersmith et al used self-reported disclosures of depression diagnosis on Twitter I was diagnosed with depression on to classify individuals suffering with depression contrasting their language with those who do not self-report such diagnoses De Choudhury et al also sought to identify new mothers who might be suffering from postpartum depression using Facebook and Twitter data After these works researchers began to replicate extend generalize and improve on these findings and in different cultural contexts and social media sites beyond just English-speaking Twitter From these seminal works on depression new studies have investigated new psychiatric disorders new social network platforms and new modalities Research has examined other disorders such as post-traumatic stress disorder anxiety schizophrenia eating disorders and suicidal ideation Work also now explores the symptomatology of mental disorders such as the severity of mental illness and stress connected to mental health Datasets too have expanded to social networks other than Twitter and Facebook like SinaWeibo Instagram Tumblr and Reddit Modalities other than text are now analyzed for their signals of mental health status Automated image analysis can identify self-harm photos on Flickr signs of depression through Instagram images and mental health disclosures on Reddit Finally new data sources have begun to supplement social media data like active and passive sensing technologies Ethical Considerations in Existing Research Overall the field of deriving algorithmic predictions of individuals mental health status is a growing area of research interest across sub-disciplines of  and is gaining traction in relevant domains Most though not all of this works touches on ethical and methods challenges as well as steps researchers take to mitigate risks to individuals whose data is analyzed Many papers include explicit notes about obfuscating sensitive and personally identifiable information data de-identification involvement of domain experts for responsible data handling and curation the need for ethical and privacy sensitivity in technology powered by algorithmic inferences quality of inferences among potential stakeholders and the need for cross-disciplinary collaboration and dialogue to prevent misuse and misinterpretation of algorithmic outcomes Notable Gaps However there are no accepted guidelines to navigate these challenges decisions by a particular research team omitted from papers are often invisible to the community leading to difficulties in normalizing ethical considerations Given the vulnerability and sensitivity of the population and the topic we find this concerning Discussions of consent validity underlying bias from data collection techniques or machine learning model selection is very limited even though applying algorithms in practical scenarios features prominently as an end goal of this research To frame a new set of interdisciplinary ethical guidelines in this emergent research area we look to these works to inform our analysis INSIGHTS FROM ETHICS RESEARCH Complementary to this work is a long history investigating the ethics of computing technology on broader domains In fact some of the gaps we note above such as participant consent role of ethics boards and challenges to autonomy and privacy have been discussed at length in these works Given the growing significance of machine learning and algorithms in different domains this field has received renewed attention both within the FAT community as well as the field of critical algorithms We provide a brief overview of relevant research in three spaces social media research ethics public health research and critical data studies Social Media Research Ethics Ample research has addressed issues in social media and ethics as early as Moving into the age of big data scholars are considering how new methods and data aggregation techniques impact individuals involved in this research Hargittai analyzed the snowballing effects that of unintended biased sampling of social media data on big data analyses Zimmer has examined ethical use of Facebook data and proposed a topology of ethical issues from Twitter research Finally Olteanu et al considered the methodological challenges of mining social media for information including issues of internal and external validity data curation and methods Public Health and Ethics Second we look to the history of public health research social media and ethics for population-scale predictions of disease and disorders Dredze and Paul consider social media research for public health focusing on end-to-end consideration of study design identifying target conditions methods and ethics Next Conway and Connor address advances and ethics of population-scale predictions of mental health providing an overview of the field and reflecting on how big data methods like machine learning and NLP facilitate surveillance of mental health for populations Metaphors for social surveillance of public health have been proposed like Vayena et als digital epidemiology to understand ethical obligations of researchers using public data Horvitz and Mulligan analyzed the potential legal privacy and data protection issues of big data analysis for well-being Norval and Henderson unpack various theories of privacy to analyze whether informed consent should be gathered in social media health research for patient information while Mikal et al used focus groups to understand users perceptions of social media data use for mental health research In NLP Benton et al recently considered the protocols for ethical social media health research from their own experiences in the field Their work discusses the ethical contention surrounding the use of public social media data for population health inference and its exemption from review by US Institutional Review Boards IRBs Stylistically this work is closest to our position although the ethical guidelines provided by Benton et al are geared toward public health needs not individualized predictions Critical Data Studies Finally the intersection of critical technology research and big data has led to critical data studies providing useful metaphors and case studies on the impacts of big data research In an early work boyd and Crawford push the new field of data science to critically consider its methods In response to the failure of Google Flu Trends Lazer et al cautions researchers to be cautious in applying predictive techniques Foucault-Welles brings light to the discriminatory impacts of aggregating analysis of social data that erases differences of minority groups Metcalf and Crawford discuss the difficulties of using other research relationship metaphors such as the physician-patient metaphor to illuminate how data researchers could conceptualize their users as more than just data sources These three perspectives discuss important concerns participant consent and contextual data integrity data protection anonymization and privacy methodological rigor bias and validity and implications of the research for different stakeholders Drawing from these two larger domains the state-of-the-art on mental health status prediction and surrounding discussion we identify three areas of tension that encapsulate concerns in this research area THREE AREAS OF ETHICAL TENSION Among the areas of ethical tension identified above first we address the research design and approval stage of the research We consider what is ethical to study if the work deserves ethics board approval and to what extent we treat social media users as research subjects in these studies Second we examine methodological concerns like feature generation and algorithm selection Finally we consider the implications of what these predictions might mean for clinicians researchers and other key stakeholders in this space Participants and Research Oversight Reacting to unethical behavior in medical and psychological experiments in the s and s many countries have adopted ethical research standards for human subjects research This standards manifest in an ethics committee whether that be an IRB Federal-wide Assurance-certified ethics board European Union EU ethics committees and corporate internal review committees Researchers and clinicians must also follow legal requirements to protect the dignity and privacy of individuals In the United States the Belmont Report and accompanying Common Rule legislation set protocols for human subject research which receives federal funding Further the Health Insurance Portability and Accountability Act HIPAA protects privacy of patients in clinical relationships with doctors in the US and privacy rights of medical records with similar protections in other countries Guided by the principles of respect beneficence and justice ethics research boards eg US IRBs deliberately transform people into research subjects in scientific inquiry this transformation prescribes people with certain rights protections and obligations that must be protected In clinical studies this obligation is at the forefront of experimental design Is predicting mental health status on social media human subjects research How do we assess harm of this mental health research without the oversight of an ethics committee In this section we discuss challenges of predicting mental health status outside a clinical setting using data-driven algorithm and impacts to participants Key Areas of Tension The Unclear Role of Ethics Committees Consent at Scale Vulnerable Populations and Risk Contextual Integrity of Communities The Unclear Role of Ethics Committees Analysis of publicly visible social media data is often exempt from research protections provided to subjects through ethics committees These studies are exempted for two primary reasons one in large-scale data analyses there is no interaction or intervention with subjects because the research is observational two the data being used was publicly available when collected Many ethics boards consider social media to be public space synonymous with gathering publicly available data that might be stored in Census records or courthouses We find this interpretation consistent across different countries and in different research environments Researchers will often cite one or both of these principles in their data collection sections there exists no relationship between researcher and social media user nor a doctor-patient relationship that would mandate medical privacy guidelines come into play Studies that do interact with subjects through surveys of crowd-workers or individuals recruited through word of mouth advertisements or through apps tend to declare appropriate ethics board approval However predicting mental health states using public social media data emphasizes whether this research should be exempt from ethics committee oversight Unlike in public health predicting mental health states even if with public data borders on medical diagnosis such as predicting the presence of schizophrenia Research is more than just the sum of its parts and extensive secondary analysis can be done from traces of social media data Mental health is a complex and sensitive area that can be isolating and stigmatizing and harm can be difficult to evaluate especially in second-order impacts Is this research human subjects research How should ethics boards handle this new research paradigm Consent at Scale In traditional human subjects research participant pools rarely exceeds several hundred This is because inference about mental health states could only be learned through clinician-patient relationships or lab studies that naturally limits the subject pool By consenting into this research participants are aware that they are part of research and therefore being surveilled Consent could meaningfully be gathered from participants and served as an important signal for participation Unlike clinical mental health studies social media datasets can contain millions of public posts and user accounts regularly exceed the hundreds of thousands obtaining consent at this scale is pragmatically impossible However there are tensions between the infeasibility of obtaining consent and conducting analysis about mood and well-being on social media This emerged in scrutinized experimental studies of Facebook data where researchers manipulated the mood of millions of Facebook users without consent In fact a recent survey study though not specific to the mental health domain found that few social media users were aware that their public content could be used by researchers and the majority felt that researchers should not be able to use tweets without consent Essentially passively collecting data transforms its initial purpose and we miss essential details of individuals experiences and symptomatology that may be gained from clinical relationships Is consent necessary in these contexts and if so what is meaningful positive or negative consent Vulnerable Populations and Risk Vulnerable populations such as prisoners expectant mothers and minors require additional protocol to protect participants in the US IRB system Even riskier research topics such illegal behaviors are protected with additional scrutiny For example the National Institutes of Health releases certificates of confidentiality that prevents research data from release to anyone including government authorities No restrictions exist for studies of public social media users no matter how vulnerable the population may be For example the median age of onset for eating disorders is between and Given that demographic attributes such as age are inferrable from social media language should we research online eating disorder communities knowing a large subset of these individuals are likely minors When should data scientists consider vulnerable populations and how should we protect this data Additionally ethics boards mandate that researchers take actions to protect against risks that a study may cause for mental health Many clinical studies include a risk management protocol where participants identified by the research team to be at an elevated mental health risk can be directed to appropriate forms of help and support resources Researchers can also intervene to stop participation in scientific research if the subject or research team believe the harms outweigh the benefits Even in studies without directed interventions the presence of researchers in communities could be triggering for individuals with mental disorders eg individuals dealing with schizophrenia and fear of mass surveillance may be upset by the knowledge that researchers are tracking their behaviors even if for beneficial outcomes Protocols for risk management and drop outs are missing or unimplemented in social media research on mental health There is no insight into what happens when users drop out of social media participation which is a close proxy to withdrawing consent Are they switching accounts exiting the platform entirely or is their mental health state dire Should we provide information to participants who may be in a dire mental health state Contextual Integrity of Communities Although online communities may post publicly to find support for anxiety to suicidality it is unclear whether social media users understand if their data can be surveilled as they discuss sensitive issues Behavior in these communities indicate that these groups may have no intention of being discovered by others and they may outright refuse participation in research When asked directly if users were comfortable with predicting depression with their Twitter profiles comfort with such research is decisively mixed Are we violating community norms with these observations We draw from the notion of contextual integrity proposed by Helen Nissenbaum in understanding privacy violations and a related follow-up by Zimmer about contextual gaps in big data research Zimmer argues that these gaps cause violations of normative bounds of how information flows within specific contexts Is is appropriate to observe online health communities for research if it violates this contextual integrity What about benign discussions on personal social media accounts As Bruckman recommends one way to resolve this contextual gap is by asking for permission through community leaders which is feasible for Reddit or public Facebook groups However most research is done on Twitter data where no formalized community structure exists and those that do like hashtags are amorphous Must we ask for consent in these scenarios to maintain contextual integrity and if so how would we do this Validity Interpretability and Methods The diversity of fields this research pulls from as well as the venues it publishes in brings many methods questions to the forefront of this work However there are documented inconsistencies and unanswered questions in this space ref section In this section we discuss ethical tensions arising from the validity and rigor or the lack thereof of new algorithms that infer mental health state Key Areas of Tension Construct Validity Data Bias Algorithmic Interpretability Performance Tradeoffs Data Protection and Anonymization Construct Validity The American Psychiatric Associations Diagnostic and Statistical Manual of Mental Disorders DSM is the best resource for identifying psychiatric symptoms and classifying mental disorders With over years of empirical support the DSM guides clinicians and researchers to make accurate psychiatric diagnoses using tested and validated constellations of symptoms and experiences obtained through clinical interviews Moreover clinically and psychometrically validated scales measure the presence and severity of mental disorders such as the Patient Health Questionnaire PHQ or the Generalized Anxiety Disorder scale GAD- It is unclear if mapping these scales to digital contexts validly reproduces results Further the complexities of patient-clinician interactions make rote application of DSM guidelines to online social media data unclear eg DSM guideline for diagnostic criteria of certain illness may be misinterpreted exaggerated or even lied about on social profiles As technology can sense psychiatric symptoms identify and potentially diagnose mental illness we must consider how best to incorporate these tools into clinical practice How do we map symptom assessment techniques to social media data in a way that preserves its validity Is it ethical to use mappings of traditional symptomatology or non-traditional ways to predict mental health Related to this is valid gold standard labels of mental health status or ground truth For prediction tasks in this space gathering ground truth data measure the target/predictor variable mental health status it is therefore a crucial part of the research process and impacts the quality of the algorithms that are built There are several standard approaches in the research on assessing ground truth of mental health status including self-disclosure of mental health state specific hashtag use and community participation Other styles directly recruit participants and administer screeners then collect social media data of these participants Most studies do not include clinical annotation however new approaches incorporate clinicians directly in labeling ground truth or validating the accuracy of other sources These approaches will vary depending on the research question and study design However there is no guidance on how to select the correct ground truth collection procedure or whether clinicians are necessary to this process Are we measuring the phenomenon we argue we are measuring Are certain kinds of measurement more appropriate for different scenarios To prevent misinterpretation of the inferences must we involve clinicians to assess ground truth states Data Bias Bias is a concern for any project for mental health status prediction bias is worrisome for the perceived validity and quality of research output We focus on population biases in datasets for an excellent analysis of bias see Olteanu et als survey Population bias refers to differences in characteristics between samples in a dataset and those of the target population we intend to measure The individuals in our datasets those with a certain mental health status on social media are a subset of the target population those with a certain mental health status By gathering data from social media we bias our data to those who use social media meaning it is likely a younger and more technologically literate sample than the population as a whole For mental health status this bias can manifest in unique ways leading to ethical lapses and challenges One well-grounded source of ground truth data is self-reported diagnosed mental health status eg I was diagnosed with schizophrenia This was pioneered by Coppersmith et al to unobtrusively identify those with mental disorders and has been validated and used in subsequent projects By sampling those who publicly self-disclose their mental health diagnoses this subsample has at least two biases First these individuals have likely been diagnosed with a mental disorder meaning they are likely to have sought professional treatment to receive those diagnoses Second they are comfortable enough to disclose their mental health status to others meaning that their forms of sharing could be different from others We acknowledge that bias is impossible to avoid in any sampled dataset however unaccounted bias can cause latent problems especially when inferences are incorporated in real life situations How should we sample and correct for bias How do we handle these biases in generalizing our results to new mental health statuses social networks or contexts Algorithmic Interpretability Next we discuss ethical challenges arising from a need for algorithmic interpretability and performance On one end of the spectrum are interpretable models as in many types of regression models like generalized linear or logistic regressions As input these models take intuitive features derived from social media behavior known symptomatology or innovations in sub-domains like character n-grams in NLP As output these models produce easy-to-understand metrics of model fit and coefficients and probabilities of salient predictors A strength of these models is that they are easily interpreted by clinicians and stakeholders who may not have technical expertise in algorithmic interpretation especially when matched to known symptomatology However interpretable models have been known to suffer from poor performance Regressions and similar algorithms are also limited by data modality as they do not handle image and video data without extensive preprocessing Sacrificing performance in the name of interpretability limits applications to applied research Simply discovering relationships between predictors and outcomes eg risk to a certain mental illness can be insightful to stakeholders like clinicians however it remains unclear how imprecise insights can be actionable during risky situations On the other hand deep learning techniques have emerged as state of the art for powerful and accurate models in prediction tasks Trained on millions of data points these algorithms can effortlessly outperform other models handle images and audio and can intuit features out of the data without human supervision Performance using deep learning techniques has seen noticeable improvements in predictive power in this space However deep learning has a key limitation they do not produce intelligible feature sets for human understanding These algorithms are black boxes producing impressive results but providing little insight into how the algorithm made its decision This can make relevant stakeholders in the process concerned about adopting these algorithms into practical scenarios Opaque models runs the risk of not only misconstrued and biased conclusions on sensitive data but also can lead to poor accountability to abide by ethical research principles as well as correcting algorithms when they fail to predict correct outcomes These models also challenge human interpretability of their outcomes How do we handle results that might not align with our clinically-grounded understanding of mental health These insights might propel research into new areas of signs of mental illness but they may also be red herrings providing false hope when in fact the algorithm has latched onto qualities of a particular training set Multiclass predictions complicate this when they discretize mental health in mutually exclusive binaries eg anxiety or not depression or not The clinical literature overwhelmingly points to mental disorders as frequently co-morbid and disorders can manifest over a continuous spectrum instead of clearly delineated outputs Existing algorithmic approaches are often not subtle enough to model this continuum or incorporate interactions between disorders and self-reported symptoms leading to artificial notions of risk PerformanceTradeoffs Risks of error in predicting mental health status should be addressed especially when these algorithms may be used in consumer-facing intervention systems False positives or incorrectly identifying the presence of a mental health status could cause dramatic consequences for individuals who are the subject of such errors Many mental disorders are stigmatizing and embarrassing and being labeled as disordered can damage someones self-esteem employment prospects and reputation as was the case of Samaritans Radar Depending on implementation false positives can also cause undue stress on individuals who may now believe something is wrong with them perhaps stifling their sharing on social platforms in the future When used in scenarios like content moderation or engagement with a clinician many false positives may overburden key stakeholders with too many requests to deploy assistance On the other hand a false negative means that mental health status was incorrectly labeled as not having a certain mental health status Pragmatically this means no intervention is triggered and no risks for interaction take place However in practical use of these systems false negatives mean that mental health status is missed and may go untreated as mentioned in prior work These risks become more concerning when dealing with grave mental health statuses such as suicidality and psychosis False negatives also raise responsibility and accountability questions for the results of these algorithms If being used in functional or practical scenarios which metric is more important to prioritize If these algorithms miss someone who is responsible for not intervening Does this reduce clinician accountability in these scenarios Data Sharing and Protection Even after careful data analysis come risks to privacy for participants We focus in this section on the risks of data sharing and publication of sensitive information for excellent overviews of privacy risks please see Zimmer and Proferes and Horvitz and Mulligan Scientists share datasets for reproducibility and consistent benchmarking of new algorithms However sharing datasets is complicated by mental health research goals These datasets are collected under specific circumstances and users may find issues with context changes Second datasets are rarely cleaned for deleted or removed data In the case of mental health discussions deleted or removed data could have particularly sensitive data or data that does not reflect the public perception a person wants to have How do we manage the joint goals of promoting scientific reproducibility while also protecting participants What does a benchmarking dataset look like for mental health Second is publication of sensitive information such as names locations and other personally identifying information When processing textual social media data algorithms can occasionally latch onto predictive textual cues this is amplified when sample size is small To combat this researchers have various levels of privacy preservation techniques such as removing usernames from data before analysis or de-identify algorithmic output later When should we curate our datasets pre or post-processing What are appropriate ways to de-identify data to preserve individual privacy while maintaining data integrity to promote good science A related risk comes from using exemplary social media postings/quotes in papers Recent work by Ayers et al found that of papers that use quotations in papers over of participants in datasets are able to be reidentified Other methods like interview studies have guidelines on modifying quotations in publications to protect participant identity and we ask similarly are quotes necessary for demonstrating the validity of the results of the paper If quotes are needed what protections can be used for privacy Implications for Stakeholders Using the perspectives of relevant stakeholders our final section deals with numerous implications in this research area We focus on the impacts to researchers in this space the individuals who are the target of predictions as well as social networks Key areas of tension Emotional Vulnerability Skillset Mismatches Role of the Clinician Designing Interventions Bad Actors and Fairness/Discrimination Emotional Vulnerability Researchers and practitioners especially those from  are not often taught how to manage complex emotions when engaging with mental health content Mental health content can contain graphic and disturbing content like pictures of self-harm or detailed discussions of suicide plans Those who engage with this content can be traumatized by these encounters and traditional approaches to research design do not take into account the researchers own emotional well-being For those who are rarely taught to handle sensitive or emotionally-laden information when annotating and interacting with data how do we train  and data scientists to handle the weight of this work Skillset Mismatches There are unique challenges in recognizing and rectifying skill gaps in interdisciplinary research collaborations Both sets of domain experts must actively work to communicate their research processes and decision-making guidelines this work As mentioned before ref Algorithmic Interpretability algorithmic output can be complex and inscrutable to outsiders  researchers are often experts in data collection feature engineering and model tuning and performance enhancement This information needs to be made interpretable to clinicians and other stakeholders with insights into the process Likewise  researchers may lack training in the skills that clinicians traditionally possess This may be in assessing valid signals of mental health acquiring ethics board approval and interpreting signal in datasets Some of these decisions may compromise the performance of models eg if a clinician suggests removing a highly predictive feature because it is not clinically relevant to predicting depression the research team will need to negotiate how to proceed For these partnerships to blossom both sets of researchers have to be mindful of making such interpretations accessible to build trust and reliability between collaborators Role of the Clinician Data collected passively/actively or continuously/intermittently may imply different responsibilities for clinicians involved in this research After entering into a physicianpatient relationship clinicians are bound by the duty to treat where they must provide treatment in accordance with their best judgment to their patients Failing to act on this knowledge would be unethical and potentially illegal For example a physician who discovers expressions of suicidal ideation by examining their patients social media may be bound to treat and therefore intervene However in this field data is both passive and actively gathered Information gathered and analyzed passively may not necessarily imply such a strong ethical responsibility for the duty to treat For example a clinician annotates an algorithmically-gathered dataset for intent to self-injure and discovers someone states that they plan to committ suicide at a specific date and time does a clinician have an obligation to intervene The obligation for intervention here may be weaker because there is no relationship developed between clinician and social media user However there also exists the duty to rescue where a bystander has an obligation to rescue another party in peril Unlike the duty to treat the duty to rescue has far more varied interpretations Does the duty to act or rescue vary depending on the type of professional on the project In many cases mental health professionals and computer scientists work in tandem  but what when they work separately Are computer scientists bound by the duty to rescue if they see someone that says they will harm themselves Another question is incorporate these new technologies effectively and ethically into clinical care How the data is collected monitored and presented to the clinical team will alter responsibilities and expectations for clinicians and researchers For example research in this space often suggests that insights from this data could be given to clinical care teams How do we design data interfaces that make sense of these algorithmic predictions for effective insights How do we not overburden clinicians with large amounts of data and direct their efforts Designing Interventions Another implication is the ability to design interventions one of the most mentioned applications of this technology in the literature With suitable performance the results of these algorithms could provide alerts to help identify moments of crisis assist in the early identification of mental illness or avoid risky episodes The potential for great societal benefit of these prediction algorithms is rooted in these interventions however design and implementation of interventions remain a key concern Outside of clinical interventions numerous stakeholders are cited as potentially invested in this work ranging from social networks crisis hotlines caregivers and individual friends to family members If we detect that a person might be suicidal should we alert experts or close family members The automated use of such technologies has been controversial when deployed for Samaritans Radar but has been better received when driven by human intervention systems on Facebook There is also risk in alerting individuals of their own mental health status a piece of information that is inferred algorithmically from passively shared social media data Are we doing more harm than good by making individuals who are not in a research study aware that they might be suffering from depression or anxiety thereby alerting them that we have gathered and analyzed their public data These concerns are also connected to issues of managing false positives and false negatives as an important performance tradeoff as discussed in Section Bad Actors and Fairness/Discrimination Another issue involves misuse of algorithmic inferences beyond the interests of the individuals themselves by other actors In one case the actor has benevolent intentions but misuse the data or violate the context of what data was gathered Samaritans Radar had good intentions of decreasing suicidality but was poorly received because it enabled other actors to harass or stalk those when they were at their most vulnerable This can also be seen in automatic screening and text processing systems like advertising recommendations which could scan Twitter posts for self-reported diagnoses of mental disorders and send advertisements for prescription drugs Is this a desirable outcome However researchers have also identified the risks of malintentioned actors using and reproducing the findings in these papers for unsavory purposes One example could be the use of this research by health insurance agencies to deny coverage for medical care or raise premiums if an individual is detected as having postpartum depression yet never sought treatment Other applications of these algorithms to other prediction systems like determining credit worthiness for loans or ability to maintain employment status are possible In some countries these predictions are illegal because mental health is a protected class however in other cases this information is not safeguarded or cleverly designed proxy variables can be engineered to get this information Can researchers in this space safeguard against bad actors or mitigate these risks A related result of these algorithms is discriminatory output it is possible that the algorithms have a strong sampling bias towards certain groups of people independent of their mental health status As mentioned above social media researchers may be sampling for younger and possibly more affluent audiences by sampling from certain social media data In their paper about postpartum depression De Choudhury et al note that they over-sample Caucasian affluent women for their data collection and interviews which makes generalizability of this algorithm to other demographics challenging If we extrapolate our algorithms to these groups how will we manage unintended biases that might lead to negative and discriminatory repercussions What impact does this sampling have on predicting on different groups of people such as those with lower socioeconomic status who do not use social media sites or older adults with lower rates of social media adoption Do these algorithms only help the proverbial rich get richer by predicting mental health status on groups already likely to seek treatment CALLS TO ACTION Research in this area will continue to grow with new algorithms data collection means and new implications for practical use of these algorithms Even if this taxonomy is not comprehensive we believe it provides an overview of where to begin to tackle problems and we are optimistic that the community can work together to solve these challenges How do we resolve these tensions in predicting mental health status from social media data Rather than prescribe a set of strict guidelines from our experiences we call the community to begin to work on these issues These challenges span both methodological areas in  as well as topical areas for ethics privacy clinical psychiatry and human-centered design In this section we propose three calls to action that could resolve these tensions and inconsistencies in formalized ways Participatory Algorithm Design Researchers should include key stakeholders in the research process including clinicians social networks and individuals who are the object of these predictions The academic community is already responding to these issues through cross-disciplinary seminars symposia and conferences offering collaborative atmospheres for people to work through these problems Examples of these venues include the recurrent Computational Linguistics and Clinical Psychology CLPsych workshop in NLP the recurrent Computing and Mental Health symposium at CHI MLHealth at NIPS in as well as FAT itself These meetings emphasize that interdisciplinary efforts in collegial environments can produce meaningful solutions In addition to such partnerships inside the field  practitioners should be eager to bring on clinicians and domain experts to this research Clinical experts provide valuable insights into construct validity validating and assessing ground truth correcting for biases managing risks and privacy tradeoffs and giving irreplaceable context to algorithmic output These collaborations are fruitful and have greatly benefit prior research Other stakeholders like ethicists designers and social media platform owners should be included as well as they both offer their own perspective and incorporate such algorithms into their systems Incorporating the knowledge of fields like psychology privacy and design we can careful craft algorithmic solutions to problems mitigate emergent issues of bias fairness and discrimination and execute thoughtful and novel intervention strategies Finally we advocate that the individuals who are the target of predictions should also be considered when developing these algorithms We especially advocate for participatory approaches of individuals through as focus groups interviews and design workshops to better understand their needs opinions and interest in this research As they are both the providers and the recipients of the algorithmic assessments of mental health status researchers have an obligation to involve them in these decision-making processes This work is beginning through interview studies and we push researchers to provide future work in this area Developing Best Practices for Methods In published work researchers should disclose study design and methods decisions to promote reproducibility and the field should agree on what best practices are The speed of advancement in this field is impressive the first papers in this area emerged only five years ago However as we note throughout this taxonomy there are divergent methodological criteria for study design methods data privacy reproducibility and ethics How can we understand what these standards are and arrive at consensus on appropriate methods and protections for research in this area To know how to resolve gaps and divergences the field must know where those gaps are One method to do so is reflective meta-analyses reviews and summative pieces that illuminate the field We envision such work to be illustrative of both the existing strengths of the fields and areas for improvement Systematic literature reviews and recommendations are beginning to be published in fact the taxonomy we present here was motivated in part because of this goal This includes knowledge of end-to-end research design decisions such as data collection and sampling strategies issues of consent and privacy management feature engineering and design and algorithmic interpretation We strongly believe that that more meta-work is necessary to document and precisely identify these inconsistencies and gaps Finally a benefit of these meta-works and resulting alignments of methodologies is that it enhances replicability of our work in the community However best practices from meta-reviews and analyses must be tempered by careful consideration of advancement in the field as well as respect for individuals as the primary contributors of data and beneficiaries of these systems Many papers already carefully document their recruitment and consent strategies privacy protections and details on methods and limitations In addition consortia such as PERVADE Pervasive Data Ethics for Computational Research and CORE Connected and Open Research Ethics offer guidance on how existing ethical codes should be adapted for computational research with sensitive data We encourage the community to use these as models for best practices in disclosure and transparency into algorithmic design and research Beyond Ethics Boards Consider and discuss the implications of this research outside of the normal considerations of ethics committees Incorporate ethics as a key value in the research process from the beginning The combinations of benign streams of public data into high-accuracy predictions of mental health status creates complex intersections of research outcomes and stakeholders Fundamentally this research is human-centered in that the predictions we make are on peoples data not on data as an abstracted notion We draw on the idea of ethics as a value in research production as science in this area has direct implications on people and on society and should be built into the research process We call researchers to consider the ethics throughout the research process rather than an afterthought when writing up publications When conducting work with direct ties to individuals we cannot ignore considering implications of this research even those that extend beyond the purview of ethics boards and oversight committees Rather than provide checklists for practitioners we encourage researchers conducting this research to consider and disclose the potentials for benefit and harm Numerous ethics researchers have cautioned transforming an ethical and sound approach to research into check lists In particular Carpenter and Dittrich argue that by relying on any one piece of ethical guidance be that an ethics board or a list of best practices we defer responsibility from considering the risks of a project onto those institutions We encourage practitioners to be transparent about implications of research in publications no matter the contribution a provocative position endorsed by ACMs Future of Computing Academy CONCLUSION Social media provides a unique perspective into individuals behaviors and moods In this paper we discussed emerging research in using social media data to predict an individuals mental health state We covered the state-of-the-art in the field and discussed three areas of ethical tension We offer calls to action to begin to solve these pressing issues in part because of our belief that this technology can be immensely beneficial in predicting and assessing mental health We hope that interdisciplinary researchers act on these ideas and begin to work on solving these pressing challenges in methods ethics privacy and consent