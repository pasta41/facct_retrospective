Problem Formulation and Fairness Formulating data science problems is an uncertain and difficult process It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems necessitating among other things the identification of appropriate target variables and proxies While these choices are rarely self-evident normative assessments of data science projects often take them for granted even though different translations can raise profoundly different ethical concerns Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model Building on six months of ethnographic fieldwork with a corporate data science team and channeling ideas from sociology and history of science critical data studies and early writing on knowledge discovery in databases we describe the complex set of actors and activities involved in problem formulation Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic and rarely worked out with explicit normative considerations in mind In so doing we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways and why specific formulations prevail in practice even in the face of what might seem like normatively preferable alternatives We conclude by discussing the implications of our findings arguing that effective normative interventions will require attending to the practical work of problem formulation CONCEPTS Information systems Data mining Computing methodologies Machine learning Human-centered computing Ethnographic studies KEYWORDS Data Science Machine Learning Problem Formulation Fairness Target Variable INTRODUCTION Undertaking a data science project involves a series of difficult translations As Provost and Fawcett point out business problems rarely are classification problems or regression problems or clustering problems They must be made into questions that data science can answer Practitioners are frequently charged with turning amorphous goals into well-specified problems that is problems faithful to the original business objectives but also problems that can be addressed by predicting the value of a variable Often the outcome or quality that practitioners want to predict the target variable is not one that has been well observed or measured in the past In such cases practitioners look to other variables that can act as suitable stand-ins proxies This process is challenging and far from linear As Hand argues establishing the mapping from the clients domain to a statistical question is one of the most difficult parts of statistical analysis and data scientists frequently devise ways of providing answers to problems that differ from those that seemed to motivate the analysis In most normative assessments of data science this work of translation drops out entirely treating the critical task as one of interrogating properties of the resulting model However ethical concerns can extend to the formulation of the problem that a model aims to address not merely to whether the model exhibits discriminatory effects To aid in hiring decisions for example machine learning needs to predict a specific outcome or quality of interest One might want to use machine learning to find good employees to hire but the meaning of good is not self-evident Machine learning requires specific and explicit definitions demanding that those definitions refer to something measurable While an employer might want to find personable applicants to join its The term data science as used in this paper refers to the practice of assembling organizing processing modeling and analyzing data using computational and statistical techniques In this paper we focus specifically on supervised machine learning but the analysis applies more broadly PROBLEM FORMULATION AND FAIRNESS sales staff such a quality can be difficult to specify or measure What counts as personable And how would employers measure it Given the challenge of answering these questions employers might favor a definition focused on sales figures which they may find easier to monitor In other words they might define a good employee as the person with the highest predicted sales figures In so doing the problem of hiring is formulated as one of predicting applicants sales figures not simply identifying good employees As Barocas and Selbst demonstrate choosing among competing target variables can affect whether a model used in hiring decisions ultimately exhibits a disparate impact There are three reasons why this might happen First the target variable might be correlated with protected characteristics In other words an employer might focus on a quality that is distributed unevenly across the population This alone would not constitute illegal discrimination as the quality upon which the employer hinges its hiring decisions could be rational and defensible But the employer could just as well choose a target variable that is a purposeful proxy for race gender or other protected characteristics This would amount to a form of disparate treatment but one that might be difficult to establish if the decision rests on a seemingly neutral target variable The employer could also choose a target variable that seems to serve its rational business interests but happens to generate an avoidable disparate impact for instance the employer could choose a different target variable that serves its business objective at least as well as the original choice while also reducing the disparate impact Second the chosen target variable might be measured less accurately for certain groups For example arrests are often used as a proxy for crime in applications of machine learning to policing and criminal justice even though arrests are a racially biased representation of the true incidence of crime In treating arrests as a reliable proxy for crime the model learns to replicate the biased labels in its predictions This is a particularly pernicious problem because the labeled examples in the training data serve as ground truth for the model Specifically the model will learn to assign labels to cases similar to those that received the label in the training data whether or not the labels in the training data are accurate Worse evaluations of the model will likely rely on test data that were labeled using the same process resulting in misleading reports about the models real-world performance these metrics would reflect the models ability to predict the label not the true outcome Indeed when the training and test data have been mislabeled in the same way there is simply no way to know when the model is making mistakes Choosing a target variable is therefore often a choice between outcomes of interest that are labeled more or less accurately When these outcomes are systematically mismeasured by race gender or some other protected characteristic a model designed to predict them will invariably exhibit a discriminatory bias that does not show up in performance metrics Finally different target variables might be more difficult to predict than others depending on the available training data and features If the ability to predict the target variable varies by population then the model might subject certain groups to greater errors than others Across all three cases we find that whether a model ultimately violates a specific notion of fairness is often contingent on what the model is designed to predict Which suggests that we should be paying far greater attention to the choice of the target variable both because it can be a source of unfairness and a mechanism to avoid unfairness The non-obvious origins of obvious problems This might not be surprising because some problem formulations may strike us as obviously unfair Consider the case of financial-aid leveraging in college admissions the process by which universities calculate the best possible return for financial aid packages the brightest students for the least amount of financial aid To achieve this bargain the university must predict how much each student is willing to pay to attend the university and how much of a discount would sway an applicant from competitors In economic terms financial-aid leveraging calculates each applicants responsiveness to price which enables the university to make tailored offers that maximize the likely impact of financial aid on individual enrollment decisions As Quirk explains Take a scholarship the full tuition for a needy student at some schools Break it into four scholarships each for wealthier students who would probably go elsewhere without the discount but will pay the outstanding tuition if they can be lured to your school Over four years the school will reap an extra which can be used to buy more rich students or gifted students who will improve the schools profile and thus its desirability and revenue Such strategies are in effect in schools throughout the United States and the impact has been an increase in support for wealthier applicants at the expense of their equally qualified but poorer peers One might therefore conclude as Danielson does that data mining technology increasingly structures recruiting to many US colleges and universities and that the technology poses a threat to such important values as equality and meritocracy Alternatively one could find like Cook in a similar thought experiment that the results would have been different if the goal were to find the most diverse student population that achieved a certain graduation rate after five years In this case the process was flawed fundamentally and ethically from the beginning For Cook agency and ethics are front-loaded a poorly formed question returns undesirable if correct answers Data science might be the enabling device but the ethical issue precedes the analysis and implementation The objective was suspect from the start For Danielson however certain ethics seem to flow from data mining itself Data science is not merely the enabling device but the impetus for posing certain questions Its introduction affords new and perhaps objectionable ways of devising admissions strategies Though they are quite different these positions are not necessarily incompatible data science might invite certain kinds of questions and financial-aid leveraging could be one such example One might say that data science promotes the formulation of questions that would be better left unasked PROBLEM FORMULATION AND FAIRNESS But this is a strangely unhelpful synthesis while according agency to the person or people who might formulate the problem it simultaneously imparts overwhelming influence to the affordances of data science The effort of getting the question to work as a data science problem drops out entirely even though this process is where the question actually and ultimately takes shape The issues of genuine concern how universities arrive at a workable notion of student quality how they decide on optimizing for competing variables student quality financial burden diversity etc how the results are put to work in one of many possible ways are left largely out of view The indeterminacy of the process where many of the ethical issues are actually resolved disappears Problem formulation in practice While a focus on the work of problem formulation in real-world applied settings has the potential to make visible the plethora of actors and activities involved in data science work it has not been the focus of much empirical inquiry to date We still know very little about the everyday practice of problem formulation In this paper we attempt to fill this gap How and why are specific questions posed What challenges arise and how are they resolved in everyday practice How do actors choices and decisions shape data science problem formulations Answers to these questions we argue can help us to better understand data science as a practice but also the origin of the qualities of a data science project that raise normative concerns As researchers work to unpack the normative values at stake in the uses of data science we offer an ethnographic account of a special financing project for auto lending to make visible the work of problem formulation in applied contexts In so doing we show how to trace the ethical implications of these systems back to the everyday challenges and routine negotiations of data science In the following sections we first situate the paper in a longer history attending to the practical dimensions of data science specifically the task of problem formulation We then describe our research site and methodology before moving to the empirical case-study We conclude by discussing the implications of our findings positioning the practical work of problem formulation as an important site for normative investigation and intervention BACKGROUND Our understanding of the role of problem formulation in data science work draws from a long line of research within the history and sociology of science that describes how scientific methods are not just tools for answering questions but in fact influence the kind of questions we ask and the ways in which we define and measure phenomena Through different methods scientists mathematize the world in specific ways producing representations that are both contingent ie they change with a change in methods and real ie they provide actionable ways to analyze the world Our practical understanding of a given phenomenon is contingent on the data we choose to represent and measure it with The emerging field of critical data studies has brought similar insights to data science data scientists do not just apply algorithms to data but work with algorithms and data iteratively and often painstakingly-aligning the two together in meaningful ways Data science work Passi and Jackson argue is not merely a collection of formal and mechanical rules but a situated and discretionary process requiring data analysts to continuously straddle the competing demands of formal abstraction and empirical contingency Algorithmic results embody specific forms of data vision-rule-based as opposed to rule-bound applications of algorithms necessitating judgment-driven work to apply and improvise around established methods and tools in the wake of empirical diversity Data science requires thoughtful measurement careful research design and creative deployment of statistical techniques to identify units of measurement clean and process data construct working models and interpret quantified results Subjective decision making is necessary throughout the process Each of these practical choices can have profound ethical implications of which data scientists are sometimes well aware Their everyday work is shot through with careful thinking and critical reflection Neff et al through ethnographic work on academic data science research show how data scientists often acknowledge their interpretive contributions and use data to surface and negotiate social values Data the authors argue are the starting and not the end points in data science In academic and research settings the contexts that inform most of our current understanding of data science the work of data science comes across mainly as the work of data scientists Data science projects in applied corporate settings however are inherently collaborative endeavors a world as much of discretion collaboration and aspiration as of data numbers and models In such projects several actors work together to not only make sense of data and algorithmic results but also to negotiate and resolve practical problems Passi and Jackson through an ethnography of a corporate data science team describe how specific issues with data intuition metrics and models pose challenges for corporate data science work and how organizational actors collaborate through specific strategies to manage these problems in the service of imperfect but ultimately pragmatic and workable forms of analysis As the authors conclude project managers product designers and business analysts are as much a part of applied real-world corporate data science as are data scientists These strands of research call attention to the role of the work of problem formulation within data science The relationship between formulated problems and the data we choose to address them is not a one-way street data are not merely things to answer questions with Instead the very formulations of data-driven problems ie the kind of questions we can and do ask are determined by contingent aspects such as what data are available what data we consider relevant to a phenomenon and what method we choose to PROBLEM FORMULATION AND FAIRNESS process them Problem formulation is as much an outcome of our data and methods as of our goals and objectives Indeed defining the data science problem is not only about making the data science process fit specific and specifiable objectives but also making the objectives fit the data science process Data miners have long grappled with the role of human judgment and discretion in their practice The field of Knowledge Discovery in Databases KDD an important predecessor to what we now call data science emerged to address how choices throughout the data mining process could be formalized in so-called process models Knowledge Discovery in Databases The iterative process of applied data mining While KDD is commonly associated with data mining and machine learning the history of the field has less to do with innovations around these techniques than with the process that surrounds their use Dissatisfied with a lack of applied research in artificial intelligence scholars and practitioners founded the new sub-field to draw together experts in computer science statistics and data management who were interested and proficient in the practical applications of machine learning see The significance of this move owed to a shift in the fields professional focus not to a change in the substance of its computational techniques When KDD established itself as an independent field it also instituted a method for applying machine learning to real-world problems the KDD process consisting of a set of computational techniques and specific procedures through which questions are transformed into tractable data mining problems Although the terms KDD and data mining are now used interchangeably if they are used at all the original difference between the two is telling While data mining referred exclusively to the application of machine learning algorithms KDD referred to the overall process of reworking questions into data-driven problems collecting and preparing relevant data subjecting data to analysis and interpreting and implementing results The canon of KDD devoted extensive attention not only to the range of problems that lend themselves to machine learning but also to the multi-step process by which these problems can be made into practicable instances of machine learning In their seminal paper Fayyad Piatetsky-Shapiro and Smyth for example insist on the obvious applicability of data mining while paradoxically attempting to explain and advocate how to apply it in practice that is how to make it applicable KDD covered more than just a set of computational techniques it amounted to a method for innovating and executing new applications The focus on process led to the development of a series of process models formal attempts to explicate how one progresses through a data mining project breaking the process into discrete steps The Cross Industry Standard Process for Data Mining CRISP-DM the most widely adopted model seems to simultaneously describe and prescribe the relevant steps in a projects lifecycle Such an Semi-annual workshops started in Annual conferences began in approach grows directly out of the earliest KDD writing Fayyad Piatetsky-Shapiro and Smyth make a point of saying that data mining is a legitimate activity as long as one understands how to do it suggesting that there is a particular way to go about mining data to ensure appropriate results Indeed the main impetus for developing process models were fears of mistakes missteps and misapplications rather than a simple desire to explicate what it is that data miners do As Kurgan and Musilek explain the push to formally structure data mining as a process results from an observation of problems associated with a blind application of data mining methods to input data Notably CRISP-DM like the earlier models that preceded it in the academic literature emphasized the iterative nature of the process and the need to move back and forth between steps The attention to feedback loops and the overall dynamism of the process were made especially evident in the widely reproduced visual rendering of the process that adopted a circular form to stress cyclicality Negotiated not faithful translations Business understanding the first step in the CRISP-DM model is perhaps the most crucial in a data mining project because it involves the translation of an amorphous problem a high-level objective or a business goal into a question amenable to data mining CRISP-DM describes this step as the process of understanding the project objectives and requirements from a business perspective and then converting this knowledge into a data mining problem definition This process of conversion however is underspecified in the extreme Translating complex objectives into a data mining problem is not self-evident a large portion of the application effort can go into properly formulating the problem asking the right question rather than into optimizing the algorithmic details of a particular data-mining method Indeed the openendedness that characterizes such forms of translation work is often described as the art of data mining Recourse to such terms reveals the degree to which the creativity of the translation process resists its own translation into still more specific parts and processes ie it is artistic only insofar as it resists formalization But it also highlights the importance of this initial task in determining the very possibility of mining data for some purpose CRISP-DM and other practical guidance for data miners or data scientists see tend to describe problem formulation mainly as part of a projects first phase an initial occasion for frank conversations between the managers who set strategic business goals the technologists that manage an organizations data and the analysts that ultimately work on data Predictably those involved in data science work face the difficult challenge of faithful translation finding the correct mapping between say corporate goals organizational data and computational problems Practitioners themselves have long recognized that even when project members reach consensus in formulating the problem it is a negotiated translation contingent on the discretionary judgments of PROBLEM FORMULATION AND FAIRNESS various actors and further impacted by the choice of methods instruments and data These insights speak to the conditions that motivate data science projects in a way that escapes the kind of technological determinism or data imperative that pervades the current discourses as if the kinds of questions that data science can answer are always already evident Getting the automation of machine learning to return the desired results paradoxically involves an enormous amount of manual work and subjective judgment The work of problem formulation of iteratively translating between strategic goals and tractable problems is anything but self-evident implicated with several practical and organizational aspects of data science work As Hand points out textbook descriptions of data mining tools and articles extolling the potential gains to be achieved by applying data mining techniques gloss over these difficulties In the following two sections we look at the work of data science that is traditionally glossed over We first describe our research site and methods before moving on to the empirical case-study through which we show how the initial problem formulation comprises a series of elastic translations a set of placeholder articulations that is susceptible to change as the project moves through its many iterations RESEARCH SITE AND METHODS This paper builds on six months of ethnographic fieldwork with DataVector a multi-billion-dollar US-based e-commerce and new media organization Established in the s DataVector owns several companies in domains such as health and automotive Many of these are multi-million-dollar companies with several thousand clients each DataVector has a core data science team based on the west coast of the United States that works with companies across different domains There are multiple teams of data engineers software developers and business analysts both at DataVector and its subsidiaries One of us worked as a data scientist with the organizations core data science team between June and November serving as the lead scientist on two corporate data science projects not reported in this paper and participating in many others During ethnographic research the data science team had eight to eleven members including one of the authors The team is headed by CliffDataVectors Director of Data Science with years of industry experience in major technology firms Cliff and the team report directly to BillDataVectors Chief Technology Officer with years of experience in the technology industry During the six-month period one of us conducted interviews with data scientists product managers business analysts project managers and company executives and produced pages of fieldwork notes and photographs Interviews and fieldwork data were transcribed and coded according to the principles of grounded-theory analysis inductively analyzing data through several Organization personnel and project names in this paper have been replaced with pseudonyms to preserve participant anonymity rounds of qualitative analysis In our analysis we coded the data in two rounds focusing on the identification of key categories themes and topics as well as the relation between them in the data While we focus on a specific corporate data science project in this paper we observed similar dynamics across several other projects We chose this case because the work of problem formulation was particularly salient in this project CASE-STUDY SPECIAL FINANCING CarCorp a DataVector subsidiary collects special financing data information on people who need car financing but have either low/bad credit scores between  or limited credit histories The companys clientele mainly consists of auto dealers who pay to receive this data called lead data that include information such as name address mortgage and employment details sometimes even the make of the desired automobile The company collects lead data primarily online people who need special financing submit their data so that interested dealers can contact them People requiring special financing face several challenges ranging from the lack of knowledge about available credit sources to difficulties in negotiating interest rates As liaisons between borrowers and lenders companies such as CarCorp and its affiliates act as important sometimes necessary intermediaries for people requiring special financing CarCorp serves several dealers across the country Few dealers collect their own lead data as the money effort and technical skills required to do so is enormous This is a key reason why dealers pay companies such as CarCorp to buy lead data CarCorps technology development and project manager Brian wanted to leverage data science to improve the quality of leads Improving lead quality Brian argued will ensure that existing dealers do not churn ie they continue to give their business to CarCorp Brian project manager The main goal is to improve the quality of our leads for our customers We want to give actionable leads That is what helps us make money makes customers continue to use our services Interview November Initial discussions between the business and data science teams revolved around two themes a defining lead quality and b finding ways to measure it Defining lead quality was not straightforward There were many stakeholders with different opinions about leads ibid Some described lead quality as a function of a leads salary data while some argued that a lead was good if the dealer had the leads desired car in their inventory Everyone on the business team however agreed on one thing as CarCorps business analyst Ron put it a good lead provided business to the dealer Ron business analyst The business team has been talking about lead quality for a long time We have narrowed down the lead quality problem to how likely is someone to purchase or to be able to finance a car when The exact number is omitted to preserve company anonymity PROBLEM FORMULATION AND FAIRNESS you send them to that dealer Interview November Lead quality was equated with lead financeability It was however difficult to ascertain financeability Different dealers had different special financing approval processes A lead financeable for one dealer can be for various reasons unfinanceable for another The goal thus was to determine dealer-specific financeability ie predicting which dealer was most likely to finance a lead The teams settled on the following definition of quality a good lead for a dealer was a lead financeable for that dealer This in turn framed the problem as one of matching leads to dealers that were most likely to finance them CarCorp had a large amount of historical lead data In alone the company had processed close to two million leads CarCorp however had relatively less data on which leads had been approved for special financing let alone data on why a lead was approved The business team asked the data science team to contact data engineers to identify and assess the relevant data sources The data science team after investigating the data sources however declared that there wasnt enough data on dealer decisions without adequate data they argued it was impossible to match leads with dealers Few dealers in CarCorps network shared their approval data with the company The scarcity of this data stemmed from the challenge of creating business incentives for dealers to provide up-to-date feedback The incentives for dealers to share information about their approval process with CarCorp were too attenuated While the data science team instructed the business team to invest in the collection of up-to-date data on dealer decisions further discussions ensued between the two teams to find alternate ways to predict dealer-specific financeability using the data that happened to be available already In debates over the utility of the available data business analysts and data scientists however voiced several concerns ranging from inconsistency eg discrepancies in data values to unreliability eg distrust of data sources Business analyst Ron for instance was wary of the multiple ways in which the data was collected and generated Ron business analyst The entire complex lead ecosystem to outsiders does not make any sense Some data came from an affiliate They give a credit score range for a bunch of those leads So not exactly the score but they could say this person is between  and  different buckets Never realistic but we pay money and we have this data We also generate leads organically online then there are leads that we buy from third-parties Different pieces of information are appended to those leads depending on where it came from Interview November Only a few CarCorp affiliates augmented lead data with additional information such as credit scores Dealers run background checks on leads with their consent as part of the financing procedure and in the process get a leads exact credit Third-party lending agencies offer services to help put borrowers in touch with multiple lenders These agencies get consent from people to perform a soft-pull on their official credit reports from credit bureaus such as Equifax score from credit bureaus such as Equifax The Fair Credit Reporting Act FCRA prohibits CarCorp from getting a leads exact credit score from credit bureaus without a leads explicit consent Leads have no reason to authorize CarCorp to retrieve their credit data because the company does not make lending decisions it only collects information about a leads interest in special financing CarCorp had to rely on either leads self-reported credit scores that were collected by a few affiliates or on credit scores provided as part of lead data bought from third-party lending agencies Business affiliates and third-party agencies provide credit scores in the form of an approximate range eg  CarCorp had hoped that this data would help them to for example differentiate between a subset of leads that appeared identical but exhibited different financeability Ron business analyst Two individuals with the same age same income same housing payment same everything could have wildly different credit scores You have those two and you send them to the same dealer From our perspective lead A and B are maybe not exactly the same but close But the dealer can finance person A and they cannot finance person B So when they dealers are evaluating whether they would renew their product with us if we had sent them a bunch from bucket B and none from A they are likely to churn But we have no way of knowing who is in bucket A and who is in bucket B You can have two individuals who measure the same on data points and they have two different credit scores Interview November It is not surprising that a lead with a credit score greater than another lead is more likely to secure special financing even when the leads are otherwise identical Credit score data is a significant factor in special financing approval processes CarCorp had this data for about of their leads While business analysts found it challenging to predict credit score ranges from the other features using traditional statistical analyses adding credit score ranges in as an additional feature did improve the companys ability to predict lead financeability The business team wondered if it was possible to use data science to predict credit score ranges for the remaining of the leads ie perhaps machine learning could work where traditional analysis had failed If successful credit scores could help ascertain lead financeability a financeable lead was a lead with a high credit score The data science teams attempt to assess if they could predict credit scores however faced a practical challenge As mentioned above credit score data received from affiliates took the form of ranges eg  not discrete numbers Different affiliates marked ranges differently For example one affiliate may categorize ranges as   etc while another as   etc It was not possible to directly use the ranges as class labels for training as the ranges overlapped The data science team first needed to reconcile different ranges As data scientist Alex started working to make the ranges consistent business analysts came up with a way to make this PROBLEM FORMULATION AND FAIRNESS process easier Pre-existing market analysis and to some extent word-of-mouth business wisdom indicated that having a credit score higher than greatly increased a leads likelihood of obtaining special financing approval This piece of information had a significant impact on the project With as the crucial threshold only two credit score ranges were now significant below- and above- Alex did not need to figure out ways to reconcile ranges such as  and but could bundle them in the below- category The above- credit score range could act as the measure of financeability a financeable lead was a lead with a credit score above- The matching problem which leads are likely to get financed by a dealer was now a classification task which leads have a credit score of over Decreasing the number of classes to two helped attenuate the difficulty of reconciling different ranges but did not help to circumvent it Alex data scientist If the credit score is below the dealer will simply kill the deal The problem is there are too many records in the  range This makes it difficult Fieldwork Notes June Leads in the  range were an issue because this range contained not only below- leads but also above- leads Making mistakes close to the decision boundary is especially consequential for special financing where you want to find people just above the threshold Alex tried many ways to segregate the leads in this range but the models according to him didnt work Their accuracy at best was slightly better than a coin flip Alex attributed the models bad performance not only to the presence of leads in the  range but also to the limited number of available features ie the data did not present sufficient information for the model to meaningfully differentiate between leads While the in-house lead dataset was a good starting point the data scientists knew that accurately classifying leads would require not only creative ways to work with available data but also a wide variety of data They had already been scouting for external datasets to augment the in-house lead dataset Director of data science Cliff had tasked data science project manager Marcus with the work of finding third-party datasets that could help with classification Their approach was first to use freely available data to see how far we get before buying paid data from companies such as Experian Free yet reliable datasets however were hard to come by Marcus found only a few datasets Internal Revenue Service IRS zip-code-level data on features such as income range tax returns and house affordability ie how much an owner might be able to pay for a property Data scientist Alex tried each dataset but declared that none improved model performance Members of the data science team wondered if it was worth investing in high-quality paid datasets Alex I used all the data but the model does not converge Marcus What about Experian data We can get it if you think it will help For example datasets such as Experians Premier Aggregated Credit Statistics We will have to buy it first for me to know if it helps with prediction Cliff director of data science Check out the online info on it and then you and Marcus can figure it out Fieldwork Notes June Without access to the data Alex argued it was not possible to clearly know its usefulness If the data was not going to be helpful however it made no sense to buy it the decision needed to be made based on the available description on Experians website They eventually ended up not investing in it Even after analyzing the datasets description Alex was not convinced that the data would increase model performance Two months later the project was halted in the absence of actionable progress Different actors justified the projects seeming failure in different ways Data scientist Alex felt that the data was the culprit Business analyst Ron felt that perhaps the business team unreasonably expected magic from data science For him the culprit was the nature of the problem itself Ron business analyst It is a selection-bias problem We are not dealing with a random sample of the population These individuals why are they submitting a lead with us Because it was not easy for them to get financed By definition our population is people with at least not great credit and usually bad credit Why do you have bad credit There is like one reason why you have good credit There are a thousand reasons why you have bad credit If you show me someone with good credit I will show you they pay bills on time have a steady income etc If you show me someone with bad credit they can have a gambling problem they can be divorced they could prove income now but maybe it has been unstable in the past and we have no way of knowing that There are literally thousands of reasons that arent that capture-able Interview November Business analyst Ron described the failure not in terms of the initially articulated business goal but in terms of the projects current data science problem formulation not the difficulty of defining the quality of a lead but the challenge of classifying leads with scores in a specific part of the credit score spectrum He believed it was possible to classify people with high/low credit scores on the full  credit score spectrum eg differentiating between a person with a score and a person with a score He argued however that CarCorps focus on the special financing population meant that the goal was not to classify high/low scores on the full credit spectrum but to demarcate between different kinds of low scores on one side of the spectrum roughly between and Note how Ron a business analyst describes the projects failure in relation to a specific definition of lead quality it was difficult to know which leads were above or below the credit score threshold of The project was halted when developing an accurate model based on this definition proved impossible The business and data science team could not figure out any other way to formulate the problem at this stage with the data they had statistics html Even such data only contained aggregated information on credit scores and ranges and not lead-specific credit scores which required consent PROBLEM FORMULATION AND FAIRNESS DISCUSSION Through the above description we see how the data science problem was formulated differently at different points in the project based on two different sets of targets variables and their possible proxies Proxy Dealer Decisions The business team initially described the project goal as the improvement of lead quality a formulation of what the business team thought the dealers wanted Note that this goal was in turn related to the broader objective of minimizing churn rate a formulation of what CarCorp itself wanted In this way the problem specification was just as much about keeping clients as it was about satisfying their business objectives These high-level goals impacted the actors initial understanding of the projects goal the quality of leads was seen in relation to dealers and CarCorps own success CarCorp decided that if dealers could finance a lead it was a good lead The fact that different dealers had different special financing approval processes further impacted the contingent relationship between quality dealers and financeability if a lead was financeable by a specific dealer it was a good lead for that dealer The data science problem therefore became the task of matching leads with dealers that were likely to finance them Proxy Credit Score Ranges Data available to support the use of dealer decisions as a proxy however were limited While business analysts did not fully understand how dealers made decisions they acknowledged based on market research the import of credit scores in the special financing approval process leads with scores higher than were highly likely to get special financing Credit scores thus became a proxy for a dealers decision which was itself a proxy for a leads financeability which was by extension a proxy for a leads quality indeed a chain of proxies The data science problem thus became the task of classifying leads into below- and above- classes Problem formulation is a negotiated translation At face value the relationship between the projects high-level business goal improving lead quality and its two different problem formulations the two sets of target variables and their proxies may seem like a one-to-many relation different translations of in effect the same goal Such an understanding however fails to account not only for the amorphous nature of high-level goals ie the difficulty of defining the quality of a lead but also for the projects iterative and evolving nature ie problem formulations are negotiated dependent on for instance actors choice of proxy In our case actors equated in order lead quality with financeability financeability with dealer decisions and dealer decisions with credit score ranges Each of these maneuvers produced different formulations of the objective in turn impacting actors articulation and understanding of the projects high-level goal as seen for instance in the way business analyst Ron ultimately accounts for the projects failure Of course depending on the threshold some leads will never be sent This is not to argue that the high-level goal to improve lead quality at some point transformed into a completely different objective Instead it shows that the translation between high-level goals and tractable data science problems is not a given but a negotiated outcome stable yet elastic Throughout the project the goal of improving lead quality remains recognizably similar but practically different evident in the different descriptions target variables and proxies for lead quality Each set of target variables and proxies represents a specific understanding of what a leads quality is and what it means to improve on it The quality of a lead is not a preexisting variable waiting to be measured but an artifact of how our actors define and measure it The values at stake in problem formulation Scholars concerned with bias in computer systems have long stressed the need to consider the original objectives or goals that motivate a project apart from any form of bias that may creep into the system during its development and implementation On this account the apparent problem to which data science is a solution determines whether it happens to serve morally defensible ends Information systems can be no less biased than the objectives they serve These goals however rarely emerge ready-formed or precisely specified Instead navigating the vagaries of the data science process requires reconceiving the problem at hand and making it one that data and algorithms can help solve In our empirical case we do not observe a data science project working in the service of an established goal about which there might be some normative debate Instead we find that the normative implications of the project evolve alongside changes in the different problem formulations of lead quality On the one hand for the proxy of dealer-decisions leads are categorized by their dealer-specific financeability a lead is only sent to the dealer that is likely to finance them In formulating the problem as a matching task the company is essentially catering to dealer preferences This approach will recommend leads to dealers that align with the preferences expressed in dealers previous decisions In this case lead financeability operates on a spectrum Financeability emerges as a more/less attribute each lead is financeable some more than others depending on the dealer Effectively each lead has at least a chance of being sent to a dealer ie the dealer with the highest probability of financing a lead above some threshold On the other hand for the credit-score proxy leads are categorized into two classes based on their credit score ranges and only leads with scores greater than are considered financeable In formulating the problem as the task of classifying leads above or below a score the company reifies credit scores as the sole marker for financeability Even if dealers had in the past financed leads with credit scores less than this approach only recommends leads with scores higher than shaping dealers future financing practices In this approach financeability operates as a binary variable a lead is financeable only if its credit score is higher than PROBLEM FORMULATION AND FAIRNESS Consequently leads in the below- category may never see the light of day discounted entirely because the company believes that these leads are not suitable for dealers Different principles different normative concerns Seen this way the matching version of the problem formulation may appear normatively preferable to the classification version But is this always true If we prioritize maximizing a persons lending opportunities the matching formulation of the problem may seem better because it increases a leads chances of securing special financing If however we prioritize the goal of mitigating existing biases in lending practices ie of alleviating existing dealer biases the classification problem formulation may come across as the better alternative because it potentially encourages dealers to consider leads different from those they have financed in the past Through the two scenarios we see how proxies are not merely ways to equate goals with data but serve to frame the problem in subtly different ways and raise different ethical concerns as a result It is far from obvious which of the two concerns is more serious and thus which choice is normatively preferable shifting our normative lens alters our perception of fairness concerning the choice of target variables and proxies In this paper we have demonstrated how approaching the work of problem formulation as an important site for investigation enables us to have a much more careful discussion about our own normative commitments This in turn provides insights into how we can ensure that projects align with those commitments Always imperfect always partial Translating strategic goals into tractable problems is a labored and challenging process Such translations do necessary violence to the world that they attempt to model but also provide actionable and novel ways to address complex problems Our intention to make visible the elasticity and multiplicity of such translations was thus not to criticize actors inability to find the perfectly faithful translation Quite the opposite we recognize that translations are always imperfect and partial and wanted to instead shift the attention to the consequences of different translations and the everyday judgments that drive them Our actors however did not explicitly debate the ethical implications of their own systems neither in the way we as researchers have come to recognize normative issues nor in the way we as authors have analyzed the implications of their problem formulations in this paper Practical and organizational aspects such as business requirements the choice of proxies the nature of the algorithmic task and the availability of data impact problem formulations in much more significant and actionable ways than for instance the practitioners normative commitments and beliefs Indeed our analysis of the empirical case makes visible how aspects such as analytic uncertainty and financial cost impact problem formulations For example the high cost of datasets coupled with the challenge of assessing the datas efficacy without using it made it particularly challenging for actors to leverage additional sources of information Yet as we show in this paper normative implications of data science systems do in fact find their roots in problem formulation work the discretionary judgments and practical work involved in translations between high-level goals and tractable problems Each translation galvanizes a different set of actors aspirations and practices and in doing so creates opportunities and challenges for normative intervention upstream sites for downstream change As Barocas et al argue A robust understanding of the ethical use of data-driven systems needs substantial focus on the possible threats to civil rights that may result from the formulation of the problem Such threats are insidious because problem formulation is iterative Many decisions are made early and quickly before there is any notion that the effort will lead to a successful system and only rarely are prior problem formulation decisions revisited with a critical eye If we wish to take seriously the work of unpacking the normative implications of data science systems and of intervening in their development to ensure greater fairness we need to find ways to identify address and accommodate the iterative and less visible work of formulating data science problems how and why problems are formulated in specific ways CONCLUSION In this paper we focused on the uncertain process by which certain questions come to be posed in real-world applied data science projects We have shown that some of the most important normative implications of data science systems find their roots in the work of problem formulation The attempt to make certain goals amenable to data science will always involve subtle transformations of those objectives along the way transformations that may have profound consequences for the very conception of the problem to which data science has been brought to bear and what consequently appear to be the most appropriate ways of handling those problems Thus the problems we solve with data science are never insulated from the larger process of getting data science to return actionable results As we have shown these ends are very much an artifact of a contingent process of arriving at a successful formulation of the problem and they cannot be easily decoupled from the process at arriving at these ends In linking the normative concerns that data science has provoked to more nuanced accounts of the on-the-ground process of undertaking a data science project we have suggested new objects for investigation and intervention which goals are posed and why how goals are made into tractable questions and working problems and how and why certain problem formulations succeed