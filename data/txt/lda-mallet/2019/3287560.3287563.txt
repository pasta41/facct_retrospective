Disparate Interactions An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments Despite vigorous debates about the technical characteristics of risk assessments being deployed in the US criminal justice system remarkably little research has studied how these tools affect actual decision-making processes After all risk assessments do not make definitive decisions they inform judges who are the arbiters It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them This paper takes a step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk We found several behaviors that call into question the supposed efficacy and fairness of risk assessments our study participants underperformed the risk assessment even when presented with its predictions could not effectively evaluate the accuracy of their own or the risk assessments predictions and exhibited behaviors fraught with disparate interactions whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants These results suggest the need for a new algorithm-in-the-loop framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract If risk assessments are to be used at all they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential CONCEPTS Human-centered computing Human computer interaction HCI Applied computing Law KEYWORDS fairness risk assessment behavioral experiment Mechanical Turk INTRODUCTION Across the United States courts are increasingly using risk assessments to estimate the likelihood that criminal defendants will engage in unlawful behavior in the future These tools are being deployed during several stages of criminal justice adjudication including at bail hearings to predict the risk that the defendant if released will be rearrested before trial or not appear for trial and at sentencing to predict the risk that the defendant will recidivate Because risk assessments rely on data and a standardized process many proponents believe that they can mitigate judicial biases and make objective decisions about defendants Risk assessments have therefore gained widespread support as a tool to reduce incarceration rates and spur criminal justice reform Yet many are concerned that risk assessments make biased decisions due to the historical discrimination embedded in training data For example the widely-used COMPAS risk assessment tool wrongly labels black defendants as future criminals at twice the rate it does for white defendants Prompted by these concerns machine learning researchers have developed a rapidly-growing body of technical work focused on topics such as characterizing the incompatibility of different fairness metrics and developing new algorithms to reduce bias Despite these efforts current research into fair machine learning fails to capture an essential aspect of how risk assessments impact the criminal justice system their influence on judges After all risk assessments do not make definitive decisions about pretrial release and sentencing they merely aid judges who must decide whom to release before trial and how to sentence defendants after trial In other words algorithmic outputs act as decision-making aids rather than arbiters Thus whether a risk assessment itself is accurate and fair is of only indirect concern the primary considerations are how it affects decision-making processes and whether it makes judges more accurate and fair No matter how well we characterize the technical specifications of risk assessments we will not fully understand their impacts unless we also study how judges interpret and use them This study sheds new light on how risk assessments influence human decisions in the context of criminal justice adjudication We ran experiments using Amazon Mechanical Turk to study how people make predictions about risk both with and without the aid of a risk assessment We focus on pretrial release which in many respects resembles a typical prediction problem By studying Although there have been several generations of criminal justice risk assessments over the past century throughout this paper we use risk assessments to refer to machine learning algorithms that provide statistical predictions After someone is arrested courts must decide whether to release that person until their trial This is typically done by setting an amount of bail or money that the defendant must pay as collateral for release The broad goal of this process is to protect individual liberty while also ensuring that the defendant appears in court for trial behavior in this controlled environment we discerned important patterns in how risk assessments influence human judgments of risk Although these experiments involved laypeople rather than judges limiting the extent to which our results can be assumed to directly implicate real-world risk assessments they highlight several types of interactions that should be studied further before risk assessments can be responsibly deployed in the courtroom Our results suggest several ways in which the interactions between people and risk assessments can generate errors and biases in the administration of criminal justice thus calling into question the supposed efficacy and fairness of risk assessments First even when presented with the risk assessments predictions participants made decisions that were less accurate than the advice provided Second people could not effectively evaluate the accuracy of their own or the risk assessments predictions participants confidence in their performance was negatively associated with their actual performance and their judgments of the risk assessments accuracy and fairness had no association with the risk assessments actual accuracy and fairness Finally participant interactions with the risk assessment introduced two new forms of bias which we collectively term disparate interactions into decision-making when evaluating black defendants participants were more strongly influenced to increase their risk prediction at the suggestion of the risk assessment and were more likely to deviate from the risk assessment toward higher levels of risk Further research is necessary to ascertain whether judges exhibit similar behaviors The chain from algorithm to person to decision has become vitally important as algorithms inform increasing numbers of highstakes decisions To improve our understanding of these contexts we introduce an algorithm-in-the-loop framework that places algorithms in a sociotechnical context thus focusing attention on human-algorithm interactions to improve human decisions rather than focusing on the algorithm to improve its decisions Rigorous studies of algorithm-in-the-loop systems are necessary to inform the design and implementation of algorithmic decision-making aids being deployed in the criminal justice system and beyond Despite some indications that risk assessments impact judges decisions little is known about the specific ways in which they influence judges The most extensive study of this topic evaluated the changes prompted by Kentucky mandating in that risk assessments be used to inform all pretrial release decisions Although the risk assessment recommended immediate non-financial release for of defendants in practice the release rate increased only marginally to before declining back toward the original release rate The analysis found that the risk assessments had no effect on racial disparities Two sets of related work provide further hints regarding how judges might use or otherwise respond to the predictions made by risk assessments and does not commit any crimes while released whether the defendant is guilty of the offense that led to the arrest is not a factor at this stage In order to make pretrial release decisions judges must determine the likelihood or the risk that the defendant if released will fail to appear in court or will be arrested People are bad at incorporating quantitative predictions The phenomenon of automation bias suggests that automated tools influence human decisions in significant and often detrimental ways Two types of errors are particularly common omission errors in which people do not recognize when automated systems err and commission errors in which people follow automated systems without considering contradictory information Heavy reliance on automated systems can alter peoples relationship to a task by creating a moral buffer between their decisions and the impacts of those decisions Thus although automated decision support tools are designed to improve decision effectiveness and reduce human error they can cause operators to relinquish a sense of responsibility and subsequently accountability because of a perception that the automation is in charge Even when algorithms are more accurate people do not appropriately incorporate algorithmic recommendations to improve their decisions instead preferring to rely on their own or other peoples judgment One study found that people could not distinguish between reliable and unreliable predictions and another found that people often deviate incorrectly from algorithmic forecasts Compounding this bias is the phenomenon of algorithm aversion through which people are less tolerant of errors made by algorithms than errors made by other people Information through existing biases Previous research suggests that information presumed to help people make fairer decisions can fail to do so because it through peoples preexisting biases For example ban-the-box policies which are intended to promote racial equity in hiring by preventing employers from asking job applicants whether they have a criminal record actually increase racial discrimination by allowing employers to rely on stereotypes and thereby overestimate how many black applicants have criminal records Similarly peoples interpretations of police-worn body camera footage are significantly influenced by their prior attitudes about police Studies have shown that judges harbor implicit biases and that racial disparities in incarceration rates are due in part to differential judicial decisions across race In Florida for example white judges give harsher sentences to black defendants than white ones who have committed the same crime and received the same score from the formula the state uses to set criminal punishments STUDY DESIGN We conducted this study in two stages developing a risk assessment for a population of criminal defendants and second running experiments on Mechanical Turk to determine how people incorporate these assessments into their own predictions Before running our experiments we made three hypotheses Hypothesis Performance Participants presented with a risk assessment will make predictions that are less accurate than the risk assessments This study was reviewed and approved by the Harvard University Area Institutional Review Board and the National Archive of Criminal Justice Data Hypothesis Evaluation Participants will be unable to accurately evaluate their own and the algorithms performance Hypothesis Bias As they interact with the risk assessment participants will be disproportionately likely to increase risk predictions about black defendants and to decrease risk predictions about white defendants Defendant population and risk assessment Stage of the study involved developing a risk assessment for criminal defendants being considered for pretrial release to predict the likelihood that if released they would be arrested before trial or fail to appear in court for trial The goal of this stage was not to develop an optimal pretrial risk assessment but to develop a risk assessment that resembles those used in practice and that could be presented to participants during the experiments in Stage We used a dataset collected by the US Department of Justice that contains court processing information about felony defendants who were arrested between and in of the most populous counties in the US We restricted our analysis to defendants whose race was recorded as either black or white and who were released before trial thus providing us with ground truth data about outcomes for each defendant This yielded a dataset of defendants Table A We pooled together failing to appear and being rearrested denying any incidence of one or both of these outcomes as violating the terms of pretrial release of released defendants committed a violation After splitting the data into train and test sets we trained a model ie the risk assessment using gradient boosted trees The model was based on ve features about each defendant age offense type previous failures to appear and number of prior arrests and convictions We excluded race and gender from the model to follow common practice among risk assessment developers Because our experiment participants would be predicting risk in increments of see Section we rounded each risk assessment prediction to the nearest The model achieves an area under the curve AUC of on the test set indicating comparable accuracy to COMPAS the Public Safety Assessment and other risk assessments We also evaluated the risk assessment model for fairness and found that it is well-calibrated Figure A We focused on calibration not as an ideal metric for fairness recognizing that no perfect metric for fairness can exist but because it is the most commonly-used approach for evaluating risk assessments in practice Based on these attributes our risk assessment resembles those used within US courts We selected from the test set an experimental sample of defendants whose proles would be presented to both the control and treatment groups during the experiments Table A The full details of how we developed the risk assessment and selected the sample population are available in the Appendix Experimental setup In Stage of the study we conducted behavioral experiments on Amazon Mechanical Turk to determine how people use and are influenced by machine learning algorithms when making predictions about pretrial release Each trial consisted of a consent page Figure An example of the prompt presented to participants in the treatment group Participants in the control group saw the same prompt but without the sentence about the risk score algorithm a tutorial with a description of the task and background information about pretrial release an intro survey Figure A a series of predictions described below and an exit survey Figure A Both the intro and exit surveys included a simple question designed to ensure participants were paying attention We also included a comprehension test with several multiple choice questions at the end of the tutorial participants were not allowed to participate in the experiment until they correctly answered all of these questions We restricted the task to Mechanical Turk workers who had an historical acceptance rate of and were inside the US Each worker was allowed to participate in the experiment only once The prediction task required participants to assess the likelihood that criminal defendants who have been arrested will commit a crime or fail to appear in court if they are released before trial on a scale from to in intervals of Each participant was presented with narrative proles about a random sample of defendants drawn from the -person experiment sample population These proles included the ve features that the risk assessment incorporated as well as the race and gender of each defendant we included these latter two features in the proles because judges are exposed to these attributes in practice While making predictions participants could reference the tutorial to look up background information about pretrial release and the definitions of key terms When participants entered the experiment they were randomly sorted into a control or treatment group participants in the control group were shown the demographic information for each defendant while participants in the treatment group were shown the risk assessments prediction in addition to demographic information Figure We presented the same set of defendants to both the control and treatment groups allowing us to directly measure the impact on predictions of showing a risk assessment Participants were paid a base sum of for completing the survey with the opportunity to gain an additional reward of up to based on their performance during the experiment We allocated rewards according to a Brier score function mapping the Brier reward bounded see Section for each prediction to a payment using the formula payment reward since the test population is restricted to defendants who were released before trial we have ground truth data with which to evaluate each prediction Because the Brier score is a proper score function participants were incentivized to report their true estimates of crime risk We explicitly articulated this to participants during the tutorial and included a question about the reward structure in the comprehension test to ensure that they understood RESULTS We conducted trials on Mechanical Turk over the course of a week in June in batches over weekdays and weekend days at times ranging from morning to evening to account for variations in the population of Turk workers workers completed the experiment we excluded all data from participants who failed at least one of the attention check questions or who required more than three attempts to pass the comprehension test This process yielded a population of participants Table A The participants were male and white and the majority have completed at least a college degree We asked participants to selfreport their familiarity with machine learning and the US criminal justice system on a scale from Not at all to Extremely During the exit surveys participants reported that the experiment paid well was clear and was enjoyable Participants earned an average bonus of median making the average total payment Participants completed the task in an average of minutes median and earned an average wage of per hour median Out of participants who responded to a free text question in the exit survey asking for any further comments mentioned that the experiment length and payment were fair Participants were also asked in the exit survey to rate how clear and enjoyable the experiment was on a scale from to The average rating for clarity was of participants rated the experiment clarity a and the average rating for enjoyment was rated the experiment enjoyment a or The participants cumulatively made predictions about defendants providing us with predictions about each defendants risk under each of the two experimental conditions Analysis We evaluated the accuracy and calibration of each prediction using the Brier reward reward prediction outcome where prediction and outcome thus reward When presented with a defendant who does not violate pretrial release for example a prediction of risk would yield a reward of a prediction of would yield a reward of and a prediction of would yield a reward of We also measured false positive rates using a threshold of Because we presented the same set of defendants to both the control and treatment groups we could measure the influence of the risk scores on the predictions about each defendant by comparing the predictions made by the control and treatment groups For each defendant we the risk scores influence Ij tj c r c where tj and c are the average predictions made about that defendant by participants in the treatment and control groups respectively and r is the prediction made by the risk assessment An I means that on average the treatment group makes identical predictions to the control group completely discounting the risk score while an I means that the treatment group makes identical predictions to the risk score This measure of influence is similar to the weight of advice metric that has been used to measure how much people alter their decisions when presented with advice Comparing the distributions of predictions made by the control and treatment groups indicates that the risk assessment influences the full distribution of predictions made by the treatment group not just the average Figure A To obtain reliable measurements when evaluating algorithm influence we excluded all predictions about the defendants for whom r c We used a variant of Equation to measure the influence of the risk assessment on each participant in the treatment group For every prediction made by a participant we measured the risk assessments influence by taking that prediction in place of the average treatment group prediction We then averaged these influences across the predictions that the participant made That is the influence of the risk assessment on participant is i ri refers to participant prediction about the defendant out of presented Our primary dimension of analysis was to compare behavior and performance across the race of defendants which has been at the crux of debates about fairness in criminal justice risk assessments Similar audits should be conducted across other intersecting forms of identity such as gender and class Hypothesis Performance Participants in the treatment group earned a larger average reward and a lower false positive rate than participants in the control group Table A two-sided t-test and test confirm that these differences are statistically significant both with p A regression of each participants performance on their treatment and personal characteristics found that being in the treatment group was associated with a higher average reward p The only personal attribute that had a significant relationship with average reward was gender women performed slightly better than men with p Yet although presenting the risk assessment improved the performance of participants the treatment group significantly underperformed the the risk assessment Table Despite being presented with the risk assessments predictions the treatment group achieved a lower average reward and a higher false positive rate than the risk assessment both with p Only of participants in the treatment group earned a higher average reward than the risk assessment over the course of their trial compared to who earned a lower reward than the risk assessment Figure A We broke these results down by race to compare how participants and the risk assessment performed when making predictions about black and white defendants As Figure indicates a similar Although I will mostly fall between and it is possible for I to fall outside these bounds if participants move in the opposite direction than the risk assessment suggests or adjust beyond the risk assessment Control Treatment Risk assessment N N N Average reward False positive rate Table The two columns show the performance of participants within the control and treatment groups and the third column shows the performance of the risk assessment N is the total number of predictions made Two-sided t-tests and tests confirm that the average rewards and the false positive rates respectively of all three prediction approaches are statistically distinct from one another all with p Control Treatment Risk assessment Control Treatment Risk assessment Average reward Fa e po si e ra te Race a a Black White Figure Performance of the control group treatment group and risk assessment broken down by defendant race In both cases the treatment group outperforms the control group but underperforms the risk assessment pattern was true for both races the treatment group outperformed the control group but underperformed the risk assessment Taking the control group performance as a lower bound and the risk assessment performance as an upper bound the treatment group achieved a similar relative improvement in its predictions about both races for average reward of possible improvement for black defendants and for white defendants for false positive rate of possible improvement for black defendants and for white defendants neither difference across race is statistically significant The actual performance level differs significantly across race however All three prediction approaches ie the control group the treatment group and the risk assessment achieve a larger reward and lower false positive rate for white defendants than for black defendants all with p Most notably the treatment group attains a higher average reward for white than black defendants and its false positive rate for black defendants is more than double its false positive rate for white defendants Hypothesis Evaluation To assess whether participants could evaluate the quality of their predictions we compared their self-reported confidence from the exit survey to their actual performance as measured by their average Brier reward during the task The average participant confidence was on a scale from to with the reward decreasing as reported confidence increases Figure A We regressed confidence on performance controlling for each participants treatment demographic information and exit survey responses and found that average reward was negatively associated with confidence p In other words the more confidence participants expressed in their predictions the less well they actually performed This pattern holds across both the control and treatment groups We next analyzed whether participants in the treatment group could evaluate the risk assessments accuracy as measured by its average Brier reward on the defendants presented to the participant these average rewards ranged from to We regressed the participants evaluations of the risk assessments accuracy against the risk assessments actual performance while controlling for each participants performance demographic information and exit survey responses The participants evaluation of the risk assessments accuracy did not have any significant relationship with the risk assessments performance during the task suggesting that participants were unable to perceive any differences in risk assessment accuracy over the samples they observed Figure A We also considered whether participants could discern how fairly the risk assessment made predictions As a rough measure of algorithmic fairness during each trial we measured the difference between the risk assessments false positive rates for black and white defendants on the defendants presented to the participant in order to focus on the most salient aspect of bias we restricted this analysis to the of participants for whom the risk assessment had a greater or equal false positive rate for black than white defendants Regressing participant evaluations of the risk assessments fairness on the risk assessments false positive rate differences controlling for each participants performance demographic information and exit survey responses along with the risk assessments performance found no significant relationship between perceived and actual fairness Figure A Finally we evaluated whether participants in the treatment group could recognize how heavily they incorporated the risk assessment into their decisions Regressing the participants selfreports of influence on the extent to which they were actually influenced by the risk assessment using the risk score influence measure introduced in Equation and controlling for each participants performance demographic information and exit survey responses along with the risk assessments performance indicates that participants could generally discern how strongly they were influenced by the risk assessment p Figure A Hypothesis Bias We interrogated Hypothesis through two complementary approaches by taking the control groups predictions as the baseline participant predictions to measure the risk assessments influence on the treatment group and second by taking the risk assessments predictions as the starting point to measure how much and in which direction the treatment group participants deviated from those predictions Although we could not precisely discern how participants made decisions the responses to an optional free response question in the exit survey about how participants used the risk scores Question in Figure A suggest that people predominantly followed a mix of these two approaches Out of the participants who described their strategy used the risk assessment as a baseline made their own judgment and then incorporated the risk assessment followed the risk assessment completely and ignored the risk assessment entirely Table A The group that followed the risk assessment earned the largest average reward while the group that ignored the risk assessment earned the lowest The other two groups both earned average rewards of and were statistically indistinguishable Analyzing behavior through the lens of the two most common strategies yields complementary evidence for disparate interactions ie interactions with the risk assessment that lead participants to disproportionately make higher risk predictions about black defendants and lower risk predictions about white defendants Influence of risk scores Because we presented the same population of defendants to the control and treatment groups we could directly measure how presenting the risk score to participants affected the predictions made about each defendant For each defendant we measured the influence of the risk assessment on the treatment groups predictions as described in Equation excluding the defendants for whom r c The risk assessment exhibited an average influence of as this number is greater than it suggests that treatment group participants placed more weight on the risk assessment than on their own judgment A twosided t-test found no statistically significant difference between the risk assessments influence when its prediction was less or greater than the control groups prediction r c or r c respectively Splitting the defendants by race tells a more complex story Figure When the risk score was lower than the control groups average prediction r c the risk assessment exerted a similar influence on participants regardless of the defendants race vs p Yet when the risk assessment predicted a higher risk than the control group r c it exerted a stronger average influence on predictions about black defendants than on predictions about white defendants vs a two-sided t-test p and of the difference in means This outcome cannot be explained by differences in the raw disparities between the risk assessments and the control groups predictions ie the value of r c since the values of r c do not differ significantly across defendant race the average disparity for both races is when r c and when r c Breaking out Figure based on the value of r c indicates that the risk assessment exerts an equal influence on predictions about both races at all values of r c except for when r c Figure A Thus the risk assessment leads to larger increases in risk for black defendants as measured by the shift in participant predictions precipitated by the risk assessment is identical when r c the risk assessment generates an average reduction of for both black and white defendants when r c the average increase for black defendants is while the average increase for white R A in flu en ce Race Black White Figure The influence of the risk assessment RA on participant predictions broken down by whether the risk score is less or greater than the control groups average prediction r c and r c respectively and compared across the race of defendants While the risk assessments influence is nearly identical across race when r c when r c the risk assessment exerts a stronger influence on participants who are evaluating black defendants p defendants is Although these results are not significant a two-sided t-test p and difference in means considering each prediction from the treatment group independently rather than taking averages for each defendant ie replacing tj with in Equation yields further evidence for this result the average increase for black defendants is compared to for white defendants a larger average increase with p and difference in means Moreover among defendants for whom r c the increase in participant risk prediction instigated by the risk assessment is larger for black defendants p Figure A We ran linear regressions to see what determines the risk assessments influence on participants We split defendants into two categories those for whom r c Group and those for whom r c Group For each group we regressed the algorithms influence on predictions about each defendant Equation on that defendants demographic attributes and criminal background along with the value of r c For Group the risk assessment exerted more influence as r c increased but less influence for defendants with a previous failure to appear on their records For Group the risk assessment similarly was more influential as r c increased Three other attributes were also statistically significant the risk assessment exerted more influence on participants making predictions about black defendants defendants who were arrested for a violent crime and defendants with more prior convictions Thus when r c participants were more strongly influenced to increase their risk predictions for black defendants in two ways they responded both directly to race and to a feature that is correlated with race prior convictions Table A Percent of predictions White Black Increase risk risk Figure The rate at which participants deviated from the risk assessments prediction toward higher and lower levels of risk broken down by defendant race When evaluating black defendants participants were more likely to deviate positively from the risk assessment and less likely to deviate negatively participant predictions matched the risk assessment at an equal rate for both races Participant deviations from risk scores For each prediction made by participants in the treatment group we measured how far and in which direction that prediction deviated from the risk assessments recommendation That is we measured p r The average deviation among the treatment group predictions was with a median deviation of Participants deviated to a higher risk prediction of the time matched the risk assessment of the time and deviated to a lower risk prediction of the time The results from Section suggest that these deviations tend to make participant predictions less accurate than the risk assessment As in the previous section these statistics differ by defendant race While the average deviation for white defendants was the average deviation for black defendants was p difference in means This difference emerged because participants were more likely to deviate positively from the risk assessment when evaluating black defendants and to deviate negatively when evaluating white defendants the average deviation magnitude was the same across race for both positive and negative deviations As Figure depicts participants deviated to a higher risk prediction of the time for black defendants compared to of the time for white defendants more and conversely deviated to a lower risk prediction of the time for black defendants compared to of the time for white defendants less Participants matched the risk assessment in of predictions when evaluating both races We regressed each deviation on the characteristics of the defendant and the participant the prediction made by the risk assessment and the participants status in the experiment ie which in the sequence of predictions the participant was making Since these deviations include repeated samples for each defendant and participant we used a linear mixed-effects model with random effects for the defendant and participant identities Several characteristics of defendants had statistically significant associations with the deviations participants were more likely to deviate positively from the risk assessment when evaluating younger defendants defendants arrested for a violent crime defendants with more prior arrests and convictions and defendants with a prior failure to appear Neither the defendants race nor any attributes of participants had a statistically significant relationship with deviations These results suggest that while participants did not deviate from the risk assessment based explicitly on race they deviated based on attributes that are unevenly distributed across race compared to white defendants black defendants on average have more prior arrests convictions and failures to appear Table A DISCUSSION This study presents initial evidence regarding how risk assessments influence human decision-makers confirming our three hypotheses our results indicate that people underperform risk assessments even when provided with its advice are unable to evaluate the performance of themselves or the risk assessment and engage in disparate interactions whereby their use of risk assessments leads to higher risk predictions about black defendants and lower risk predictions about white defendants This work demonstrates how theoretical evaluations are necessary but insufficient to evaluate the impacts of risk assessments what appears to be a fair source of information can depending on how people interact with it become a leverage point around which discrimination manifests It is necessary to place risk assessments into a sociotechnical context so that their full impacts can be identified and evaluated Our results highlight a significant but often overlooked aspect of algorithmic decision-making aids introducing risk assessments to the criminal justice system does not eliminate discretion to create objective judgments as many have argued Instead risk assessments merely shift discretion to different places which include the judges interpretation of the assessment and decision about how strongly to rely on it This reality must become a central consideration of any proposals for and evaluations of risk assessments especially given that previous attempts to standardize the criminal justice system sentencing reform efforts in the s shifted discretion to prosecutors generating a racially-biased rise in excessive punishment A particular danger of judicial discretion about how to incorporate risk assessments into decisions is the potential for disparate interactions biases that emerge as an algorithmic prediction through a person into a decision Our experiment participants were more strongly influenced by the risk assessment to increase their risk prediction when evaluating black defendants than white ones leading to a larger average increase for black than white defendants due the risk assessment Moreover participants were more likely to deviate positively from the risk assessment and less likely to deviate negatively from the risk assessment when evaluating black defendants Although it is possible that participants predicted higher risk for black defendants to account for the racial bias in arrests we do not believe this was an important factor since no participants mentioned any such thought process in the exit survey when describing their behavior These disparate interactions emerged through both direct and indirect bias while race had a direct role in increasing the risk scores influence on participants the disparities in influence and deviations also arose due to participants responding to particularly salient features that are unevenly distributed by race such as number of prior convictions essentially double-counting features for which the risk assessment had already accounted This behavior resembles that of machine learning algorithms which can be racially biased even when race is not included as an explicit factor and highlights the importance of studying the complex mechanisms through which discrimination can manifest Future work should explore how different ways of presenting and explaining risk assessments and of training people to use them could improve performance and in particular reduce disparate interactions An important research direction that could guide such efforts is to study the processes through which people make decisions when provided with risk assessments Our participants followed several approaches when evaluating defendants the most common being using the risk assessment to influence their initial judgment and using the risk assessment as a baseline Table A Analyzing participant behavior from both of these perspectives indicated related forms of disparate interactions Meanwhile the most successful strategy was to directly follow the risk assessment While in theory it is possible for people to synthesize the risk assessment with their own judgment to make better decisions than either could alone in practice we found no evidence that any strategy taken by participants leads them to outperform the risk assessment A major limitation to peoples use of risk assessments is their inability to evaluate their own and the risk assessments performance Many proponents defend the deployment of risk assessments on the grounds that judges have the say and can discern when to rely on the predictions provided But our results indicate that this is an unrealistic expectation our participants judgments about their own performance were negatively associated with their actual performance and their evaluations of the risk assessment had no statistically significant relationship with its actual performance other research has similarly shown that people struggle to detect algorithmic mistakes across a variety of conditions Given these results it is no wonder that participants in the treatment group underperformed the risk assessment How can we expect people to navigate the balance between their own judgment and a risk assessments when they are unable to accurately assess their own or the algorithms performance in the place Determining how to incorporate a risk assessment into ones own prediction is arguably a more challenging task that requires more expertise than merely making a prediction The results of this study raise one of the most important but rarely-discussed issues at the heart of debates about risk assessments how should risk assessments be incorporated into existing practices On the one hand risk assessments alone achieve better performance than individuals both with and without a risk assessments aid in terms of accuracy and false positive rates Yet there This result assumes a comparison between a single individual and a risk assessment This is in contrast to a recent study suggesting that humans are just as accurate as COMPAS that result holds only when the predictions of humans are aggregated to create a wisdom of the crowd effect in fact that study similarly found COMPAS to be more accurate than individuals are many reasons to be wary of relying too heavily on risk assessments including due process concerns their embedding of discriminatory and punitive approaches to justice and their potential to hinder more systemic criminal justice reforms Meanwhile the current approach of presenting predictions to judges without sufficient guidelines or training comes with the issues of poor interpretation and disparate interactions The between these positions are apparent in how the Wisconsin Supreme Court severely circumscribed the role of risk assessments in its decision in State v Loomis regarding the use of COMPAS in sentencing Despite defending the use of COMPAS on the grounds that it has the potential to provide sentencing courts with more complete information the Court also mandated that risk scores may not be used to determine whether an offender is incarcerated or to determine the severity of the sentence If COMPAS is not supposed to influence the sentence there are few purposes that the more complete information it provides can serve and few ways to ensure that it serves only those purposes In that case why show it at all An Algorithm-in-the-Loop Framework As computational systems permeate everyday life and inform critical decisions it is of paramount importance to study how algorithmic predictions impact human decision-making across a broad range of contexts Risk assessments are just one of an emerging group of algorithms that are intended to inform people making decisions other examples include predictions to help companies hire job applicants and to help doctors diagnose patients Yet despite robust research into the technical properties of these algorithms we have a limited understanding of their sociotechnical properties most notably whether and how they actually improve decision-making To answer these questions it is necessary to study algorithms following the notion of technologies as social practice which is grounded in the understanding that technologies are constituted through and inseparable from the specifically situated practices of their use A natural body of work from which to draw inspiration in studying human-algorithm collaborations is human-in-the-loop HITL systems In settings such as social computing and active learning computational systems rely on human labor such as labeling photos and correcting errors to overcome limitations and improve their performance But where HITL processes privilege models and algorithms utilizing people where necessary to improve computational performance settings like pretrial release operate in reverse using algorithms to improve human decisions This distinction suggests the need for an alternative framework algorithm-in-the-loop AITL systems Instead of improving computation by using humans to handle algorithmic blind spots such as analyzing unstructured data AITL systems improve human decisions by using computation to handle cognitive blind spots such as patterns in large complex datasets This framework centers human-algorithm interactions as the locus of study and Although previous studies have used the phrase algorithm-in-the-loop they have it in the context of simulation and modeling rather than in relation to human-in-the-loop computations and human-algorithm interactions prioritizes the humans decision over the algorithms as the most important outcome An algorithm-in-the-loop perspective can inform essential sociotechnical research into algorithms Recent work related to interpretability provides one important direction where progress is already being made Future analysis should focus on how to develop and present algorithms so that people can most effectively and fairly incorporate them into their deliberative processes with particular attention to improving evaluations of algorithm quality and reducing disparate interactions This may involve altering the algorithm in unintuitive ways previous research suggests that in certain situations a seemingly suboptimal algorithm actually leads to better outcomes when provided to people as advice It will also be important to study the efficacy of different mechanisms for combining human and algorithmic judgment across a variety of contexts Most algorithm-in-the-loop settings involve simply presenting an algorithmic output to a human decision-maker relying on the person to interpret and incorporate that information Yet research within human-computer interaction and crowdsourcing suggests that alternative approaches could lead to a better synthesis of human and computer intelligence Which mechanisms are most effective and desirable from an ethical and procedural standpoint will likely vary depending on the situation Finally given that automation can induce a moral buffer it is necessary to study how using algorithms affects peoples sense of responsibility for their decisions Given the all-too-common expressions from engineers that they do not bear responsibility for the social impacts of their technologies the potential for automation bias raises the unsettling specter of situations in which both the engineers developing algorithms and the people using them believe the other to be primarily responsible for the social outcomes It is of vital importance to study whether algorithms create a moral buffer and to nd ways to avoid such scenarios Limitations Given that our experiments were conducted on a population of Mechanical Turk workers rather than actual judges in the courtroom it is necessary to circumscribe the interpretation of these results Judges have more expertise than laypeople at predicting pretrial risk and are generally given more information about the risk assessments in use Interestingly however judges have been shown to release many high-risk defendants and detain many low-risk ones Judges may also be more reluctant to rely on risk assessments believing that their own judgment is superior previous research has shown that people with more expertise are less willing to take advice and a recent survey found that less than of judges believed that an actuarial assessment could outperform their own predictions of risk Our study also fails to capture the level of racial priming that could influence judges use of risk assessments While our experiment tells participants that a defendant is black or white a judge would also see the defendants name and physical appearance Studies have shown that employers discriminate based on racially indicative names and that judges are harsher toward defendants with darker skin and more Afrocentric features Thus it is possible that the disparate interactions we observe in our experiments could be heightened in the courtroom where race is more salient Future research should study how people respond to risk assessments as racial priming increases The short length of each trial predictions over approximately minutes means that we could not capture how the relationships between people and risk assessments evolve over extended periods of time This is an important factor to consider when deploying algorithmic systems especially given research demonstrating that the changes instigated by risk assessments are short-lived The immediate impacts of introducing algorithms into decision-making processes may not indicate the long-term implications of doing so This is particularly true within the criminal justice system where political incentives and manipulation can distort the use of risk assessments over time Thus while this study hints at issues that may arise in the courtroom it remains an open question how closely our results resemble the outcomes of real-world implementation Further studies must be done in both experimental and natural settings before risk assessments can be seriously considered for broader deployment in the criminal justice system if they are to be used at all