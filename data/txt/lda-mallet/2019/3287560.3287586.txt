Classification with Fairness Constraints A Meta-Algorithm with Provable Guarantees Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts Several recent works have focused on studying classification with respect to specific fairness metrics modeled the corresponding fair classification problem as constrained optimization problems and developed tailored algorithms to solve them Despite this there still remain important metrics for which there are no fair classifiers with theoretical guarantees primarily because the resulting optimization problem is non-convex The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple nondisjoint and multi-valued sensitive attributes and which comes with provable guarantees In particular our algorithm can handle non-convex linear fractional constraints which includes fairness constraints such as predictive parity for which no prior algorithm was known Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family Empirically we observe that our algorithm is fast can achieve near-perfect fairness with respect to various fairness metrics and the loss in accuracy due to the imposed fairness constraints is often small CONCEPTS Computing methodologies Supervised learning by classification KEYWORDS Classification Algorithmic Fairness INTRODUCTION Classification algorithms are increasingly being used in many societal contexts such as criminal recidivism predictive policing and job screening There are growing concerns that these algorithms may introduce significant bias with respect to certain sensitive attributes eg against African-Americans while predicting future criminals granting loans or NYPD stop-and-frisk and against women while recommending jobs The US Executive Office also voiced concerns about discrimination in automated decision making including health care delivery and education Further introducing bias may be illegal due to antidiscrimination laws and can create social imbalance Thus developing classification algorithms that are fair with respect to sensitive attributes has become an important problem In classification one is given a data vector and the goal is to decide whether it satisfies a certain property The algorithm is allowed to learn from a set of labeled data vectors that may be assumed to come from an unknown distribution The accuracy of a classifier is measured as the probability that the classifier correctly predicts the label of a data vector drawn from the same distribution Each data vector however may also have a small number of multi-valued sensitive attributes such as race gender and political opinion and each setting of a sensitive attribute gives rise to potentially non-disjoint groups of data points Since fairness could mean different things in different contexts a number of different metrics have been used to determine how fair a classifier is with respect to a sensitive group when compared to another eg statistical parity equalized odds and predictive parity In fact there are currently at least well-accepted fairness metrics and counting see Several recent works use the sensitive attributes and the desired notion of group fairness to place constraints on the classifier formulating it as a constrained optimization problem that maximizes accuracy and develop tailored algorithms to find such classifiers eg constrained to statistical parity or equalized odds However these algorithms do not always come with provable guarantee because often the resulting optimization problem turns out to be non-convex eg for statistical parity and equalized odds Further it is open whether such approaches would work for other important measures of disparate mistreatment such as predictive parity Predictive parity that measures whether the fractions over the class distribution for the predicted labels are close between different group that are important in predicting criminal recidivism stopping-and-frisking pedestrians and predicting heart condition left as an open problem to find algorithms to solve the fair classification problem with false discovery or false omission parity two types of predictive parity Our contributions We present a new classification algorithm that takes as input any one of a large class of fairness metrics which can be phrased as linear-fractional constraints and produces an approximately fair solution Technically we achieve this by identifying a family of classification problems with linear constraints see Section developing an algorithm to solve this constrained classification problem see Section and reducing classification with linear-fractional constraints to solving a small number of linear classification problems above for carefully chosen parameters see Section Our approach is very flexible it allows us to provide classifiers that are fair with respect to a host of fairness metrics corresponding to both of linear and non-linear constraints see Table examples include several prevalent fairness metrics In particular we obtain classifiers with predictive parity-type constraints for which there was no previous result with provable guarantees Additionally our algorithmic framework can handle multiple fairness metrics simultaneously and the metrics can be defined with respect to complex sensitive attributes eg multiple attributes non-disjoint attributes and/or multi-valued attributes Further we conduct an empirical evaluation of our algorithm on the Adult German credit and COMPAS datasets and compare it against state-of-the art approaches in fair classification see Section The results show that our algorithm can often achieve higher fairness than prior work and that the loss in accuracy due to imposing fairness constraints is often small Thus we provide a meta-algorithm for fair classification which makes it flexible and easy to use in a variety of applications is approximately optimal for whichever fairness metric is selected and performs well in practice OUR MODEL We consider the Bayesian model for classification Let denote a joint distribution over the where X is the feature space Each sample X Z Y is drawn from where each pi i n represents a sensitive attribute and Y is the label of X Z that we want to predict For the sake of readability we discuss the case where there is only one sensitive attribute Z p in the main text This can be generalized to multiple sensitive attributes by adding fairness constraints for all sensitive attributes and is discussed in Appendix E Fixing different values of Z partitions the domain D into p groups Gi x D Let denote the collection of all possible classifiers Given a loss function L that takes a classifier and a distribution as arguments there are two models for fair binary classification which have been studied in the literature and we consider is not used for prediction then the goal is to learn a classifier X that minimizes Lf In this model X If Z is used for prediction then the goal is to learn a classifier X p that minimizes Lf In this model Denote by the probability with respect is clear from context we simply denote by Pr A commonly used loss function is the prediction error ie Lf Y Here with some abuse of notation we use to represent X for the first model and X Z for the second model Apart from minimizing the loss function as usual in fair classification problems the goal is also to achieve similar performance across all There are many metrics to measure this group performance including statistical rate true positive rate accuracy rate or false discovery rates see Table For example the statistical rate of Gi is of the form Gi ie the probability of an event conditioned on another event Gi Group performance can be defined in a general form as follows Definition Group performance group performance function Given a classifier and i p we call the group performance of Gi if Pr E Gi E for some events E E that might depend on the choice of Define a group performance function p for any classifier as When is clear from context we denote by q At a high level a classifier is considered to be fair to q if for all i Consider the following examples of q Accuracy Rate Here E Y and E ie is the accuracy of the classifier on we can rewrite as follows see Lemma A in Appendix A Pr Y Gi Pr Y Gi Pr Y Gi Pr Y Gi Pr Y Gi ie a linear combination of conditional probabilities Pr Gi Y and Pr Gi Y False Discovery Rate Here E Y and E ie is the prediction error on the sub-group of Gi with positive predicted labels we can rewrite as follows see Lemma A in Appendix A Pr Y Gi Pr Gi Y Pr Gi Pr Gi ie the fraction of two conditional probabilities Pr Gi Y and Pr Gi In both these can in terms of probabilities Pr Gi as either a linear combination or as a quotient of linear combinations Below we define two general classes of group performance functions that generalize these two examples respectively Definition Linear-fractional/linear group performance functions A group performance function q is called linear-fractional if for any and i p can be written as i i Gi A i i l i Gi B i Table Summary of prior work The symbol or represents that the corresponding framework works for or can be extended to handle the corresponding fairness metric The events E and E determine the group performance of the while represents whether this group performance function is linear or linear-fractional Q lin This paper E E a i r n e s s m e t r i c s statistical Q lin conditional statistical X S Q lin false positive Y Q lin false negative Y Q lin true positive Y Q lin true negative Y Q lin accuracy Y Q lin false discovery Y Q false omission Y Q positive predictive Y Q negative predictive Y Q for two integers l events Ai i B i i l that are independent of the choice of and parameters i i i i l R that may depend on but are independent of the choice of Denote Q to be the collection of all linear-fractional group performance functions Specifically if l and i i p q is said to be linear Denote Q lin Q to be the collection of all linear group performance functions In Appendix A we show that all q in Table are linear-fractional and in fact many are linear see the Q lin column A classifier is said to satisfy to a given group performance function q if mini p p see The closer is close to the fairer is with respect to q Assume there are m fractional group performance functions q Q and Lf Y Given our main objective is to solve the following fair classification program induced by that we refer to as ρ-Fair min Pr Y s t min p q i p q i i m ρ-Fair It follows that by setting q appropriately ρ-Fair captures several existing constrained classification problems as special cases eg statistical rate true positive rate or predictive rate Remark If the program above computes a classifier with perfect fairness to This setting is well studied in the literature However perfect fairness is known to have deficiencies and hence prior work considers the relaxed fairness metric where Another relaxed fairness metric is defined by mini p maxi p Computing a classifier such that has also been investigated in the literature We refer the reader to a survey for other relaxed fairness metrics eg AUC and correlation Computationally the constraints of ρ-Fair are non-convex making the problem of solving it even approximately intractable in general To bypass this we introduce a fair classification problem with linear constraints which we call Group-Fair that has additional parameters corresponding to lower and upper bounds and corresponding fairness constraints Definition Group-Fair Given u i for all i m and p we consider the following classification problem with fairness constraints min Pr Y s t i q i u i i m p Group-Fair On the positive side these constraints are linear resulting in a convex programming problem and give us a finer control over the group performance function In particular by selecting nonuniform parameters i u i Group-Fair can treat different groups differently while all groups are symmetrically regarded in ρ-Fair Moreover it is easy to see that for any feasible classifier of Group-Fair and any i m satisfies i u i to While being tractable ρ-Fair raises the problem of finding the appropriate values of the lower and upper bounds that are not part of the input to Group-Fair In Section we show how to solve Group-Fair by a small number of calls to ρ-Fair where we set the lower and upper bound parameters in each call to ρ-Fair carefully Remark We remark that our algorithms assume the existence of an oracle that evaluates sufficiently well for any given classifier To overcome this issue we note that we can estimate E Gi E by the empirical probability of samples drawn from ie the ratio between the number of samples satisfying E Gi E and the number of samples satisfying Gi E RELATED WORK From a technical perspective the most relevant prior work includes which considered the Bayesian classification model for statistical parity or equalized odds reduce their constrained classification problems to unconstrained optimization problem by the Lagrangian principle while aim to find optimal threshold rules or use regularizers to find a fair classifier Our framework also uses the Lagrangian principle but works for a much wider class than these works ie any linear-fractional group performance function Some very recent work has also taken steps towards providing a unifying approach to fair classification encode fairness constraints including statistical parity equalized odds and predictive parity as a distance between the distributions for different values of a single binary sensitive attribute and then use the privileged learning framework to optimize loss with respect to fairness constraints While this results in an interesting heuristic they do not provide theoretical guarantees for their approach give a method to compute a nearly optimal fair classifier with respect to statistical parity or equalized odds by the Lagrangian principle In particular their framework supports fairness constraints that are linearly dependent on the conditional moments of the form E where is a function that depends on the classifier along with features of the element while E is an event that does not depend on However linear-fractional constraints cannot be directly represented in this form since here the event we condition on E depends on the classifier which is why their framework does not support constraints like predictive parity use post-processing techniques to achieve calibration as false-positive or false-negative parity but do not provide any provably guarantee with respect to predictive parity We also note that our linear fairness constraints are inspired by works on other fundamental algorithmic problems such as data summarization ranking elections and personalization There are increasingly many works on fairness in machine learning with provable guarantees including that provide different classification algorithms with constraints on statistical parity or equalized odds and for fairness in multi-armed bandit settings or ranking problems respectively To the best of our knowledge our algorithm is the first unifying framework for all current fairness metrics with provable guarantees Many alternate approaches to improve the fairness of classification have also been studied One approach is to make predictions without the information of sensitive attributes which avoids disparate treatment However since the learning data may contain historical biases classifiers trained on such data may still have indirect discrimination for certain sensitive groups Another approach is to modify the classification problem to incorporate constraints of some kind For example one approach proposes other fairness metrics as a proxy of statistical parity or equalized odds eg propose a covariance-type constraint for statistical parity and equalized odds Their model does not require the sensitive attribute Z to be explicitly provided during prediction thereby preventing disparate treatment Yet another approach postprocesses a baseline classifier by shifting the decision boundary can be different for different groups eg use the sensitive attribute Z during prediction Their goal can be regarded as learning a different classifier for Alternatively another line of research is to pre-process on the training data and achieve an unbiased dataset for learning eg This approach is quite different from ours since we focus on learning classifiers and investigating the accuracy-fairness tradeoff from the feeding dataset Beyond group fairness recent works also proposed other notions of fairness in classification and discussed a notion of individual fairness that similar individuals should be treated similarly defined preference fairness based on the concepts of fair division and envy-freeness in economics Moreover discussed procedural fairness that investigates which input features are fair to use in the decision process and how including or excluding the features would affect outcomes Finally and investigated the inherent tradeoff between equalized odds and predictive parity called well-calibrated in their papers THEORETICAL RESULTS In this section we present an efficient algorithm to approximately solve ρ-Fair Theorem Section Towards this goal we first show that ρ-Fair can be efficiently reduced to a family of programs with linear fairness constraints Group-Fair Section Subsequently we show that there exists a polynomial time algorithm that computes an approximately optimal classifier for Group-Fair Section For convenience we only in this section ie there is only one group performance function q and we require for some This can be generalized to multiple group performance functions as discussed in Appendix E Reduction from ρ-Fair to Group-Fair We first show the generality of Group-Fair ie approximately solving ρ-Fair can be reduced to solving a family of Group-Fair A β-approximate algorithm for Group-Fair is an efficient algorithm that computes a feasible classifier with prediction error at most times the optimal prediction error of Group-Fair Theorem ρ-Fair Given let denote an optimal fair classifier for ρ-Fair Given a for Group-Fair and any there exists an algorithm that calls A at most times and computes a classifier such that Pr Y Pr Y mini p maxi p Theorem asserts that the ρ-Fair program can be solved efficiently by solving at most different Group-Fair programs The resulting classifier slightly violates the since there is an additive error term in the right side of As the error goes to the violation becomes small and hence the resulting classifier is guaranteed to be more fair with respect to q However the running time becomes longer since it depends on the term Proof of Theorem For each t T denote at t and t For each t T we construct a Group-Fair program Pt with at and for all p Then we apply A to compute as a solution of Pt Among all we output such that Pr Y is minimized Next we verify that satisfies the conditions in the theorem Note that at for each t T We have min i p max i p On the other hand assume that t mini p t for some t T Since is a feasible solution of ρ-Fair max i p mini p t Hence is a feasible solution of Program Pj By the definitions of A and we have Pr Y Pr Y Pr Y The above theorem can be generalized to any loss function instead of the prediction error The reduction also holds for them case The only difference is that we need to call roughly times This enables us to simultaneously handle multiple fairness requirements see Appendix E for details The reduction is efficient with respect to running time as well and so to efficiently solve a ρ-Fair program we just need to construct an algorithm for the Group-Fair program Algorithm for Group-Fair In this section we propose an algorithm for Group-Fair Due to space limitations we omit many details see Appendix B For concreteness we first consider the setting where X and q Q lin and subsequently discuss the q Q case By Definition assume that i i Pr Gi A i for and i p Without fairness constraints it can be shown that I Pr Y X x is an optimal classifier minimizing the prediction error Pr Y where I is the indicator function But such a classifier might not satisfy all the fairness constraints Hence we introduce a regularization parameter and study the following problem arg min Pr Y i p Now we can control by adjusting Intuitively increasing leads to an increase in By selecting suitable we can expect that satisfies all fairness constraints We will show that there exists some such that Group-Fair is equivalent to by the Lagrangian principle Moreover can be shown to be an instance-dependent threshold function with the threshold Pr Y X x i p x where x i Pr Gi A i Pr Gi A i X x is the scaling factor of that is determined by the form of Observe that the term Pr Y X x is exactly the threshold for the unconstrained optimal classifier and the remaining term i p x can be regarded as a threshold correction induced by Theorem Solution characterization and computation for q Q lin Given any parameters p there exist optimal Lagrangian parameters such that X is an optimal fair classifier for Group-Fair Moreover can be computed in polynomial time as a solution to the following convex program arg min arg min i p i i p max This theorem asserts that Group-Fair can be solved efficiently up to an arbitrary accuracy first compute the optimal Lagrangian parameters via and then output the fair classifier X The running time depends on how fast we can solve Program Since Program is convex and we can apply standard convex optimization algorithms eg the stochastic subgradient method to compute an ε-approximate such in time The proof of this theorem reduces Group-Fair to an unconstrained optimization problem by the Lagrangian principle Appendix B We then derive as the dual program to Group-Fair and show that is an optimal solution to Appendix B Consequently Theorem leads to an algorithm Group-Fair that computes an optimal fair classifier for the Group-Fair program Theorem can also be directly extended to by replacing X to X Z everywhere Menon and Williamson Algorithm also propose an algorithmic framework for fair classification with respect to statistical rate and true positive rate using the Lagrangian principle However they only analyzed the characterization but did not show how to compute the optimal Lagrangian parameters Our approach can be naturally applied to their setting for computing the optimal Lagrangian parameters see Appendix for details Theorem can be generalized Q The key observation is that we can rewrite the fairness constraint as i i Pr Gi A i i l i Pr Gi B i By rearranging the above inequality is expressible as a linear constraint a b This also holds for which implies that Group-Fair is a linear program of Hence introducing fairness constraints can handle predictive parity Q but the prior work can not due to the fact that the constraint may not be convex in general lin we also apply the Lagrangian principle The only difference is that we need to introduce two regularization parameters and respectively for constraints and However the objective function of linear-fractional programming is a ratio of linear functions and this results in a simple reduction from it to linear programming see In our reduction the constraints are linear-fractional and it does not seem easy to reduce to a single linear program Then similar to for any regularization parameters R p we define an instance-dependent threshold function x Pr Y X x i p i i i x l i i i x i i i x l i i i x which consists of the term Pr Y X x that is the threshold for the unconstrained optimal classifier and threshold correction terms induced by and Then we prove the following theorem which indicates that q Q can also be solved efficiently by first computing the optimal Lagrangian parameters and then outputting the fair classifier X Theorem Solution characterization and computation for q Q Suppose X and q Q Given any parameters i p there exists R p such that x is an optimal fair classifier for Group-Fair Moreover we can compute the optimal Lagrangian parameters and in polynomial time as a solution of the following convex program arg min EX X i p i i i p i i Algorithm for ρ-Fair We proceed to designing an algorithm that handles the fairness metric In real-world settings instead of knowing we only haveN samples xi i N drawn To handle this we use the idea inspired by estimate by eg via Gaussian Naive Bayes or logistic regression on samples and then compute a classifier based on by solving a family of Group-Fair programs as stated in Theorem see Algorithm By Theorems and the running time of Algorithm is polynomial in N Analyzing Algorithm Intuitively if is close to then the quality of in both accuracy and fairness should be comparable to an optimal fair classifier for ρ-Fair under Define max i pf as the error introduced in when by Let denote the total variation distance between and Theorem Quantification of the output classifier Let be a fair classifier minimizing the prediction error Y subject to the relaxed min i p max i p Then Algorithm outputs a classifier such that Y Y mini p q i maxi p q i Algorithm An algorithm for ρ-Fair Input Samples xi i N from distribution a linear-fractional group performance function Q a fairness parameter and an error parameter Output A classifier Compute an estimated distribution eg via Gaussian Naive Bayes on xi i N T For each t T at t and t For each t T let Group-Fair at p p Return Y We defer the proof to Appendix C The key is to show is feasible for ρ-Fair under This can be inferred by the assumption that mini p q i maxi p q i and the definition of Then we prove by Theorem that Pr Y Pr Y and mini p q i maxi p q i To account for the error when going from to the terms and are introduced Note that is only an approximately optimal fair classifier for ρ-Fair due to the additional error Assume the optimal fair classifier for ρ-Fair is Since we do not have access to only to it is unknown whether satisfies the with respect to Hence we can only compare the performance of the output to instead of the optimal classifier If the number of samples N is large we can expect that and are close and hence are small Then the performance of is close to over Specifically if we have The output classifier then satisfies the properties of Theorem with which implies that is an approximately optimal fair classifier for ρ-Fair Remark For the fairness metric introduced in Remark we can also design an algorithm similar to Algorithm We only need to modify Line by L recall ai i and bi i The quantification of the output cf is similar to Theorem The main differences are arg min Pr Y and the output satisfies that Y Y The details are discussed in Appendix D Remark Since the distribution is constructed via samples from we can study the number of samples required such that and are close enough ie max i pf Note that given the inequality learning is exactly a classic distribution learning problem in which the sample complexity is bounded under a certain assumption model of eg mixtures of a constant number of Gaussian distributions We refer interested readers to the survey for distribution estimation techniques For the second inequality when the constraint is linear we can use Chernoff bound to show that samples from the underlying there exists an algorithm that computes an estimated distribution such that max i pf where mini p Gi E The formal statement and discussion on sample complexity are presented in Appendix C EMPIRICAL EVALUATION Experimental Setup We compare the empirical performance of our algorithm against the state-of-the-art techniques for fair classification on three datasets that are commonly used to evaluate the fairness of algorithms Algorithms and Benchmarks We compare three versions of Algorithm which provide fair classification results with respect to different fairness metrics Subject to τsr-fair Algo ie fairness constraint with respect to the statistical rate Subject to -fair Algo ie fairness constraint with respect to the false discovery rate which is a kind of predictive parity constraint Subject to τsr-fair and Algo ie fairness constraints with respect to both the statistical rate and the false discovery rate We benchmark our approach against four state-of-the-art algorithms developed in aims to constrain statistical rate SHIFT developed in designed to ensure equalized odds constrain fpr and FPR-COV and FNR-COV presented in aim to eliminate disparate mistreatment control the ratios fpr and fnr REDUCTION developed in designed to constrain statistical parity and equalized odds constrain against because they are state-of-the-art algorithms for their respective fairness metrics We also select REDUCTION to compare against because it provides a different meta-algorithm which works for a subset of the fairness metrics we consider Measurements Let D denote the empirical distribution over the testing set Given a group performance function q we denote to be the fairness metric under the empirical distribution D For instance given a classifier min i p Pr D Z i i p Pr D Z i represents the fairness of the output classifier over the testing set while represent the input fairness constraint desired with which uses the Least squares classifier as the base classifier respect to the underlying distribution However this may not always be satisfied in practice if the estimated distribution is not a good fit for the underlying distribution Hence we report as this is the output fairness obtained by the classifier For completeness we also report the correspondence between the output fairness and the input constraint Datasets We conduct our experiments on the following three datasets which are commonly used for benchmarking in the fairness literature Adult This is an income dataset which records the demographics of individuals along with a binary label indicating whether the income of an individual is greater than USD We use the pre-processed dataset We take gender to be the sensitive attribute which is binary in the dataset German This dataset records the attributes corresponding to around individuals with a label indicating positive or negative credit risk We use the pre-processed dataset by Friedler et al We take gender to be the sensitive attribute which is binary in the dataset COMPAS This dataset compiled by Propublica is a list of demographic data of criminal offenders along with a risk score We refer the reader to for more details on how the data was analysed and compiled For our experiment we use the following features for classification sex age race juvenile felony count decile score juvenile misdemeanor count other juvenile charges count priors count days in jail charge degree and try to predict the is label which represents whether individuals recidivated within two years or not We take race as the sensitive attribute and consider the subset of the data corresponding to individuals for which the race attribute is either black or white Implementation Details We perform five repetitions in which we divide the dataset uniformly at random into training and testing sets and report the average statistics of the above algorithms In Algorithm we set the error parameter to and fit the estimated distribution using Gaussian Naive Bayes using SciPy For each dataset we for and plot the resulting and accuracy We solve the optimization problem using Gradient Descent methods Results Accuracy vs Output Fairness on the Adult Fig summarizes the tradeoff between the accuracy and the observed fairness with respect to the statistical rate The red points represents the mean value of and accuracy of Algo for different input values of with the error bars representing the standard deviations respectively For other algorithms we report only the point with largest mean value and the axes of the ellipse around the point are the standard deviations of the fairness and accuracy respectively We observe can achieve higher than than other methods However this gain in fairness comes at a loss accuracy is decreasing in for Algo albeit always above Even for lower values of corresponding to weaker constraints Figure vs for Adult dataset For Algo we plot value of accuracy and observed for different values of input For other methods we plot the datapoint with largest mean and the ellipse around it represents the standard deviation Algo can achieve better fairness with respect to than any other method albeit at a loss to accuracy Figure vs for Adult dataset For Algo we plot value of accuracy and observed fairness for different values of input For other methods we plot the datapoint with largest mean and the ellipse around it represents the standard deviation Algo achieves better fairness with respect to than any other method and the loss in accuracy is small Figure vs Algo for different values of input on Adult dataset Figure vs Algo for different values of input on Adult dataset the accuracy is worse than that of and SHIFT This is likely due to the fact that we use a simple model for estimating the empirical distribution which will affect the overall accuracy of the algorithm see Theorem we expect that the performance would improve if we were to tune the fit see also Section and the observed fairness with respect to the false discovery rate The red points represents the mean value of and accuracy of Algo for different input values of while we report only the point with largest mean for the other algorithms Here we observe that Algo can attain the highest observed fairness for appropriate input values Furthermore its accuracy even for the highest fairness values is comparable to that of other methods Note that the overall fairness for all methods is higher than this is likely because the unconstrained optimal classifier for Algorithm achieves see Table ie the Adult dataset is already relatively fair across genders with respect to Quadrianto and Sharmanska also provide a heuristic meta-algorithm for multiple fairness metrics However we were unable the average results for all five training-test splits of the dataset Within each partition they are monotone to compare against their approach directly due to the unavailability of their code online and our inability to replicate their results via our own implementation Comparing against the raw numbers reported in their paper they achieve accuracy overall while the accuacy-difference their metric of fairness across groups is on the Adult dataset Algo can achieve a similar accuracydifference for an overall accuracy of and can achieve a smaller accuracy-difference for overall accuracy Relationship Between and on the Adult Dataset Empirically we find that the observed fairness is almost always close to the target constraint The output fairness and accuracy of the classifier against the input measure is depicted in Fig and Fig We plot all points from all the training/test splits in these figures Results on COMPAS and German datasets A similar evaluation on the COMPAS and German dataset are presented in Appendix and we simply summarize the primary observations here The performance of Algo with respect to other algorithms on German dataset is depicted in Figures and From Fig we observe that the classifier is able to satisfy the input fairness constraint almost every time ie for almost all values of input the observed fairness of the classifier is greater than or almost equal to Furthermore as shown in Fig the maximum Table The performance mean and standard deviation in of different fair classification with respect to accuracy and the fairness metrics from in Table on the Adult dataset We present the performance of an unconstrained optimal classifier for Algorithm for comparison Fairness Metrics fpr fnr for Unconstrained T i s p a p e r Algo Algo Algo B a s e l i n e s FPR-COV FNR-COV SHIFT REDUCTION value achieved is around while amongst other algorithms the maximum achieved is around Similarly for Algo whose results are presented in Figures and we see that for almost all values of input we satisfy the input fairness constraint except when in which case observed Figures and depict the performance of Algo with respect to other algorithms on the COMPAS dataset Algo achieves a maximum of around while other algorithms are able to achieve value around For lower values of input we achieve similar accuracy as other methods however for higher values of input we incur a loss in accuracy Similarly the performance of Algo is presented in Figures and Once again the input fairness constraint is almost always satisfied and we achieve higher values than other algorithms Effect of Constraints on Other Fairness Metrics We also examine the performance of our methods and the baselines with respect to other fairness metrics and report their mean and standard deviation we consider only classifiers corresponding to while for Algo FPR-COV FNR-COV and SHIFT we choose the classifier corresponding to Different methods are better at optimizing different fairness metrics the key difference is that Algo can optimize different metrics depending on the given parameters whereas other methods do not have this flexibility eg here we constrain fairness with respect to and for which the maximal values of and are attained but we could instead constrain with respect to any other q if desired Interestingly although Algo and Algo do not achieve the highest accuracy overall both have significantly higher accuracy parity than other methods Furthermore we can consider multiple fairness constraints simultaneously Algo can achieve both and while remaining methods can not or Unfortunately this does come at a loss of accuracy likely due to the difficulty of simultaneously achieving accuracy and multiple fairness metrics CONCLUSION We present an efficient meta-algorithm for classification with nonconvex linear-fractional constraints Linear-fractional constraints capture many existing fairness definitions in the literature and thus our algorithm can be used to derive several old and new results for classification with fairness constraints In particular to the best of our knowledge our framework is the first that works for predictive parity with provable guarantees which addresses an open problem proposed in Empirical evaluation of our algorithm on realworld datasets shows that our algorithm almost always satisfies the fairness constraints and the loss in accuracy is small This paper opens several possible directions for future work As observed in the empirical results and predicted by Theorem the performance of our framework depends on the quality of the estimated distribution It would be interesting to optimize the approach in this regard either empirically or theoretically We also believe it would be valuable to extend this framework to other commonly used loss functions eg l-loss or AUC and other classifiers eg margin-based classifiers or score-based classifiers It would be interesting to get bounds on sample complexity for classification with linear fractional constraints Finally while in this paper we consider fairness constraints introduced by the other fairness constraints such as AUC and correlation see the survey might also be worth considering