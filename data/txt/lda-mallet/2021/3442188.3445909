A Pilot Study in Surveying Clinical Judgments to Evaluate Radiology Report Generation The recent release of many Chest X-Ray datasets has prompted a lot of interest in radiology report generation To date this has been framed as an image captioning task where the machine takes an RGB image as input and generates a  sentence summary of findings as output The quality of these reports has been canonically measured using metrics from the NLP community for language generation such as Machine Translation and Summarization However the evaluation metrics eg BLEU CIDEr are inappropriate for the medical domain where clinical correctness is critical To address this our team brought together machine learning experts with radiologists for a pilot study in co-designing a better metric for evaluating the quality of an algorithmically-generated radiology report The interdisciplinary collaborative process involved multiple interviews outreach and preliminary annotation to design a larger scale study which is now underway to build a more meaningful evaluation tool CONCEPTS Computing methodologies Machine learning Human-centered computing Collaborative and social computing design and evaluation methods KEYWORDS Radiology Report Generation Participatory ML INTRODUCTION Over the past few years there has been considerable interest in the task of creating deep learning models to interpret medical images Many of the use cases include tumor detection/localization automatic segmentation of  and X-ray scans and image registration All of these scenarios have structured outputs such as pixel labels or image labels which allows standard loss functions and evaluation metrics to lead to models which converge towards high performance The situation is different however for tasks which generate clinical text such as document summarization clinical question answering and report generation Evaluation metrics used in language generation tasks are not as reliable both general and domain-specific settings and are an area of active research In this work we provide three main contributions We develop and conduct an annotation task to collect clinical judgment on candidate reports from radiology images This provides much-needed guidance on what makes a report good or bad We examine what radiologists look at when evaluating a report This is useful both to understand the current limitations of the task framing and also to help inform future development of a better evaluation metric for Chest X-Ray report generation We demonstrate some of the outreach tools we used for initial contact with domain experts to help create discussions and eventual partnerships Our findings highlight the need for data scientists to work closely with clinical experts to build meaningful tasks and models Without domain knowledge and wisdom it is far too easy to fall into the trap of incorrect modelling assumptions such as treating radiology report generation as a simple image captioning task with readily available labels in the reports that accompany frontal radiographs And once the task is properly defined interdisciplinary teams must work together to develop sound evaluation metrics which service as sound proxies for clinical usefulness The rest of the paper is as follows Section contextualizes this effort in relation to related work Section describes the methodological design of the pilot report candidates and evaluation metrics Section shows the results of the pilot and related analyses Section discusses the implications of these results Section offers a conclusion of our results and Section discusses the limitations of this study and how they inform future work BACKGROUND AND RELATED WORK Generating and Evaluating Radiology Reports In recent years several Chest X-Ray datasets including both images and clinical reports have been made publicly available Some efforts in radiology AI have worked to model text and images jointly in order to predict disease severity of illness identify regions of interest over chest X-ray images and allow information retrieval Since these datasets have come out many works have generated radiology reports including with template-informed approaches as well as reinforcement learning and radiology-derived metrics in the objective function Once text gets generated by a model it needs to be evaluated both in training and at inference time to have an understanding of the model performance The gold standard would be a bespoke human evaluation but this has many challenges including the bottlenecks of speed cost and scale The machine learning community designed automatic metrics to solve some of these challenges Existing metrics can be broadly categorized into n-gram matching count the number of n-grams that occur in the reference and candidate text potentially with reweightings embedding matching compare the soft similarities of dense fuzzy representations for words and sentences learned metrics metrics are trained to optimize correlation with human judgments radiology-specific metrics rule-based parsing to identify the presence or absence of specific diagnoses Evaluation metrics are meant to be proxies for bespoke human evaluations therefore any new metric that is developed is measured to demonstrate its correlation with some form of human judgment The original BLEU paper had human evaluators score candidate translations from very bad to very good and then demonstrated that BLEU ranks systems in the same order that human annotators do CIDEr was created in for image captioning and became very popular for benchmarking on the MS COCO dataset Annotators were asked to decide which of two candidate captions better agrees with a reference caption and then CIDEr was shown to agree with the annotator rankings more strongly than previous metrics like BLEU and METEOR Several papers have proposed moving away from correlation and more towards multidimensional evaluation where sentence corruptions are introduced as unit tests for how well the metric responds Many of the most popular metrics have been widely criticised In the context of text simplification BLEU has a very weak and in some cases negative correlation with human judgment on grammaticality meaning preservation and simplicity Even in the context of machine translation for which it was originally created BLEU correlates poorly with human judgment on both adequacy ie whether the hypothesis sentence fully captures the meaning of the reference sentence and on fluency ie the quality of language in the hypothesis sentence Table Linguistic characteristics of PASCAL-S CIDErs annotations and the MIMIC-CXR radiology reports Average Characteristic PASCAL-S MIMIC-CXR number of references sentence count word count dale-chall readability score Gunning-Fog index Flesch readability score Challenges in the Clinical Domain Although these metrics have been used in evaluating radiology report generation they are often designed for specific contexts with underlying assumptions about the number of reference sentences available as well as the complexity of the sentences to be analyzed When CIDEr was introduced its authors showed that humans and CIDEr agree with a high correlation but they did so when there were  reference captions per images as well as very low-complexity descriptions eg a cow is standing in a field It is not clear whether these findings would hold in the clinical domain where there is only one reference report many more tokens and a strong emphasis on factual correctness To better understand the differences between the simple general domain image descriptions and clinical text we use standard readability scores to assess the complexity of a given piece of text The dale-chall readability formula and Gunning-Fog index measures the years of formal education a person needs to understand the text on the first reading In the case of Gunning-Fog the score is meant to directly indicate the number of school years eg means th grade means th grade etc and Dale-Chall works similarly but on a  scale The Flesh readability score rates documents on a -point scale based on the number of words and sentence and syllables per word Unlike the previous two indices higher Flesch scores indicate easier-to-read documents Table demonstrates many of the differences between PASCALS a dataset introduced in the CIDEr paper and MIMIC-CXR We observe that PASCAL-S indeed has reference reports each of which has times fewer tokens than a MIMIC-CXR report Additionally the Dale-Chall and Gunning-Fox readability scores suggest that nearly twice as many years of formal education are required to understand radiology reports than simple image descriptions Clinical text is demonstrably more complicated than the general domain text that previous metrics were developed for METHODOLOGY The long-term goal is to eventually design a better evaluation metric for determining whether an automatically-generated radiology report is good However such a task requires the close collaboration with clinicians on the design and improvement of the metric As such we begin by focusing on the process of this participatory design and will then apply these lessons learned in a standalone work This collaborative effort was done in two parts qualitative discussions to design the framing/task and a pilot study Computer Figure Three different annotation tasks we considered for the radiologists to perform a Direct Assessment Radiologist would be asked to select how good the generated caption is for the image from b Caption Ranking Radiologist would be asked to rank proposed captions based on how well each describes the given image c Image Selection Radiologist would be asked to select which image is the one being described by a given caption scientists conducted interviews with radiologists from three hospitals including from Boston MA and Atlanta GA After these conversations an interdisciplinary team of computer scientists and radiologists conducted a pilot study based on feedback during the initial outreach Designing the Annotation Task Based on prior work in radiology report generation and evaluation metric creation we had a rough idea of the collaboration and data collection we had in mind radiologists need to read generated reports and decide whether it is good or not The simplest way to go about this could be to display an image report and ask the radiologist to rank how good it is from  Unfortunately this approach suffers from broader design issues behavioral economics demonstrates that humans can be inconsistent We can see an example of this from the Sentences Involving Composition Knowledge SICK dataset where prior work observes that the same kind of sentence transformations can be scored inconsistently A man is holding a frog had a  similarity with There is no man holding a frog A man is playing soccer had a  similarity with There is no man playing soccer Although some of these annotator calibration concerns can be solved with mean normalization ensuring that a particularly harsh annotator doesnt distort the average the larger problem is that annotators are not only inconsistent with one another but they can also be inconsistent with themselves depending on their context and priming With this in mind we explored a few potential ways to pose the annotation task for doctors Figure demonstrates a few ways to ask annotators to make judgments such as a ranking-based approach b or image selection c Interviews with Radiologists In order to reach out to radiologists to discuss this project we created a pager to send to them before our call inspired by the Collabsheets list of simple questions for computer scientists and clinicians to discuss The pager is shown in Figure and its purpose of the document is to give a background on where we are coming from and focus the conversation on the kinds of questions that seemed important to us On many questions there was a strong consensus among the radiologists They all agreed that clinical correctness is the most important factor in determining whether a report is good Additionally each radiologist talked about their fields move towards more structured templated reports with some suggesting that perhaps an evaluation metric should try to favor regularity Additionally there was overall agreement that for an annotation task like this where they were not being asked to write their own reports it might be nice to have a DICOM image viewer that could allow them to zoom adjust contrast etc but such functionality would not be necessary During the course of the interviews there were a few other concepts raised by radiologists which we had not considered when designing the pager in Figure including Many images are simply normal heart normal lungs etc We should purposefully select a diversity of diagnoses in the annotation set When designing a metric eventually it may be useful to look for words conveying levels of uncertainty eg consistent with vs suggests There was however some disagreement among the clinicians One radiologist questioned whether any of the proposed annotation tasks were the most meaningful thing to measure they suggested perhaps we should create an interface where the generated report is a first draft for the annotator to modify until they are satisfied with the final product This would however be a much more involved task for our annotators Additionally radiologists disagreed over whether it was worth including background information about the patient eg y/o female suffering from cough Pilot Study Annotation Task Based on the feedback from initial conversations we conducted a data collection pilot study Two radiologists annotated images captions a piece Figure demonstrates one instance of this task for a given image radiologists needed to rank possible Interestingly the notion that different doctors could disagree on healthcare expert opinions was surprising for some computer scientists A helpful analogy for how experts in a field have different opinions is to consider the strong opinions computer scientists have on Bayesian vs Frequentist statistics No field is a monolith Figure This pager document was sent to radiologists during outreach when setting up initial conversations about this projects goals reports based on how well they describe the findings of the image Each radiologist viewed the same images in the same order For each image we presented the annotator with the following statements The four following reports are all trying to describe this image some of them might be factually incorrect Please rank them from best to worst Briefly describe how you arrived at this ordering a few simple bullet points is fine Figure An example HIT of the chosen annotation task Confidence that another radiologist would arrive at the same choice for best report Not confident at all Very confident For each image we generate four different reports using the following methods reference gram nearest neighbor -NN and random-report The reference report refers to the actual report written by the radiologist logged in the EHR The nearest neighbor -NN report is produced by returning the report of the closest image in the DenseNet-induced feature space training set Similarly the random-report is the associated report of a randomly selected image from the training set Finally the gram is produced by retrieving the closest images and fitting a tri-gram model from their associated reports Performance of Evaluation Metrics Ranking the four reports for a single image produces comparisons ie best the other three the second best the other two the third best the worst though of those comparisons involve the reference sentence To determine how strongly a given metric agrees with radiologist judgment we compute the specific metric score for each of the non-reference candidates and determine the number of pairwise comparisons where the metric agrees with the experts For the metrics we evaluate many evaluation metrics including baselines random-score length readability scores Dale-Chall n-gram BLEU CIDEr embedding BERTScore and CheXpert accuracy RESULTS For images ranking reports results in binary comparisons Of those comparisons the annotators agreed with each other We do not compute the metric on the reference because by definition it would score as you would be comparing the reference against itself Table Of the consensus comparisons how often would each metric rank the two reports the same way the radiologists did Metric Percent Agree Percent Ties random-score choose shorter report Dale-Chall Readability Index BLEU- BLEU- ROUGE- CIDEr BERTScore chexpert-accuracy chexpert-accuracy CIDEr on ie of the time Of the rankings which did not include the reference report radiologists agreed on rankings ie of the time In line with prior work the clinical correctness of the gram model is higher than the random-report model but the nearest neighbor achieves the highest level Table shows how often each metric agreed with consensus radiologist rankings We include the random-score metric not to be confused with the random-report method as a sanity check if the metric were randomly assigning numbers it would get the ranking correct of the time The Percent Ties column denotes how often a given metric was not able to pick either report eg There were data entry errors where two reports were given the same ranking eg ranking of or even though the task did not allow for ties For those five entries the authors used the given explanations to infer what the annotator meant and break the ties defined as the macro-average recall of CheXpert labels Table Top n-grams from the explanations provided by annotators for decision-making Phrases containing stop words were removed unigram Count factually all wrong not correct bigram Count even though most correct hard to factually wrong all but trigram Count are factually wrong one and two not sure if all but one is hard to Table Readability scores of the n-gram-generated reports vs the random-report candidates Average Characteristic random-report gram dale-chall readability score Gunning-Fog index Flesch readability score if two reports were each correct on  findings then chexpert accuracy would be tied at a piece Because this is so common for chexpert-accuracy metrics we also report how well it would perform when CIDEr is used to break the ties ie CIDEr BERTScore attains the top performance of CheXpert only achieves DISCUSSION In this section we explore unexpected or interesting results in an attempt to better understand how they arose Self-Reported Annotator Rationale We were curious to test what would happen when radiologists needed to rank reports based on criteria which included style factual correctness grammar and potentially other factors One of the annotators qualitatively described their process for completing the ranking task in an interview They made it clear that the top criteria is factual correctness It doesnt matter how nice or brief a report is If its factually wrong then its bad To rank the candidates they would do an initial pass to group reports into two buckets plausible and wrong From there they would look at each bucket and identify which errors were more egregious eg the report with a rare type of error was ranked worse than a report with a common type of error Whenever two reports were both plausible without any disqualifyingly bad mistakes they would look to see which one was more complete especially since some omissions eg failure to mention a lung lesion would be more glaring than others Based on the quantitative results the other annotator seemed to agree about the importance of correctness After each images ranking annotators were asked to Briefly Describe How You Arrived at This Ordering a few simple bullet points is fine Table depicts the top- most frequent unigrams bigrams and tri-grams of the rationales experts gave in response We can see through uses of phrases like correct wrong factually and are factually wrong that they are explaining their decisions as decisions of factual correctness Additionally they convey the challenges of comparing two non-perfect candidates through phrases like most correct and all but one In both the qualitative and quantitative analyses there was little to no discussion of readability or grammaticality Why Do Radiologists and CheXpert Disagree on grams One surprising part about these factual correctness-based explanations is that Table shows the Dale-Chall Readability Index agreed with the radiologists more often than any of the CheXpert-based metrics This finding continues the discrepancy initially discovered in the CXR-Baselines paper where gram outperformed the random-report model on CheXperts clinical correctness but underperformed on standard evaluation metrics We can see in Table that ngram-based reports tend to have higher variance of readability suggesting there may be especially difficult-to-read reports which are still coherent enough for CheXperts rules to parse correctly It may be the case that especially ungrammatical reports came across as non-sensical to annotators which could be considered part of correctness LIMITATIONS AND FUTURE WORK Many of the reports in the dataset were essentially incomplete because they did not capture everything a radiologist would have when really performing their work Over half of all reports in the annotation pilot referenced previous radiographs which meant annotators needed to make their best guesses about unseen data Additionally although we only show the frontal chest x-ray reports are usually written using multiple views such as frontal and lateral On top of that because there are so many diagnostic labels it can be hard to know the focus of the report in the true data generation process the radiologist knows the reason for the exam eg check ETT tube placement This could lend itself more naturally to a question-answering task where the reason is the question the radiograph is the corpus and the report is the answer Another limitation encountered in this work is that the caption generation methods were very simple When radiologists were completing the survey at least one of them performed an initial plausible or wrong screening to filter out obviously inappropriate reports In the followup study we hope to run we will also use more advanced report generation techniques Though we will continue to use random-report to serve as a strong baseline for whether doctors can identify when a report looks good but is actually irrelevant to the context CONCLUSION In this work we performed a pilot study to assess clinician judgment for what makes one radiology report better than another one The gold standard of a reports good-ness would be how well it improves outcomes for the patient or hospital perhaps it catches more illnesses than a bad report would or perhaps it saves time/money for hospital operations Of course one cannot run a randomized control trial because a bad model would result in significant harm to patients That is why the clinical domain needs an appropriate metric to serve as a proxy when the true outcome itself cannot be obtained The difficult challenge is in determining whether a proxy is appropriate BLEU is a proxy but the field has spent over a decade pointing out many flaws it has In addition to the clinical content of this work the second main contribution of this study is in the collaborative process itself Many domains eg healthcare criminal justice social science etc are getting a lot of attention from computer scientists but meaningful progress can only be made through meaningful engagement with the domain experts and when possible the stakeholders In this work we emphasize the tools we used for outreach and the conversations for co-designing a refined version of this task based on lessons learned from the pilot study