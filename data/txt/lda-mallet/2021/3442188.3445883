Removing Spurious Features can Hurt Accuracy and Affect Groups Disproportionately Spurious features interfere with the goal of obtaining robust models that perform well across many groups within the population A natural remedy is to remove such features from the model However in this work we show that removing spurious features can surprisingly decrease accuracy due to the inductive biases of over-parameterized models In noiseless over-parameterized linear regression we completely characterize how the removal of spurious features affects accuracy across different groups more generally test distributions In addition we show that removal of spurious features can decrease the accuracy even on balanced datasets where each target co-occurs equally with each spurious feature and it can inadvertently make the model more susceptible to other spurious features Finally we show that robust self-training produces models that no longer depend on spurious features without affecting their overall accuracy The empirical results on the Toxic-Comment-Detection and CelebA datasets show that our results hold in non-linear models INTRODUCTION Machine learning models are susceptible to fitting spurious features For example models for toxic comment detection assign different toxicity scores to the same sentence with different identity terms Im gay and Im straight and models for object recognition make different predictions on the same object against different backgrounds A common strategy to make models robust against spurious features is attempting to remove such features eg removing identity terms from a comment removing background of an image or learning a new representation from which it is impossible to predict the spurious feature However removing spurious features can lower accuracy and moreover this drop varies widely across groups within the population Core features Target Spurious features z y s Figure We compare two models the core model Ms which predicts y from only the core features z and the full model Ms which predicts y from both z and the spurious feature s Previous work has identified two reasons for this accuracy drop i Core non-spurious features are noisy or not expressive enough so spurious features are needed even by the optimal model to achieve the best accuracy ii removing spurious features corrupts core features especially when learning a new representation uncorrelated with the spurious features In this work we show that even in the absence of the aforementioned two reasons removing spurious features can still lead to a drop in accuracy due to the effects of inductive bias in over-parameterized models For our theoretical analysis we consider noiseless linear regression We have d core features z and the spurious feature s See Figure for the causal graph Importantly i the spurious feature s adds no information about y beyond what already exists in z and ii removing s does not corrupt z since we are not required to remove the correlated features to s We consider two models the core model Ms which only uses z to predict y and the full model Ms which also uses the spurious feature s In this simple setting one might conjecture that removing the spurious feature should only help accuracy However in this work we show that this is not always the case and accuracy can drop by removing the spurious feature We exactly characterize groups or more generally test distributions that removing the spurious feature s hurts their accuracy In the over-parameterized regime since number of training examples is less than the number of features there are some directions of data variation that are not observed in the training data For instance Table shows a simple example with only one training data and two core features z and z In this example we do not observe any information about the second feature z The core model assigns weight to the unseen directions weight for the second feature in Table c On the other hand in the presence of a spurious feature the full model can fit the training data perfectly with a smaller norm by assigning a non-zero weight for the spurious feature weight for the feature s in Table c This non-zero weight for the spurious feature leads to different assumptions for the unseen directions In particular the full model does not assign weight to the unseen directions In Table d the full model True Parameters target y z spurious s a Training Example z s y b Model estimated parameters Core model Full model c Model prediction Core model y z Full model y z s d Table A simple linear regression example that shows when removing a spurious feature increases the error a There are core features z z z one spurious feature s and the target is y b There is only one training example with no information about z c Core models estimate for the parameters is however the full model interpolates data with a smaller norm by putting weight for the spurious feature s d For the prediction we can replace s by which results in the full model implicitly assigning weight of for the second feature while the core model assigns wight of to the second feature As a result if then removing s increases the error implicitly assigns weight to the second feature unseen direction at training while the core model assigns weight As a result if is small then the full model which uses s has lower error and if is large then core model which does not use s has lower error Intuitively by using the spurious feature the full model incorporates into its estimate The true target parameter and the true spurious feature parameters agree on some of the unseen directions and do not agree on the others Thus depending on which unseen directions are weighted heavily in the test time removing s can increase or decrease the error We formalize the conditions under which removing the spurious feature s increases the error We state that in addition to the true parameters the distribution of core-features in train and test data are also critical for identifying the impact of removing s on error Consequently conditions only on the true parameters or only on the distribution of the spurious feature and the target cannot guarantee that error does not increase after removal of the spurious feature Therefore removing the spurious feature may aggravate the error even under favorable conditions such as balanced datasets y s in the train and test data or if disjoint features determine the target and the spurious feature i for all i We then study multiple spurious features and show that removing one spurious feature inadvertently makes the model more susceptible to the rest of the spurious features in line with the recent empirical results Finally we show how to leverage unlabeled data and the recently introduced Robust Self-Training RST to remove the spurious features but retain the same performance as the full model The new model CoreRST model is robust to changes in s and it can perform when s is not available Empirically we analyze the effect of removing spurious features by training a convolutional neural network on three datasets the CelebA dataset for predicting if a celebrity is wearing lipstick where we use wearing earrings as the spurious feature the Comment-Toxicity-Detection dataset for predicting the toxicity of a comment where we use identity terms as the spurious features and finally a synthetically-generated dataset where we concatenate each MNIST image with another image and use the label of the new image to be the spurious feature Our empirical results are four folds Removal of the spurious feature lowers the average accuracy and disproportionately affects different groups increase for some groups and decrease for others The full model is not robust against the spurious feature and changing the spurious feature at the test time lowers its accuracy substantially The CoreRST achieves similar average accuracy as the full model while being robust against the spurious feature In the CelebA dataset we show that removing the spurious hair color makes the model less robust against wearing earrings SETUP Let z Rd denote the core features which determine the prediction target y z Let s denote a spurious feature We study the over-parameterized regime where we observe n d triples si as training data For an arbitrary loss function the standard error for a model M Rd R R is error s where the expectation is over test points z We are also interested in robustness of a model against spurious feature Let S be the set of different values for the spurious feature We define the robust error as follows Robust-error max s S s Robust-Error measures the worst case error for each data point z with respect to change in s Ideally we want our model prediction to be robust with respect to change in the spurious feature ie error Robust-error Robust-Error is a common definition in robust machine learning against input perturbation Also Robust-Error is close to counterfactual notions of fairness such as Counterfactual Token Fairness Let C denote a function that measures the complexity of a model eg the L-norm of its parameters and be a parameter to tune the trade-off between the models complexity and its empirical risk We are interested in the performance of the following two models Full model Ms which uses the core features z and the spurious s to predict y M s argmin M i si Core model which only uses the core features z to predict y M argmin M i True parameters Training example z s y Model estimated parameters Core model Full Model Model prediction Core model y z Full model y z s z Table There is only one training example with no information about z and z The core model assigns weight of to these two features The full model uses the spurious feature s which results in a good estimation for real weight of z but not z The core model does not use the spurious feature therefore its Robust-Error is equal to its Error However there might be a large gap between Error and Robust-Error for the full model WHEN DOES THE FULL MODEL OUTPERFORM THE CORE MODEL Over the last few years researchers have observed some surprising phenomena about deep networks that conflict with classical machine learning For example more training data might hurt accuracy A line of work explain these unintuitive results in simple models such as linear regression in over-parameterized regime Accuracy drops due to the removal of spurious features is also in contrast with classical machine learning Removing spurious features should decreases the error in the under-parameterized regime Analogous to the mentioned work we now explain this unintuitive result in noiseless over-parameterized linear regression We assume there are true parameters Rd such that the prediction target y and the spurious feature s in Section we study multiple spurious features and their interactions Motivated by recent work in deep learning which speculate that gradient descent converges to the minimum-norm solution that fit training data perfectly we consider the minimum-norm solution ie the complexity function C in and returns the L-norm of the parameters with regularization strength tending to We consider squared-error for the loss function Let Z be the matrix of observed core features Y be the targets and S be the spurious features We first analyze the minimum-norm estimate for the core model Let denote the estimated parameters of the core model which can be obtained through solving the following optimization problem argmin s t Y Let be the projection matrix to columns of Z then The full model uses the spurious feature in addition to the core features Let and denote the estimated parameters for the full model which can be obtained through solving the following optimization problem argmin s t Y denote the weight assigned to the spurious feature Note that in we can always set to obtain so the full model only achieves smaller norm by optimizing over In particular instead of having norm of the full model can use s with weight and have norm instead The optimal value for which minimizes the norm is Intuitively is larger if and are similar in column space of the training data See Appendix A for details The estimated parameters for the full and core models are shown in Table first and second rows To understand the difference between the performance of the core model and full model on different groups we extend the example in the introduction to include two unseen directions see Table In this example there are d core features and only one training example with no information about the second and the third features Let which results in y Let which results in s Without the spurious feature the estimated parameter is however by using s the full model fits the training data perfectly with a smaller norm by setting By substituting s with the full model implicitly assigns weights of and to the second and third features respectively Therefore removing s reduces the error for a test distribution with high deviations from zero on the second feature whereas removing s increases the error for a test distribution with high deviations from zero on the third feature The following proposition which is our main result provides general conditions that characterize precisely when the core model removing the spurious feature increases the error over the full model Proposition Let denote the covariance matrix of the test data or covariance matrix for any group and let denote the column space of training data errors error sign sign I and I Intuitively this proposition states that removing the spurious feature increases the error if the projection of on in column space of training data seen directions is similar to the projection of on in the null-space of training data unseen directions scaled by the covariance matrix Applying to the example in Table shows that removing s reduces the error at the test time with covariance matrix if This is in line with our intuition since the full model recovers exactly however its estimation for is worse than the core models estimate True parameters Training examples z s y Estimated parameters Test Examples Models predictions z s y Core model Full model Table An example demonstrating that s and y can be exactly the same in train and test time blue filled cells but the full model which uses s has a worse performance in comparison to the core model which does not use s Disjoint Parameters and Balanced Dataset are not Enough Proposition characterizes when removing the spurious feature increases and when it decreases the overall error Now we investigate whether removing the spurious feature can always decrease the error for some special settings For example if the features that determine the spurious feature s and the target y respectively are disjoint ie for all i we have i or when spurious features and the target are independent ie s y in the train and test distribution The following corollary states that there is no condition on the true parameters that guarantees error reduction by removing the spurious feature Corollary Disjoint parameters are not enough For d consider any non-zero Rd such that there is no scalar c where For any n d we can construct Z as training and Z Z as test data such that if we train Ms and using Z as training data then errors error on Z and errors error on Z On the contrary if then training both models on any Z results in errors error on any Z See Appendix A for the proof Intuitively for any and we can choose the training data Z such that i has positive projection on in the column space of Z ii there are two directions in the null space of Z where has a positive projection on in one of them and negative projection in the other one We can then select the test data from one of these two unseen directions Note that even in the special case of disjoint parameters i for all i removing s can increase the error Lets now consider conditioning on the vector of observed spurious features and the targets in train and test data In fact one of the proposed ways to reduce the sensitivity of the model against the spurious features is having a balanced dataset ie collecting the dataset such that y and s be independent in train and test data Py s Py For example in comment toxicity detection Dixon et al suggest adding new examples in training data to equalize the number of toxic comments and non-toxic comments for each identity term Although they show some mitigation with this method Wang et al demonstrate that a balanced dataset is not enough and the model can still be sensitive to the spurious features We now show that no measure based on the spurious features and the target can guarantee no error rise after removing the spurious feature Corollary Balanced datasets are not enough Consider any d n d and where there is no scalar c such that Y We can construct Z Z Z such that if we train Ms and using Z then the errors error on Z and error errors on Z On the contrary if Y then training both models on any Z results in errors error on any Z See Appendix A for the proof At a high level for proving this corollary we first rewrite the formulation of in terms of S and Y We then construct Z such that the dot product of S and Y projected on is non-zero We then construct Z Z such that Y Z Z and S Z Z Furthermore we select Z to have high variance on the unseen directions that projection of on is negative while choosing Z to have high variance on the unseen directions with positive projection of on There are two interesting special cases for Corollary i Even when there is no shift in the distribution of the spurious feature and target ie S and Y are exactly the same in train and test full model which uses s can have higher error than the core model see Table for a simple example ii Even when targets and spurious features are independent at train and the test balanced datasets the full model which uses s can have lower error than the core model Corollary and together state that we cannot compute the sensitivity of a model to the spurious feature or the effect of removing spurious feature by only observing the relation between s and y or knowing the relationship between and Therefore naively collecting a balanced dataset is not enough and we need to consider other features as well Robust-Error analysis Proposition compares the error of the full model and core model and show that each model can outperform the other on some conditions In this section we show that Robust-Error of the full model is always larger than the Robust-Error of the core model recall that for the core model Robust-error error Intuitively for any z s data point we can perturb s such that the full model makes the same prediction as to the core model therefore the full models Robust-Error is always lower than the core model error Proposition If z then Robust-error Robust-errors Note that without any bounds on the spurious feature the Robust-Error can be infinity By bounding the norm of z we can bound the True parameters Training example z s s y Estimated parameters with s and s equivalently with only s equivalently Table An example with two spurious features s and s There are two models one model which only uses s and another model which uses s and s Removing s increases the weight for s weight in comparison to blue filled cells As a result the model that only uses s is more susceptible to change in s ie increase the Robust-Error with respect to s and it performs worse on groups with high variance on z weight of instead of while the true weight is red filled cells perturbation set of the spurious feature as S where indicates the dual norm see Appendix A for the proof Multiple spurious features We now extend our framework to spurious features s sk where si z We characterize the effect of removing one spurious feature on the models sensitivity to the other spurious features In particular we show that removing one spurious feature can make the model more susceptible to other spurious features Extending the notation from Section let Z be the core features Y be the target and S denote the spurious features Similar to the previous section we obtain the minimum-norm estimate by solving the following optimization problem argmin s t Y where Rk denote the optimal weights for the spurious features Proposition The weight of the ith spurious feature of the minimum norm estimator is wi Consider the special case of with s and s as spurious features where and have positive dot product in the column space of training data Table shows a simple example of this setup As shown in removing s increases the weight for s which makes the model more sensitive against changes in s For instance in Table after removing s the weight of s changed from to green cells In addition recall that using s causes high error for groups with high variance on the unseen directions where differs from Removing s leads to even higher error for these groups In our example using s and s the models estimate for is which is different from the true value of therefore groups with high variance on the second feature incur high error Removing s changes the estimate of from to which exacerbates the error on the groups with high variance on z This is in line with the recent empirical results suggesting that making a model robust against one type of noise might make it more vulnerable against other types of noise SELF-TRAINING Can we have a model that is robust against change in the spurious feature but have the same assumptions about the unseen directions as the full model In the previous examples shown in Table and Table we showed that the full model M s has an equivalent form which has weight on the spurious feature In addition to being robust against change in s this equivalent form is useful when s is not available We now explain how to recover the equivalent s-oblivious form with finite labeled data and access to many unlabeled data In order to recover the s-oblivious equivalent form of the full model we use Robust Self Training RST Intuitively RST leverages unlabeled data to understand the assumptions for the unseen directions for a model that has low error but high Robust-Error It then incorporates these assumptions for the unseen directions to a robust model to achieves a robust model with low error Assume in addition to n labeled examples si we have access to m unlabeled examples s u i We are interested in a model that it is robust against s and it has the same prediction as the full model M s on unlabeled data We obtain M RST which we call coreRST by solving the following optimization problem M RST argmin M M s u i where are parameters to tune the trade-off between the labeled data unlabeled data and complexity of the model We now show that in the noiseless linear regression setup M RST is a s-oblivious equivalent form of M s Form d let Zu Rmd and Su term denote the unlabeled data Let RST denote the estimated parameters of RST model obtained by solving the following model weight for s weights for z Error M M s M RST wI Table Estimated parameters and error for different models optimization problem RST argmin st Y Zu where and are the estimated parameters for the full model The following proposition states the optimal parameters for and proves that for any data point M RST has the same prediction as M s Proposition The optimum parameters for are wI and for any data point z s we have s See Appendix A for details Intuitively M RST learns from unlabeled data and replace with See Table for the estimated parameters of the three introduced models For real data in Section we pseudo-label all the unlabeled data with the full model and then train the coreRST model on labeled data and pseudo-labeled data EXPERIMENTS We now investigate the effects of removing spurious features in non-linear models trained on real-world datasets Although our theory assumptions do not hold anymore we find similar results as Section In particular we show that removal of a spurious feature lowers the average accuracy has disproportionate effects on different groups and makes the model less robust to other spurious features We then show coreRST model can achieve higher accuracy than the core model while being robust against the spurious feature Datasets and Setup Double-MNIST The MNIST dataset consists of images of handwritten digits between to We synthetically construct a new dataset from the MNIST dataset which we call Double-MNIST We concatenate each image in MNIST with another random image from the same class with probability of and a random image from other classes with probability The original images label is the target y and the concatenated images label is the spurious feature s Note that the features that determine the target the first image are completely disjoint from the feature that determine the spurious feature the second image We train a two-layer neural network with hidden units on this dataset Using for the training data the model achieves accuracy However for our experiments where we need unlabeled data we used labeled examples unlabeled examples and for test data CelebA The CelebA dataset contains photos of celebrities along with different attributes We choose wearing lipstick which indicates if a celebrity is wearing lipstick as the target and wearing earrings as the spurious feature We train a two-layer neural network with hidden units on this dataset For our purposes in this work we use labeled examples unlabeled examples and for test data Toxic-Comment-Detection The Toxic comment dataset is a public Kaggle dataset containing Wikipedia comments Each comment is labeled by human raters as toxic or non-toxic where toxicity is defined by Dixon et al as rude disrespectful or unreasonable comment that is likely to make you leave a discussion We cleaned the data by replacing abbreviations eg changing weve to we have We used term frequency-inverse document frequency tfidf to extract features and used a logistic regression model to train on the extracted features Splitting data to the train-test we achieve a similar AUC as reported in Garg et al Dixon et al provide identity terms that a model should be robust against them We choose of these identity terms that exist in positive and negative class at least times see Figure for the list We use these terms as the spurious feature and replace them with a special token for the core model For the full model we did not remove these identity terms We choose the subset of the dataset that contains these adjectives consisting of examples We did train-test split use examples as labeled data and the rest of examples as unlabeled data See Table for a summary of datasets We run each experiment times and report the average accuracy and the standard deviation Results Core model vs Full model Removing the spurious feature decreases the overall accuracy in all three datasets see the first two rows of Table Note that the change in accuracy varies widely among different groups in the CelebA dataset removing the spurious wearing-earrings feature increases the accuracy for celebrities who only wear either lipstick or earrings and in the Double-MNIST dataset removing the second image label increases the accuracy for data points where the concatenated images have the different labels name core features z target y spurious feature s Example z y s Double-MNIST two MNIST images label of the left image label of the right image CelebA celebrities photo wearing lipstick wearing earrings True True Toxic-Comment-Detection comment w/o identity terms toxic or not identity terms cuz i shouldnt be blocked just for being non-toxic black Table A summary of the three datasets that we used in this work Double-MNIST is a synthetically generated dataset where each data point is a concatenation of two images from the MNIST dataset Double-MNIST CelebA Toxic-Comments all same labels different labels all no lipstick or earrings only earrings only lipstick both lipstick and earrings all group size in percentage full model accuracy core model accuracy full model robust accuracy coreRST accuracy Table Accuracy declines as we remove spurious feature the core model vs the full model however note that this drop varies widely among different group The coreRST achieves similar average accuracy as the full model while having high robust accuracy and lower gap among different groups accuracy For the Toxic-Comment-Detection dataset for each identity term u we construct two groups one group consist of toxic comments that have u in the comment text and another group consists of non-toxic comments that have u in the comment text resulting in total of groups The maximum gap among accuracy of the groups containing toxic comments max-TPR-gap drop from to and the maximum gap among accuracy of groups that are non-toxic max-TNR-gap drops from to Toxic-gay and non-toxic-gay groups incurred the maximum change in accuracy and respectively due to removal of identity terms Figure shows the change in true positive rate true negative rate ratio for different identity terms Robust accuracy Recall that robust accuracy is the worst-case accuracy for each data point with respect to change in the spurious feature First note that the robust accuracy of the full model third row is much lower than its accuracy first row which indicates the full models reliance on the spurious feature Furthermore this reliance helps some groups eg celebrities who wear earrings and lipstick while it does not have any effect on other groups eg celebrities who wear earrings but not lipstick Recall that the robust accuracy of the core model is exactly equal to its accuracy since it does not use the spurious feature Finally in line with our theory in Proposition the robust accuracy of the full model is always lower than the robust accuracy of the core model Robust Self-Training The forth row of the Table shows the accuracy coreRST model as explained in Section The coreRST ga y om os ex u al b lin d b la it e m u sl im m al e st ra t d je is fe m al e ch in es e yo u n g am er an in d n ch ri st n eu ro p ea n positive negative TPR TNR full model TPR TNR core model Figure The identity terms used as spurious features The Red line indicates the ratio of positive comments containing the identity terms over the negative comments containing the identity term The difference between the ratio of TPR/TNR of the core and full model can be small straight young or large male Chinese independent of positive/negative ratio for the identity term has a better average accuracy than the core model The CoreRST does not use the spurious feature therefore its robust accuracy unlike the full model is exactly equal to its accuracy In Section we prove that the full model and the coreRST should have the same predictions however unlike our theory we observe that the large gap among the accuracy of different groups in the full model is all no lipstick or earrings only earrings only lipstick both lipstick and earrings With hair color and necklace accuracy robust accuracy With only necklace accuracy robust accuracy Table Two models trained on the CelebA dataset Top model uses hair color and lipstick a binary feature indicating if a person is wearing lipstick or not bottom model only uses hair color the robust accuracy against hair color drop more rapidly for the bottom model The group that has lowest accuracy for the bottom model only earrings have a better accuracy when hair color is also used as an extra spurious feature number of training examples ac ra full model minority full model majority core model minority core model majority Figure In Double-MNIST dataset as we increase the number of training data the gap between the performance of core model and full model shrinks The majority groups contains data points where the labels of two concatenated images are the same of data and minority group contains data points where the labels of the concatenated images are different of data mitigated by the coreRST We have not observed a large accuracy boost using unlabeled data in the Toxic-Comment-Detection dataset After some error analysis we observed that as mentioned in Garg et al there are some examples that can be toxic with respect to some identity terms but not the others Furthermore we observed some biases in annotations in this dataset eg username is gay has labeled as toxic As a result some accuracy drop of the core model is inherent and cannot be mitigated by unlabeled data Effect of training data size Figure shows that the gap between the accuracy of the core model and the full model decreases as the training data size increases In particular it shows the accuracy on two different groups in the Double-MNIST dataset the majority different labels and minority same labels When we train a model on more training examples we observe more data variation directions therefore the gap between the full and the core models decreases Multiple spurious features Finally we study multiple spurious features In the celebA dataset we first use hair color and wearing earrings as a spurious feature We then remove the hair color and only use wearing earrings as the spurious feature In line with our theory in Section Table shows that The gap between robust accuracy against wearing earrings and standard accuracy increases when we remove hair color Celebrities who wear lipstick but not earrings have the lowest accuracy due to the use of earrings as a spurious feature This group accuracy drops even lower after removing the hair color attribute RELATEDWORK AND DISCUSSION This work is motivated by work in fairness in machine learning that aims to construct a model that is robust against changes in sensitive features The techniques used in this work is similar to work in robust machine learning that tries to understand the tradeoff between the robust accuracy against perturbed inputs and the standard accuracy In the following we discuss related work in these two fields Fairness in Machine Learning There are mainly two common flavors of fairness notions in machine learning concerning a sensitive feature eg nationality i The statistical notions which measure how much a model loss is different among groups according to the sensitive features These notions operate at the group level and it does not provide any guarantees at the individual level ii Counterfactual notion of fairness which measure how much two similar individuals are incurred different losses because of their sensitive feature This work is related to the counterfactual notion of fairness since we study the models that are robust against sensitive spurious features Note that there are many concerns and critics regarding counterfactual reasoning when sensitive features are immutable However there are works that try to learn a new representation entirely uncorrelated to the sensitive feature in categorical data in vision and in natural language processing There is a common trend in all of these works accuracy drops when we train the same model using the new representation Khani and Liang show that when the core features are noisy or incomplete the model can obtain better accuracy by using sensitive features As a result removing sensitive features in these cases will lead to a drop in accuracy Zhao and Gordon show that if groups according to the sensitive attribute have different base rates probability of y is different among different groups it is impossible to learn the optimum classifier from a representation uncorrelated with the sensitive features Dutta et al show how biased dataset lead to trade-off in representation learning This work shows that under a very favorable condition still removing spurious features changes the inductive bias of the model which results in different performance on average and over different groups We believe that as over-parameterized models such as deep learning models becomes more prevalent studying inductive biases and their effect on groups gain more importance Robustness in Machine Learning Since the initial demonstration of adversarial examples examples generated by changes in images that are not perceptible by humans but change the prediction of models there have been many attempts on achieving a robust model against these perturbations However making models robust to adversarially input perturbation comes with the cost of a drop in accuracy There have been many explanations regarding this drop in accuracy The recent line of work on double descent has shed some lights on this trade-off and show that unlike conventional under-parameterized regime in the new over-parameterized regime more data and fewer parameters might result in a worse error The closest work to us is Raghunathan et al where they show augmenting the training data with adversarially perturbed inputs can hurt the accuracy In contrast to their work we focused on removing a spurious feature with a known correlation with other features instead of an arbitrary data perturbation set for data augmentation As a result we were enabled to analyze the drop in accuracy in a more interpretable way and show how removing the spurious feature affects different groups There is also another difference between our work and work on robustness to input perturbation In our work by removing s we have a robust model independent of the data distribution However robustness to input perturbation always depends on the distribution CONCLUSION In this work we first showed that over-parameterized models are incentivized to use spurious features in order to fit the training data with a smaller norm Then we demonstrated how removing these spurious features altered the models assumptions on unseen data variations direction Theoretically and empirically we showed that this change in inductive bias could hurt the overall accuracy and affect groups disproportionately We then proved that robustness against spurious features or error reduction by removing the spurious features cannot be guaranteed under any condition of the target and spurious feature Consequently balanced datasets do not guarantee a robust model and practitioners should consider other features as well Studying the effect of removing noisy spurious features is an interesting future direction Reproducibility All code data and experiments for this paper are available on the CodaLab platform at