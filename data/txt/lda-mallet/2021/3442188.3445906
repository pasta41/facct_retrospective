Socially Fair k-Means Clustering We show that the popular k-means clustering algorithm Lloyds heuristic used for a variety of scientific data can result in outcomes that are unfavorable to subgroups of data eg demographic groups Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups The algorithm Fair-Lloyd is a modification of Lloyds heuristic for k-means inheriting its simplicity efficiency and stability In comparison with standard Lloyds we find that on benchmark datasets Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering while incurring a negligible increase in running time thus making it a viable fair option wherever k-means is currently used INTRODUCTION Clustering or partitioning data into dissimilar groups of similar items is a core technique for data analysis Perhaps the most widely used clustering algorithm is Lloyds k-means heuristic Lloyds algorithm starts with a random set of points centers and repeats the following two-step procedure a assign each data point to its nearest center this partitions the data into disjoint groups clusters b for each cluster set the new center to be the average of all its points Due to its simplicity and generality the k-means heuristic is widely used across the sciences with applications spanning genetics image segmentation grouping search results and news aggregation crime-hot-spot detection crime pattern analysis profiling road accident hot spots and market segmentation Lloyds algorithm is a heuristic to minimize the k-means objective choose centers such that the average squared distance of a point to its closest center is minimized Note that these centers automatically define a clustering of the data simply by assigning each point to its closest center To better describe the k-means objective and the Lloyds algorithm in the context of human-centric applications let us consider two applications In crime mapping and crime pattern analysis law enforcement would run Lloyds algorithm to partition areas of crime This partitioning is then used as a guideline for allocating patrol services to each area cluster Such an assignment reduces the average response time of patrol units to crime incidents A second application is market segmentation where a pool of customers is partitioned using Lloyds algorithm and for each cluster based on the customer profile of the center of that cluster a certain set of services or advertisements is assigned to the customers in that cluster In such human-centric applications using the k-means algorithm in its original form can result in unfavorable and even harmful outcomes towards some demographic groups in the data To illustrate bias consider the Adult dataset from the UCI repository This dataset consists of census information of individuals including some sensitive attributes such as whether the individuals self identified as male or female Lloyds algorithm can be executed on this dataset to detect communities and eventually summarize communities with their centers Figure a shows the average k-means clustering cost for the Adult dataset for males vs females The standard Lloyds algorithm results in a clustering which incurs up to higher cost for females compared to males Figure b shows that this bias is even more noticeable among the five different racial groups in this dataset The average cost for an Asian-Pac-Islander individual is up to times worse than an average cost for a white individual A similar bias can be observed in the Credit dataset between lower-educated and higher-educated individuals Figure c In this paper we address the critical goal of fair clustering ie a clustering whose cost is more equitable for different groups This is of course an important and natural goal and there has been substantial work on fair clustering including for the k-means objective Prior work has focused almost exclusively on proportionality ie ensuring that sensitive attributes are distributed proportionally in each cluster In many application scenarios including the ones illustrated above one can view each setting of a sensitive attribute as defining a subgroup eg gender or race and the critical objective is the cost of the clustering for each subgroup are one or more groups incurring a significantly higher average cost In light of this consideration we consider a different objective Rather than minimizing the average clustering cost over the entire dataset the objective of socially fair k-means is to find a k-clustering that minimizes the maximum of the average clustering cost across different demographic groups ie minimizes the maximum of the average k-means objective applied to each group a Adult census dataset b Adult census dataset c Credit dataset Figure The standard Lloyds algorithm results in a significant gap in the average clustering costs of different subgroups of the data Can social fairness be achieved efficiently while preserving the simplicity and generality of standard k-means algorithm Applying existing algorithms for fair clustering with proportionality constraints leads to poor solutions for social fairness see Figure for comparison on standard datasets so we need a different solution Our objective is similar to the recent line of work on min/max fairness through multi-criteria optimization Our results We answer the above question affirmatively with an algorithm we call Fair-Lloyd Similar to Lloyds algorithm it is a two-step iteration with the only difference being how the centers are updated a assign each data point to its nearest center to form clusters b choose new fair centers such that the maximum average clustering cost across different demographic groups is minimized This step is particularly easy for k-means average the points in each cluster We prove that the fair centers can also be computed efficiently using a simple one-dimensional line search when the data consists of two demographic groups and using standard convex optimization algorithms when the data consists of more than two groups Furthermore when the data consists of two groups the convergence of our algorithm is independent of the original dimension of the data and the number of clusters We prove convergence stability and approximability guarantees and apply our method to multiple real-world clustering tasks The results show clearly that Fair-Lloyd generates a clustering of the data with equal average clustering cost for individuals in different demographic groups Moreover its computational cost remains comparable to Lloyds method Each iteration to find the next set of centers is a convex optimization problem and can be implemented efficiently using Gradient Descent For two groups we give a line-search method which is significantly faster This extends to a fast heuristic for m groups whose distance to optimality can be tracked This approach might be of independent interest as a very efficient heuristic for similar optimization problems Due to the simplicity and efficiency of the Fair-Lloyd algorithm we suggest it as an alternative to the standard Lloyds algorithm in human-centric and other subgroup-sensitive applications where social fairness is a priority Fair k-means Objective and Algorithm To introduce the fair k-means objective we define a more general notion the k-means cost of a set of points u with respect to a set of centers C c and a partition u U Uk is i p For a set of centers c let be a partition such that if p then p min p c Then the standard k-means objective is min Cc ie to find a set of centers C c that minimizes For an illustrative example of the potential bias for different subgroups of data see Figure left The two centers selected by minimizing the k-means objective are both close to one subgroup and therefore the other subgroup has higher average cost Note that the notion of fairness based on proportionality also prefers this clustering which imposes a higher average cost on the purple subgroup To introduce our fair k-means objective and algorithm in this section we focus on the case of two demographic groups In Section we discuss how to generalize our framework to more than two groups k-means Fair k-means Figure Two demographic groups are shown with blue and purple The -means objective minimizing the average clustering cost prefers the clustering and centers shown in the left figure This clustering incurs a much higher average clustering cost for purple than for blue The clustering in the right figure has more equitable clustering cost for the two groups The fair k-means objective for two groups AB such that U A B is the larger average cost max A A B B where U A U A Uk A The goal of fair k-means is to minimize so as to minimize the higher average cost As illustrated in Figure right minimizing this objective results in a set of centers with equal average cost to individuals of different groups In fact as we will soon see the solution to this problem equalizes the average cost of both groups in most cases Next we present the fair k-means algorithm or Fair-Lloyd in Algorithm Algorithm Fair-Lloyd Input A set of points u A B and N Initialize the set of centers C c repeat Assign each point to its nearest center in C to form a partition U U Uk Pick a set of centers C that minimizes U C Line search u U until convergence return C c The second step of each iteration uses a minimization procedure to assign centers fairly to a given partition of the data While this can be done via a gradient descent algorithm we show in Section that it can be solved very efficiently using a simple line search procedure see Algorithm due to the structure of fair centers Section In Section we discuss some other properties of the fair k-means and Fair-Lloyd Algorithm More specifically we discuss the stability of the solution found by Fair-Lloyd the convergence of Fair-Lloyd and approximation algorithms that can be used for fair k-means eg to initialize the centers In summary our fair version of the k-means inherits its attractive properties while making the objective and outcome more equitable to subgroups of data Related Work k-means objective and Lloyds algorithm The k-means objective is NP-hard to optimize and even NP-hard to approximate within a factor of The best known approximation algorithm for the k-means problem finds a solution within a factor of optimal where The running time of Lloyds algorithm can be exponential even on the plane As for the quality of the solution found Lloyds heuristic converges to a local optimum with no worst-case guarantees possible It has been shown that under certain assumptions on the existence of a sufficiently good clustering this heuristic recovers a ground truth clustering and achieves a near-optimal solution to the k-means objective function For all the difficulties with the analysis and although many other techniques has been proposed over the years Lloyds algorithm is still the most widely used clustering algorithm in practice Fairness During the past years machine learning has seen a huge body of work on fairness Many formulations of fairness have been proposed for supervised learning and specifically for classification tasks The study of the implications of bias in unsupervised learning started more recently We refer the reader to for a summary of proposed definitions and algorithmic advances Majority of the literature on fair clustering have focused on the proportionality/balance of the demographical representation inside the clusters a notion much in the nature of the widely known disparate impact doctrine Proportionality of demographical representation has initially been studied for the k-center and k-median problems when the data comprises of two demographic groups and later on for the k-means problem and for multiple demographic groups Among other notions of fairness in clustering one could mention proportionality of demographical representation in the set of cluster centers or in large subsets of a cluster Our proposed notion of a fair clustering is different and comes from a broader viewpoint on fairness aiming to enforce any objective-based optimization task to output a solution with equitable objective value for different demographic groups Such an objective-based fairness notion across subgroups could be defined subjectively eg by equalizing misclassification rate in classification tasks or by minimizing the maximum error in dimensionality reduction or classification We define a socially fair clustering as the one that minimizes the maximum average clustering cost over different demographic groups To the best of our knowledge our work is the first to study fairness in clustering from this viewpoint AN EFFICIENT IMPLEMENTATION OF FAIR k-MEANS The Fair-Lloyd algorithm Algorithm is a two-step iteration where the second step is to find a fair set of centers with respect to a partition A set of centers C is fair with respect to a partition u if C argmin C In this section we show that a simple line search algorithm can be used to find C efficiently Structure of Fair Centers We start by illustrating some properties of fair centers A partition of the data induces a partition of each of the two groups and hence a set of means for each group Formally for a set of points u AB and a partition u U Uk let and be the mean of A and B respectively for i Our first observation is that the fair center of each cluster must be on the line segment between the means of the groups induced in the cluster Lemma Let U A B and U U Uk be a partition of U Let C c be a fair set of centers with respect Then is on the line segment connecting and Proof For the sake of contradiction assume that there exists an i such that is not on the line segment connecting and Note that see or Lemma in the Appendix for a proof of the following equation p p A A i p p B B i Let c i be the projection of to the line segment connecting and Then by Pythagorean theorem for convex sets we have c i c i c i c i Therefore since c i we have c i A i and c i B i Thus replacing with c i decreases the fair k-means objective The above lemma implies that in order to find a fair set of centers we only need to search the intervals B i Therefore we can find a fair set of centers by solving a convex program The following definition will be convenient Definition Given U AB and a partition u U Uk for i let A A B B and li B i Also let MA B Since B i is a line segment for the ith cluster we only need to find the distance xi of its center from Then the distance of the center from is li xi where li is the length of the line segment B i By the average cost of group A with respect to a partition u in terms of xi s i p A i A i A A i A A A i Similarly the average cost of group B is MB U B B i li xi Hence our goal is to solve the following optimization problem max A A i MB U B B i li xi subject to xi li i Note that this is a convex program because the maximum of two convex functions is a convex function One can see that the point on the line segment B i that has distance xi from is xi B i li where li is the length of B i Using a standard trick to write the objective function as a linear function we can state the problem as the following Corollary U Uk be a partition AB Then C c is a fair set of centers with respect to U if i A i x i B i li where x is an optimal solution to the following convex program min st A A i MB U B B i li xi xi li i We can solve this convex program with standard convex optimization methods such as gradient descent However as we show in the next section we can solve it with a much faster algorithm Computing Fair Centers via Line Search We first need to review a couple of facts about subgradients For a convex continuous function we say that a vector u is a subgradient of at point x if y x uT y x for any y We denote the set of subgradients of at x by x Fact Let be a convex function Then point x is a minimum for if and only if x Fact Let fm be smooth functions and x max m fj x Let m fj x x Then the set of subgradients of at x is the convex hull of union of the subgradients of fj s at x for Let fAx A A i fB x MB U B B i li xi Then we can view the convex program as minimizing x max fAx fB x st xi li i Note that x is convex since the maximum of two convex functions is convex Therefore by Fact our goal is to find a point x such that x Note that fA and fB are differentiable Hence by Fact we only need to look at points x for which there exists a convex combination of fAx and fB x that is equal to As we will see this set of points is only a one-dimensional curve in l lk When fAx fB x x has a unique gradient and it is equal to fAx Similarly when fAx fB x we have x fB x By Fact in the case that fAx fB x for any x fB x is a subgradient of x and these are the only subgradients of at x Now consider the set Z x l lk x In words set Z is the set of all points for which there exist a convex combination of gradients of fA and fB that is equal to If we find x Z such that fAx fB x then any convex combination of fAx and fB x is a subgradient of at x and therefore x Hence x is an optimal solution We first describe Z and show that there exists an optimal solution in Z Lemma Let and x Then xi li Proof We have xi fAx and xi fB x xi li Using the fact that x we have xi li Hence xi li The previous lemma gives a complete description of set Z One example of set Z is shown in Figure left for the case of The following is an immediate result of Lemma Lemma Z x xi li Note that when this recovers the all-zero vector and when x l lk Therefore these extreme points are also in Z As we mentioned before if there exists x Z such that fAx fB x then x is an optimal solution Therefore suppose such an x does not exist One can see that d li li Therefore for i xi is decreasing in Also one can see that fA is increasing in xi and fB is decreasing in xi Therefore fA is decreasing in and fB is increasing in Figure right shows an example that illustrates the change of fA and fB with respect to This implies that if there does not exist any x Z such that fAx fB x then either fA fB or fB where l lk In the former case the optimal solution to is which means that the fair centers are located on the means of points for group A In the latter case the optimal solution is which means that the fair centers are located on the means of points for group B In these extreme cases there does not exist a set of centers with equal average cost with respect to the particular chosen partition of points The above argument asserts that we only need to search the set Z to find an optimal solution Each element of Z is uniquely determined by the corresponding Our goal is to find an element x Z such that fAx fB x Since fA is decreasing in and fB is increasing in we can use line search to find such a point in Z If such a point does not exist in Z then the line search converges to or Two steps of such a line search are shown in Figure See Algorithm for a precise description Using this line search algorithm we can solve the convex program described in to error in O i log li time Fair k-means is well-behaved In this section we discuss the stability convergence and approximability of Fair-Lloyd for groups As we will show in Section these results can be extended tom groups Stability The line search algorithm finds the optimal solution to This means that for a fixed partition of the points eg the last clustering that the algorithm outputs the returned centers are optimal in terms of the maximum average cost of the groups However one important question is whether we can improve the cost for the group with the smaller average cost The following proposition shows that this is not possible assuring that the solution is pareto optimal Algorithm Line search u U Input A set of points u A B and a partition U U Uk Compute B i li M A MB See Definition for t T do xi li for i Compute fAx and fB x if fAx fB x then else if fAx fB x then else break end end li xi A i xi B i li for all i return C c Proposition Let x be the optimal solution for a fixed partition U U Uk Then there does not exist any other optimal solution with an average cost better than fAx or fB x for groups A and B respectively Proof Let y be another optimal solution Without loss of generality suppose fAx fB x If fAx fB x then by our discussion on the line search algorithm x and it is the only optimal solution Therefore y x Now suppose fAx fB x For the sake of contradiction and without loss of generality assume fAx fAy but fB x fB y Therefore fAy fB y First note that y because if fA fB then for any other x in the feasible region fAx fB x which is a contradiction because fAx fB x Hence we can decrease one of the coordinates of y by a small amount to get a point y in the feasible region If the change is small enough we have fAy fAy fB y fB y but this is a contradiction because it implies x y y which means x was not an optimal solution Convergence Lloyds algorithm for the standard k-means problem converges to a solution in finite time essentially because the number of possible partitions is finite This also holds for the Fair-Lloyd algorithm for the fair k-means problem Note that for any fixed partition of the points our algorithm finds the optimal fair centers Also note that there are only a finite number of partitions of the points Therefore if our algorithm continues until a step where the clustering does not change afterward then we say that the algorithm has converged and indeed the solution is a local optimum However note that in the case where there is more than one way to assign points to the centers ie there exists a point that have more than one closest center then we should exhaust all the cases otherwise the output is not necessarily a local optimal This is not a surprise because the same condition also holds for the Lloyds algorithm for the k-means problem For example see Figure Adjacent points have unit distance from each other The centers are optimum for the illustrated clustering However they do not form a local optimum because moving c and c to the left by a small amount decreases the k-means objective from to Initialization An important consideration is how to initialize the centers While a random choice is often used in practice for Figure Left an example of the one-dimensional curve for Right the functions fA and fB with respect to and two steps of the line search algorithm We can use a line search to find the optimal value of and an optimal solution to c A A c c A A Figure An example of k-means problem where the current clustering is not a local optimal and we need to check all the possible partitions with the current centers c c c are the centers and the points are marked with the letter A on top of them the k-means algorithm another choice that has better provable guarantees is to use a set of centers with objective value that is within a constant factor of the minimum We will show that a c-approximation for the k-means problem implies a c-approximation for the fair k-means problem and so this method could be used to initialize centers for Fair-Lloyd as well The best known approximation algorithm for the k-means problem finds a solution within a factor of optimal where Theorem If the k-means problem admits a c-approximation in polynomial time then the fair k-means problem admits a c-approximation in polynomial time Proof Let A A B B This is basically the k-means objective when we consider a weight of A for the points in A and a weight of B for the points in B Let O be an optimal solution to and S be a c-approximation solution to ie Moreover max SUS A A SUS B B SUS A A SUS B B Hence Now let O be an optimal solution for Then O A A O B B max O A A O B B Also by optimality of O for we have Therefore This implies that US c GENERALIZATION TOm GROUPS Let U A Am Then the objective of fair k-means form demographic groups is to find a set of centers c that minimizes the following max A A Am Am Let U U Uk be a partition of U and i be the mean of ie the mean of members of subgroup in cluster i Then by a similar argument to Lemma one can conclude that for a fair set of centers C c with respect is in the convex hull of m i Then we can generalize the convex program in Equation tom demographic groups as the following min st M U i i i m i m i i where i and M In the same standard trick as for the case of two groups is used to make the objective function linear and M i i i is the average cost of group The set of s found by solving the above convex program will be a fair set of centers with respect We can solve this using standard convex optimization algorithms including gradient descent However similar to the case of two groups we can find a fair set of centers by searching a standard m simplex Namely we only need to search the following set to find a fair set of centers Z C c m i i i m The following notations will be convenient For C c and m let fj C M U i i i and C max m fj C Then the convex program represented in is equivalent to min C i m i i Similar to the case of two groups set Z is the set of points for which there exist a convex combination of the gradients of fj s that is equal to i i For a vector v let vs denote its component Then we have i d s u i s Theorem Any optimum solution of is in Z Proof We can view a set of centers as a point in a d dimensional space Let s i s d be the set of standard basis of this space Then we have d dei s fj C i s By Fact and we only need to show that set Z is the set of all points for which there exists a convex combinations of fC fm C that is equal to Let such t We want to find a C such that m C Therefore for each i s we have m d dei s fj C m i s m i s i s Thus s m i i i s and m i i i This shows that the set of centers that satisfy m C for some are exactly the members of Z Note that any element is identified by a point in the standard m simplex ie such that m However the function defined on the m-simplex is not necessarily convex Indeed as we will show in Figure in the Appendix it is not even quasi-convex Thus one can either use standard convex optimization algorithms to solve the original convex program in or other heuristics to only search the set Z For our experiments we use a variant of the multiplicative weights update algorithm on set Z see Algorithm To certify the optimality of the solution one can use Fact and show that is a subgradient However the iterative algorithms usually do not find the exact optimum but rather converge to the optimum solution To evaluate the distance of a solution from the optimum we propose a min/max theorem for set Z in Section This theorem allows us to certify that the solutions found by our heuristic in the experiments are within a distance of from the optimal Multiplicative Weights Update Heuristic Note that the original optimization problem given in is convex However we can use a heuristic to solve the problem in the space One such heuristic is the multiplicative weights update algorithm precisely defined as Algorithm Algorithm Multiplicative Weights Update Input Integers m and numbers i and centers for i and m and Mj for m m for m for t T do m i i for i C c Compute fj C for all m C max m fj C C fj C for m t max m Normalize s such that m end return C Certificate of Optimality Next we give a min/max theorem that can be used to find a lower bound for the optimum value Using this theorem we can certify that in practice the multiplicative weights update algorithm finds a solution very close to the optimum Theorem Let S m and C c m i i i m and S Then max C min S fj C min C max S fj C Moreover max C min S fj C min C max m fj C Proof Let CC and let be the corresponding parameters for CC respectively Note that Z Therefore m C Hence because for any S we have S C Hence S C C C Therefore there exists a S such that fj C C C Thus because fj is convex we have fj C fj C fj C C C fj C Therefore max S fj C min S fj C Note that this holds for any CC Z and this implies the first part of the theorem Let C be the optimum solution to min max S fj C Note that by Theorem this is an optimum solution to the problem of finding a fair set of centers for the groups in S Moreover for any set of centers c outside the convex hull of the centers of groups in S we have max S fj C max S fj C proof of this is similar to Lemma Hence max S fj C min max S fj C Thus we have max C min S fj C min C max S fj C max S fj C min C max S fj C min C max m fj C Note that we can use Theorem to get a lower bound on the optimum solution of the convex program in For example suppose C is a solution returned by a heuristic Then min m fj C max C min m fj C min C max m fj C Therefore min m fj C is a lower bound for the optimum solution Hence the difference of the solution returned by the heuristic with the optimum solution is at most max m fj C min m fj C This will be very useful for the case where fj C C for all m where C is the optimum solution The reason is that in this case Theorem implies max C min m fj C min max m fj C However this might not be the case and we might have fj C C for some In this case we can use S m For example an S that gives a larger lower bound and for which max C min S fj C min max S fj C Stability and Approximability We conclude this section by a discussion on the stability and the approximability of fair k-means form groups Our stability results generalizes tom demographic groups Let C c be an optimal solution and S m Also let fj C maxi m C for S and fj C maxi m C for S Then one can see that for all i i S This uniquely determines the location of the optimal solution and thus we cannot improve the value of functions fj where S Moreover with an argument similar to Proposition one can deduce that we cannot improve the value of functions fj where S Moreover the Fair-Lloyd algorithm form demographic groups converges to a solution in finite time essentially because the number of possible partitions of points is finite Finally if the k-means problem admits a c-approximation then the fair k-means problem form demographic groups admits anmc-approximation the proof is similar to the proof of Theorem EXPERIMENTAL EVALUATION We consider a clustering to be fair if it has equal clustering costs across different groups We compare the average clustering cost for different demographic groups on multiple benchmark datasets using Lloyds algorithm and Fair-Lloyd algorithm Code is accessible here We used three datasets Adult dataset consists of records of individuals collected from census data with features The demographic groups considered are female/male for the group setting and five racial groups of Amer-Indian-Eskim Asian-Pac-Islander Black White and Other for the multiple-groups setting Labeled faces in the wild LFW dataset consists of images of celebrities The size of each image is or a vector of dimension The demographic groups are female/male and Credit dataset consists of records of individuals with features We divided the multi-categorical education attribute to higher educated and lower educated and used these as the demographic groups As different features in any dataset have different units of measurements eg age versus income it is standard practice to normalize each attribute to have mean and variance We also converted any categorical attribute to numerical ones For both Lloyds and Fair-Lloyd we tried different center initialization each with iterations We used random initial centers starting both algorithms with the same centers in each run For clustering high-dimensional datasets with k-means Principal Component Analysis PCA is often used as a pre-processing step reducing the dimension to We evaluate Fair-Lloyd both with and without PCA Since PCA itself could induce representational bias towards one of the demographic groups Fair-PCA has been shown to be an unbiased alternative and we use it as a third pre-processing option We refer to these three pre-processing choices as w/o PCA w/ PCA and w/ Fair-PCA respectively Results Figure shows the average clustering cost for different demographic groups In the first row all datasets are evaluated in their original dimension with no pre-processing applied w/o PCA In the second and third rows w/ PCA and w/ Fair-PCA the PCA/Fair-PCA dimension is equal to the target number of clusters Our first observation is that the standard Lloyds algorithm results in a significant gap between the clustering cost of individuals in different groups with higher clustering cost for females in the Adult and LFW datasets and for lower-educated individuals in the Credit dataset The average clustering cost of a female is up to higher than a male in the Adult LFW dataset when using standard Lloyds A similar bias is observed in the Credit dataset where Lloyds leads up to higher average cost for a lower-educated individual compared to a higher-educated individual Our second observation is that the Fair-Lloyd algorithm effectively eliminates this bias by outputting a clustering with equal clustering costs for individuals in different demographic groups More precisely for the Credit and Adult datasets the average costs of two demographic groups are identical represented by the yellow line in Figure For the LFW dataset we observe a very small difference in the average clustering cost over the two groups in the fair clustering and difference for without PCA with PCA and with Fair-PCA respectively Notably Fair-Lloyd mitigates the bias of the output clustering independent of whether it is applied on the original data space on the PCA space or on the Fair-PCA space In Figure we show a snapshot of performance of Fair-Lloyd versus Lloyds on the Adult dataset for all three different pre-processing choices Figure shows the maximum ratio of average cost between any two racial groups in the Adult dataset which comprised of five racial groups Amer-Indian-Eskim Asian-Pac-Islander Black White and Other Note that the max cost ratio of one indicates that all groups have the same average cost in the output clustering As we observe the standard Lloyd algorithm results in a significant gap between the cost of different groups resulting in a high max cost ratio overall As for the Fair-Lloyd algorithm as the number of clusters increases it outputs a clustering of the data with same average cost for all the demographic groups The price of fairness Does requiring fairness come at a price in terms of either running time or overall k-means cost Figure LFW dataset Adult dataset Credit dataset Figure Average clustering cost of different groups when using Fair-Lloyd algorithm versus the standard Lloyds Rows correspond to different pre-processing methods and columns to the datasets Note that the fair clustering costs for the two groups are identical or nearly identical in all datasets shows the running time of Lloyds versus Fair-Lloyd for iterations Running time for all three datasets is measured in the k-dimensional PCA space where is the number of clusters As we observe Fair-Lloyd incurs a very small overhead in the running time with only and increase on average over for the Adult Credit and LFW dataset respectively Moreover as illustrated in Figure the convergence rate of Lloyd and Fair-lloyd are essentially the same in practice Finally the increase in the standard k-means cost of Fair-Lloyd solutions averaged over the entire population was at most and for the LFW Adult Figure Adult dataset The maximum ratio of average clustering cost between any two racial groups Amer-Indian-Eskim Asian-Pac-Islander Black White and Other and Credit datasets respectively Arguably this is outweighed by the benefit of equal cost to the two groups Socially fair versus proportionally fair The first introduced notion of fairness for k-means clustering considered the proportionality of the sensitive attributes in each cluster We emphasize that improving the proportionality is at odds with improving the maximum average cost of the groups This can be seen in Figure To illustrate this more we compared our method to one of the proposed methods that guarantees the proportionality of the clusters Figure Adult dataset comparison of the standard Lloyds and Fair-Lloyd algorithm for the three different pre-processing choices of w/o PCA w/ PCA and w/ Fair-PCA LFW dataset Adult dataset Credit dataset Figure Running time seconds of Fair-Lloyd algorithm versus the standard Lloyds algorithm on the k-dimensional PCA space for iterations LFW dataset Adult dataset Credit dataset Figure Convergence rate of Fair-Lloyd algorithm versus the standard Lloyds algorithm for The plotted objective value for the standard Lloyd is the average cost of clustering over the whole population and the objective value for Fair-Lloyd is the maximum average cost of the demographic groups The reported objective values are averaged over runs and the shaded areas are the standard deviations Credit Adult Figure Comparison of socially fair k-means Fair-Lloyd to proportionally fair k-means Fair let on the Credit and Adult dataset in terms of proportionality and clustering cost on the credit and adult datasets We used the code provided in As illustrated in Figure the proportionally fair method fails to achieve an equal average cost for different populations and our methods do not achieve proportionally fair clusters DISCUSSION Fairness is an increasingly important consideration for Machine Learning including classification and clustering Our work shows that the most popular clustering method Lloyds algorithm can be made fair in terms of average cost to each subgroup with minimal increase in the running time or the overall average k-means cost while maintaining its simplicity generality and stability Previous work on fair clustering focused on proportional representation of sensitive attributes within clusters while we optimize the maximum cost to subgroups As Figure suggests and Figure shows on benchmark data sets these criteria lead to different solutions We believe that both perspectives are important and the choice of which clustering to use will depend on the context and application eg proportional representation might be paramount for partitioning electoral precincts while minimizing cost for every subgroup is crucial for resource allocation