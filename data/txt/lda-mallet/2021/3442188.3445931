I agree with the decision but they didnt deserve this Future Developers Perception of Fairness in Algorithmic Decisions While professionals are increasingly relying on algorithmic systems for making a decision on some occasions algorithmic decisions may be perceived as biased or not just Prior work has looked into the perception of algorithmic decision-making from the users point of view In this work we investigate how students in fields adjacent to algorithm development perceive algorithmic decisionmaking Participants N were asked to rate their agreement with statements regarding six constructs that are related to facets of fairness and justice in algorithmic decision-making in three separate scenarios Two of the three scenarios were independent of each other while the third scenario presented three different outcomes of the same algorithmic system demonstrating perception changes triggered by different outputs Quantitative analysis indicates that agreeing with a decision does not mean the person deserves the outcome perceiving the factors used in the decision-making as appropriate does not make the decision of the system fair and perceiving a systems decision as not fair is affecting the participants trust in the system In addition participants found proportional distribution of benefits more fair than other approaches Qualitative analysis provides further insights into that information the participants find essential to judge and understand an algorithmic decision-making systems fairness Finally the level of academic education has a role to play in the perception of fairness and justice in algorithmic decision-making CONCEPTS Human-centered computing Empirical studies in HCI Also with the Research Centre on Interactive Media Smart Systems and Emerging Technologies Nicosia Cyprus INTRODUCTION Algorithmic decision-making is widely used for contributing to decisions affecting peoples lives Job hiring healthcare education finance and criminal justice are just a few of the examples where algorithms are taking on what previously were human decision-making tasks An algorithm is even deciding on the posts and news people will see on social media While the use of algorithmic decision-making has prospects to make decision-making more efficient and reliable concerns have been raised about the fairness and justice of such decisions Algorithmic decision-making systems do not always behave as they should making decisions that may discriminate against certain groups of people There are many examples in different domains that show the misbehavior of these systems gender discrimination has been detected in a recruitment system for reviewing and ranking applicants resumes and in resume search engines auto-complete search terms can produce suggested terms which could be viewed as racist sexist or homophobic image search results are gender-biased depending on the search term used and racially-biased towards Black individuals There has been an increasing focus in the research community from various disciplines on promoting and understanding fairness in algorithmic decision-making While much effort has been devoted to developing frameworks of fairness and algorithmic models to alleviate biases there is a need to understand how algorithmic fairness is perceived by people wwwpropublicaorg/article/machine-bias-risk-assessments-in-criminal-sentencing While related work has looked into how the end users and/or the general public perceive algorithmic fairness it is important to understand how the people who are developing or will soon be involved in developing algorithmic decision-making systems perceive algorithmic fairness To our knowledge perception of fairness in algorithmic decision-making of students from fields adjacent to computing has not been studied previously To explore how future developers perceive algorithmic fairness we conducted an online survey with students in fields adjacent to algorithm development who will potentially be involved in the development of an algorithmic decision-making system We presented participants with three scenarios of algorithmic decision-making systems describing different contexts and asked them to indicate their agreement regarding six statements related to the fairness and justice constructs We then analyzed their responses in order to understand the interplay between these constructs in relation to the scenarios Two of the three scenarios their contents independent of each other were used to trigger the participants judgement on the use of particular factors used for decision-making In the third scenario participants were presented with the description of a single system and three different cases of algorithmic decision The purpose of this scenario was to examine whether participants perception was affected when presented with different outputs Our findings indicate that even when participants agreed with the decision made by the algorithm they did not believe that the person in the scenario deserved the outcome Moreover even when factors used in the decision-making were perceived as appropriate the overall process followed by the system was perceived as non-fair by the participants In addition systems that were perceived as not fair affected participants trust in the system In the third scenario participants show a preference for the proportional ratio decision as compared to the other two decisions giving all the money to one candidate splitting the money equally Our results suggest that the level of education can change participants understating of the process their agreement with the decision and appropriateness of certain factors used by the algorithm Finally qualitative analysis shows that future developers in order to judge fairness of a given algorithmic system find it essential to know more information about the Factors used and the Process followed in the decision-making and whether Sensitive Attributes eg age race gender were used in the decision Overall our findings note the complexity of understanding perceptions of fairness in algorithmic decision-making even from the developers perspective BACKGROUND With the increasing use of algorithms for making or supporting managerial decisions those that humans used to make researchers need to understand how not only users but also developers perceive these algorithms Regardless of an algorithms performance in terms of correctness and accuracy the perception people have of these algorithms can influence their adoption There is a growing body of work looking into perception of algorithmic decision-making fairness perception and trust from the end-user point of view However an equally important and interesting aspect is how developers of such algorithms perceive algorithmic decision-making and in extent fairness of algorithmic decisions Fairness in Algorithmic Decision Making Fairness is a complex construct consisting of several parameters that are considered by individuals when they are trying to define fairness in different contexts There is a lot of work on fairness and justice constructs in the psychology literature see for a comprehensive review With regards to algorithmic fairness in decision-making recent work in HCI and FAccT has looked into perceived fairness only to find that people understand fairness differently and according to the context where the system is operating Perceived fairness is multi-dimensional and there is a lack of consensus on which features are perceived as unfair by different people Thus usually scenario-based studies are employed in order to provide study participants a framing when asked to define fairness Studies revealed contradicting results as to what is perceived as fair People perceive algorithmic decision-making as less fair than human decision-making even when the decision requires human skills or more fair in other contexts like school admissions Algorithms and systems should consider social and altruistic behavior in order to be considered as fair elements that may be difficult to incorporate in mathematical modelling People tend to rate models as unfair when they consider them biased and vice versa and prefer human decision-making even if they consider the algorithmic model as fair or unbiased Accuracy was rated as more important than equality in with demographic parity best represented peoples understanding of fairness Certain attributes are not considered fair when used in defining the outcome of a system in a certain context suggesting that the use of features and attributes upon which decisions are made are context-dependent as well as output-dependent and can be perceived as fair or unfair accordingly As users are becoming more aware of the concept of algorithmic fairness they are starting to worry about potential biases in the decision as well as in the data or the algorithm interaction They seek more information about how different factors weighted in the decision and whether an algorithm uses sensitive attributes such as race or gender Variables such as the computer literacy and favourable outcome as well as development procedures of the system have also been proven to correlate with the perception of algorithmic fairness In particular people rate the algorithm as more fair when the decision is in their favour irrespective of whether it appears to be biased towards certain social groups In the same vein Pierson showed that there are gender differences in perceptions of algorithmic fairness while demographic differences contribute to the variability of opinions on fairness Education and training on algorithmic fairness appears to have an effect on students whose perception of fairness changed after an hour-long lecture and discussion on algorithmic fairness However in order for algorithms to become more fair developers need to be educated and become aware of the potential biases and discrimination that can occur due to the algorithms they develop They need to be in a position to understand the source of bias and perform the right steps to overcome it The challenge developers have to face though is that with the use of machine learning ML approaches becoming more and more popular as a driving tool These terms are usually used interchangeably in the literature in algorithmic decision-making it can get even more difficult to trace the source of bias in the system Hutchinson and Mitchell in their review of years of work on unfairness suggest that current and future work on ML should be informed by prior work and findings rather than trying to generally define a fair model They urge ML researchers to look into questions that are deeper define criteria that are context- and use-dependent and question whether all subgroup dimensions eg gender age can be served in one model or if a different approach might be required Thus in this study we are interested in examining how future developers perceive certain constructs linked to fairness in algorithmic systems and the interplay of those in specific scenarios Explanations in Decision Making Opacity of algorithmic decision-making and whether different transparency approaches might enhance the perceptions of fairness of those systems has also been a cause for discussion in the recent literature With ML models being exploited for predicting sensitive individual information classifying individuals in categories and providing decisions that were previously taken by humans there is a need for interpreting those models in a way that the user would understand In light of the recently drafted General Data Protection Regulation GDPR people who interact with systems that involve automated decision-making have a right to obtain meaningful explanations of the logic involved Hence there is a need for methods and approaches that will allow the user to understand the output of these opaque and automated processes in context At the same time the user should be able to understand the potential consequences of applying the decision in the real world Hence explanations should be informative and easy to be interpreted by the person they are created for Edwards and Veale argue that pedagogical approaches to explanation explanations that teach how the model works might be more promising for the general public than decompositional approaches breaking the model down with the risk of trading secrets and intellectual property breach Furthermore ML tools such as debiasing or transparency systems will also need to take into consideration the contextual challenges early on Explaining a systems decision though is not trivial Different level of explanation is needed according to the audience and the purpose especially for black-box models According to local explanations focus on explaining a particular output global explanations explain how a set of outputs emerges from a particular input and counterfactual explanations attempt to help the user understand how their input could change the output of the system by resembling everyday human conversation Studies with users however are inconclusive as to what type and level of explanation they prefer Binns et al ran an experiment using different explanation styles input influence sensitivity case-based demographics They found significant differences in justice perception between different explanation styles Particularly case-based explanations presenting a case from the models training data which is most similar to the decision affected the judgments of justice negatively compared to sensitivity-based explanations explaining how much the value of a variable used in the model affects the output However when people were exposed to the same explanation style in different scenarios they observed none of the above Rader et al found that explanations in any form help to create awareness of how the system works and understand potential bias in the systems output but offer little in evaluating the correctness of the output Explanations in group recommendations have been proven to improve the perception of fairness when all or the majority of group members preferences are taken into account emphasizing how fairness is subjective to each individual person The challenge of dealing with and explaining potentially harmful outputs has been demonstrated in several occasions Take as an example the Google photos incident where a Black American and his friend were mistakenly labeled by the system as gorillas After a two-year effort at Google to solve the problem the final solution was just a work-around of removing the label from their lexicon This demonstrates the difficulties that companies like Google and in extend their developers face in understanding and explaining possible unwanted decisions of their own ML-based systems Holstein et al provide some important insights on how developers are struggling to find a balance between fairness in their systems and providing a product for their companies They are calling for procedures processes and training on concepts related to fairness accountability transparency and ethics for developers who are already in the business Thus it is important for us in this work to understand how future developers deal with certain explanations provided in the scenarios they were given Furthermore we look into how different decisions change their perception of fairness and whether the context and the output also have an impact on their perceptions METHODOLOGY In order to understand how future developers perceive fairness in algorithmic decision-making we conducted an online survey that ran between September and May Scenarios Participants were presented with three scenarios where algorithms made decisions that influenced humans We selected contexts that our target population is familiar with Two of the three scenarios were used to trigger the participants judgement on the use of particular factors eg demographics considered for decision-making and explanations of the decision given In the third scenario three different decisions were presented with the purpose of examining whether participants perception changes according to different outcomes Scenario A car insurance companys premiums dynamically-priced based on personal details and driving behaviour This scenario was adopted from Binns et al Scenario Passengers on over-booked airline flights being automatically selected for re-routing Airline X is using a system for automatically selecting and re-routing passengers on over-booked flights based on the passengers marital status number of children the passenger has whether they are part of a group booking and their age and gender Based on the above information the system decided to re-route Frank who was single traveling alone was years of age male instead of Lisa who was single traveling alone years old female Scenario Applying for a personal financial loan This scenario was adapted from Saxena et al There are two candidates Person A and Person B they are identical in every way except their race and loan repayment rates Both of them have applied for a loan to start a business and the loan officer only has a Case A Taking into consideration the Gender Race and Individual loan repayment rate the system decided to split the money between the two candidates giving to Person A and to Person B Case B Taking into consideration the Gender Race and Individual loan repayment rate the system decided to give Person A which is proportional to that persons payback rate of and give Person B which is proportional to that persons payback Case C Taking into consideration the Gender Race and Individual loan repayment rate the system decided to give all the money to Person A For each scenario participants were asked to rate their agreement in five statements according to in addition to Trust A -point Likert scale ranging from Strongly Disagree to Strongly Agree was employed for each of the six statements S Agreement I agree with the decision S Understanding I understand the process by which the decision was made S Appropriateness of factors The factors considered in the decision were appropriate S Fair process The decision-making process was fair S Deserved outcome The individual deserved this outcome given their circumstances or behaviour S Trust I would trust this systems decision more than a humans decision Participants were also asked to explain using free-text Q Was the information provided in the above scenario sufficient Participants free-text responses were coded as Yes No Unsure Finally participants self-reported Yes/No/Other write-in whether they have taken Q any training/course on Fairness Accountability and Transparency in algorithmic systems and Q assessed their knowledge on Fairness in algorithmic decision-making systems using a -point Likert scale Not at all Very Knowledgeable Participants We recruited respondents using snowball sampling We emailed the survey to colleagues at other universities all over the world inviting them to pass the survey on to their students We also shared the survey on our social media accounts where the authors have a lot of computing-related students as connections We recruited undergraduate and postgraduate students from the fields related to Computer Science One participant was removed due to providing non-serious answers thus respondents were considered Participation was voluntary and all participants provided us with written informed consent for their data to be used The study has received ethical clearance by the Cyprus National Bioethics Committee of our respondents were male with in the age group of between between and above years old Most of the participants identified themselves as a postgraduate student and of that group were Masters students The rest of the participants were self-identified as undergraduate students of them being in their third or fourth year and being in their first or second year of studies The majority of the participants are enrolled in the following degree programs in Computer Science in Information Systems in Data Science in Machine Learning/Artificial Intelligence in Human-Computer Interaction/Human-Robot Interaction in Computer Science with Mathematics and in other programs The majority of participants are studying at institutions in Europe and the UK in the USA in Israel and in China Brazil and Australia QUANTITATIVE FINDINGS Table Descriptive Statistics for the variables used in the analysis Mean Std Deviation Agreement Understanding Appropriateness Fair Deserved Trust Quantitative analysis was employed in order to understand participants perception of each individual construct for Scenario and Scenario and to examine whether their perception changes if they are presented with the same scenario but a different algorithmic decision Scenario Do constructs Agreement Understanding Appropriateness Fair process Deserved Outcome and Trust correlate across different algorithmic decision-making scenarios Based on the literature on perceived fairness and justice and recent work on perceptions of algorithmic justice we expected to find correlations between all constructs To examine this we calculated Pearson correlations Descriptive statistics for all of the variables used in the Pearson correlations are available in Table Although we were expecting that all constructs will correlate similar to we were surprised to see that understanding of the process followed correlates with appropriateness of the factors and understanding of the process with deserved outcome see Table Perception and Interplay of Constructs To examine a number of hypotheses regarding participants perception of the Fairness constructs in Scenarios we run a series of Wilcoxon signed ranked tests Table Pearson Correlations for the six constructs of justice Agreement Understanding Appropriateness Fair Deserved Trust Agreement Pearson Correlation -tailed Understanding Pearson Correlation -tailed Appropriateness Pearson Correlation -tailed Fair Pearson Correlation -tailed Deserved Pearson Correlation -tailed Trust Pearson Correlation -tailed People who agreed with the decision also believe that the person in the scenario deserved the outcome We were expecting that the people who indicated agreement with the systems decision would also believe that the person in the scenario deserved the outcome Surprisingly we found significant statistical differences in their opinions Scenario z p Scenario z p In Scenario there was a considerable number of participants who selected options and on the Agreement scale while selected options and on the Deserved scale indicating that they agreed with the decision but the person in the scenario did not deserve the outcome In scenario fewer participants but still a considerable number selected options and on the Agreement Scale indicating they agree with the decision while selected options and on the Deserved scale People who found the factors used in the decision making process appropriate will also think that the decision making process is fair The results show significant differences between the responses of the participants in Scenario p with participants in their majority selected and on the scale for S reporting that the factors used in the decision-making were appropriate however they do not believe that the decisionmaking process was fair selected and on the scale for S In Scenario we do not have a statistical significant difference between the two scales where participants in their majority agree that the factors used in the decision making processes were not appropriate and believe that the decision making process was not fair Qualitative results see below show that in Scenario participants felt that the use of gender and age as factors to determine the decision were not appropriate which explains this result People who indicated the the decision making process was not fair would not trust this systems decision more than a humans decision For both Scenario and Scenario we did not get any significant differences between the two scales Specifically of the participants in Scenario and of the participants in Scenario believe the decision making process was not fair and of the participants in Scenario and of the participants in Scenario would not trust the systems decision more than a humans Next we wanted to examine whether the different decisions in Scenario Case A Case B and Case C affected participants perception of the above constructs Does the participants perception of Agreement Understanding Appropriateness Fair Process Deserved Outcome and Trust change according to the decision of the system given the same scenario To compare the responses in Scenario we followed a within-subject analysis using ANOVA repeated measures followed by a Bonferroni post-hoc test There were significant differences for Agreement p Appropriateness p Fairness p Deserved Outcome p and Trust p in responses provided by the participants Bonferroni posthoc tests showed that participants perceived the decision in Case B proportional outcome as the most just while the decision on Case C as the least Similarly comparing their responses in question Q in all three cases in Scenario we observed significant statistical differences p with the post-hoc test revealing that participants felt that the information provided in Case B was perceived as sufficient indicated sufficient information provided in Case B compared to in Case A and in Case C In all scenarios we did not find any differences in the participants responses between self-reported gender groups Previous training and self reported knowledge on topics related to algorithmic decision making did not have an impact on the responses of participants in our sample Differences between Undergraduate and Postgraduate Participants Since this study ran with undergraduate and postgraduate students in fields adjacent to algorithmic development it is natural to examine whether the participants level of education made a difference in their responses A series of Mann-Whitney U tests were run to determine if there were differences between the two groups Distributions of the engagement scores for undergraduates and postgraduates were similar as assessed by visual inspection in all cases Firstly we wanted to examine whether there is a difference between undergraduates and postgraduates in understanding the process by which the decision was made Scenario was the only scenario where statistical significant difference in understanding were found Median engagement score was moderately statistically significantly higher in postgraduates than in undergraduates U z p indicating that postgraduates understood the process that the system is following in making a decision better compared to undergraduates There was no significant difference between the two groups with respect to the other parameters Since we had indications from the previous analysis where the different cases in Scenario perceived differently we wanted to see whether there is a difference between undergraduates and postgraduates in the perception of sufficiency of information provided In Case A and Case C we did not find any significant differences between the two groups In Case B median engagement score was statistically significantly lower in undergraduates than in postgraduates U z p indicating that undergraduates find the information provided less sufficient in this case compared to postgraduates Furthermore we examined whether there is a difference in the agreement with the decision between undergraduates and postgraduates in our sample In Case A and Case B we did not find any statistically significant differences between the two groups For Case C median engagement score was statistically significantly lower in undergraduates than in postgraduates U z p indicating that undergraduates agreed less with the decision of the system compared to the postgraduates Following the same line of thought we examined whether there is a difference in the perception of appropriateness of the factors considered for the systems decision between undergraduates and postgraduates Similar to above in their responses regarding Case A and B we did not have any significant differences In Case C median engagement score was statistically significantly lower in undergraduates than in postgraduates U z p indicating that undergraduates considered the factors used in the system for making the decision less appropriate compared to the postgraduates Finally there is a marginal statistical difference between undergraduates and postgraduates in their indication of whether the decision-making process was fair in Case C Median engagement score was statistically significantly lower in undergraduates than in postgraduates U z p indicating that undergraduates considered the decision-making process less fair compared to the postgraduates QUALITATIVE FINDINGS For Q participants were asked whether they had sufficient information The free-text responses that simply stated a yes/no were excluded from the qualitative analysis To analyse participants free-text responses we used content analysis by coding responses for the themes mentioned in relation to the concepts in question Two researchers analyzed the responses independently to define emerging categories We allowed multiple categories per answer The categories identified by the two researchers were then compared the disagreements discussed and sometimes a dimensions definition amended to come to a final consensus Scenario participants elaborated on their response to Q for Scenario where six thematic areas emerged from their responses Table Most often participants discussed Missing Factors important factors about the situation that were not taken into consideration These included context of the day of accident time weather participant p road infrastructures p drivers attitude and her family history p and condition of the car p Interestingly some participants even mentioned the need to consider other factors even when they indicated they found the information sufficient of the participants referred to the Similar Cases on which the prompt said the decision was based Although the prompt explicitly stated twice that the decision was based on thousands of similar cases from the past and went on to give one similar case only as an example participants often remarked that a single example is not enough to adequately explain decisions p Some participants questioned the exact number of cases in the dataset p seemingly arguing what others explicitly stated If the data is quite large I think the decision is trustful p The third-most common theme was the decision-making Process with a total of responses Most participants wanted to know how much each factor contributed to the decision p some specifically asking for additional explanation on how age driving at night etc affects the probability of having an accident p A few participants wanted Specific Information which seemed to be missing from the scenario such as the criteria p or cost p of the cheapest tier as well as more examples of similar cases p These participants did not ask about other factors missing from the scenario but for the specific values of factors already mentioned The remaining themes received few responses Three participants mentioned the need to think about the Human/Company Policy of the scenario such as participant who said that a human being would able talk to the driver and better understand drivers attitude responses fell under the catch-all Other category which includes responses that do not mention the other themes or responses where the participant indicated they dont understand the question p Scenario In Scenario participants elaborated on their answer from which five thematic areas emerged Table The most often discussed theme was the Process of the decision-making appearing in almost half of the responses Similar to Scenario most of the responses wondered about what makes certain features less preferable than others p Other responses commented on specific elements such as age should factor more into the algorithm p even though the scenario description did not disclose how much each factor influenced the decision Table Themes emerged in Scenario Theme Description Missing Factors Not considering all the appropriate factors Similar cases Comparison with similar cases data used to train the model Process Procedures followed by the model features weights Specific information Specific value of a factor missing from the given scenario Human/Company policy Deferring to humans following companys policy Other falls outside of the established themes Table Themes emerged in Scenario Theme Description Process Procedures followed by the model features weights Factors Consideration of irrelevant factors and/or missing important factors Age Consideration of Age in the decision Gender Consideration of Gender in the decision Other falls outside of the established themes The second most common theme with responses consisted of responses discussing the Factors The vast majority of the responses asked about and offered other important factors p that the system should consider in this context such as health condition p p reason of their flight p p p p and disability status p p Some participants argued that the factors mentioned in the prompt were irrelevant to the scenario p A number of responses specifically mentioned Age and Gender in their responses with responses mentioning both While some participants disputed only the use of age p and gender p in such systems some argued that neither should be used to make such decisions p p Some referred to the law specifying that the use of factors such as gender and age is illegal p and breaks lots of UK laws p Interestingly one participant p discussed a personal experience similar to that of the scenario and argued that the decision should be based on the time the check-in was made sic responses fell under the catch-all Other category as they did not mention any of the other themes Scenario The three cases in Scenario were analysed together to compare the effect of the different outcomes on participants perceptions In addition to five main themes that emerged the responses in Cases B and C were also coded for whether the participant made references to their response to an earlier case see Table Case A had Case B had and Case C had responses that were analyzed In Case A the majority of the participants out of asked about Specific Information missing from the description of the given scenario however only participants out of in Case B and out of in Case C discussed this theme In Cases A and C most of the participants noted that they wanted to know the loan repayment rates of the individuals and how they differed eg p Case A Interestingly a few participants also wanted to know the specific loan repayment rate in Case B where the rate for one applicant was explicitly stated and the other implied via ratios The remaining responses for Cases A and C mainly focused on the race and gender of the applicants while only one participant mentioned them for Case B p Process was the second most common theme in Case A and the most common theme discussed by the participants in Case C but was mentioned in only responses out of for Case B These responses often noted that there was no information on the decision process Interestingly for Case B participants mentioned the proportional outcome as an indication of the calculation/reasoning of the algorithm in contrast with the other cases the outcome was a reason to question the process leading to the decision Some participants wondered about the influence of the different factors on the final decision one remarking that Yes I had sufficient information but as long as the parameters are awful the system is biased Other participants specifically asked about the role of gender and race in fact many of the participants discussing Process also discussed Race/Gender in Case A in Case B in Case C Race/Gender was the most common theme discussed in Case B and a popular theme in Case A and Case C While some responses simply questioned the role of race/gender in the decision-making process others argued that race and gender were not relevant to the decision and should not be taken into account Certain participants specifically said that the use of these features were illegal Less often participants made references to other missing Factors to be considered by the system in Case A in Case in Case C Among the factors mentioned were the applicants ability their job stability annual income or financial situation and the risks of the business they proposed One participant argued that the factors are not sufficient and that a human is needed to analyze the business proposal Overall responses in Case A in Case B in Case C fell under the catch-all Other category which includes responses that do fall under any of the other themes as well as responses Table Themes emerged in Scenario Case A Case B and Case C Theme Description A B C Specific information Specific value of a factor missing from the given scenario Process Procedures followed by the model features weights Race/Gender Consideration of race and/or gender in the decision Factors Consideration of irrelevant factors and/or missing important factors Other falls outside of the established themes Same as above Same answer as the previous cases where the participant indicated they dont really understand all the questions DISCUSSION We investigated the relationship between six constructs related to fairness and justice in algorithmic decision-making Agreement with the decision Understanding of the decision-making process Appropriateness of factors considered Fairness of the decision-making process whether the individual Deserved the outcome and Trust in the systems decision over a humans Although we expected all constructs to correlate with one another we found that specifically understanding of the process was highly correlated with factors used appropriately as well as with that the individuals in the scenario deserved the outcome Similar to previous work we found that factors are context- and output-dependent something that is also obvious in our qualitative analysis Surprisingly in the scenarios describing a car insurance premium Scenario and an airline re-routing Scenario decisions participants tended to both agree with the decision and believe that the person in the scenario did not deserve the outcome given their circumstances or behavior Considering that participants often noted that some important factors were missing this could imply that the participants find that while the calculations with the given information are accurate agreeing with the outcome the process needs to take into account other factors and therefore does not actually calculate the most just decision deserved outcome Participants generally find that some important factors were not considered highlighting the complexity of real life situations and the context-dependent nature of algorithms In both scenarios participants responses focused heavily on the factors that were involved in the decision making as well as the actual decision-making process followed However in Scenario statistical evidences show that although the participants found the factors used in the decision-making process as appropriate they did not believe that the decision-making process was fair In contrast in Scenario participants indicated in their majority that the factors used in the decision-making process were not appropriate and hence the process was not fair Qualitative results confirm that the participants found the use of some factors specifically age and gender inappropriate in Scenario so they were reluctant to believe the process was fair Our findings confirmed our expectations and are in line with that people who believed the decision-making process was not fair would also not trust the systems decision more than a humans decision Looking closer at the way different outcomes can affect the perception of fairness and justice in algorithmic decision-making systems our results for Scenario showed that dividing resources proportionally based on a factor considered relevant was perceived as more fair than dividing the resources equally which was still more fair than giving all resources to one individual over another Our finding aligned with where the ratio decision was found to be more fair than the equal decision supporting thus Liu et al calibrated fairness instead of the treating similar people in a similar way approach The lack of information/explanations provided in the scenarios prompted participants to comment on those heavily Sometimes participants did not necessarily think other factors were required but instead needed specific information from a factor already mentioned in the scenario For example nearly half of the participants asked about the specific gender race and/or loan repayment rate of the individuals in Scenario Case A Fewer people but still a notable amount asked for similar information in the other cases of Scenario as well as in Scenario Scenario in contrast disclosed the specific age gender and other details about the individuals in the scenario accordingly participants did not ask for further specific information about the individuals but instead focused on the generalized more abstract discussion of the use of gender and age as factors This implies that participants can judge the process the decision and their fairness better if the specific information eg gender of the individuals involved are disclosed Therefore for developers to enable full judgement of an algorithmic process it may not be enough to give information about the process in the abstract but provide concrete details about the cases involved However this may also introduce the human bias we are trying to minimize by automating Another very common theme discussed was the process or reasoning of the algorithm appearing often in every scenario Participants often asked about the weight of the factors/features in general or stated that one factor often gender race or age should/should not have more influence than the others It seems that our sample hence developers are in favour of a decompositional approach to explanations which requires more specific information about the system the process the weights etc rather than pedagogical approaches that might be more suitable for end users As was expected we have found differences related to the participants level of academic education Postgraduate students appeared to understand the decision-making process in Scenario more than undergraduate students did but questioned whether sufficient information was provided Postgraduates in their majority replied that they were not sure whether the information provided for this scenario was sufficient while undergraduates in their majority replied positively This shows the experience that postgraduates students have over undergraduates especially considering their comments My answer is no in my opinion information should also include the health condition of certain drivers which were involved in the research p postgraduate or No they lack psychosocial characteristics such as driving attitude eg I like speed when I can not respect the signals etc or the ability to react to stress eg if I had a bad day or Im late change my driving style Finally at the level of behavior monitoring driving routines could be established on a weekly basis considering both the driving environment eg motorways vs busy areas and the timing p postgraduate Clearly postgraduate students understood the system well enough to be able to challenge the factors values/weights and the model overall In the scenario for different loan distributions Case B where the loan repayment rate for the two individuals was disclosed was thought to have sufficient information by postgraduates in contrast undergraduates tended to think more information should be provided Similar to Scenario this could be a result of the postgraduate students being more familiar with the type of decision-making systems using proportionality Three more differences were observed between these groups in the case where all the loan amount was given to one individual Postgraduates tended to agree with the decision of the system find the factors used appropriate and find the decision process fair more so than the undergraduates Clearly education has a great role to play in affecting the development of fair algorithmic decision-making systems According also to previous work there is a need for incorporating seminars modules and training courses in the computing related degrees as well as professional training courses for recently graduated practitioners Pierson et al reported evidences of statistically significant changes in perception and attitudes of students towards algorithmic fairness and transparency after just an hour of lecture and discussion Thus in order for future algorithmic decision-making systems to be fair we need to ensure that the people developing them are aware of concepts related to Fairness Accountability Transparency and Ethics in algorithmic systems They also need to be aware that the systems they are developing have an impact positive or negative to the society Limitations It is important to note that this study as any empirical study faced several limitations The numbers in the thematic analysis indicate only whether a theme was mentioned or not within a response Themes were mentioned in many ways from information that should have been included in the scenario to concepts positively or negatively affecting fairness in algorithmic systems to personal subjective opinions In contrast to previous work the participants gender and previous training/self-reported knowledge on algorithmic fairness did not appear to affect the individuals perception of the constructs we were examining This can be due to the limited number of participants in our study and should be taken into account when one interprets this result Also due to the initial goals of the survey design our data does not include race information about the participants and therefore the dimension of race is missing from our analysis of the participants perceptions Finally the number of the participants N allowed us to run some quantitative analysis over the collected data however the reader should take into account that this number is relatively small the subjects were students in degrees with varying distance from algorithmic development and they were from a limited set of countries so our findings may not be representative of the general public CONCLUDING REMARKS Algorithmic decision-making systems are becoming very popular prompting us to rely more and more on their decisions with potentially serious consequences for the affected social groups Developers have an important role to play when they are called to develop algorithms that will drive these decisions Algorithmic fairness might be a first step in understanding how people perceive and assess the decisions and the explanations provided Most importantly we need to understand how developers perceive fairness in the systems they develop which will potentially decide on behalf of a human and in some occasions for matters with real social impact This paper provides some insights on how future developers perceive algorithmic fairness in algorithmic decision-making It suggests that their level of academic education has a role to play in their understanding of the decision-making process as well as their critical thinking on the factors and the decision-making process involved Factors that are employed are context- and output-dependent and appropriate factors might not presuppose the fairness of the decision-making process Future developers in our sample were in favour of a ratio decision rather than the others provided We hope that this work will act as a starting point for understanding the concept of fairness from the developers perspective instead of the user/person affected in order to inform policies procedures and guidelines for the respective industry