Mitigating Bias in Set Selection with Noisy Protected Attributes Subset selection algorithms are ubiquitous in AI-driven applications including online recruiting portals and image search engines so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race Currently fair subset selection algorithms assume that the protected attributes are known as part of the dataset However protected attributes may be noisy due to errors during data collection or if they are imputed as is often the case in real-world settings While a wide body of work addresses the effect of noise on the performance of machine learning algorithms its effect on fairness remains largely unexamined We find that in the presence of noisy protected attributes in attempting to increase fairness without considering noise one can in fact decrease the fairness of the result Towards addressing this we consider an existing noise model in which there is probabilistic information about the protected attributes eg and ask is fair selection possible under noisy conditions We formulate a denoised selection problem which functions for a large class of fairness metrics given the desired fairness goal the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability Although this denoised problem turns out to be NP-hard we give a linear-programming based approximation algorithm for it We evaluate this approach on both synthetic and real-world datasets Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes and compared to prior noise-oblivious approaches has better Pareto-tradeoffs between utility and fairness INTRODUCTION The subset selection problem arises in various contexts including online job portals where an algorithm shortlists candidates to show to the recruiter university admissions where a panel admits a subset of students and online search where the platform selects a subset of the results in response to a user query The basic problem is as follows There items and each item i m has a utility ie the value it adds to the subset The goal is to select a subset of n m items which has the largest total utility Given the pervasiveness of subset selection tasks it is crucial to ensure that subset selection algorithms do not propagate social biases Consequently there has been extensive work on developing fair algorithms for selection and for the related problem of ranking see for an overview Many of these approaches ensure that the number of individuals selected from different socially salient groups eg those defined by gender or race satisfy some fairness constraints and/or improve along a given fairness metric Towards this these algorithms assume exact access to the corresponding protected attributes of individuals However in practice these attributes can be erroneous unavailable for some individuals or missing entirely For instance in healthcare patients ethnic information can be incorrectly recorded or left blank When this data is missing probabilistic methods based on other proxy information are used to impute these protected attributes For instance when assessing if lenders comply with fair lending policies the Consumer Financial Protection Bureau uses last name and geolocation to impute consumers race Similar approaches have also been used in the context of healthcare Additionally online job platforms such as LinkedIn use a users data to infer their demographic information based on the data they have on other users Furthermore in some cases such as with images on the internet protected attributes are missing for the entire datasets and labeling all images is not viable Inferring protected attributes is bound to have errors which can affect the groups differently Thus using imputed attributes as a black-box in subsequent fair algorithms without accounting for their noise can have an unexpected and adverse impact on the fairness achieved For instance observe that noise oblivious fair algorithms do not satisfy their fairness guarantee in the presence of noise To gain some intuition consider the setting where we are given a set of candidates and would like to ensure proportional representation across individuals with different skin-tones coded as White and non-White Assume that the utilities of all candidates have a similar distribution and so picking candidates with top n utilities proportionately represents them Further assume that the labels have a higher amount of noise for non-Whites than Whites One can show that any fair algorithm which assumes that these noisy labels are correct and selects a proportionate number of White and non-White candidates based on them would violate proportional representation In this case adding fairness constraints increased the disparity This leads us to the question addressed in this paper Can we develop a framework for selection which outputs an approximately fair subset despite noisy protected attributes Recently this received public attention when attempting to estimate the racial disparities in COVID infections showed large discrepancies For instance as observed in commercial image-based gender classifiers Our contributions Building on prior work on fairness constraints we develop a framework for fair selection in the presence of noisy protected attributes This framework allows for multiple and intersectional groups and given access to unbiased probabilistic information about the true protected attributes it can satisfy a large class of fairness constraints including demographic parity proportional representation and the rule with high probability Formally we would like to solve an ideal optimization problem Program Target which satisfies the fairness constraints for the true and unknown protected attributes Such problems have been studied by prior works eg However since we do not have the true protected attributes we cannot solve it directly using their approaches Instead we formulate a denoised problem Program Denoised such that an optimal solution of Program Denoised has the optimal utility for Program Target and violates the fairness constraints of Program Target by at most a small multiplicative factor with high probability Lemma Although Program Denoised turns out to be NP-hard Theorem we develop a linear-programming based approximation algorithm for it This in turn implies an approximation algorithm for Program Target We empirically study the fairness achieved by this approach with respect to standard fairness metrics eg risk difference on both synthetic and real-world datasets We also study the performance of existing fair algorithms in the presence of noise and benchmark our approach with them We observe that our approach achieves the highest fairness and has a Pareto-optimal tradeoff between utility and fairness on changing the strength of constraints Interestingly these observations also hold in our empirical results where unlike what our theoretical results assume we have skewed probabilistic information of the noisy attributes Finally our empirical results hint at potential applications of this approach eg in online recruiting portals and image search engines Related work Mitigating bias An extensive body of work strives to mitigate bias and improve diversity in subset selection and the closely related ranking problem We refer the reader to for a comprehensive overview of work on diverse selection and an excellent talk which discusses work on curtailing bias in rankings Closest to our setting are approaches which use protected attributes to impose fairness constraints on algorithms for selection and ranking However if the attributes are noisy these could even increase the bias A different approach is to learn unbiased utilities by either using a causal model to capture the relation between attributes and utilities or by casting it as a multi-objective unconstrained optimization problem The former approach explicitly uses the protected attributes to generate counterfactuals so it can lead to unfair outcomes in the presence of noise also see Section And the latter approach can lead to sub-optimal fairness if noisy data is not accounted for as shown by works on fair classification In it is empirically shown that when protected attributes are missing proxy attributes can be used to improve fairness in classification However they do not consider how necessary noise resulting from the proxy attributes affects the fairness or accuracy Here unbiased refers to the statistical notion of an unbiased estimator Mitigating bias with noise Works on curtailing bias with noisy information are relatively recent Closest to this paper are those which consider noise in the protected attributes In conditions on the noise under which the popular post-processing method for fair classification by reduces bias in terms of equalized odds are characterized However they only consider noise in the training samples and assume that the test samples are not noisy which often doesnt hold in practice In an in-processing approach to fair classification is suggested they show that applying tighter fairness constraints in existing fair classification frameworks can mitigate bias in terms of equalized odds and statistical parity with binary protected attributes However this approach does not extend to nonbinary protected attributes and to other definitions of fairness In an in-processing approach for fair classification which can mitigate bias with nonbinary and noisy protected attributes is developed However they assume that the noise only depends on the unknown underlying protected attributes whereas we also allow the noise to vary with nonprotected attributes and utility Furthermore mitigate bias in classification tasks and it is not clear how to extend these methods to subset selection In methods to reliably assess disparity in the setting where the protected attributes are entirely missing are proposed We consider a similar noise model as the one they propose however the problem is fundamentally different as their goal is assessment rather than mitigation Noise models in literature Several works in the machine learning literature consider noise in the predicted labels as opposed to in attributes protected or otherwise In this paper we consider a noise model that arises from this line of work but applied to the protected attributes rather than the label MODEL For a natural number n N by n we denote the set n and for a real number x R by we denote ex We use I to denote the indicator function o to denote to denote the uniform distribution on interval ab Given a natural number p N p denotes the standard p-simplex Selection problem and noise model Selection problem In the classical selection problem one is given m items where each item i m has a utility wi An items utility is the value it adds to the selection The goal is to find a subset of n items which has the most total value It is convenient to encode a subset with a binary selection vector x m Then the classical selection problem is max m m i st m i xi n Protected attributes We consider s N protected attributes such as gender or race where for s the k-th protected attribute can take pk N values such as different genders or races Let X be the domain of all other nonprotected attributes Fix a joint distribution B R p ps X Then each item i m is represented by the tuple wi z i z s i ai R p ps X and is drawn independently from this joint distribution We observe the utility wi and nonprotected attributes ai but do not observe the protected attributes z s i Instead we observe a noisy version z s i of them for each i m For each attribute-value pair s and there is a unknown group G m items whose k-th attribute has value G B i m i Groups unknown For example if the k-th protected attribute is race then for different values of pk G is the subset candidates whose race is However we only have noisy information about the protected attributes of each item so only noisy information of this subset Intersectional groups In the above model each protected attribute takes a unique value It may appear that this does not allow for intersectional groups eg say multiracial candidates But this is only a matter of encoding and is remedied by using attributes such as has-race A and has-race B which take Yes or No values Definition Noise For each item i m and s we have a probability vector i pk such that the k-th protected attribute of item i takes value pk with probability q conditioned on wi z i z s i ai B Pr i G wi z i z s i ai The event that i G is independent of all other items mi and all other attributes in sk Note that for all i m and s q Discussion of the noise model The above model says that given the utility wi noisy protected attributes z s i and nonprotected attributes ai of an item i there is probabilistic information about its protected attributes If items represent candidates for a job and the protected attribute is race then we can use the candidates last name encoded in ai to derive probabilistic information about their race This has been used in practice eg by We can also consider multiple nonprotected attributes such as both lastname and location eg as used by As discussed in Section this could be relevant for an online hiring platform which may not have demographic information of some or all of its users and image search engines where the images do not have gender labels Target problem Studies have found that in the absence of other constraints the selection problem can overrepresent individuals with certain protected attributes at the expense of others Towards mitigating this bias we consider lower bounds and upper bounds on the number of items of a given protected attribute selected Formally the constraints ensure that for each attribute-value pair s and pk the selection has at least Lk and at most U items from G Then a selection x m satisfies the target fairness constraints if for all s and pk Lk i G xi U Fairness constraints Constraints similar to Equation have been studied by several works in algorithmic fairness and are rich enough to encapsulate a variety of fairness and diversity metrics eg see Thus for the appropriate L the subset satisfying constraints would be fair for one from a large class of fairness metrics Overall our constrained subset selection problem is max x m m i Target st Lk i G xi U s pk m i xi n If we know the protected attributes and in for s and pk then we can hope to solve Program Target directly Indeed prior works consider similar problems in rankings or its generalization to multiple Matroids constraints However with only noisy information about protected attributes we can not even verify if a selection vector x is feasible for Program Target To overcome this we must go beyond exact algorithms which always satisfy fairness constraints Denoised problem The difficulty in solving Program Target is that we do not know the constraints as we do not We propose to solve a different problem Program Denoised which uses the noise estimates q to approximate the constraints of Program Target For some small we define the denoised program as the following max x m m i Denoised st Lk m i xi U s pk m i xi n Here m i xi is the expected number of items selected by x whose k-th protected attribute is Then intuitively we can see Program Denoised that satisfies the constraints of Program Target in expectation where the expectation is taken over the noise However just satisfying constraints in expectation is not sufficient For instance this would allow algorithms that in each use violate the fairness constraints by a large amount but average out their errors in aggregate Instead our goal is to find an algorithm which violates the constraints by at most a small amount almost always Before presenting our theoretical results we discuss an alternate noise model and why it is not suitable in our setting Group-level noise model Recent works on noisy fair classification consider a different noise model which adapted to our setting uses the following probabilities B z i z s i Notice that unlike Definition does not condition on the utility or nonprotected attributes ai Thus its estimates are the same for all items with the same set of noisy protected attributes We call this the group-level noise model GLN In the next example we discuss why GLN is not sufficient to mitigate bias in subset selection Toy example Consider a setting where there is one protected attribute which takes two values ie s and the relevant fairness metric is equal representation Let the two groups unknown be AB m and their observed noisy versions be A and B According to q each candidate i B has the same probability of being in A In this noise model these candidates are indistinguishable apart from their utilities so if one picks nb N candidates from B they would naturally be the ones with the highest utility However suppose that most individuals in A have a higher utility than most individuals in B In this case the probabilities q will be distorted by the utilities such that candidates with higher utility in B are more likely to be in A than those with lower utility in B In fact is much larger than n then most of the top nb candidates in B would in fact be from A This example can be extended to more realistic settings with more than two groups and a smaller amount of bias in the utilities Even then to overcome this distortion in probabilities one needs to consider a stronger noise model in which the noise estimate varies with utility as in Definition either implicitly through proxy nonprotected attributes ai or explicitly with utility wi Remark In Sections and for the sake of simplicity we only consider the setting with one protected attribute s which takes p values We obtain analogous results for the the general case in Supplementary Material B When s we let p B p and drop superscripts representing the protected attribute from all variables THEORETICAL RESULTS Our main algorithmic result is an efficient approximation algorithm for Program Target Theorem An approximation algorithm for Program Target There is an algorithm Algorithm that given an instance of Program Target for s and noise q from Definition outputs a selection x m such that with probability at least p over the noise in the protected attributes of each item the selection x has a value at least as high as the optimal value of Program Target violates the cardinality constraint by at most p additive and violates the fairness constraints by at most additive The algorithm runs in polynomial time in the bit complexity of input As desired the algorithm outputs subset which violates the constraints of Program Target by at most a small amount with high probability Note that the approximation is only in the constraints and not in the value with high probability x has an higher value than the optimal solution say x of Program Target i m i xi wi In most real-world contexts p is a small constant Here Theorem implies that x violates the fairness constraints Equation by a Formally A i B i A i and B i Such an bias in utilities is one reason why we need fairness constraints in the first place multiplicative factor of at most o and the constraint Equation by a multiplicative factor of at most high probability If p is large then x from Theorem can violate the constraints by a large amount However in this case it is NP-hard to even check if there is a solution to Program Denoised which violates the constraints by a constant additive factor let alone finds an optimal solution for Program Target see Theorem Algorithm crucially uses the Program Denoised it first solves the linear-programming relaxation of Program Denoised and then rounds this solution to integral coordinates In the next section we overview the proof of Theorem We defer the proof of Theorem to Supplementary Material A due to space constraints Remark We can strengthen Theorem to guarantee that Algorithm finds an x m which does not violate the lower bound fairness constraint left inequality in Equation and violates the upper bound fairness constraints by at most without changing other conditions In particular this shows that if one places only lower bound fairness constraints then subset found by Algorithm would never violate the fairness constraints Algorithm Algorithm for Program Target Input A number n N probability matrix q mp utility vector term constraint vectors R p Solve x Find a basic feasible solution to LP-relaxation of Program Denoised with inputs Set x i B xi for all i m Round solution Return x Proof overview and hardness results In this section we overview the proof of Theorem The complete proof and an extension of Theorem for multiple protected attributes ie s appear in Supplementary Materials A and B The proof of Theorem has two broad steps First we show that solving Program Denoised even approximately gives us a good solution to Program Target and then we develop an approximation algorithm along with matching hardness results for Program Denoised To prove the former we bound the difference between the true and the expected number of candidates from any one group Lemma For all and x m st m i xi n p i xi i m holds with probability at least p over the noise in the protected attributes of each item The proof of this lemma appears in Supplementary Material A Using Lemma we can show that any solution that violates the constraints of Program Denoised by a small amount with high probability also violates the constraints of Program Target by at most a small amount Let x be an optimal selection for Program Target Using Lemma we can show that x is feasible for Program Denoised with high probability It follows any solution x which is optimal for Program Denoised has value at least as large as x ie Using n if not we can set to min n and to min n m i m i xi wi These suffice to show that solving Program Denoised gives a good solution for Program Target which satisfies the claims in the Theorem It remains to solve Program Denoised Unfortunately even checking if Program Denoised is feasible is NP-hard see Theorem a constant-factor approximation in utility to Program Denoised is also NP-hard We overcome this hardness by allowing solutions to violate the constraints of Program Denoised by a small additive amount p Towards this consider the linear-programming relaxation of Program Denoised for s We show that any basic feasible solution BFS of LP-Denoised has a small number of fractional entries Lemma max m m i LP-Denoised for s st p m i m i xi n Lemma An optimal solution with p fractional entries Any basic feasible solution x m of LP-Denoised has at most min mp fractional values ie m i xi min mp The proof follows by specializing well-known properties of BFSs to LP-Denoised We remark that this result is tight see Fact A Proof sketch of Theorem Using Lemma we can show that x is feasible for Program Denoised with probability at least p Assume that this event happens Then x is also feasible for LP-Denoised Consider the basic feasible solution x to LP-Denoised from Step of Algorithm Since x is optimal for LP-Denoised it follows that x has a value at least as large as x i m i xi wi Further since the rounded solution x from Step of Algorithm only increases the utility of x Thus m i x m i xi wi This establishes the first claim in Theorem It follows from Lemma that x picks at most p more elements than x Thus x violates Equation so Equation by at most p By the same argument x violates the fairness constraints of Program Denoised by at most p additive Combining this with Lemma we can show that with probability at least p x violates the fairness constraints of Program Target by at most additive This establishes the last two claims in Theorem conditioning on the two events described above The run time follows since there are polynomial time algorithms to find a basic feasible solution of a linear program Finally taking a union bound over over the two events completes the proof Hardness results Lastly we present our hardness results their proofs appear in Supplementary Material A Theorem Hardness results Informal Consider variants of Program Denoised for values of p If p then deciding if the problem is feasible is NP-hard If p then the problem is APX-hard If p and s then for every constant c the following violation gap variant of Program Denoised is NP-hard Output YES if the input instance is satisfiable Output NO if there is no solution which violates every upper bound constraint at most an additive factor of c EMPIRICAL RESULTS We evaluate our approach on utilities and noise derived from both synthetic and real-world data We consider the following algorithms Baseline Blind As a baseline we consider the Blind algorithm which selects n candidates with the highest utility Note that Blind has the optimal unconstrained utility Noise aware FairExpec is our proposed approach see Theorem FairExpecGrp is the same as FairExpec but uses the probabilities from the group-level noise model Section Noise oblivious Impute protected attributes Bayes-optimally from q mp as i m p B if argmax p otherwise If argmax is not unique pick one at random Then we consider the following noise oblivious algorithms which take the imputed protected attributes q as input solves Program Target defined on q This is equivalent to the ranking algorithms of adapted to subset selection MultObj is a multi-objective optimization algorithm inspired by s approach for ranking Let t p be the target distribution of protected attributes in the selection For example if the target is equal representation then t B Given a constant MultObj solves max x n i n t wm m where m R m is the all one vector The first term is the value of x is the distribution of noisy protected attributes in x and entire second term is a penalty on x for being far from the target distribution t Setup and metrics Setup We consider one protected attribute s which takes p disjoint values we use p and p Our simulations either target equal-representation where t or proportional representation where t G In each simulation we do the following FairExpec FairExpecGrp and Set and n and vary from to Notice that enforces no constraints on the subset the constraints become tighter as increases and ensures the subset chooses exactly n candidates from the â„“-th group MultObj Vary from to a large value Here enforces no penalty on the objective the penalty increases as increases and forces MultObj to satisfy the target distribution exactly on the noisy attributes The code for the simulations is available at Given two vectors x y p y denotes the Kullback-Leibler divergence of x and y defined as y B p i xi We scale the second term by the average utility m i This is not necessary but ensures that does not heavily depend on the scale of the utility Let be the r choice of and For each we draw a individuals or items from the dataset For each element i M we have p R We give the details of drawing M and fixing wi with each simulation Fairness metric Given subset S m and target t p let the risk difference S t of S for target t be S t B min max p S n S Gk n tk Here a risk difference is the most fair and is the least fair When the target is proportional representation S t reduces to the usual definition of risk difference up to scaling m be the subset selected by algorithm A on input We report FA B E t where the expectation is over the choices of Utility metric to be the average utility obtained B E i wi where the expectation is over the choices of We report the utility ratio KA for different algorithms A defined as KA B U Blind Utility ratio When the algorithm A is not important or clear from context we drop the subscripts from FA and KA Synthetic data with disparate error-rates In this simulation we consider the setting where different groups have different noise levels This has been observed in practice for instance in commercial image-based gender classifiers Data We generate a synthetic dataset with one binary protected attribute p This attribute partitions the underlying population into a minority group and a majority group We assume that candidates in both groups have similar potentials so sample utilities of all candidates independently Next we sample the probabilities from a Gaussian mixture such that the resulting population has minority candidates in expectation and the imputed attributes q have a higher false discovery rate FDR for minority candidates compared to majority candidates Formally we sample as follows N N and B where is the truncated normal distribution on with mean and standard deviation Setup In this simulation we target equal representation between the majority group and the minority group and and n We report the risk difference of different algorithms as a function of for FairExpec FairExpecGrp and and as a function of for multobj in Figure Some works also define risk difference as a measure of unfairness and set it equal to t with t up to scaling The difference of in FDRs is comparable to difference in FDRs between dark-skinned females and light-skinned men observed by for a commercial classifier A A A A A A A A A A A A A A A Am or e fa ir R i s d i ff e r e n c e le fa ir this work Figure Synthetic data with disparate error-rate Section This simulation considers the setting where the minority group of total has a higher higher FDR compared to the majority group The utilities of all candidates are iid from the uniform distribution The target is to ensure equal representation between the majority and minority groups The y-axis shows the risk difference of different algorithms and the x -axis shows the constraint parameters for FairExpec FairExpecGrp and and for MultObj values are averaged over trials and the error bars represent the standard error of the mean We observe that increasing fairness constraints to noise oblivious algorithms and MultObj worsens their risk difference Whereas the risk difference of noise aware algorithms improves FairExpec and FairExpecGrp on increasing fairness constraints Remark MultObj does not guarantee a particular fairness level for any fixed Thus one should consider the limiting value of MultObj in Figure Results We observe that without any constraints ie and all algorithms have similar risk difference However on adding fairness constraints and MultObj become more unfair In fact for the strongest fairness constraint ie and they have the lowest risk difference This is because the imputed protected attributes have a higher FDR for the minority group so the algorithms pick a higher number of candidates from the majority group In contrast FairExpec and FairExpecGrp do not use the imputed protect attributes so increasing fairness constraints to FairExpec and FairExpecGrp improves their risk difference and for the strongest fairness constraint they attain the highest risk difference Finally since we sample all utilities from the same distribution it is not surprising that FairExpec and FairExpecGrp perform similarly Synthetic data with disparate utilities In this simulation we consider the setting where different groups have different distributions of utilities In particular we assume that the minority group unfairly has a lower average utility when in fact the distributions of utilities should be the same for both the majority group and the minority group Contrast this with Section where all utilities are identically drawn Such differences in utility can manifest in the real world for many reasons including the implicit biases of the committee evaluating the candidates and structural oppression faced by different groups Counterfactually fair approaches One could also consider counterfactually fair approaches to mitigate bias in selection We refer the reader to for an overview of counterfactual fairness At a high-level these approaches try to unbias the utilities across groups and then use unbiased utilities in subsequent tasks say selection or ranking In this simulation we also consider counterfactually fair algorithms by CntrFair and CntrFairResolving They correspond to non-resolving and resolving algorithms in Roughly they assume that there is a causal model parameterized by such that given the attributes ai of an individual their utility ai Then roughly they fix each individuals protected attributes and compute the unbiased utility as wi this represents the utility of the individuals had they had the same protected attribute v Data We consider a synthetic hiring dataset generated with the code provided by In the data each candidate i has one protected attribute denoting their race if the candidate is Black and otherwise and two nonprotected attributes ai and ai R ai is if the candidate has prior work experience and ai is denotes their qualifications the larger the better We sample candidates independently from a fixed distribution defined in which is such that the utility of Black candidates is unfairly lower than non-Black candidates For further details we refer the reader to Supplementary Material C Preprocessing We sample a training dataset D candidates Then using D we compute an approximation of and given a candidate i described by wi we compute Note that the candidate i may not be in D For prefer more details of the preprocessing in Supplementary Material C Adding noise The dataset does not have noise to begin with Given a noise level we generate noisy race of candidate i by flipping their race independently with probability Setup We target proportional representation of race and vary over For each noise level we sample a new instance D of the dataset and add -noise to it We fix n and the strongest constraints and for the algorithms Here CntrFair and CntrFairResolving calculated in preprocessing and FairExpec FairExpecGrpMultObj and use q or the imputed attributes q both calculated in preprocessing We report the risk difference as a function of the noise-level in Figure We also report the selection rates from each group in Supplementary Material C Results We observe that all algorithms have the highest fairness when there is no noise Here they have a similar risk difference lying between to As the noise increases we observe that the risk difference of FairExpecGrp and CntrFair approaches Blind and risk difference of MultObj This interpretation differs from who interpret both and ai as protected attributes The only difference from is that we increase the underlying bias by reducing the mean utilities for Black candidates and candidates without prior experience We do so because the dataset already had a high risk difference without adding any fairness constraints We choose as MultObjs fairness MultObj converges before A A A A A A A A A A A A A A A A A A A A Am or e fa ir R i s d i ff e r e n c e le fa ir more noise Noise less noise this work CntrFair CntrFairReCntrFai Res Figure Synthetic data with disparate utilities Section This simulation considers the setting where the utilities of a minority group have a lower average than the majority group and both groups have an identical amount of noise The target is to ensure proportional representation The y-axis shows the risk difference of different algorithms and the x -axis shows the amount of noise added values are averaged over trials and the error bars represent the standard error of the mean We observe that the risk difference of all algorithms becomes poorer with noise than without it Here FairExpec has the highest risk difference for all values of noise Finally unlike Section FairExpecGrp has a lower fairness than FairExpec since in this simulation different groups have different distributions of utilities and CntrFairResolving approaches a value between In contrast FairExpec has a better risk difference throughout The risk difference of MultObj and improves with at some values of we give a possible explanation in Remark Notice at for all candidates i m the noisy label is chosen uniformly at random and provides no information about CntrFair and CntrFairResolving use to compute the counterfactual utilities so perform poorly at Further FairExpecGrp uses the probabilities which depend on but not on wi Since the utility of candidates of different races has a different distribution can be skewed see Section We note that the utility of all algorithms decreases on adding noise In particular while FairExpec is able to satisfy the fairness constraints with noise its utility decreases on adding noise see Supplementary Material C for a plot of utility ratio vs noise Remark The risk difference of and MultObj is nonmonotonic in the noise This might be because the false discovery rate FDR of q for Black candidates is non-monotonic Specifically the FDR first increases with roughly for and then decreases The decrease in FDR after comes at the cost of fewer total positives ie q identifies fewer total Black candidates The total number of total positives drop below for higher values of Correspondingly the of and MultObj first decreases as FDR reduces then increases as FDR increases until the number of total positives is larger than roughly and finally decreases as the number of total positives drops below Remark We do not consider counterfactual approaches in Section because there the utilities are already unbiased and so CntrFair and CntrFairResolving reduce to Blind Further CntrFair and CntrFairResolving only ensure proportional representation We find that the datasets considered in Sections and are already fair more fair Risk difference less fair this work Figure Real-world data for candidate selection Section This simulation considers race as the protected attribute which takes p values The simulation uses last-name as a proxy to derive noisy information of race and draws the utility of each candidate from a fixed distribution depending on their race The target is to ensure equal representation across race The y-axis shows the utility ratio of different algorithms and the x -axis shows the risk difference of different algorithms both and values are averaged over trials and the error bars represent the standard error of the mean We observe that FairExpec reaches the highest risk difference and has a better tradeoff between utility and risk difference compared to other algorithms when the target is proportional representation in both cases Blind Therefore we omit these algorithms from those simulations Real-world data for candidate selection In this simulation we consider the problem of selecting candidates under noisy information about their race Similar to what has been used in applications eg we use a candidates last name to predict their race We consider a candidates utility as their previous-salary which we model using the race-aggregated income dataset This dataset provides the income distribution of families from different races elaborated below This problem could be relevant in the context of an online hiring platform which would like to display a race-balanced set of candidates but only has noisy information about each candidates race Data US Census dataset The dataset contains distinct last names which occurred at least a times in the US census last names occur at least a times For each last name i the dataset has its total occurrences per people Z and a vector representing the fraction of individuals who are White Black Asian and Pacific Islander API American Indian and Alaskan Native only AIAN multiracial or Hispanic respectively We do not use AIAN and two or more races categories ie and as they do not occur in the income dataset Then for each last name i we define the probability vector as the normalized version of the vector Income dataset We use family income data aggregated by race This was compiled by the US Census Bureau from the Current Population Survey The dataset provides income data of families It has four races White Black Asian and Hispanic age categories and income categories For each set of race age and income categories the dataset has the number of families whose reference person see definition here belongs to these categories For each race r we consider the discrete distribution Dr of incomes of families with race r derived from the income dataset see Figure in supplementary material for some statistics of Dr Setup We consider race as a protected attribute with four labels p and target equal representation based on race and n For each choice of and we draw a last names uniformly from the entire population with replacement The i-th last name is drawn with probability proportional to For each last name i M we sample a ground-truth race ri unknown to the algorithms according to the distribution and then sample the income wi We report the utility ratio as a function of the risk difference for different algorithms in Figure We also report as a function of for FairExpec FairExpecGrp and and as a function of for multobj in Supplementary Material C Results FairExpec reaches the highest risk difference of followed by FairExpecGrp which reaches a risk difference of reaches MultObj reaches and Blind has We do not expect the algorithms to outperform the unconstrained utility ie that of Blind We observe that FairExpec has a better Pareto-tradeoff compared to other algorithms ie for any desired level of risk difference it has a better utility ratio than other algorithms In contrast while FairExpecGrp also has a high maximum risk difference it is not Pareto-optimal All algorithms lose a large fraction of the utility up to This is because the unconstrained and constrained optimal are very different without any constraints we would roughly select candidates from some races However ensuring equal representation requires selecting roughly times as many candidates from these races When the difference between unconstrained and constrained optimal is smaller we expect to lose a smaller fraction of the utility Real-world data for image search In this simulation we consider the problem of selecting images under noisy information about the gender of the person depicted in the image We derive noisy information about the gender of the person depicted in an image using a CNN-based gender classifier and use this information to select a gender-balanced set of images This could be relevant in mitigating gender bias in image search engines which have been observed to over-represent the stereotypical gender in job-related searches Here one could first select a balanced subset of images to display on each page and then order this subset in decreasing order of utility from top to bottom of each page Data We use a recent image dataset named the Occupations Dataset by The dataset contains top Google Image Search results from December for occupations related queries For each image the dataset has a gender coded as men women or The age categories are to to to The income categories are and in USD per annum Male Female NA Total Dark Light NA Total Figure Statistics of the Occupations dataset other skin-tone coded as dark or light and the images position in the search result an integer in We present aggregate statistics from the dataset in Figure Gender classifier We use an off-the-shelf face-detector to extract faces of people from the images and then use a CNN-based classifier to predict the supposed gender of people from their faces For each image i the classifier outputs a prediction which is the uncalibrated likelihood that the image is of a man women We calibrate this score as described next As a preprocessing step we remove all images which either have a gender label of NA and for which the face-detector did not detect any face Then we calibrate outputs of the classifier on all remaining images This calibration is only done once and is used to compute the noise-information for the entire simulation For more details of the preprocessing used we refer the reader to Supplementary Material C Selecting occupations Next we infer the occupations which have considerable gender stereotype Towards this we fix a threshold and partition the occupations into three sets occupations for which at least -fraction of images were labeled to appear to depict women occupations for which at least -fraction of images were labeled to appear to depict men and all other occupations We fix This gives us and and images with occupations in for a list of the occupations see Table in Supplementary Material C Remark Note that we calibrate q on all occupations and only consider images in a subset of occupations less than half This means that qs may not be an unbiased estimate of the protected attributes which is a hard case for FairExpec Setup In this simulation we consider gender as the protected attribute with two values p We say a particular gender is stereotypical for a given occupation if the majority of images of this occupation are labeled to appear to depict a person with this gender For example men are stereotypical for occupations in and women are stereotypical for occupations in We call an image stereotypical if the dataset labels the person depicted in the image to appear to be of the stereotypical gender for its occupation We call an image anti-stereotypical if it is not stereotypical We would like to ensure equal representation between stereotypical and anti-stereotypical images While there could be richer and nonbinary gender categories many commercial classifiers and the classifier by categorizes images as either male or female Note that we do not check if the detected faces are correct This introduces some error which is also expected in practice A A A A A A A A A A A A A A A A A A A A A U t i l i t y r a t i o more fair Risk difference less fair this work Figure Real-world data for image search Section This simulation considers gender as the protected attribute and uses a CNN-based classifier to derive noisy information about the gender of the person depicted in the image The target is to ensure equal representation equal between genders The y-axis shows the utility ratio of different algorithms and the x -axis shows the risk difference of different algorithms and values are averaged over trials and the error bars represent the standard error of the mean We observe that FairExpec reaches the highest risk difference and has a better tradeoff between utility and fairness compared to other algorithms For each choice of and we draw a subset M of m images uniformly from all images with occupation in For each image i M let its rank be ri We compute as discussed earlier and set its utility B log ri We report the utility ratio as a function of the risk difference for different algorithms in Figure We also report as a function of for FairExpec FairExpecGrp and and as a function of for multobj in Supplementary Material C Remark Since the underlying application in this simulation is ranking image results we also considered Normalized DCG as the utility metric We observed similar results for this For completeness we present the plot in Supplementary Material C Furthermore we also tried other functions for utilities wi including ri and and observed similar results Results The risk difference of Blind ie the risk difference without any interventions is All algorithms reach a better risk difference than Blind Among them FairExpec reaches the highest risk difference of While the next best algorithm FairExpecGrp has Since the algorithms satisfy fairness constraints we do not expect them to have a higher utility than Blind hence it is not surprising that Among the algorithms we consider we observe that FairExpec has the Pareto-optimal tradeoff between utility and fairness it has a higher utility ratio compared to other algorithms for a given value of risk difference Additional empirical results We present additional empirical results with selection lift in Supplementary Material C We observe that indeed FairExpec is able to mitigate discrimination with respect to selection lift and reaches the most-fair selection lift in all experiments Further it has the Pareto-optimal tradeoff between utility and fairness compared to this work Figure Risk difference on varying Remark Towards analyzing the robustness of our approach to the fraction of items selected we fix m and vary n from to in Simulation The y-axis shows the risk difference of different algorithms and the x -axis shows n values are averaged over trials and the error bars represent the standard error of the mean We find that increasing on n the difference in the fairness of different algorithms remains roughly the same other algorithms in all but one case in the simulation from Section while FairExpec has a lower utility than MultObj for some levels of selection lift We believe this is because the constraint region induced by threshold of selection lift is different from the constraint region of Program Target One could correct this eg by using Theorem to reduce the constraint from selection lift to that of multiple lower and upper bound constraints Remark Risk difference on varying Different applications could require selecting different fractions of results from the set of available items For example an image search engine might select a small fraction of available results whereas a job platform can select a larger fraction Towards analyzing the robustness of our approach to the fraction of items selected we fixed m and varied n in simulations We find that on increasing n holding m fixed the difference in algorithms fairness remains roughly the same See Figure for a plot for the simulation in Section we present the plots for simulations from Sections and in Supplementary Material C LIMITATIONS AND FUTURE WORK We consider the natural setting where the utility of a subset is the sum of utilities of its items A useful extension to this work could consider submodular objectives which are relevant when the goal is to select a subset which summarizes a collection of items Apart from this some works also study other variants of subset selection for example diverse and fair subset selection in the online settings Studying and mitigating bias in the presence of noise under this variant is an interesting direction for future work Further our approach assumes access to probabilistic information about the true protected attributes If this information is itself is skewed or incorrect then our approach can have a poor performance Although empirical results on real-world data suggest that our approach can improve fairness even when there is some skew in This reduction uses multiple lower bound and upper bound constraints to provably approximate the constraints of selection lift the noise information see eg Remark Still determining probabilistic information more reliably is an important problem and recent works have made some progress toward this goal Furthermore while we focus on the subset selection problem our results can also extend to the ranking problem where after selecting a subset it must be ordered by satisfying the fairness constraints in the top-k positions for a small number choices of say this reduces the high-probability guarantee from p to This is particularly relevant in the setting where results are displayed one page at a time Satisfying the constraints for a larger number of positions with high probability might require stronger information about noise and is an interesting direction for future work Empirically we could report fairness on several other metrics eg selection lift or extended difference We focus on risk difference as it is closer to our approach Nevertheless Program Target can mitigate discrimination with respect to selection lift and other metrics as well eg see Empirically evaluating this would be an important direction for future work Finally we note that bias is a systematic issue and this work only addresses one aspect of it Indeed any such approach is limited by how people and the broader system uses the subset presented to them eg a recruiter on an online hiring platform might deliberately reject minority candidates even when presented with a representative candidate subset Thus it is important to complement our approach with other necessary tools to mitigate bias and counter discrimination CONCLUSION We consider the problem of mitigating bias in subset selection when the protected attributes are noisy or are missing for some or all of the entries and must be imputed using proxy variables We note that accounting for real-world noise in algorithms is important to mitigate bias and not accounting for noise can have unintended adverse effects eg adding fairness constraints in a noise oblivious fashion can even decrease fairness when the protected attributes are noisy Section We consider a model of noise where we have probabilistic information about the protected attributes and develop a framework to mitigate bias which given this information can satisfy one from a large class of fairness constraints with at most a small multiplicative error with high probability Section In our empirical study we observe that this approach achieves a high level of fairness on standard fairness metrics eg risk difference even when the probabilistic information about protected attributes is skewed Remark and Section and this approach has a better tradeoff between utility and fairness compared to several prior approaches Section