Fairness Through Robustness Investigating Robustness Disparity in Deep Learning Deep neural networks DNNs are increasingly used in real-world applications eg facial recognition This has resulted in concerns about the fairness of decisions made by these models Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm or benefit particular subgroups of the population In this paper we argue that traditional notions of fairness that are only based on models outputs are not sufficient when the model is vulnerable to adversarial attacks We argue that in some cases it may be easier for an attacker to target a particular subgroup resulting in a form of robustness bias We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR- CIFAR- Adience and UTKFace and show that in almost all cases there are subgroups in some cases based on sensitive attributes like race gender etc which are less robust and are thus at a disadvantage We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs thus making mitigation of such biases a non-trivial task Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making Code to reproduce all our results can be found here CONCEPTS Computing methodologies Neural networks General and reference Evaluation INTRODUCTION Automated decision-making systems that are driven by data are being used in a variety of different real-world applications In many cases these systems make decisions on data points that represent humans eg targeted ads personalized recommendations hiring credit scoring or recidivism prediction In such scenarios there is often concern regarding the fairness of outcomes of the systems This has resulted in a growing body of work from the nascent Fairness Accountability Transparency and Ethics FATE community that drawing on prior legal and philosophical doctrine aims to define measure and attempt to mitigate manifestations of unfairness in automated systems Most of the initial work on fairness in machine learning considered notions that were one-shot and considered the model and data distribution to be static Recently there has been more work exploring notions of fairness that are dynamic and consider the possibility that the world ie the model as well as data points might change over time Our proposed notion of robustness bias has subtle difference from existing one-shot and dynamic notions of fairness in that it requires each partition of the population be equally robust to imperceptible changes in the input eg noise adversarial perturbations etc We propose a simple and intuitive notion of robustness bias which requires subgroups of populations to be equally robust Robustness can be defined in multiple different ways We take a general definition which assigns points that are farther away from the decision boundary higher robustness Our key contributions are as follows We define a simple intuitive notion of robustness bias that requires all partitions of the dataset to be equally robust We argue that such a notion is especially important when the decision-making system is a deep neural network DNN since these have been shown to be susceptible to various attacks Importantly our notion depends not only on the outcomes of the system but also on the distribution of distances of data-points from the decision boundary which in turn is a characteristic of both the data distribution and the learning process We propose different methods to measure this form of bias Measuring the exact distance of a point from the decision boundary is a challenging task for deep neural networks which have a highly non-convex decision boundary This makes the measurement of robustness bias a non-trivial task In this paper we leverage the literature on adversarial machine learning and show that we can efficiently approximate robustness bias by using adversarial attacks and randomized smoothing to get estimates of a points distance from the decision boundary We do an in-depth analysis of robustness bias on popularly used datasets and models Through extensive empirical evaluation we show that unfairness can exist due to different partitions of a dataset being at different levels of robustness for many state-of-the art models that are trained on common classification datasets We argue that this form of unfairness can happen due to both the data distribution and the learning process and is an important criterion to consider when auditing models for fairness Related Work Fairness in ML Models that learn from historic data have been shown to exhibit unfairness ie they disproportionately benefit or harm certain subgroups often a sub-population that shares a common sensitive attribute such as race gender etc of the population This has resulted in a lot of work on quantifying measuring and to some extent also mitigating unfairness Most of these works consider notions of fairness that are one-shot that is they do not consider how these systems would behave over time as the world ie the model and data distribution evolves Recently more works have taken into account the dynamic nature of these decision-making systems and consider fairness definitions and learning algorithms that fare well across multiple time steps We take inspiration from both the one-shot and dynamic notions but take a slightly different approach by requiring all subgroups of the population to be equally robust to minute changes in their features These changes could either be random eg natural noise in measurements or carefully crafted adversarial noise This is closely related to Heidari et al s effort-based notion of fairness however their notion has a very specific use case of societal scale models whereas our approach is more general and applicable to all kinds of models Our work is also closely related to and inspired by Zafar et als use of a regularized loss function which captures fairness notions and reduces disparity in outcomes There are major differences in both the approach and application between our work and that of Zafar et als Their disparate impact formulation aims to equalize the average distance of points to the decision boundary E G our approach instead aims to equalize the number of points that are safe ie E G g see section for a detailed description Our proposed metric is preferable for applications of adversarial attack or noisy data the focus of our paper whereas the metric of Zafar et al is more applicable for an analysis of the consequence of a decision in a classification setting Robustness Deep Neural Networks DNNs have been shown to be susceptible to carefully crafted adversarial perturbations which imperceptible to a human result in a misclassification by the model In the context of our paper we use adversarial attacks to approximate the distance of a data point to the decision boundary For this we use state-of-the-art white-box attacks proposed by Moosavi-Dezfooli et al and Carlini and Wagner Due to the many works on adversarial attacks there have been many recent works on provable robustness to such attacks The high-level goal of these works is to estimate a tight lower bound on the distance of a point from the decision boundary We leverage these methods to estimate distances from the decision boundary which helps assess robustness bias defined formally in Section Fairness and Robustness Recent works have proposed poisoning attacks on fairness Khani and Liang analyze why noise in features can cause disparity in error rates when learning a regression We believe that our work is the very rst to show that different subgroups of the population can have different levels of robustness which can lead to unfairness We hope that this will lead to more work at the intersection of these two important sub fields of ML HETEROGENEOUS ROBUSTNESS In a classification setting a learner is given data D G consisting of inputs G R and outputs C which are labels in some set of classes C These classes form a partition on the dataset such that D CG The goal of learning in decision boundary-based optimization is to draw delineations between points in feature space which sort the data into groups according to their class label The learning generally tries to maximize the classification accuracy of the decision boundary choice A learner chooses some loss function L to minimize on a Figure A toy example showing robustness bias A the classifier solid line has accuracy for blue and green points However for a budget g dotted lines of points belonging to the round subclass showed by dark blue and dark green will get attacked while only of points in the cross subclass will be attacked This shows a clear bias against the round subclass which is less robust in this case B shows a different classifier for the same data points also with accuracy However in this case with the same budget g of both round and cross subclass will be attacked thus being less biased a Three-class classification problem for randomly generated data b Proportion samples which are greater than g away from a decision boundary Figure An example of multinomial logistic regression training dataset parameterized by parameters while maximizing the classification accuracy on a test dataset Of course there are other aspects to classification problems that have recently become more salient in the machine learning community Considerations about the fairness of classification decisions for example are one such way in which additional constraints are brought into a learners optimization strategy In these settings the data D G B is imbued with some metadata which have a sensitive attribute S B BC associated with each point Like the classes above these sensitive attributes form a partition on the data such that D B B B Without loss of generality we assume a single sensitive attribute Generally speaking learning with fairness in mind considers the output of a classifier based o of the partition of data by the sensitive attribute where some objective behavior like minimizing disparate impact or treatment is integrated into the loss function or learning procedure to nd the optimal parameters There is not a one-to-one correspondence between decision boundaries and classifier performance For any given performance level on a test dataset there are infinitely many decision boundaries which produce the same performance see Figure This raises the question if we consider all decision boundaries or model parameters which achieve a certain performance how do we choose among them What are the properties of a desirable high-performing decision boundary As the community has discovered one undesirable characteristic of a decision boundary is its proximity to data which might be susceptible to adversarial attack This provides intuition that we should prefer boundaries that are as far away as possible from example data Let us look at how this plays out in a simple example In multinomial logistic regression the decision boundaries are well understood and can be written in closed form This makes it easy for us to compute how close each point is to a decision boundary Consider for example a dataset and learned classifier as in Figure a For this dataset we observe that the brown class as a whole is closer to a decision boundary than the yellow or blue classes We can quantify this by plotting the proportion of data that are greater than a distance g away from a decision boundary and then varying g Let G be the minimal distance between a point G and a decision boundary corresponding to parameters For a given partition P of a dataset D such that D P we define the function b g G G g If each element of the partition is uniquely defined by an element say a class label or a sensitive attribute label B we equivalently will write b g or bB g respectively We plot this over a range of g in Figure b for the toy classification problem in Figure a Observe that the function for the brown class decreases significantly faster than the other two classes quantifying how much closer the brown class is to the decision boundary From a strictly classification accuracy point of view the brown class being significantly closer to the decision boundary is not of concern all three classes achieve similar classification accuracy However when we move away from this toy problem and into neural networks on real data this difference between the classes could become a potential vulnerability to exploit particularly when we consider adversarial examples ROBUSTNESS BIAS Our goal is to understand how susceptible different classes are to perturbations eg natural noise adversarial perturbations Ideally no one class would be more susceptible than any other but this may not be possible We have observed that for the same dataset there may be some classifiers which have differences between the distance of that partition to a decision boundary and some which do not There may also be one partition P which exhibits this discrepancy and another partition P which does not Therefore we make the following statement about robustness bias D A dataset D with a partition P and a classifier parameterized by exhibits robustness bias if there exists an element P for which the elements of are either significantly closer to or significantly farther from a decision boundary than elements not in A partition P may be based on sensitive attributes such as race gender or ethnicity or other class labels For example given a classifier and dataset with sensitive attribute race we might say that classifier exhibits robustness bias if partitioning on that sensitive attribute for some value of race the average distance of members Figure An example of robustness bias in the UTKFace dataset A model trained to predict age group from faces is fooled for an inputs belonging to certain subgroups black and female in this example for a given perturbation but is robust for inputs belonging to other subgroups white and male in this example for the same magnitude of perturbation We use the UTKFace dataset to make a broader point that robustness bias can cause harms In the specific case of UTKFace and similar datasets the task definition of predicting age from faces itself is awed as has been noted in many previous studies of that particular racial value are substantially closer to the decision boundary than other members We might say that a dataset partition and classifier do not exhibit robustness bias if for all P and all g PGD G g G PGD G g G Intuitively this definition requires that for a given perturbation budget g and a given partition one should not have any incentive to perturb data points from over points that do not belong to Even when examining this criteria we can see that this might be particularly hard to satisfy Thus we want to quantify the disparate susceptibility of each element of a partition to adversarial attack ie how much farther or closer it is to a decision boundary when compared to all other points We can do this with the following function for a dataset D with partition element P and classifier parameterized by RB g PG D G g G PG D G g G Observe that RB g is a large value if and only if the elements of are much more or less adversarially robust than elements not in We can then quantify this for each element a more pernicious variable to handle is g We propose to look at the area under the curve b for all g b c c Note that these notions take into account the distances of data points from the decision boundary and hence are orthogonal and complementary to other traditional notions of bias or fairness eg disparate impact/disparate mistreatment etc This means that having lower robustness bias does not necessarily come at the cost of fairness as measured by these notions Consider the motivating example shown in Figure the decision boundary on the right has lower robustness bias but preserves all other common notions eg as both classifiers maintain accuracy Real-world Implications Degradation of Quality of Service Deep neural networks are the core of many real world applications for example facial recognition object detection etc In such cases perturbations in the input can occur due to multiple factors such as noise due to the environment or malicious intent by an adversary Previous works have highlighted how harms can be caused due to the degradation in quality of service for certain sub-populations Figure shows an example of inputs from the UTKFace dataset where an perturbation of could change the predicted label for an input with race black and gender female but an input with race white and gender male was robust to the same magnitude of perturbation In such a case the system worked better for a certain sub-group white male thus resulting in unfairness It is important to note that we use datasets such as Adience and UTKFace described in detail in section only to demonstrate the importance of having unbiased robustness As noted in previous works the very task of predicting age from a persons face is a awed task definition with many ethical concerns MEASURING ROBUSTNESS BIAS Robustness bias as defined in the previous section requires a way to measure the distance between a point and the closest decision boundary For deep neural networks in use today a direct computation of G is not feasible due to their highly complicated and non-convex decision boundary However we show that we can leverage existing techniques from the literature on adversarial Figure For each dataset we plot b g for each class in each dataset Each blue line represents one class The red line represents the mean of the blue lines ie C b g for each g Figure For each dataset we plot bgB for each sensitive attribute B in each dataset Figures for other models can be found in Appendix A attacks to efficiently approximate G We describe these in more detail in this section Adversarial Attacks Upper Bound For a given input and model one can compute an upper bound on G by performing an optimization which alters the input image slightly so as to place the altered image into a different category than the original Assume for a given data point G we are able to compute an adversarial image G then the distance between these two images provides an upper bound on distance to a decision boundary ie kG G G We evaluate two adversarial attacks DeepFool and Carlini-Wagners L attack We extend b for DeepFool and carlini-wagner as d G g kG G and d G g kG G respectively We use similar notation to define and as defined in While these methods are guaranteed to yield upper bounds on G they need not yield similar behavior to b or We perform an evaluation of this in Section Randomized Smoothing Lower Bound Alternatively one can compute a lower bound on G using techniques from recent works on training provably robust classifiers For each input these methods calculate a radius in which the prediction of G will not change ie the robustness certificate In particular we use the randomized smoothing method since it is scalable to large and deep neural networks and leads to the state-of-the-art in provable defenses Randomized smoothing transforms the base classifier to a new smooth classifier by averaging the output of over noisy versions of G This new classifier is more robust to perturbations while also having accuracy on par to the original classifier It is also possible to calculate the radius in the distance in which with high probability a given inputs prediction remains the same for the smoothed classifier ie G A given input G is then said to be provably robust with high probability for a perturbation where is the robustness certificate of G For each point we use its calculated using the method proposed by as a proxy for G The magnitude of for an input is a measure of how robust an input is Inputs with higher are more robust than inputs with smaller Again we extend b for Randomized Smoothing as c G g We use similar notation to define see EMPIRICAL EVIDENCE OF ROBUSTNESS BIAS IN THE WILD We hypothesize that there exist datasets and model architectures which exhibit robustness bias To investigate this claim we examine several image-based classification datasets and common model architectures Datasets and Model Architectures We perform these tests of the datasets CIFAR CIFAR using both classes and super classes Adience and UTKFace The rst two are widely accepted benchmarks in image classification while the latter two provide significant metadata about each image permitting various partitions of the data by final classes and sensitive attributes More details can be found in Appendix A Our experiments were performed using PyTorchs torchvision module We rst explore a simple Multinomial Logistic Regression model which could be fully analyzed with direct computation of the distance to the nearest decision boundary For convolutional neural networks we focus on Alexnet VGG ResNet DenseNet and Squeezenet which are all available through torchvision We use these models since these are widely used for a variety of tasks We achieve performance that is comparable to state of the art performance on these datasets for these models Additionally we also train some other popularly used dataset specific architectures like a deep convolutional neural network we call this Deep CNN and PyramidNet U depth no bottleneck for CIFAR- We re-implemented Deep CNN in pytorch and used the publicly available repo to train PyramidNet We use another deep convolutional neural network which we refer to as Deep CNN CIFAR and PyramidNet U depth with bottleneck for CIFAR and CIFAR-Super For Adience and UTKFace we additionally take simple deep convolutional neural networks with multiple convolutional layers each of which is followed by a ReLu activation dropout and maxpooling As opposed to architectures from torchvision which are pre-trained on ImageNet these architectures are trained from scratch on the respective datasets We refer to them as UTK classifier and Adience classifier respectively These simple models serve two purposes they form reasonable baselines for comparison with pre-trained ImageNet models fine-tuned on the respective datasets and they allow us to analyze robustness bias when models are trained from scratch In sections and we audit these datasets and the listed models for robustness bias In section we train logistic regression on all the mentioned datasets and evaluate robustness bias using an exact computation We then show in section and that robustness bias can be efficiently approximated using the techniques mentioned in and respectively for much more complicated models which are often used in the real world We also provide a thorough analysis of the types of robustness biases exhibited by some of the popularly used models on these datasets EXACT COMPUTATION IN A SIMPLE MODEL MULTINOMIAL LOGISTIC REGRESSION We begin our analysis by studying the behavior of multinomial logistic regression Admittedly this is a simple model compared to modern deep-learning-based approaches however it enables is to explicitly compute the exact distance to a decision boundary G We t a regression to each of our vision datasets to their native classes and plot b g for each dataset Figure shows the See Appendix A Table for model performances additional quantitative results and supporting figures distributions of b g from which we observe three main phenomena the general shape of the curves are similar for each dataset there are classes which are significant outliers from the other classes and the range of support of the g for each dataset varies significantly We discuss each of these individually First we note that the shape of the curves for each dataset is qualitatively similar Since the form of the decision boundaries in multinomial logistic regression are linear delineations in the input space it is fair to assume that this similarity in shape in Figure can be attributed to the nature of the classifier Second there are classes which indicate disparate treatment under b g The treatment disparities are most notable in UTKFace the superclass version CIFAR- and regular CIFAR- This suggests that when considering the dataset as a whole these outlier classes are less susceptible to adversarial attack than other classes Further in UTKFace there are some classes that are considerably more susceptible to adversarial attack because a larger proportion of that class is closer to the decision boundaries We also observe that the median distance to decision boundary can vary based on the dataset The median distance to a decision boundary for each dataset is for CIFAR- for CIFAR for the superclass version of CIFAR- for Adience and for UTKFace This is no surprise as G depends both on the location of the data points which are fixed and immovable in a learning environment and the choice of architectures/parameters Finally we consider another partition of the datasets Above we consider the partition of the dataset which occurs by the class labels With the Adience and UTKFace datasets we have an additional partition by sensitive attributes Adience admits partitions based o of gender UTKFace admits partition by gender and ethnicity We note that Adience and UTKFace use categorical labels for these multidimensional and socially complex concepts We know this to be reductive and serves to minimize the contextualization within which race and gender derive their meaning Further we acknowledge the systems and notions that were used to reify such data partitions and the subsequent implications and conclusions draw therefrom We use these socially and systematically-laden partitions to demonstrate that the functions we define b and depend upon how the data are divided for analysis To that end the function b is visualized in Figure We observe that the Adience dataset which exhibited some adversarial robustness bias in the partition on C only exhibits minor adversarial robustness bias in the partition on S for the attribute Female On the other hand UTKFace which had significant adversarial robustness bias does exhibit the phenomenon for the sensitive attribute Black but not for the sensitive attribute Female This emphasizes that adversarial robustness bias is dependant upon the dataset and the partition We will demonstrate later that it is also dependant on the choice of classifier First we talk about ways to approximate G for more complicated models EVALUATION OF ROBUSTNESS BIAS USING ADVERSARIAL ATTACKS As described in Section we argued that adversarial attacks can be used to obtain upper bounds on G which can then be used to measure robustness bias In this section we audit some popularly a UTK classifier DeepFool b UTK classifier carlini-wagner c UTK classifier Rand Smoothing d ResNet DeepFool e ResNet carlini-wagner ResNet Rand Smoothing g Alexnet DeepFool Alexnet carlini-wagner i Alexnet Rand Smoothing vgg DeepFool vgg carlini-wagner l vgg Rand Smoothing m Densenet DeepFool n Densenet carlini-wagner o Densenet Rand Smoothing p Squeezenet DeepFool q Squeezenet carlini-wagner r Squeezenet Rand Smoothing Figure UTKFace partitioned by race We can see that across models that different populations are at different levels of robustness as calculated by different proxies DeepFool on the left carlini-wagner in the middle and Randomized Smoothing on the right This suggests that robustness bias is an important criterion to consider when auditing models for fairness used models on datasets mentioned in Section for robustness bias as measured using the approximation given by adversarial attacks Evaluation of c and d To compare the estimate of G by DeepFool and carlini-wagner we rst look at the signedness of and For a given partition captures the disparity in robustness between points in relative to points not in see Considering all possible partitions based on class labels and sensitive attributes where available for all ve datasets both carlini-wagner and DeepFool agree with the signedness of the direct computation times ie sign sign sign sign Further the mean difference between and or ie is for DeepFool and for carlini-wagner with variances of and respectively There is agreement between the direct computation and the DeepFool and carlini-wagner estimates of b This behavior provides evidence that adversarial attacks provide meaningful upper bounds on G in terms of the behavior of identifying instances of robustness bias Audit of Commonly Used Models We now evaluate ve commonly-used convolutional neural networks CNNs Alexnet VGG ResNet DenseNet and Squeezenet We trained these networks using PyTorch with standard stochastic gradient descent We achieve comparable performance to documented state of the art for these models on these datasets A full table of performance on the test data are described in Table Appendix After training each model on each dataset we generated adversarial examples using both methods and computed for each possible partition of the dataset An example of the results for the UTKFace dataset can be see in Figure With evidence from Section that DeepFool and carlini-wagner can approximate the robustness bias behavior of direct computations of we rst ask if there are any major differences between the two methods If DeepFool exhibits adversarial robustness bias for a dataset and a model and a class does carlini-wagner exhibit the same and vice versa Since there are different convolutional models we have different comparisons to make Again we rst look at the signedness of and and we see that sign sign This means there is agreement between DeepFool and carlini-wagner about the direction of the adversarial robustness bias To investigate if this behavior is exhibited earlier in the training cycle than at the final fully-trained model we compute and for the various models and datasets for trained models after epoch and the middle epoch For the rst epoch of the partitions were internally consistent ie the signedness of was the same in the rst and last epoch and were internally consistent We see that at the middle epoch of the partitions were internally consistent for DeepFool and were internally consistent for carlini-wagner Unsurprisingly this implies that as the training progresses so does the behavior of the adversarial Our full slate of approximation results are available in Appendix A robustness bias However it is surprising that much more than of the final behavior is determined after the rst epoch and there is a slight increase in agreement by the middle epoch We note that of course adversarial robustness bias is not necessarily an intrinsic value of a dataset it may be exhibited by some models and not by others However in our studies we see that the UTKFace dataset partition on Race/Ethnicity does appear to be significantly prone to adversarial attacks given its comparatively low and values across all models EVALUATION OF ROBUSTNESS BIAS USING RANDOMIZED SMOOTHING In Section we argued that randomized smoothing can be used to obtain lower bounds on G which can then be used to measure robustness bias In this section we audit popular models on a variety of datasets described in detail in Section for robustness bias as measured using the approximation given by randomized smoothing Evaluation of c To assess whether the estimate of G by randomized smoothing is an appropriate measure of robustness bias we compare the signedness of and When has positive sign higher magnitude indicates a higher robustness of members of partition as compared to members not included in that partition similarly when is negatively signed higher magnitude corresponds to lesser robustness for those members of partition see We may interpret shared signedness of both where G is deterministic and where G is measured by randomized smoothing as described in Section as positive support for the c measure Similar to Section we consider all possible partitions across CIFAR- CIFAR- CIFAR-Super UTKFace and Adience For each of these partitions we compare to the corresponding We nd that their sign agrees times ie sign sign thus giving a agreement Furthermore the mean difference between and ie is with a variance of This provides evidence that randomized smoothing can also provide a meaningful estimate on G in terms of measuring robustness bias Audit of Commonly Used Models We now evaluate the same models and all the datasets for robustness bias as measured by randomized smoothing Our comparison is analogous to the one performed in Section using adversarial attacks Figure shows results for all models on the UTKFace dataset Here we plot for each partition of the dataset on x-axis and for each model y-axis A darker color in the heatmap indicates high robustness bias darker red indicates that the partition is less robust than others whereas a darker blue indicates that the partition is more robust We can see that some partitions for example the partition based on class label and the partition based on race black tend to be less robust in the final trained model for all models indicated by a red color across all models Similarly there are partitions that are more robust for example the partition Figure Depiction of and for the UTKFace dataset with partitions corresponding to the class labels C and the gender and race/ethnicity These values are reported for all ve convolutional models both at the beginning of their training after one epoch and at the end We observe that largely the signedness of the functions are consistent between the ve models and also across the training cycle Figure Depiction of for the UTKFace dataset with partitions corresponding to the class labels C and the gender and race/ethnicity A more negative value indicates less robustness bias for the partition Darker regions indicate high robustness bias We observe that the trend is largely consistent amongst models and also similar to the trend observed when using adversarial attacks to measure robustness bias see Figure based on class and race asian end up being robust across different models indicated by a blue color Figure takes a closer look at the distribution of distances for the UTKFace dataset when partitioned by race showing that for different models different races can be more or less robust Figures and we see similar trends for CIFAR- CIFAR- CIFAR-Super and Adience which we report exhaustively in Appendix A lead us to the following key conclusions Dependence on data distribution The presence of certain partitions that show similar robustness trends as discussed above eg see final trained model in Figs and the partitions by class and race asian are more robust whereas the class and race black are less robust across all models point to some intrinsic property of the data distribution that results in that partition being more or less robust regardless of the type decision boundary Thus we conclude that robustness bias may depend in part on the data distribution of various sub-populations Dependence on model There are also certain partitions of the dataset eg based on the classes and as per Fig that show varying levels of robustness across different models Moreover even partitions that have same sign of across different models have very different values of This is also evident from Fig which shows that the distributions of G as approximated by all our proposed methods for different races can be very different for different models Thus we conclude that robustness bias is also dependent on the learned model Role of pre-training We now explore the role of pre-training on our measures of robustness bias specifically we pre-train ve of the six models Resnet Alexnet VGG Densenet and Squeezenet on ImageNet and then fine-tune on UTKFace We also train a UTK classifier from scratch on UTKFace Figures and shows robustness bias scores after the rst epoch and in the final fully-trained model At epoch we mostly see no robustness bias indicated by close-to-zero values of for UTK classifier This is because the model has barely trained by that rst epoch and predictions are roughly equivalent to random guesses In contrast the other ve models already have pre-trained ImageNet weights and hence we see certain robustness biases that already exist in the model even after the rst epoch of training Thus we conclude that pre-trained models bring in biases due to the distributions of the data on which they were pre-trained and the resulting learned decision boundary after pre-training We additionally see that these biases can persist even after fine-tuning Comparison of Randomized Smoothing and Upper Bounds We have now presented two ways of measuring robustness bias via upper bounds and via randomized smoothing While there are important distinctions between the two methods it is worth comparing them To do this we compare the sign of the randomized smoothing method and the upper bounds as sign sign i and sign sign i We see that there is some evidence that the two methods agree The Adience UTKFace and CIFAR- dataset have strong agreement at or above between the randomized smoothing for both types of upper bounds DeepFool and Carlini-Wagner while the CIFAR dataset has a much weaker agreement above but closer to and CIFAR-Super has an approximately agreement It is important to point out that it is not entirely appropriate to perform a comparison in this way Recall that the upper bounds provide estimates of using a trained model However the randomized smoothing method estimates not directly with the trained model instead it rst modifies smooths the model of interest and then performs an estimation Since the upper bounds and randomized smoothing methods are so different in practice there may be no truly appropriate way to compare the results therefrom Therefore too much credence should not be placed on the comparison of these two methods Both methods indicate the existence of the robustness bias phenomenon and can be useful in distinct settings We present the full results table in Appendix C AN OBVIOUS MITIGATION STRATEGY Having demonstrated the existence of this robustness bias phenomenon it is natural to look ahead at common machine learning techniques to address it In Appendix B we have done just that by adding to the objective function a regularizer term which penalizes for large distances in the treatment of a minority and majority group We write the empiric estimate of RB g as RB g a full derivation can be found in Appendix B Formally RBg G G G g G G G g Details of a full implementation of this loss function and the results can be found in Appendix B Experimental results based on that implementation reported in Appendix B support the idea that regularization a typical approach taken by the fairness in machine learning community can reduce measures of robustness bias However we do believe that this type of experimentation belies the larger point of the present largely descriptive work that robustness bias is a real and significant artifact of popular and commonly-used datasets and models Surely there are ways to mitigate some of the effects or manifestations of this bias as we show with our fairly standard regularization-based mitigation technique However we believe that any type of mitigation should be taken in concert with the contextualization of these technical systems in the social world and thus leave mitigation research to ideally application-specific future work involving both machine learning practictioners and the stakeholders of particular systems DISCUSSION AND CONCLUSION We propose a unique definition of fairness which requires all partitions of a population to be equally robust to minute often adversarial perturbations and give experimental evidence that this phenomenon can exist in some commonly-used models trained on real-world datasets Using these observations we argue that this can result in a potentially unfair circumstance where in the presence of an adversary a certain partition might be more susceptible ie less secure Susceptibility is prone to known issues with adversarial robustness such as sensitivity to hyperparameters Thus we call for extra caution while deploying deep neural nets in the real world since this form of unfairness might go unchecked when auditing for notions that are based on just the model outputs and ground truth labels We then show that this form of bias can be mitigated to some extent by using a regularizer that minimizes our proposed measure of robustness bias However we do not claim to solve unfairness rather we view analytical approaches to bias detection and optimization-based approaches to bias mitigation as potential pieces in a much larger multidisciplinary approach to addressing these issues in fielded systems Indeed we view our work as largely observational we observe that on many commonly-used models trained on many commonly-used datasets a particular notion of bias robustness bias exists We show that some partitions of data are more susceptible to two state-of-the-art and commonly-used adversarial attacks This knowledge could be used for attack or to design defenses both of which could have potential positive or negative societal impacts depending on the parties involved and the reasons for attacking and/or defending We have also defined a notion of bias as well as a corresponding notion of fairness and by doing that we admittedly toe a morally-laden line Still while we do use fairness as both a higher-level motivation and a lower-level quantitative tool we have tried to remain ethically neutral in our presentation and have eschewed making normative judgements to the best of our ability