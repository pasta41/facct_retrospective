Can You Fake It Until You Make It Impacts of Differentially Private Synthetic Data on Downstream Classification Fairness The recent adoption of machine learning models in high-risk settings such as medicine has increased demand for developments in privacy and fairness Rebalancing skewed datasets using synthetic data created by generative adversarial networks GANs has shown potential to mitigate disparate impact on minoritized subgroups However such generative models are subject to privacy attacks that can expose sensitive data from the training dataset differential privacy DP is the current leading solution for privacy-preserving machine learning differentially private GANs DP GANs are often considered a potential solution for improving model fairness while maintaining privacy of sensitive training data We investigate the impact of using synthetic images from DP GANs on downstream classification model utility and fairness We demonstrate that existing DPGANs cannot simultaneously maintain model utility privacy and fairness The images generated from GAN models trained with DP exhibit extreme decreases in image quality and utility which leads to poor downstream classification model performance Our evaluation highlights the friction between privacy fairness and utility and how this directly translates into real loss of performance and representation in common machine learning settings Our results show that additional work improving the utility and fairness of DP generative models is required before they can be utilized as a potential solution to privacy and fairness issues stemming from lack of diversity in the training dataset INTRODUCTION Machine learning models are being trained for use in high-stakes settings on sensitive data in domains such as medicine and consumer finance These models can have disparate impacts on minoritized subgroups for a number of reasons including due to class imbalance and small minority sample sizes in the underlying training dataset These impacts often amplify existing prejudices against minorities in society under the guise of automated impartiality The use of balanced synthetic datasets created by Generative Adversarial Networks GANs to augment classification training has demonstrated some benefits for reducing disparate impact due to minoritized subgroup imbalance However GANs and the synthetic data generated from them are vulnerable to privacy attacks due to their tendency to memorize the training distribution This makes GANs vulnerable to membership inference attacks which can expose sensitive information about individuals in the underlying training data to malicious entities differential privacy DP is the leading technique for preserving privacy in statistical and machine learning settings It has seen successful applications at Google Apple and in the US Census resulting in its increased popularity and use DP guarantees privacy by ensuring that the inclusion or exclusion of any particular individual does not significantly change the output distribution of an algorithm differentially private stochastic gradient descent DP-SGD is a popular method for training deep learning models with DP While DP-SGD has strong theoretical guarantees for privacy and generalization its application has been shown to disparately impact model performance for minoritized subgroups in imbalanced datasets theoretically and empirically in the fields of computer vision and natural language processing GANs trained without DP have been extremely successful at generating high quality synthetic images Dataset augmentation using GANs has been shown to improve downstream classification performance Training classification models on differentially private synthetic data has been proposed as a technique to ensure the privacy of training data Empirically GANs trained with DP have been shown to generate useful synthetic images on simpler datasets such as MNIST Most investigations of DP GANs on image datasets have used lower dimensional datasets such as MNIST and SVHN though some have utilized high-dimensional facial datasets such as CelebA and more challenging tabular datasets However to our knowledge there have been no investigations into whether DP synthetic images provide the same improvements to utility and fairness in downstream classifications that synthetic images generated without privacy do Furthermore past DP GAN research has not taken advantage of state-of-the-art SOTA GAN architectures such as Progressively Growing GANs PG-GANs that have been advantageous in generating high-dimensional image data Such composite analysis of the fairness utility and privacy impacts of DP is essential for its application in real-life systems In this work we investigate the viability of DP synthetic data as a solution to improving both utility and fairness of downstream classification while maintaining privacy We focus on two chest x-ray datasets the MIMIC Chest X-Ray MIMIC-CXR dataset and CheXpert CXP and two facial datasets CelebA-HQ and FairFace one dermatology datasets the ISIC archive To succeed in this analysis we extend the current theory for the sequential adaptive composition of heterogeneous Rényi differential privacy RDP sampled Gaussian mechanisms and provide new guarantees to support training state-of-the-art DP GANs such as Progressively Growing GANs PG-GANS Based on these new guarantees we empirically investigate the effect that DP synthetic data generated from PG-GANs has on the utility and fairness of classification models We compare the generated image quality as we increase privacy levels and investigate whether there is disparate impact to image quality across subgroups Next we investigate whether DP protects all subgroups equally from membership inference attacks on GANs Finally we investigate the impacts of training downstream classification models on balanced DP synthetic datasets We quantify the impacts on both model utility and fairness We demonstrate that DP does not introduce unfairness into the data generation process or to standard group fairness measures in the downstream classification models but does unfairly increase the influence of majority subgroups DP also significantly reduces the quality of the images generated from the GANs which decrease the utility of the synthetic data in downstream tasks Our results demonstrate that more research into new techniques is needed to improve the utility and quality of synthetic image data generated using DP GANs Currently applying DP-SGD to GANs does not result in high quality high-dimensional private synthetic image datasets that are useful for training downstream classification models Contributions We provide a new upper bound on the privacy loss of the sequential adaptive composition of heterogeneous RDP mechanisms to support private training of PG-GANs Current adaptive composition bounds for sampled Gaussian RDP mechanisms assume constant sampling probability batch size across all steps taken by the learning algorithm Thus those bounds do not apply to PG-GAN architectures which have varying sampling probabilities during training Our method allows us to reliably quantify the privacy guarantees from PG-GANs Our new bounds are an important addition to understanding the sequential adaptive composition of the heterogeneous sampled Gaussian RDP mechanism DP significantly decreases image quality but there are no trends in the difference in image quality between subgroups Generated image quality is reduced by DP overall but there is no observed disparate subgroup impact in our chosen datasets DPGANs defend against membership inference attacks in the presence of dataset shift We show that GANs trained without privacy are subject to membership inference attacks when the test set is shifted from the training set and demonstrate that DP defends against these attacks DP synthetic images decrease downstream model performance We nd that due to the poor quality of the DP synthetic images the performance of downstream classification models significantly degrade DP synthetic images have the potential to give unfair influence to majority subgroups but result in no changes to group fairness measures We nd that the use of DP synthetic images results in no significant changes to classification model group fairness measures However the influence of the majority subgroup on the downstream predictions can increase with increased privacy for highly imbalanced datasets BACKGROUND AND RELATEDWORK differential Privacy differential privacy DP ensures that the output of statistical analyses are robust to the inclusion or exclusion of a particular individual It provides important theoretical guarantees for privacy and has been shown empirically to defend against privacy attacks DP-SGD is currently the leading private learning algorithm for training neural networks Other private learning algorithms such as objective perturbation are only applicable to linear models because they assume convex loss functions The privacy loss in DP-SGD is tracked using methods such as the Moments Accountant or the Rényi differential privacy RDP Accountant DP learning algorithms focus on learning the body of the joint data and label distribution By doing so they omit the tails of the distribution Recent work shows that this leads to disparate impact for minoritized subgroups due to the combination of gradient clipping and noise addition Additional background about differential privacy has been included in Appendix A GANs for Image Generation GANs learn to model a data distribution They can then sample from the modeled distribution to generate novel data and are extremely effective at learning high-dimensional distributions such as image data The training procedure follows a minimax optimization problem between a generator and a discriminator Conditional GAN models are modified to also input a label to both the generator and the discriminator in order to learn a conditional distribution based on that label Progressive Growing GANs Many of the current state-of-the-art GAN models such as PG-GAN and StyleGAN use progressive growing architectures These architectures progressively grow the generator and discriminator networks by gradually adding additional layers This helps the model to learn global features at low resolutions and local features at high resolutions and mitigates common GAN training issues such as mode collapse differentially Private GANs GANs tend to output data samples that are closer to the training data than to unseen data raising privacy concerns when the training data contains sensitive information differentially private GANs DP GANs and Private Aggregation of Teacher Ensembles GANs PATE-GANs are able to achieve differential privacy in GANs by employing the DP-SGD or PATE mechanism during training Since PATE-GAN has not yet been shown to be effective on image data we focus our investigation on DP-SGD We have chosen to employ the progressively growing technique with the conditional version of DP GANs in our work DP-CGANs Additional background about GANs has been included in Appendix A differential Privacy and Fairness differentially private training has been shown to degrade model accuracy This accuracy drop is more significant in DP-SGD for minoritized subgroups in the training dataset Recent attempts to make differentially private learning fair have looked at Lagrangian dual approaches and post-processing techniques similar to equalized odds and multicalibration Most current work in DP dataset synthesis focuses on low-dimensional image or tabular data and does not investigate the effects on downstream classification fairness Improving Model Performance With Synthetic Data Augmenting the original training dataset using generated images from GANs has shown promise in improving performance in downstream tasks Downstream Training with DP DP is immune to postprocessing This means the output of a differentially private mechanism cannot be made less private regardless of any auxiliary information or the unbounded computational resources a malicious entity may have This is important for using DP synthetic images because downstream models trained on these generated images do not need to use DP in their training process Therefore we can train classification models on generated differentially private data and maintain privacy EXTENDING THE RDP ACCOUNTANT TO PROGRESSIVELY GROWING TRAINING We use the TensorFlow Privacy library to train our models with DP-SGD and to track the privacy loss using an RDP accountant for sampled Gaussian mechanisms SGM This implementation is based on the theoretical analyses in which assumes that batch size remains constant throughout training PG-GANs decrease batch size during training due to memory constraints Existing work on DP PG-GANs naively applies the RDP Accountant and provides underestimates of the privacy loss We extend the RDP for SGM privacy analysis to account for varying batch sizes throughout training in Section We use this modification to measure the privacy of our model using the derived n guarantee Lower n values mean higher privacy Note that the privacy guarantees we provide are underestimates of the true privacy loss because we do not add the privacy loss incurred due to hyper-parameter search Extending the RDP Accountant We derive a new tight upper bound on the privacy loss for our PGGAN training and provide a loose upper bound which generalizes more broadly to progressively growing learning procedures We use this modification to the RDP for SGM accountant to track privacy loss Progressively Growing Private Training Analysis We present a theoretical analysis on the upper bound of the RDP of our models trained with progressively growing training We extend the theoretical work of for our specific use case Preliminaries definition Rényi Divergence Let there be two probability distributions and with densities and respectively We define the distribution on X over the same probability space The Rényi Divergence for a finite order U between and is defined as U U G G U definition Rényi differential Privacy We say that a mechanism is U n with order U if for all neighboring datasets U n definition Sampled Gaussian Mechanism Let be a function mapping subsets toR in our case this maps mini-batches to gradients We define the Sampled Gaussian Mechanism SGM parameterized with sampling parameter and the noise G G with sampling probability q N where each element of is sampled independently with probability without replacement and is a d-dimensional spherical Gaussian with per coordinate variance T RDP for Sampled Gaussian Mechanism The upper bound of the RDP for an SGM that is U as proven in under their assumptions is n U U T Constant Sampling Probability Using the RDP composition theorem and tight bounds on the RDP SGM let the composition of homogeneous U sampled Gaussian mechanisms with constant sampling probability resulting in an U n mechanism be n U U Figure Pipeline for training and evaluations We train PG-GANs on imbalanced datasets with varying levels of noise for differential privacy We then evaluate the quality of the synthesized images from these generative models across different privacy levels We perform privacy attacks on the generative models to observe the effect of privacy on the success of the attacks We generate balanced synthetic datasets using the PG-GANs and use the balanced datasets to train downstream classification models The performance of these models are evaluated against real test images We also analyze the fairness of these downstream classifiers across various fairness metrics We analyze the influence of each data point in the balanced synthetic dataset on the output predictions of a downstream classification model Analysis DP-SGD can be viewed as an adaptive sequential composition of U mechanisms Previous analyses such as the Moments Accountant and RDP Accountant assume that the sampling probability for each mechanism is constant In our work we examine the composition when the sampling probability decreases during training due to the progressive growing learning algorithm thus motivating the extension of existing composition upper bounds for decreasing sampling probability over the sequential adaptive mechanisms We extend the composition theorems for RDP and tight bounds for RDP sampled Gaussian mechanisms to this unique case of sequential adaptive composition with decreasing sampling probability that appears when training PG-GANs We prove both theorems in Appendix E T Decreasing Sampling Probability With Constant Decay and Steps Let there be T RDP sampled Gaussian mechanisms SGMs We partition this set of mechanisms times Each partition is made up of U n mechanisms where For each partition the original sampling probability decays by where The upper bound of the composition of this set of heterogeneous RDP SGMs is n U U We provide a tight upper bound on given these assumptions and the assumptions on and from the previous theorem In our previous theorem we provide a tight upper bound based on the assumptions regarding the varying sampling probabilities and the number of steps taken at each sampling probability Typically the sampling probability decreases over time because we reduce the batch size Since we reduce the batch size the number of steps taken increases compared to previous batch sizes For our theoretical analysis we make the prior theorem more generalizable by only assuming that the number of steps for each sampling probability is greater than that of the previous sampling probability ie that the number of step T Varying Sampling Probability With Varying Decay and Step Sizes Let there be T RDP sampled Gaussian mechanisms SGMs We partition this set of mechanisms times each partition B B B mechanisms being made up of U n mechanisms where For each partition the original sampling probability decays by where The upper bound of the composition of this set of heterogeneous RDP SGMs is n U B U METHODS Figure shows our model training and evaluation pipeline We rst trained GANs on real image data with and without progressive growing to compare which architecture produces the best images when training with differential privacy We focus our evaluations on PG-GANs as they produce higher quality images than non-growing GANs Section GAN Architecture Our GAN model is a conditional variation of the PG-GAN trained with a differentially private discriminator model This training process allows for the generator model to be differentially private as well via the Post-Processing Theorem as shown in Appendix A The GAN is trained with an Adam optimizer with gradient clipping and noise addition based on DP-SGD We condition our PG-GAN using both the protected and target labels to ensure that we can generate a balanced synthetic dataset to train our downstream classification models All images used in the GAN training process and the rest of our evaluation pipeline have been resized to be pixels Progressively Growing Training Progressively growing training modifies the standard GAN training procedure by gradually fading layers into the model rather than training all the layers at the same time This process trains the model at lower resolution images rst allowing it to learn the high-level structure of the data before learning the low-level details In order to accommodate for memory constraints the mini-batch size is decreased as the model size and image resolution increases This procedure has been shown to improve the resulting generated image quality differentially Private Training We train models without privacy guarantees using stochastic gradient descent SGD Models are trained with DP-SGD which is the only currently suitable differentially private learning method for deep neural networks DP-SGD makes two modifications to SGD clipping gradients computed on a per-example basis to have a maximum norm and then adding Gaussian noise to these gradients before applying parameter updates We select the clipping norm to be from hyper-parameter tuning We use three different noise multipliers of corresponding to low medium and high privacy settings respectively For each task four increasing levels of privacy are explored No Privacy Standard PG-GAN Model Low Privacy Medium Privacy and High Privacy We use our modification to the RDP accountant discussed in Section to track the privacy loss of the model The epsilon values for each privacy level and dataset are presented in Appendix Downstream Model Training We generate balanced synthetic datasets using the trained PG-GANs for each real dataset at each privacy level Each synthetic data is the same size as the corresponding real dataset but is balanced across both the protected attribute and the target prediction attribute The attributes for each dataset are listed in Table We then use these balanced synthetic datasets to train different types of downstream classification models to predict the target label linear regression LR random forest RF and multi-layer perceptron MLP EVALUATION Evaluation of Synthetic Image Quality We evaluate the quality of synthesized images from these GANs using Frechet Inception Distance FID FID estimates the quality and diversity of a set of synthetic images as compared to the real dataset It extracts the activations of the real and generated images from the last pooling layer of Inception v These activations are used to measure the distance between the distribution of the real images and the distributions of the generated images Evaluating Privacy Attacks for GANs We perform membership inference attacks on the GANs across all privacy levels to measure the ability of DP to defend against these attacks Membership inference exploits the memorization of training examples in the parameters of neural networks We assume that the malicious entity has access to the trained generator leading to a white-box attack This is a reasonable assumption given that trained GAN models are often released publicly We use a white box membership inference attack based on the reconstruction loss for each image The attacker uses the parameters of the generator model to optimize the random latent vector inputted to the model to reproduce the given images as closely as possible The reconstruction loss between this reproduced generated image and the real image is lower for images that can be reproduced more closely The images with the lowest reconstruction loss are taken by the attacker to be members of the original training set If this attack has a high success rate measured by the Area Under Receiver Operating Characteristic Curve AUC then we conclude that the model has high memorization and is vulnerable to membership inference privacy attacks Downstream Model Evaluation classification Performance We measure the performance of models using AUC on real test images from the original real dataset Group Fairness Metrics We evaluate classification fairness under four different standard measures of group fairness performance gap parity gap recall gap and specificity gap We provide definitions for these metrics in Appendix B Group influence Metrics We also measure fairness via the level of influence as defined by Cooks distance for each subgroup Cooks Distance captures the influence of a particular training point on a linear regression model DATA We conduct our experiments on two chest x-ray datasets the MIMIC Chest X-Ray MIMIC-CXR dataset and CheXpert CXP two facial datasets CelebA-HQ and FairFace and one dermatology dataset the ISIC archive Table reports the label distributions for these datasets Detailed dataset distributions are reported in Appendix D We intentionally selected a variety of balanced and imbalanced datasets in order to investigate the effect of imbalanced training data All datasets use the same methods and evaluation pipeline as described in Sections and Chest X-Ray Datasets We take M/F to be our protected attribute based on prior work demonstrating disparities in chest x-ray classifier performance for this label We pick the E label to be our target label out of a possible labels because it is one of the most prevalent labels in MIMIC-CXR positive rate and CXP positive rate We only use the frontal images in both datasets Our experiments on this data investigate if our methods are able to improve the fairness of models in the medical imaging eld Dermatology Dataset We take S T to be our protected attribute in for ISIC We generate the binary skin tone label based on Fitzpatrick Skin Tone categories using a similar automated procedure to the one Dataset Name Protected Label Target Label Majority Protected Attribute Cross-Dataset Test Set Total Images MIMIC-CXR Sex Male/Female No Male CXP CXP Sex Male/Female No Male MIMIC-CXR CelebA-HQ Skin Tone Light/Dark Young/Old Light Skin Tone FairFace FairFace Skin Tone Light/Dark Young/Old Dark Skin Tone CelebA-HQ ISIC Skin Tone Light/Dark No Melanoma/Melanoma Light Skin Tone N/A Table Datasets and labels used in our analysis All of the labels listed are treated as binary labels in our evaluations We use the protected label and target labels to condition our PG-GAN In the classification models we only predict the target label In privacy attack experiments for MIMIC-CXR CXP CelebA-HQ and FairFace we also test each GAN model on a different dataset in the same domain as the dataset the model was trained on Cross-Dataset Test Set Detailed dataset distributions are included in Appendix D described in The procedure is included in Appendix C We pick the presence of M to be our target label for this dataset Facial Datasets We take S T as our protected attribute in our facial datasets We generate the binary skin tone label based on Fitzpatrick Skin Tone categories using the same process as for the dermatology dataset FairFace only contains labels for the attributes of race gender and age We formulate a label Y to be the target predictor label in FairFace by splitting the provided age buckets in half We select as the target label in CelebA-HQ to match our FairFace labels Facial Datasets Recognition and Fairness There have been several facial datasets that have recently been constructed by the computer vision community such as CelebA-HQ and FairFace The collection and use of these datasets has led to concerning and unethical research due to the inherent systemic biases sexism racism colorism and ageism from the collection of the images and their labels We use CelebA-HQ and FairFace in our experiments for prediction tasks to illustrate our hypotheses about privacy and fairness Our analysis serves to demonstrate the negative effects of using these labels We do not condone use of and prediction on these datasets in practical scenarios We emphasize that irresponsible and unethical usage of these labels and datasets could have harmful impacts on the research community and on society-at-large EXPERIMENT EVALUATING QUALITY OF SYNTHETIC DP DATA First we analyze the effect of increasing levels of privacy on the quality of images generated by the DP PG-GANs conditioned on our protected attributes and target labels Experimental Setup We evaluate the quality of generated images compared to real images across GANs of increasing privacy levels using Frechet Inception Distance FID We also measure this on a subgroup level to quantify possible disparate impact Image Quality Metrics A low FID score indicates that the set of generated images is close in distribution to the images in the training set and therefore realistic The minimum possible FID Figure FID Scores of PG-GANs vs GANs Closer to is better The PG-GANs achieve much better FID scores than the non-PG models in the DP case and similar FID Scores in the non-DP case Models trained with DP have poorer FID scores than models trained without DP score is which corresponds to the score for when the generated image distribution exactly matches the training image distribution Result Progressive Growing Training Improves DP Synthetic Image Quality We rst confirmed that progressively growing training with DP improved the resulting image quality Figure shows the differences in image quality between DP vs non-DP models trained on CelebAHQ The models trained with DP are trained in the medium privacy setting The models trained with DP show significant improvements when using PG training even when non-DP models show negligible improvements This shows that the improvements in FID experienced by large high-quality image models such as StyleGAN are extensible to models trained with DP as well It also demonstrates the need for our formulated privacy accountant that accurately supports progressively growing learning in Section Therefore we focus on PG-GANs in the remainder of our experiments and results Result DP Degrades Quality of Synthetic Images High privacy levels result in lower image quality across all datasets Figure b shows the computed FID metric for each privacy level and Figure a Generated images from GANs trained on MIMIC-CXR Qualitatively models trained with higher privacy produce worse-quality images b Left Comparison of FID scores of PG-GAN models across different DP privacy levels Overall image quality decreases significantly with DP when compared to the non-DP models Right difference in FID between the protected subclasses listed in Table Negative FID difference means that the minority protected subclass has relatively worse images Note that models trained with DP generally have higher differences in FID between their protected subclasses but we observe no directional trends with relation to the majority protected class attribute Additional results are included in Appendix G a White Box Attack AUC at different privacy levels b AUC-Difference in White Box Attacks between protected subclasses c Samples reconstructed from the training set using GAN models trained on CelebAHQ at different privacy levels Figure Comparison of white-box inference attack success rate in dataset as well as from different datasets within the same domain Cross-Dataset as listed in Table Attacks using test images from a different dataset of the same domain are more successful lower AUC is better The cross-dataset white box attacks are less successful for DP-trained GANs Therefore DP protects against membership inference attacks difference in attack AUC between protected subclasses increases as privacy increases for CelebA-HQ From the reconstructed images we can see that the model trained without privacy is able to more closely reconstruct the input images than the models trained with privacy indicating potential memorizing and over-fitting to the training set Additional attack results are included in Appendix the corresponding generated images The FID worsens as the privacy levels increases Figure a qualitatively visualizes the output of the GAN models trained on the MIMIC-CXR dataset The images become noisier and more unrecognizable as privacy increases Synthetic chest X-ray images were also reviewed post-submission by a licensed radiologist to confirm that the quality of the x-ray images decreased as privacy levels increased Finally we nd no trends in the FID difference across the protected subgroups with increasing privacy levels Figure b EXPERIMENT EVALUATING ROBUSTNESS TO PRIVACY ATTACKS We measure the efficacy of different levels of DP for protecting PG-GANs against white-box membership inference attacks compared to PG-GANs trained without DP Experimental Setup We use the vulnerability to white-box attacks of PG-GANs trained without DP as our baseline for this evaluation We then measure the attack success on PG-GANs with increasing levels of privacy across all datasets The attack is successful if it is able to accurately predict which data points are from the original training set of the model We evaluate whether increased privacy reduces attack success We evaluate both the setting where the training and test set of images are from the same distribution within the same dataset and the setting where the training set and test are from different datasets in the same domain ie across CelebA-HQ and FairFace White-Box Membership Inference Attack In a whitebox attack the adversary attempts to infer whether a particular image belonged to the original training set of a generative model based on the models ability to reconstruct that image The model parameters are used to optimize a latent variable input to the model in order to minimize reconstruction loss and produce a synthetic image that most closely resembles the given image Performing this attack on both training and test images allows us to estimate training set membership since models with memorization will be able to better reconstruct the training images than unseen images We use training images and unseen test images images total The model takes the images with the lowest reconstruction loss to be members of the training set The AUC of the white-box attacks represents the success level of the attack at predicting training set membership Higher AUC means that the attacker can more accurately infer if an individuals data was used to train the GAN Result DP Protects GANs From Membership Inference Prior work demonstrates that membership inference is unsuccessful in GANs that are trained on large enough datasets In Figure a we observe this to be true across all privacy levels when the training and test sets are from the same distribution When the training and test set are from different datasets within the same domain GANs trained without privacy are vulnerable to membership inference In this setting of dataset shift DP protects GANs from membership inference We qualitatively analyze the effectiveness of this white-box attack in Figure c Models trained without privacy can reproduce the training set images almost exactly In real-life applications dataset shift is a common occurrence so this result presents dangerous privacy implications for deploying generative models trained without DP in real settings Therefore differential privacy is essential for preventing these attacks especially when a model is publicly released We observe that increasing privacy results in less protection for the minoritized subgroup in the presence of dataset shift for CelebA-HQ the most imbalanced dataset use in our evaluations Figure b There is no trend observed amongst the other datasets used in our evaluation EXPERIMENT FAIRNESS OF MODELS TRAINED ON PRIVATE SYNTHETIC DATA We measure the effect of DP synthetic data on downstream classification model fairness in order to evaluate the relationship between privacy and utility We perform the evaluation across all datasets and privacy levels Experimental Setup Three different downstream algorithms are evaluated linear regression LR random forest RF and multilayer perceptron MLP The DP PG-GANs from our previous experiments are used to create balanced synthetic datasets at each privacy level for downstream training The downstream algorithms are evaluated on the real test images from each corresponding dataset predicting the target label as listed in Table Our baseline is the classification performance when trained on the entire unbalanced corpus of real training data For training on the synthesized data a dataset of the same size as the real data was generated balanced across both the protected attribute and the target attribute Utility and Fairness Metrics We evaluate model utility using AUC We evaluate model fairness according to standard group fairness measures between the protected attribute classes performance gap AUC-difference between subclasses parity gap specificity gap and recall gap Result DP Reduces classification Model Utility The AUC of the classification models decreases as the privacy level increases across all datasets Figure Therefore the reduced image quality due to DP results in poor data utility and poor downstream model performance Result DP Has No Impact on Standard Group Fairness Metrics We observe no trends in any group fairness metrics for the downstream classification models Figure as privacy increases Results for additional group fairness metrics like parity gap specificity gap and recall gap have been included in Appendix I EXPERIMENT SUBGROUP INFLUENCE ON DOWNSTREAM CLASSIFICATION MODELS Experimental Setup In addition to standard group fairness metrics on downstream models we also measure fairness in terms of the influence of each synthetic training data point For each generated dataset we use Cooks distance to evaluate the influence of each training point on a downstream linear regression model influence Metrics Cooks Distance quantifies the effect that the training point has on the downstream models predictions based on the change in the models prediction values when that point is excluded from the training set We provide a detailed definition for this metric in Appendix B A highly influential point is considered to be any point that has a Cooks Distance value of over where is the total number of training data points We measure influence fairness as the percentage of highly influential points from the majority class To be determined as fair when trained on the balanced synthetic datasets the set of highly influential points should contain equal proportions of the protected subclasses a MIMIC-CXR b CXP c CelebA-HQ d FairFace Figure effect of DP on Downstream classification AUC and Performance Gap AUC difference between subclasses Training on images generated from models trained with DP significantly decreases the performance of the classification model but has no trend on the group fairness metrics Additional downstream evaluations and fairness metrics are included in Appendix I Figure Percentage of influential Points as defined by Cooks Distance from the majority class at different privacy levels Closer to is more fair DP synthetic data increases the influence of the majority class as noise increases for highly imbalanced datasets influence in the high privacy setting has been excluded due to the low utility of the model rendering any influence on the prediction output essentially meaningless Additional influence results are included in Appendix Result DP Can Disparately Impacts influence on Highly Imbalanced Datasets As shown in Figure we observe that synthetic data created by GAN models trained without DP improves or approximately maintains influence fairness when compared to the real training set for all datasets As the privacy level increases more influence tends to be given to points from the majority subgroup in imbalanced datasets CelebAHQ the most imbalanced dataset used in our evaluation displays a significant increase in influence unfairness in the presence of any DP Models trained on synthetic data created from more balanced datasets used in our evaluations do not experience the same level of disparate impact or do not display any consistent trends Therefore even when the synthetically generated dataset is balanced imbalances in the protected attribute in the original training set can still potentially result in unfair and harmful over-influence of majority subgroups in downstream evaluations DISCUSSION The Need For Diverse Real Data Lack of diversity in training datasets is known to amplify societal biases for minoritized subgroups For example such deficiencies in facial recognition algorithms have led to significantly worse recognition rates for black female faces in commercially deployed algorithms Similarly the lack of variation in medical chest x-ray datasets can cause models to have consistently poor performance on important subsets of the data Our analysis emphasizes that biased datasets may still have disparate impacts on downstream classification influence even when the real dataset is not directly used in the downstream training process and even when the synthetic dataset used for the training is balanced Therefore supplementing or replacing datasets with synthetic data does not mitigate the fairness concerns caused by the existing biases in imbalanced datasets Privacy Risks of Synthetic Data Synthetic data augmentation is often explored as a solution to missing data or to data privacy issues such as in credit card fraud detection models and sports medicine research Although synthetic data can be realistic and appear promising to use as a replacement for real data the privacy risk associated with doing so remains high While commonly used for creating synthetic data generative models do not guarantee privacy by design We have shown that GANs trained without DP are vulnerable to membership inference privacy attacks especially in the presence of dataset shift Releasing a GAN model therefore exposes the training set to attackers and can result in the leakage of sensitive data The Price of Privacy Our investigation into the effect of DP on state-of-the-art PG-GAN models in terms of downstream model utility privacy and fairness shows the negative reality of the trade-offs We have demonstrated that training with DP can address privacy concerns but has the potential to skew the influence towards the majority subgroups exacerbating already existing inequities in the data collection processes As well while DP has strong theoretical guarantees it can significantly degrade model utility when applied in real-word settings The loss in utility of the synthetic data as privacy increases causes much higher rates of misclassifications in downstream models This decrease in model performance can lead to disastrous effects in important settings such as weapons identification and pedestrian identification for autonomous cars Limitations Of Our Analysis We caution that the automatic detection of facial labels is only performed as an example and should not be considered a valid task for deployed applications Many labels that are associated with facial datasets such as Y S or A all available labels in the CelebA-HQ dataset are disproportionately biased towards women and other minoritized groups When these predictors are applied in settings such as automatic hiring algorithms they can be used against these minoritized groups in the screening process The analysis we present also contains limitations in the automated skin tone categorization since Fitzpatrick categories are known to be subject to lighting conditions and are inherently subjective even to human dermatologists Cameras are also known to less accurately capture darker skin tones We also reiterate the inherent flaws in our labels for our downstream models specifically predicting age attributes Labels with less problematic implications should be used in the future provided they are available CONCLUSION AND FUTURE WORK Our analysis shows that balanced differentially private synthetic datasets are effective against privacy attacks However DP GANs display a decrease in dataset image quality and downstream classification accuracy The utility of the synthetic images decreases significantly with increased privacy for standard downstream tasks resulting in close to random classification predictions We conclude that more work needs to be done on differentially private data synthesis before it is ready to be applied in real-world settings such as high-dimensional medical images or facial images Additional future work can also be done around the training of progressively growing models with differential privacy When training at low resolutions with DP-SGD the noise added during the training process prevents the model from effectively learning essential low level structures of the images resulting in much worse quality images than if the noise is only added when the model is learning the high-level details Theoretical work must be done around how this technique would affect privacy guarantees