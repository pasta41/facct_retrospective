Fairness Violations and Mitigation under Covariate Shift We study the problem of learning fair prediction models for unseen test sets distributed differently from the train set Stability against changes in data distribution is an important mandate for responsible deployment of models The domain adaptation literature addresses this concern albeit with the notion of stability limited to that of prediction accuracy We identify sufficient conditions under which stable models both in terms of prediction accuracy and fairness can be learned Using the causal graph describing the data and the anticipated shifts we specify an approach based on feature selection that exploits conditional independencies in the data to estimate accuracy and fairness metrics for the test set We show that for specific fairness definitions the resulting model satisfies a form of worst-case optimality In context of a healthcare task we illustrate the advantages of the approach in making more equitable decisions CONCEPTS Computing methodologies Learning under covariate shift Causal reasoning and diagnostics KEYWORDS algorithmic fairness domain adaptation covariate shift causal inference INTRODUCTION Deployment of machine learning algorithms to aid consequential decisions such as in medicine criminal justice and employment require revisiting the dominant paradigms of training and testing such algorithms Particularly the assumption that the data distribution in training and deployment will be the same is not always warranted Examples of the impact of distribution shift can be found in medical imaging tasks where the algorithms trained on one chest radiography dataset performed poorly on other datasets Similarly Nestor et al find that models for critical care tasks degraded in performance over time resulting from changes in the instrumentation of the electronic health records Given the safety-critical nature of the decisions the decision-making process should account for these shifts in distributions to ensure high predictive accuracy of the algorithms Many methods exist to learn under distribution shifts including recent work from a causal inference perspective Such methods have significant appeal since they allow learning accurate models for arbitrary shifts including those in unseen future data This is achieved by exploiting causally-relevant factors in data that are generalizable to unseen test sets as opposed to fitting to the factors specific to the training sets However the focus of the methods has been on average case prediction performance alone In certain circumstances while high predictive accuracy is a necessary requirement decisions made using the algorithms should also not lead to or perpetuate past disparities among groups in the data Without any design changes algorithmic solutions for mitigating distribution shifts that do not account for disparities in training data can result in disparate impact while predicting under distribution shifts We discuss a concrete example later At the same time most work in algorithmic fairness addresses the setting with a single learning task or domain under the assumption that the data distribution does not change between train and test settings Under this assumption minimizing classification risk along with constraints on the fairness metric in training data is likely to generalize to identically distributed test data Thinking about shifts in fair machine learning is also important though since deployment of a fair decision-making tool might affect what data is collected in future eg selectively policing locations with high predicted risk or might incentivize individuals to strategically adapt their features for favourable outcomes thus causing distribution shift In addition due to data-scarcity such as in medical decision-making the models may be applied to newer settings such as hospitals than the ones seen during training The issue of ensuring fairness when deployment environment differs from the training one has received little attention Due to the variety of train-test shifts that can occur conceptualizing and addressing the problem has been challenging Our contributions We address the problem of learning fair models under mismatch in train-test distributions when either limited or no data is available from the test distribution We consider the setup of causal domain adaptation where possible shifts are expressed using causal graphs with the goal of learning models with stable performance under the specified shifts Our main contribution is to formulate the fair learning problem in this setup and provide sufficient conditions that enable estimation of model accuracy and fairness metrics in the test domain For a subset of covariate shifts and for several well-known group-fairness metrics we show that the resulting solution is worst-case optimal We operationalize the sufficient conditions in an algorithm based on a reduction to the standard fair learning problem Finally we present a case study on a medical decision-making task which demonstrates applicability of the approach RELATEDWORK Domain adaptation and fair machine learning are both widely studied problems Thus we primarily focus the discussion on literature at their intersection Fairness A number of fairness metrics have been proposed that make different normative statements on the machine learning models output see for a review Depending on the application context different metrics might be appropriate or mandatory by law Consequently fairness methods have been developed to build/modify models that satisfy different fairness criteria We focus on a class of methods that pose the problem as that of constrained optimization Domain adaptation The seminal work of Ben-David et al relates the target domain error to the source domain error and the distance between the distributions This inspired many domain adaptation methods based on adversarial training of representations to align the distributions One drawback is that the methods require some data from test distribution while training When causal structure of the domains is known recent work on causal domain adaptation identify predictors with stable accuracy under unseen changes in distribution To accomplish this the methods exploit the principle of invariance of causal mechanisms Sec that says interventions or shifts in certain mechanisms in the graph keep the other mechanisms unchanged The invariant mechanisms can be used to build stable predictors Similar to we adopt a setting where a causal graph specifies anticipated distribution shifts and no target domain data is given but can be used if available The goal is to construct predictors that are invariant to all anticipated shifts without necessarily observing the corresponding data The setting is particularly well-suited for consequential decision-making where we want to proactively guard against shifts that may result in harm before deploying the model and collecting target data However none of these methods consider the possibility of unfair outcomes after adaptation Fairness and domain adaptation On multiple benchmark datasets Friedler et al found that fair machine learning methods showed high variance in achieved accuracy and fairness on randomly split train-test sets To mitigate this Huang and Vishnoi propose adding a regularization term to the constrained ERM problem that guarantees stability However the term stability is used for changes in the fairness metric as a training data sample is removed/added as opposed to changes under different distributions In authors propose algorithms for generalisation of fairness constraints but to an iid test set In the authors propose learning feature representations using adversarial training which result in fair classifiers when trained on the representations They do not address changes in distribution of the features and their representations across domains In the same setup as ours under the assumption of covariate shift but with the availability of unlabelled target data give weighting-based estimators and take a robust optimization approach Other works that assume some labelled data from the target domain include For instance learns a representation from multiple domains with guarantees on generalization to the target domain but requires labelled target data to fine-tune classifiers and a low-rank assumption that constrains dis-similarity between the domains In authors restrict to shifts in feature means and propose ways to flag a potentially unfair model under such shifts Further concurrent work posits a set of test distributions defined as weighted combinations of the training data and find a fair classifier minimizing the worst loss across such distributions Instead we rely on distributional assumptions expressed using a causal graph Considering the causal structure of the problem allows the modeller to express plausible distribution shifts more intuitively by denoting the mechanisms instead of the statistical properties that can change It also guides the construction of estimators that are robust against shifts of arbitrary magnitude rather than only the shifts in the observed datasets Our work is related in spirit to who consider building fair models from biased training data Here we provide a complementary set of results on fairness under train-test distribution mismatch avoiding assumptions on specific generative processes for the shift Instead we use causal graphs to make weaker assumptions on where the mismatch is This allows us to give a general characterization of the addressable mismatch settings Moreover at a conceptual level our focus is on addressing mismatch with multiple future test sets rather than a biased training set PROBLEM SETUP Let us denote all the variables associated with the system being modelled as V X where is the sensitive attribute X is a non-empty set of covariates other than and is an outcome of interest We will consider a binary sensitive attribute a d ie advantaged and disadvantaged group and the binary classification case thus For simplicity of exposition consider the case with only two domains a source and a target with joint probability distributions source and target respectively Crucially the two distributions may be different eg data from two hospitals with different care practices Bold letters are used for vectors uppercase for random variables and lowercase for instantiations Fair classifier Consider that the classifier is built from the feature subset S X and outputs the binary prediction S We will operate in the empirical risk minimization framework for learning classifiers and introduce additional fairness constraints in the objective to control the inter-group disparity a commonly-used approach Each constraint is given by some function of the prediction outcome and features Denote the constraint by S S with S target S and some hyperparameter allowing for approximate fairness If there are multiple constraints we write the set of constraints succinctly as target Note that the desired fairness constraint G is assumed to be the same in both the domains The classification error ie probability of a misclassification is written as S Then the fair domain adaptation DA problem amounts to finding a minimizer Fair DA target argmin target S target ie a function target in the set of learnable functions S of features S that minimizes classification error as well as satisfies fairness constraints Fairness metrics We will focus on group-fairness metrics defined based on some notion of parity across groups These have received much attention in the fair machine learning literature due to the relative ease of communicating their implications to stakeholders and the ease of computing them from observational data Definition DP A classifier is said to satisfy demographic parity for some distribution if S S Thus the constraint is S a S d Definition A classifier is said to satisfy equalized odds for some distribution if S S for Thus the constraints G are S a S d for We define two more metrics derived from If we condition only on the resulting metric is known as true positive rate equality TPR or more commonly equality of opportunity Similarly for the metric is known as true negative rate equality TNR Solving requires estimating the error target S and the fairness constraint target Given enough samples from target standard fair learning methods eg return a solution But this is not possible in the Fair DA setting as we do not have access to the complete target data Thus the central question we ask is Under what assumptions can we still find target For arbitrary distribution shifts it is not possible to answer this question in affirmative With background knowledge of how the distributions differ past work provides methods to bound the target domain Note that S can contain as we assume that disparate treatment is allowed in the problems of our interest error Crucially such methods still do not guarantee target domain fairness and using fairness constraints from the source domain naturally does not solve Through the following example we illustrate that these design choices can significantly affect accuracy and fairness of the models It also shows how the causal inference framework for domain adaptation allows for the specification of shifts and design of predictors An illustrative example Consider a simplified version of the flu diagnosis task from The associated data generating process is shown in Figure a Flu status of a person is to be predicted from three measurements The disease has two known causes say virus-exposure risk and age group indicating adult or child respectively In addition a noisy yet predictive symptom of flu is observed as say body temperature which is expressed differently depending on the age group A categorical variable indicates different data collection sites the domains which differ on i how well the temperature is measured eg self-reported vs clinician-tested and ii the proportion of demographics across sites Suppose a classifier is to be built using data from a single site source domain and used in multiple sites target domains to allocate scarce healthcare resources testing kits medical consultation to individuals The model designer would like to mitigate differential error rates across age groups and chooses to use as the fairness constraint while learning We compare three ways of designing the model that account differently for the possibility of shift and unfairness Figures b c show results from a simulation discussed in detail in Section As we vary the magnitude of distribution shift between the sites the Standard classifier built by regressing on from source data degrades in accuracy blue curve on target data By accounting for the shift CausalDA a domain adaptation approach that only uses the features remains stable orange curve Surprisingly domain adaptation leads to higher levels of fairness violations as shown in Figure c To mitigate this we would want to learn CausalDA with fairness constraints which is complicated as discussed earlier since we cannot evaluate the constraints for unseen target domains However following the method proposed in Section learning CausalDA with fairness constraints on the source domain red curve retains both the desired properties consistently high accuracy and low unfairness Thus the example illustrates the need to consider fairness constraints while adapting for the shifts Next we describe the joint causal graphs in more detail that allow us to represent the potential shifts followed by our main results on learning fair and stable predictors under specific shifts JOINT CAUSAL INFERENCE AND DOMAIN ADAPTATION Following recent work we consider a joint causal graph which represents the data distribution for all domains This allows us to reason about the invariant distributions under shifts which is key to addressing the fair domain adaptation problem Assume that all the source and the target domains are characterized by a set of variables V which are observed under different a Example causal graph annotated to show anticipated shifts in the distributions and/or Magnitude of Shift Ac ra o n Ta rg et D at a Standard CausalDA CausalDAFairLearn b Accuracy vs Shift Magnitude of Shift Fa es s V io l on T ar ge t D at a Standard CausalDA CausalDAFairLearn c Fairness Violation vs Shift Figure Flu diagnosis example a Data generating process for source and target domains represented as a causal graph where domains are indicated by the context variable Edges from represent shifts between the domains are features with sensitive attribute and outcome bc Classification accuracy and fairness violation with varying magnitude of shifts for synthetic data Section for the example Fairness violation is computed as the maximum violation of equalized odds constraint across and Median values are plotted over runs and error bars show first and third quartiles Proposed approach CausalDAFairLearn achieves both stable accuracy and fairness in the shifted target domains contexts eg experimental settings particular to each domain Joint Causal Inference Sec framework provides a way of representing the data generating process for all domains as a single causal graph representing an underlying causal model In addition to the system variables V the framework introduces an additional set of exogenous variables named context variables C that represent the modelers knowledge of how the domains differ from one another given by the causal relations among the system and context variables We include the formal definition of framework in Appendix D along with the necessary assumptions on faithfulness and Markov property For the example in Figure a system variables are With a binary context variable and correspond to joint distributions for the two domains source and target More generally setting context variable to a particular value say C c can be seen as an intervention that results in the data distribution for a domain V C c A class of causal domain adaptation problems is to learn a predictor that generalizes to different target data distributions which correspond to different settings of the context variables in the causal graph In authors propose learning a predictor using only a subset of the features that guarantee invariance of the outcome distribution conditional on the chosen feature subset More specifically if V X and C are the context variables the desired subset of features S X satisfies C S implying that the conditional distribution of outcome given the features S is invariant to the effect of domain changes The set S is referred to as a separating set as it d-separates and C in the joint causal graph This criterion generalizes the covariate shift criterion which assumes independence between conditioned on all the features Note that the separating set criterion excludes graphs where In a related concept selection diagrams also add auxiliary variables to a causal graph to represent the distributions that can change across different domains More discussion on the relationship between the two can be found in Under the assumptions of framework discussed in Appendix D this is the same as Cc where C c denotes an intervention C directly causes known as label shift The predictor using the separating set satisfies a desirable optimality property As shown in it has the lowest mean squared loss against any distribution having the same outcome distribution S as in the source However using a separable set in itself does not guarantee fairness For example separating sets for Figure a are S But neither satisfies the condition required for in general ie S Thus to ensure both invariance and fairness we restrict our search space in Fair DA to S ie the set of predictors built using the separating set S Next we describe the assumptions that allow us to solve this problem All proofs are included in Appendices AC in the supplemental material FAIR DOMAIN ADAPTATION Now we return to our problem of finding fair classifiers for the target domain and describe how the joint causal graph helps in solving In the context variable notation we are interested in finding argmin S where represents the target domain We start by noting the need for further assumptions Proposition Fair DA problem is not solvable in general without further assumptions Proposition follows by the impossibility results on domain adaptation Even when domain adaptation is possible ie target domain error is identifiable uniquely estimable in terms of source domain distribution the fairness constraint is not guaranteed to be identifiable We make this point by constructing an example with group-specific measurement error in features Thus the natural question is under what conditions on distributions and assumptions on data availability can we identify the error S and the fairness constraint We make the following two assumptions for the selected features S X for the classifier Assumption Invariance of classification error Features S form a separating set ie S Assumption Invariance of fairness constraint Depending on the fairness metric assume that For demographic parity DP S satisfies S For equalized odds S satisfies S For true positive rate equality TPR S satisfies S For true negative rate equality TNR S satisfies S For example the condition for DP asserts that the characteristics in terms of features S of the sensitive groups are invariant across domains Similarly the condition for says that feature distribution for groups defined by the label and the sensitive attribute is invariant across domains This ensures that we can evaluate and hence balance the corresponding fairness constraint irrespective of the domain Next we consider two scenarios to state the quality of the solution that can be found under the two assumptions i when labelled source and unlabelled target domain data is available alternatively ii when only the labelled source domain data is available Fair domain adaptation with limited target domain data Proposition Given Assumptions and hold then using only labelled source and unlabelled target data the Fair DA problem can be solved exactly by a data re-weighting method Proof sketch This follows since the error is invariant ie S S due to Assumption This implies that S S S where weights S are the ratio of feature densities Under Assumption the fairness constraint is invariant ie To solve we find argmin S Both the error and the constraint are estimable as we have labelled source data sampled from The remaining term is the density ratio S used to re-weight the error Since we have features from both source and target in this scenario S can be computed for instance using a probabilistic classifier for discriminating between the domains This solution strategy is akin to the importance-weighting approach of addressing covariate shift with the distinction being the use of the separating feature set instead of all the features Fair domain adaptation with no target domain data In the scenario when only the labelled source data is available we cannot use Proposition since we cannot estimate the weights A Demography sensitive attribute Y Disease status R Risk factors L Lab tests T Treatment a A Gender sensitive attribute L Age Y Credit risk level R Repayment duration Credit amount S Savings Housing b Figure Examples of addressable causal graphs a Disease risk scoring under population shift and treatment policy shift b Credit scoring under population shift Following Assumptions and including in the feature set blocks the effect of population shift eg the paths in magenta and excluding from the feature set blocks the effect of treatment policy shift eg the path in green Instead we use the source data with the selected features argmin S with S satisfying Assumptions and Next we show that this solution minimizes the worst-case error under fairness constraints among target distributions satisfying the two assumptions with respect to the feature subset S Such a property might be desirable for models aiding consequential decisionmaking as it guarantees good performance under the worst possible target distribution In other words the solution to will perform well for any target distribution we may encounter as long as the distribution adheres to the stated assumptions Denote the set of continuous functions which satisfy the fairness constraints G with respect to the distribution by G C where C denotes the set of all continuous functions Let P denote the distributions over X that satisfy Assumptions and for some features S Then the set G is the same for any distribution P Lemma P By Assumption if holds then also holds Thus the two sets are the same Therefore we can denote the set of fair functions by For the next result we will restrict to three fairness definitions DP TPR or TNR and assume that the conditional outcome ie the random variable X has strictly positive density on This technical condition allows us to characterize the optimal predictors in following Corbett-Davies et al Theorem Worst-case optimality Consider the set of distributions P satisfying Assumptions and which are absolutely Algorithm Fair domain adaptation via reduction to standard fair learning Input Joint causal graph G source data D source fairness metric Output Classifier target S or No_solution Initialize val for S X do Solve min source S and compute error on validation set val val end for Sort val in increasing order Traverse val and select S satisfying Assumptions and say S by checking for d-separation in graph G if S exists then Solve Fair DA problem with features S and return output else return No_solution end if continuous with respect to the same product measure and a set of fair functions satisfying either DP TPR or TNR Assume that the conditional outcome has strictly positive density Then the proposed classifier satisfies argmin sup P X That is the proposed approach achieves minimum worst-case error amongst the fair predictors with respect to the distributions satisfying the two assumptions We note that the assumption of absolute continuity in Theorem is made to avoid cases where source and target distributions have disjoint support which would make generalization challenging if some parts of the feature space are not observed at all in the source domain Practicality of assumptions Assumptions and together describe the types of shifts that our approach can address Graphically these are characterized as a shifts with causal paths to which all include ie with all arrows toward and b shifts with non-causal paths to ie for some feature S This means that any shift causing change in the distribution of the sensitive attribute as well as any shift in variables with a non-causal path to can be addressed Figure gives an example of both the cases described in more detail in Appendix Shifts in distribution of sensitive attribute are common when there is sample selection bias eg patient demographics being different between rural and urban hospitals In Section we demonstrate a general class of shifts in medical diagnosis tasks where both the assumptions are satisfied Finally we note that the assumptions barring those for DP are untestable without access to labelled target data The reason for untestability is the same as that for no unmeasured confounding we do not observe the counterfactual target data and hence cannot test for conditional independence Thus background knowledge of plausible shifts are critical Proposed algorithm The approach described in suggests a simple algorithm based on feature selection followed by solving the standard fair learning problem We assume that the following are given a causal graph for the system of interest G and data from a source domain D source The steps outlined in Algorithm are as follows a Iterate over all feature subsets to rank them in increasing order of their empirical error on the source domain b Starting from the feature set with the least error check for Assumptions and using ùëë-separation in G c Solve the fair learning problem with D source limited to model class S This can be achieved by a fair learning algorithm such as chosen based on the model class and the fairness definition If there is no S satisfying the assumptions we do not return a solution The time complexity is dominated by the search over feature subsets in a which is exponential in number of features To reduce the combinatorial search we can run a feature selection procedure eg the lasso in case of linear models Chapter to prune non-predictive features Another heuristic is to start with the set of causal parents of Y which satisfy Assumption and prune it to get a subset satisfying Assumption Extension to Counterfactual Fairness Another set of fairness definitions based on the causal effect of the sensitive attribute on the prediction have been proposed We consider one version of these counterfactual fairness definitions Definition A classifier X is said to be counterfactually fair if the counterfactual distribution of conditioned on all observed values is the same under a and d ie X x X x for and a d One method to build a classifier S satisfying is to only use feature set S X that does not contain any descendant of in the causal graph Lemma Thus the counterpart of Assumption for solving Fair DA under is that the selected feature set contains the non-descendants of Combined with Assumption we select non-descendants of which form a separating set in order to solve Fair DA Since only requires change in feature subset and does not include any fairness constraints in the fair learning problem we can show the worst-case optimality result as well described in Appendix E However we note that there are multiple ways of defining counterfactual fairness For instance require that causal effects of on through particular paths should be zero or small Further work should explore approaches to solve Fair DA under broader definitions of counterfactual fairness EXPERIMENTS The experiment settings explained next are designed to evaluate performance accuracy and fairness of the proposed classifier trained using a source dataset on unseen target datasets The constrained learning problem in is solved using the algorithm by referred henceforth as FairLearn which converts the problem into a sequence of weighted cost-sensitive classification problems Predictive performance is measured using accuracy percentage correct Bernoulli N Bernoulli N N N a Data generating process Accuracy on Target Data Fa es s V io l on T ar ge t D at a Standard CausalDA OTDA StandardFairLearn CausalDAFairLearn b High shift magnitude Accuracy on Target Data Fa es s V io l on T ar ge t D at a Standard CausalDA OTDA StandardFairLearn CausalDAFairLearn c Low shift magnitude Figure a Data for the domains with shift governed by highlighted in red bc Accuracy and fairness metrics on synthetic data example with different magnitude of shifts Median values are reported over runs and error bars show first and third quartiles Proposed approach CausalDAFairLearn is both accurate and fair under large shifts area under ROC and precision-recall curves For the experiments presented here we use as the desired fairness constraint To evaluate unfairness we report the maximum violation of the constraint ie max S S Baselines We consider five baselines which account for either distribution shift unfairness both or none of these Standard is the optimal un-constrained classifier with all available features ie V CausalDA is the classifier with the separating set ie S st S OTDA is an optimal transport-based method for unsupervised domain adaptation StandardFairLearn is V trained with FairLearn Finally CausalDAFairLearn is the proposed method ie S trained with FairLearn where S satisfies Assumptions and Results on another method anchor regression are included in Appendix Since this method requires data from multiple sources we evaluate it against the above methods in a separate experiment setting Hyperparameters used for the methods are reported in Appendix I Code for reproducing results on synthetic data is at Synthetic data example Setup For the flu example in Figure a we generate data from a structural equation model described in Figure a with linear relationships and logit link function for binary variables To generate target domains we perform soft interventions to shift distributions of and The shift magnitude is governed by a multiplier in the linear equations In total pairs of source and target datasets are simulated with samples in each dataset The proportion of disadvantaged group in source is kept at roughly In target domains with an extreme value for the ratio shifts to roughly Class ratio is varied from to with increase in From Figure a we observe that satisfies the two assumptions Adding a collider to S makes the predictor dependent and thus unstable We use logistic regression models in all experiments In Figure b the goal is to find a classifier performing well on both accuracy and fairness ie one close to the right-hand bottom corner Results For a high magnitude of shift Figure b domain adaptation CausalDA leads to considerably higher accuracy than using all features Standard but results in high unfairness Learning with fairness constraints CausalDAFairLearn which results in low unfairness with a minimal loss in accuracy even when the domains differ significantly As seen in Figure c for low magnitudes of shift CausalDAFairLearn still has low unfairness but results in a pessimistic accuracy estimate as it accounts for larger shifts than are seen in the target domain Thus in practice the choice of method will depend on the expected magnitude of shift Synthetic data example additional results Varying magnitude of shift To check robustness of different models to distribution shift we generate target datasets with different values of in the linear structural equations in Section Figure abc included at the end shows two predictive performance metrics Accuracy percentage correct AUROC and one fairness metric maximum fairness violation for different magnitudes of shift We observe the same trends as reported in Section ie CausalDA orange curve achieves stable predictive performance but leads to high unfairness whereas CausalDAFairLearn red curve achieves both stable predictive performance and low unfairness Results with demographic parity Figure report results on the synthetic example with demographic parity DP as the fairness constraint instead of In case of DP the fairness violation is quantified as S S We observe similar trends as compared to the plots for a Graph for diagnosis D Demography age sex A race Y diagnosis M Comorbidities X Lab tests Vitals including BUN Context variables b Variable descriptions on Target Data Fa es s V io l on T ar ge t D at a Standard CausalDA StandardFairLearn CausalDAFairLearn c Class ratio is Figure a Postulated causal graph for Bi-directed edge denotes unmeasured confounding between disease outcome and lab test values due to unobserved common causes b Legend for variables in the graph c Accuracy and fairness metrics for data Median values are reported over runs and error bars show first and third quartiles Proposed approach improves fairness with small loss in accuracy even on shifted target data Case study diagnosing Acute Kidney Injury Acute Kidney Injury is a condition characterized by an acute decline in renal function affecting of hospitalized patients and more than of patients in the intensive care unit ICU The condition can develop over a few hours to days and early prediction can greatly reduce the fatalities associated with the condition Hence building models for predicting risk from clinical data is an active area of research Such models can be used to risk-stratify patients to screen them for close monitoring or to perform further diagnostics to guide course of treatment Importantly incidence has well-documented disparities across groups defined by race and sex Thus introduction of risk prediction tools for guiding clinical care has a potential to perpetuate such disparities or alternatively to address them through a more deliberate design of the prediction tools A recent study showed good predictive performance for based on patient data provided by the US Department of Veteran Affairs However the female population was severely underrepresented in the data which raises concern over differential error rates when deployed in a different population Therefore to analyze the fairness across sensitive groups we conduct experiments on MIMIC III a publicly-available critical care dataset We extract variable types mentioned in caption of Figure for around patients Pre-processing steps are described in Appendix I We use a simplified causal graph for the diagnosis task Figure a based on the one used by for a sepsis diagnosis task The group sex female is taken as the sensitive attribute to assess fairness of the predictions In this case study the risk score is not intended to prescribe treatment but to flag a patient for extra care resources eg by alerting clinical staff Thus the potential harm that we want to avoid is groups having unequal opportunity to such care resulting from group differences in prediction errors Setup Patient encounters are randomly split into source and target data We artificially introduce two types of shifts a change in female proportion and b change in measurement policy where a lab test is prescribed less often some of the factors affecting model performance across clinical settings We randomly downsample female population by rejecting each row in the source data from that group with probability This shifts the proportion of females from to Also we randomly choose encounters in the target data and add missing values for the Blood Urea Nitrogen BUN test a biomarker of Results with other missingness proportions are included in Appendix I From Figure a we note that satisfies the two assumptions We report in Figure c instead of accuracy as it is less sensitive to class imbalance class ratio is All results are reported for classifiers trained with gradient boosting trees We drop OTDA from comparison due to its low accuracy and high running time for this dataset Results We find that classifiers with separating feature set perform significantly better in compared to those with all features exact numbers are reported in Appendix I Further CausalDAFairLearn improves fairness in target domain reducing fairness violation by with decrease in Thus the experiments provide preliminary evidence that our method can learn stable classifiers while being fair for a class of shifts in diagnosis tasks denoted by Figure a Note that the setup has some limitations namely adding missing values to perturb target data conflates the effectiveness of the procedure for handling missing data mean value imputation in our case with the procedure for domain adaptation We plan to validate the approach on datasets across multiple hospitals or time points to address these limitations LIMITATIONS AND DISCUSSION Knowledge of causal graph Our approach requires the causal graph for the system being studied to check whether the two assumptions are satisfied for any given subset While this is a requirement made by multiple domain adaptation methods this can be relaxed when data from multiple domains are available In such settings causal discovery methods can be used to posit a graph and validate with domain experts Such a procedure is demonstrated in An important direction for future work includes identifying the desired feature subsets with causal discovery algorithms We recommend that the causal graph be postulated conservatively ie only adding conditional independencies that are well-substantiated by domain knowledge In this case if the separating features are not found our method will output that a fair predictor is not possible instead of incorrectly returning a model that will not be fair Addressable shifts In Section we described shifts that our approach can address and presented examples However these are only a part of the possible shifts that a modeller may worry about For example shifts in direct causes of the outcome are excluded due to Assumption Such shifts can result in arbitrary changes to the outcome within each group making it impossible to balance group-specific statistics in the fairness constraint see Appendix A for an example These are difficult to address without making strict assumptions on magnitude of the shift or assuming access to target data Thus for some joint causal graphs Algorithm might not yield any feature set In such cases an alternative is to return the set with the least source domain risk but such a set has no generalization guarantee Algorithmic fairness in healthcare to promote health equity Disparities in health outcomes and healthcare access across different groups eg based on race and gender arise from multiple reasons such as socio-economic inequities eg due to structural racism and clinician bias Such disparities can result in differential model performance across groups as Obermeyer et al finds in context of a model for identifying patients who need extra care resources Left unaddressed allocating resources using biased models may worsen health disparities As a consequence a growing body of work aims to develop algorithms embodying fairness principles specific to healthcare This includes constraining prediction errors across groups for the tasks of predicting risk of cardiovascular events or predicting healthcare costs However such group-level fairness constraints including the ones we consider may not match ethical desiderata in all possible healthcare settings Some alternative constraints have been defined for example using counterfactuals or preference between group or aggregate-level models We plan to investigate fair domain adaptation under broader notions of fairness We have motivated the approach on healthcare tasks due to the importance of ensuring reliable model performance under distribution shifts in this domain We note that the approach is more broadly applicable to other domains involving high-stakes decisions CONCLUSION AND FUTURE WORK In absence of data from new environments in which a machine learning model will be deployed giving performance guarantees regarding predictive performance and fairness is challenging We find that methods to address distribution shift while controlling for decay in accuracy can result in fairness violations As a countermeasure we show that it is possible to obtain accurate and fair predictors for widely-studied fairness definitions and under a large class of shifts particularly prevalent in healthcare tasks Future work includes studying fair domain adaptation under parametric assumptions on shifts adaptation for counterfactual definitions of fairness and finite sample properties of the estimators We hope that the problem setup presented here will enable further work at the intersection of fairness and causal inference