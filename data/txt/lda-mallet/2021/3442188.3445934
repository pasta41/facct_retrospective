Chasing Your Long Tails Differentially Private Prediction in Health Care Settings Machine learning models in health care are often deployed in settings where it is important to protect patient privacy In such settings methods for differentially private DP learning provide a general-purpose approach to learn models with privacy guarantees Modern methods for DP learning ensure privacy through the addition of calibrated noise The resulting privacy-preserving models are unable to learn too much information about the tails of a data distribution resulting in a loss of accuracy that can disproportionately affect small groups In this paper we study the effects of DP learning in health care We use state-of-the-art methods for DP learning to train privacy-preserving models in clinical prediction tasks including x-ray classification of images and mortality prediction in time series data We use these models to perform a comprehensive empirical investigation of the tradeoffs between privacy utility robustness to dataset shift and fairness Our results highlight lesser-known limitations of methods for DP learning in health care models that exhibit steep tradeoffs between privacy and utility and models whose predictions are disproportionately influenced by large demographic groups in the training data We discuss the costs and benefits of differentially private learning in health care with open directions for differential privacy machine learning and health care CONCEPTS Applied computing Health informatics Security and privacy Usability in security and privacy Computing methodologies Machine learning KEYWORDS machine learning health care privacy fairness robustness INTRODUCTION The potential for machine learning to learn clinically relevant patterns in health care has been demonstrated across a wide variety of tasks However machine learning models are susceptible to privacy attacks that allow malicious entities with access to these models to recover sensitive information eg HIV status or zip code of patients who were included in the training data Others have shown that anonymized electronic health records EHR can be re-identified using simple linkages with public data and that neural models trained on EHR are susceptible to membership inference attacks Differential privacy DP has been proposed as a leading technique to minimize re-identification risk through linkage attacks and is being used to collect personal data by the US Census user statistics in iOS and MacOS by Apple and Chrome Browser data by Google DP is an algorithm-level guarantee used in machine learning where an algorithm is said to be differentially private if its output is statistically indistinguishable when applied to two input datasets that differ by only one record in the dataset DP learning focuses with increasing intensity on learning the body of a targeted distribution as the desired level of privacy increases Techniques such as differentially private stochastic gradient descent DP-SGD and objective perturbation have been developed to efficiently train models with DP guarantees but introduce a privacy-utility tradeoff This tradeoff has been well-characterized in computer vision and tabular data but have not yet been characterized in health care datasets Further DP learning has asymptotic theoretical guarantees about generalization that have been established There have not been any theoretical or empirical investigations on connections between differential privacy and out-of-distribution generalization which we term privacy-robustness tradeoffs Finally more unique minority data may not be well-characterized by DP leading to a noted privacy-fairness tradeoff in vision and natural language settings To date there has not been a robust characterization of utility privacy robustness and fairness tradeoffs for DP models in health care settings Patient health and care are often highly individualized with a heavy tail due to the complexity of illness and treatment variation and any loss of model utility in a deployed model is likely to hinder delivered care Privacy-robustness tradeoffs may also be high cost in health care as the data distribution is constantly changing in response to new conditions clinical practice shifts and underlying EHR systems changing Privacy-fairness tradeoffs are perhaps the most pernicious concern in health care as there are well-documented prejudices in health care Importantly the data of patients from minority groups also often lie even further in the data distribution tails because lack of access to care can impact patients EHR presence and leads to small sample sizes of non-white patients In this work we investigate the feasibility of using DP methods to train models for health care tasks We characterize the impact of DP learning in both linear and neural models on accuracy robustness and fairness First we establish the privacy-utility tradeoffs within two health care datasets NIH Chest X-Ray data and MIMIC-III EHR data as compared to two vision datasets MNIST and Fashion-MNIST We find that DP models have severe privacy-utility tradeoffs in the MIMIC-III EHR setting using three common tasks mortality long-length of stay LOS and an intervention vasopressor onset Second we investigate the impact of DP on robustness to dataset shifts in EHR data Because medical data often contains dataset shifts over time we create a realistic yearly model training scenario and evaluate the robustness of DP models under these shifts Finally we investigate the impact of DP on fairness in two ways loss of performance and loss of influence We define loss of performance through the standard and often competing group fairness metrics of performance gap parity gap recall gap and specificity gap We examine fairness further by looking at loss of minority data importance with influence functions In our audits we focus on loss of population minority influence eg importance of Black patient data and label minority influence eg importance of positive class patient data across levels of privacy low to high and levels of dataset shift least to most malignant Across our experiments we find that DP learning algorithms are not well-suited for off-the-shelf use in health care outlining a series of technical challenges for the DP community First DP models have significantly more severe privacy-utility tradeoffs in the MIMIC-III EHR setting and this tradeoff is proportional to the size of the tails in the data This tradeoff holds even in larger datasets such as NIH Chest X-Ray We further find that DP learning does not increase model robustness in the presence of small or large dataset shifts except for one instance This observation warrants further empirical and theoretical investigations on connections between differential privacy and out of distribution generalization Finally we do not find a significant drop in standard group fairness definitions unlike other domains likely due to the dominating effect of utility loss We do however find a large drop in minority class influence Specifically we show that Black training patients lose helpful influence on Black test patients Finally based on our empirical observations we outline a series of open problems that future work should address to make DP learning feasible in health care settings Contributions In this work we evaluate the impact of DP learning on linear and neural networks across three tradeoffs privacy-utility privacy robustness and privacy-fairness Our analysis contributes to a call for ensuring that privacy mechanisms equally protect all individuals We present the following contributions Privacy-utility tradeoffs scale sharply with tail length We find that DP has particularly strong tradeoffs as tasks have fewer positive examples resulting in unusable classifier performance Further increasing the dataset size does not improve utility tradeoffs in our health care tasks There is no correlation between privacy and robustness in EHR shifts We show that DP generally does not improve shift robustness with the mortality task as one exception Despite this we find no correlation between increasing privacy and improved shift robustness in our tasks most likely due to the poor utility tradeoffs Thus given the one instance we believe further theoretical and empirical research should explore connections between differential privacy and out-of-distribution generalizaton DP gives unfair influence to majority groups that is hard to detect with standard measures of group fairness We show that increasing privacy does not result in disparate impact for minority groups across multiple protected attributes and standard group fairness definitions because the privacy-utility tradeoff is so extreme We use influence functions to demonstrate that the inherent group privacy property of DP results in large losses of influence for minority groups across patient class label and patient ethnicity labels RELATEDWORK Differential Privacy DP provides much stronger privacy guarantees over methods such as k-anonymity and t-closeness to a number of privacy attacks such as reconstruction tracing linkage and differential attacks The outputs of DP analyses are resistant to attacks based on auxiliary information meaning they cannot be made less private Such benefits have made DP a leading method for ensuring privacy in consumer data settings Further theoretical analyses have demonstrated improved generalization guarantees for out of sample data Inspired by these theoretical guarantees we are interested in investigating empirically if differential privacy improves out of distribution generalization Our goal is to motivate future theoretical analyses connecting differential privacy and domain generalization Other theoretical analyses demonstrate that a model that is both private and approximately fair can exist in finite sample access settings However they show that it is impossible to achieve DP and exact fairness with non-trivial accuracy Further theoretical analyses show the unavoidable tradeoff between privacy and utility on long-tailed datasets This is empirically shown in DP-SGD which has disparate impact on complex minority groups in vision and NLP Differential Privacy in Health Care Prior work on DP in machine learning for health care has focused on the distributed setting where multiple hospitals collaborate to learn a model This work has shown that DP learning leads to a loss in model performance defined by area under the receiver operator characteristic AUROC We instead focus on analyzing the tradeoffs between privacy robustness and fairness with an emphasis on the impact that DP has on subgroups Utility Robustness and Fairness in Health Care Utility Needs in Health Care Tasks Machine learning in health care is intended to support clinicians in their decision making Dataset Data Type Outcome Variable Classification Task Tail Size Protected Attributes Evaluation health care mimic_mortality Time Series in-ICU mortality Binary Large Ethnicity UR mimic_los_ Time Series length of stay days Binary Small Ethnicity UR mimic_intervention Time Series vasopressor administration Multiclass Small Ethnicity UR NIH_chest_x_ray Imaging multilabel disease prediction Multiclass multilabel Largest Sex Vision Baselines mnist Imaging number classification Multiclass None N/A U fashion_mnist Imaging clothing classification Multiclass None N/A U Table We analyze tradeoffs in two vision baseline datasets and two health care datasets We use three prediction tasks in MIMIC-III with different tail sizes and focus our utility U robustness R and fairness analyses on these tasks Finally we choose NIH Chest X-Ray which is a larger dataset with the largest tail to examine whether increasing the dataset size has an impact on utility and fairness tradeoffs which suggests that models need to perform similarly to physicians The specific metric is dependent on the the task as high positive predictive value may be preferred over high negative predictive value In this work we focus on predictive accuracy as AUROC and AUPRC characterizing this loss as privacy levels increase Robustness to Dataset Shift The effect of dataset shift has been studied in non-DP health care settings demonstrating that model performance often deteriorates when the data distribution is nonstationary Recent work has demonstrated that performance deteriorates rapidly on patient LOS and mortality prediction tasks in the MIMIC-III EHR dataset when trained on past years and applied to a future year We focus on this setting for a majority of our experiments leveraging year-to-year changes in population as small dataset shifts and a change in EHR software between and as a large dataset shift Group Fairness Disparities exist between white and Black patients resulting in health inequity in the USA Further even the use of some sensitive data like ethnicity in medical practice is contentious and has been called into question in risk scores for instance in estimating kidney function Much work has described the ability of machine learning models to exacerbate disparities between protected groups even state-of-the-art chest X-Ray classifiers demonstrate diagnostic disparities between sex ethnicity and insurance type We leverage recent work in measuring the group fairness of machine learning models for different statistical definitions in supervised learning We complement these standard metrics by also examining loss of data importance through influence functions influence functions have also been extended to approximate the effects of subgroups on a models prediction They demonstrate that memorization is required for small generalization error on long tailed distributions DATA Details of each data source and prediction task are shown in Table The four datasets are intentionally of different sizes with respective tasks that represent distributions with and without long tails Vision Baselines We use MNIST and FashionMNIST to demonstrate the benchmark privacy-utility tradeoffs in non-health settings with no tails We use the NIH Chest X-Ray dataset images details in Appendix B to benchmark privacy-utility tradeoffs in a medically based but still vision-focused setting with the longest tails of all of our tasks MIMIC-III Time Series EHR Data For the remainder of our analyses on privacy-robustness and privacy-fairness we use the MIMIC-III database a publicly available anonymized EHR dataset of intensive care unit ICU patients unique patient stays details in Appendix B We focus on two binary prediction tasks of predicting ICU mortality class imbalanced LOS greater than days class balanced and choose one multiclass prediction tasks of predicting intervention onset for vasopressor administration class balanced Source of Distribution Shift In MIMIC-III there is a known source of dataset shift after due to a transition in the EHR used There are also smaller shifts in non-transition years as the patient distribution is non-stationary METHODOLOGY We use both DP-SGD and objective perturbation across three different privacy levels to evaluate the impact that DP learning has on utility and robustness to dataset shift Given the worse utility and robustness tradeoffs using objective perturbation we focus our subsequent fairness analyses on DP-SGD in health care settings Model Classes Vision Baselines We use different convolutional neural network architectures for the MNIST and FashionMNIST prediction tasks based on prior work We use DenseNet pretrained on ImageNet for the NIH Chest X-Ray experiments MIMIC EHR Tasks For the MIMIC-III health care tasks analyses we choose one linear model and one neural network per task based on the best baselines trained without privacy outlined in prior work creating benchmarks for the MIMIC-III dataset For binary prediction tasks we use logistic regression LR and gated recurrent unit with decay GRU-D For our multiclass prediction task we use LR and D convolutional neural networks Differentially Private Training We train models without privacy guarantees using stochastic gradient descent SGD DP models are trained with DP-SGD which is the de-facto approach for both linear models and neural networks We choose not to train models using PATE because it requires access to public data for semi-supervised learning and this is unrealistic in health care settings In the Appendix we provide results for models trained using objective perturbation which provides It is only applicable to our linear models We focus on DP-SGD due to its more optimal theoretical guarantees regarding privacy-utility tradeoffs and objective perturbations limited applicability to linear models The modifications made to SGD involve clipping gradients computed on a per-example basis to have a maximum norm and then adding Gaussian noise to these gradients before applying parameter updates Appendix E We choose three different levels of privacy to measure the effect of increasing levels of privacy by varying levels of epsilon We selected these levels based on combinations of the noise level clipping norm number of samples and number of epochs Our three privacy levels are None Low Clip Norm Noise Multiplier and High Clip Norm Noise Multiplier We provide a detailed description of training setup in terms of hyperparameters and infrastructure in Appendix Privacy Metrics We measure DP using the bound derived analytically using the Renyi DP accountant for DP-SGD Larger values of reflect lower privacy Note that the privacy guarantees reported for each model are underestimates because they do not include the privacy loss due to hyperparameter searches PRIVACY-UTILITY TRADEOFFS We analyze the privacy-utility tradeoff by training linear and neural models with DP learning We analyze performance across three privacy levels for the vision MIMIC-III and NIH Chest X-Ray datasets The privacy-utility tradeoffs for these datasets and tasks have not been characterized yet Our work provides a benchmark for future work on evaluating DP learning Experimental Setup We train both linear and neural models on the tabular MIMIC-III tasks We train deep neural networks on NIH Chest X-Ray image tasks and the vision baseline tasks We first analyze the effect that increased tail length in MIMIC-III has on the privacy-utility tradeoff Next we compare whether linear or neural models have better privacy-utility tradeoffs Finally we use the NIH Chest X-Ray dataset to evaluate if increasing dataset size while keeping similar tail sizes results in better tradeoffs Time Series Utility Metrics For MIMIC-III we average the model AUROC across all shifted test sets to quantitatively measure the utility tradeoff We measure the privacy-utility tradeoff based on the difference in performance metrics as the level of privacy increases The average performance across years is used because it incorporates the performance variability between each of the years due to dataset shift Results for AUPRC for MIMIC-III can be found in Appendix Both our AUROC and AUPRC results show extreme utility tradeoffs in health care tasks Both metrics are commonly used to evaluate clinical performance of diagnostic tests Imaging Utility Metrics For the NIH Chest X-Ray experiments the task we experiment on is multiclass multilabel disease prediction We average the AUROC across all disease labels For the MNIST and FashionMNIST vision baselines the task we experiment on is multiclass prediction labels for both where we evaluate using accuracy Health Care Tasks Have Steep Utility Tradeoffs We compare the privacy-utility tradeoffs in Table DP-SGD generally has a negative impact on model utility The extreme tradeoffs in MIMIC-III mortality prediction and NIH Chest X-Ray diagnosis exemplify the information DP-SGD looses from the tails because the positive cases are in the long tails of the distribution There is a and drop in the AUROC between no privacy and high privacy settings for mortality prediction for LR and GRU-D respectively There is a drop in AUROC between the no privacy and high privacy settings for the NIH Chest X-Ray prediction task which has a much longer tail than mortality prediction Our results for objective perturbation show worse utility tradeoffs than those presented by DP-SGD Appendix G Linear Models Have Better Privacy-Utility Tradeoffs Across all three prediction tasks in the MIMIC-III dataset we find that linear models have better tradeoffs in the presence of long tails This is likely due to two issues small generalization error in neural networks often requires memorization in long tail settings and gradient clipping introduces more bias as the number of model parameters increases Long Tails Are Still Difficult to Overcome With Larger Datasets By definition of differential privacy privacy-utility tradeoffs can be improved with larger datasets and less noise is required to achieve the same privacy guarantee with more data We find that the NIH Chest X-Ray dataset also has extreme tradeoffs Despite its larger size the datasets positive labels in long tails are similarly lost Additionally we downsample the MIMIC-III Mortality task according to different percentages and find it still has similar privacy-utility tradeoffs to when the model is trained on the entire training set Table PRIVACY-ROBUSTNESS TRADEOFFS A potential motivation for using DP despite extreme utility tradeoffs are the recent theoretical generalization guarantees These guarantees hold for out of sample data that is still from the same distribution as the training sample which differs from our setting In our setting we empirically analyze whether there is motivation Vision Baselines Dataset Model None Low High MNIST CNN FashionMNIST CNN MIMIC-III Task Model None Low High Mortality LR gru-d Length of Stay LR gru-d Intervention Onset LR CNN NIH Chest X-Ray Metric Model None Low High Average AUC DenseNet Best AUC DenseNet Hernia Edema Pleural Thickening Worst AUC DenseNet Infiltration Fibrosis Pleural Thickening Table Health care tasks have a significant tradeoff between the High and Low or None setting The tradeoff is better in tasks with small tails length of stay and intervention onset and worst in tasks such as mortality and NIH Chest X-Ray with long tails We provide the guarantees in parentheses where represents the privacy loss lower is better and represents the probability that the guarantee does not hold lower is better Figure The effect of DP learning on robustness to non-stationarity and dataset shift One instance of increased robustness in the column for mortality prediction in the high privacy setting A but this does not hold across all tasks and models Performance drops in the column for LOS in both LR and GRU-D B and a much worse drop in the high privacy CNN for intervention prediction C for theoretical analyses about the effect of DP on out-of-distribution generalization guarantees We investigate the impact of DP to mitigating dataset shift for time series MIMIC-III tasks by analyzing model performance across years of care We first record generalization as the difference in performance when a model is trained and tested on data drawn from versus performance on a shifted test set drawn from and the malignancy of the shift We then measure the malignancy of the yearly shifts using a domain classifier Finally we perform a Pearsons correlation test between the models generalization capacity and the shift malignancy Experimental Setup We analyze the robustness of DP models to dataset shift in the MIMIC-III health care tasks We use year-to-year variation in hospital practices as a small shifts and a change in EHR software between as a source of major dataset shift We define robustness as the difference in test accuracy between in-distribution and out-distribution data For instance to measure model robustness from the to we would train a model on data from test the model on data from and test the same model on data from The difference in these two test accuracies is the model robustness Robustness Metrics To measure the impact of DP-SGD on robustness to dataset shift we measure the malignancy of yearly shifts from to for the MIMIC-III dataset We then measure the correlation between malignancy of yearly shift and model performance As done by others we use We use a binary domain classifier model class is chosen best on data type trained to discriminate between in-domain and out-domain The malignancy of the dataset shift is proportional to how difficult it is to train on and perform well in Other methods such as multiple univariate hypothesis testing or multivariate hypothesis testing assume that the data is iid A full procedure is given in Appendix A with complete significance and malignancies for each year in Appendix D DP Does Not Impart Robustness to Yearly EHR Data Shift While we expect that DP will be more robust to dataset shift across all tasks and models we find that model performance drops when the EHR shift occurs across all privacy levels and tasks Fig We note one exception high privacy models are more stable in the mortality task during more malignant shifts Fig Despite this we find that there are no significant correlations between model robustness and privacy level Table Our analyses find that the robustness guarantees that DP provides do not hold in a large tabular EHR setting We note that the privacy-utility tradeoff from Section is too extreme in health care to conclusively understand the effect on model robustness PRIVACY-FAIRNESS TRADEOFFS Prior work has demonstrated that DP learning has disparate impact on complex minority groups in vision and NLP We expect similar disparate impacts on patient minority groups in the MIMIC-III and NIH Chest X-Ray datasets based on known disparities in treatment and health care delivery We evaluate disparities based on four standard group fairness definitions and on loss of minority patient influence We focus on the disparities between white and Black patients in MIMIC-III based on prior work showing classifier variation in the low number of Black patients We focus on male and female patients in NIH Chest X-Ray based on prior work exposing disparities in chest x-ray classifier performance between these two groups Group Fairness Experimental Setup and Metrics We measure fairness according to four standard group fairness definitions We did not observe this improvement when training with objective perturbation Appendix G performance gap parity gap recall gap and specificity gap The performance gap for our health care tasks is the difference in AUROC between the selected subgroups The remaining three definitions of fairness for binary prediction tasks are presented in Appendix A Influence Experimental Setup and Metrics We use influence functions to measure the relative influence of training points on test set performance equations in Appendix A Influences above are helpful in minimizing the test loss for the test patient in that column and influences below are harmful in minimizing the test loss for that patient Our influence function method assumes a smooth convex loss function with respect to model parameters and is therefore only valid for LR We focus group privacy analyses on the LR model in no and high privacy settings for mortality prediction First we aim to confirm that the gradient clipping in DP-SGD bounds the influence of all training patients on the loss of all test patients For the utility tradeoff we measure the group influence that the training patients of each label group has on the loss of each test patient For the robustness tradeoff we measure the individual influence of all training patients on the loss of test patients between the least malignant and most malignant dataset shifts Finally we measure the group influence of training patients in each ethnicity on the white test patients and Black test patients separately DP Has No Impact on Group Fairness on Average But Reduces Variance Over Time To measure the average fairness gap in MIMIC-III we average group fairness measures across all years of care In the NIH Chest X-ray data we average across all disease labels We find that DP-SGD has little impact on any tested fairness definitions in both MIMIC-III Table and NIH Chest X-Ray likely due to the high privacy-utility tradeoff DP-SGD does confer lower variance in fairness measures on MIMIC-III tasks over time Appendix DP Learning Gives Unfair Influence to Majority Groups We find that DP-SGD reduces the influence of all training points on individual test points Fig because gradient clipping tightly bounds influence of all training points across test points Influence-Utility Tradeoff We find the worst privacy-utility tradeoff in the mortality task Non-DP models find the patients who died to be the most helpful in predictions of mortality Fig and Table However because positive labels ie death are rare DP models focus influence on patients who survived resulting in unfair over-influence Influence-Robustness Tradeoff We see improved robustness in LR for the mortality task which has the most malignant dataset shift Figure We find that the variance of the influence is fairly low for non-DP models during lower malignancy shifts In more malignant shifts the variance of the influence is high with many training points being harmful Fig This is likely due to gradient clipping reducing Figure DP bounds the individual influence of training patients on the loss of test patients A which improved robustness for mortality prediction between the least malignant shift in B and the most malignant in C Individual influence of training data in the no privacy setting on test patients with highest influence variance Each column on the x-axis is an individual test patient A unique colour is plotted per column/test patient for ease of assessment The influence value of each patient in the training set on a specific test point is plotted as a point in that patients column Influence of training points is bounded in the high privacy setting red dotted line Privacy Level Average Survived Influence Average Died Influence Most Helpful Group Most Harmful Group Influence None Died Survived Low Survived Survived High Survived Survived Table Group influence summary statistics of training data by class label in all privacy levels for all test patients Privacy changes the most helpful group the patients who died minority to the patients who survived majority DP learning minimizes the helpful influence of minority groups resulting in worse utility influence variance and is entangled with the poor privacy-utility tradeoff Influence-Fairness Tradeoff We approximate the collective group influence of different ethnicities in the training set on the test loss in Fig and Table We show that group privacy results in white patients having a more significant influence both helpful and harmful on test patients in the high privacy setting DISCUSSION On Utility Robustness and Trust in Clinical Prediction Tasks Poor Utility Impacts Trust While some reduced utility in long tail tasks are known the extreme tradeoffs that we observe in Table are much worse than expected Machine learning can only support the decision making processes of clinicians if there is clinical trust If models do not perform well as or better than Figure Group influence of training data by class label in no privacy A and high privacy B settings on test patients with highest influence variance In the no privacy setting patients who died have a helpful influence despite being a minority class High privacy gives the majority group the most influence due to the group privacy guarantee White Test Patients Privacy Level Average White Influence Average Black Influence Most Helpful Ethnicity Most Harmful Ethnicity None White White Low White White High White White Black Test Patients Privacy Level Average White Influence Average Black Influence Most Helpful Ethnicity Most Harmful Ethnicity None Black White Low White White High White White Table Group influence summary statistics across all privacy levels for white majority and Black minority training patients on both white and Black test patients in MIMIC-III Privacy changes the most helpful group from Black patients to the majority white patients and minimizes their helpful influence This needs careful consideration as the use of ethnicity is still being investigated in medical practice clinicians once we include privacy there is little reason to trust them Importance of Model Robustness Despite the promising theoretical transferability guarantees of DP the results in Fig and Table demonstrate these do not transfer in our health care setting While we explored changes in EHR software as dataset shift there are many other known shifts in healthcare data eg practice modifications due to reimbursement policy changes or changing clinical needs in public health emergencies such as COVID- If models do not maintain their utility after dataset shifts catastrophic silent failures could occur in deployed models Majority Group Influence Is Harmful in Health Care We show in Figure that the tails of the label distribution are minority-rich results in poor mortality prediction performance under DP Prior work in evaluating model fairness in health care has focused on standard group fairness definitions However these definitions do not provide a detailed understanding of model fairness under reduced utility Other work has shown that large utility loss can wash out fairness impacts Our work demonstrates that DP learning does harm group fairness in such washed out poor utility settings by giving majority groups eg those that survived and white patients the most influence on predictions across all subgroups Why Influence Matters Disproportionate assignment of influence is an important problem Differences in access practice or recording reflect societal biases and models trained on biased data may exhibit unfair performance in populations due to this underlying variation Further while patients with the same diagnosis are usually more helpful for estimating prognosis in practice labels in health care often lack precision or in some cases may be unreliable In this setting understanding what factors are consistent high-influence in patient phenotypes is an important task Loss Of Black Influence Ethnicity is currently used in medical practice as a factor in many risk scores where different risk profiles are assumed for patients of different races However the validity of this stratification has recently been called into question by the medical community Prior work has established the complexity of treatment variation in practice as patient care plans are highly individualized eg in a cohort of million patients of diabetes and depression patients and almost of hypertension patients had a unique treatment pathway Thus having the white patients become the most influential in Black patients predictions may not be desirable Anchoring Influence Loss in Systemic Injustice Majority over-influence is prevalent in medical settings and has direct impact on the survival of patients Many female and minority patients receive worse care and have worse outcomes because clinicians base their symptomatic evaluations on white and/or male patients Further randomized control trials RCTs are an important tool that of treatments are based on However prior work has shown that RCTs have notorious exclusive criteria for inclusion in one salient example only of asthmatic patients would have been eligible to enroll in the RCT that resulted in their treatments RCTs tend to be comprised of white male patients resulting in their data determining what is an effective treatment By removing influence from women Hispanics and Blacks naive machine learning practices can exacerbate systemic injustices There are ongoing efforts to improve representation of the population in RCTs shifting away from the majority having majority influence on treatments Researchers using DP should follow suit and work to reduce the disparate impact on influence to ensure that it does not perpetuate this existing bias in health care One solution is to start measuring individual example privacy loss instead of a conservative worst bound across all patients Currently DP-SGD uses constant gradient clipping for all examples to ensure this constant worst bound Instead individual privacy accounting can help support adaptive gradient clipping for each example which may help to reduce the disparate impact DP-SGD has on influence We also encourage future privacy-fairness tradeoff analyses to include loss of influence as a standard metric especially where the utility tradeoff is extreme Less Privacy In Tails Is Not an Option The straightforward solution to the long tail issue is to provide less or no privacy for the tails In some settings this solution may make sense In health care this solution could amplify existing systemic biases against minority subgroups and minority mistrust of medical institutions For example Black mothers in the US are most likely to be mistreated dying in childbirth at a rate three times higher than white women In this setting it is not ethical choose between a non-private prediction that will potentially leak unwanted information eg prior history of abortion and a private prediction that will deliver lower quality care On the Costs and Benefits of Privacy in Health Care Privacy Issues With Health Care Data Most countries have regulations that define the protective measures to maintain patient data privacy In North America these laws are defined by the Health Insurance Portability and Accountability Act HIPAA in the US and Personal Information Protection and Electronic Documents Act PIPEDA in Canada These laws are governed by the General Data Protection Regulation GDPR in the EU Recent work has shown that HIPAAs privacy regulations and standards such as anonymizing data are not sufficient to prevent advanced re-identification of data In one instance researchers were able to re-identify individuals faces from MRIs using facial recognition software Privacy attacks such as these demonstrate the fear of health care data loss Open Problems for DP in Health Care While health care has been cited as an important motivation for the development of DP our work demonstrates that there are a number of open areas of research to make it well suited for machine learning for healthcare The theoretical benefits of DP apply in extremely large data collection settings such as the successful deployment of DP US Census data storage We highlight potential areas of research that both the DP and machine learning communities should focus on to make DP usable in health care data Connections Between Differential Privacy and Domain Generalization In our robustness to dataset shift experiments we observe one instance where differential privacy improves model robustness Dataset shift can be viewed as a type of domain generalization Thus an interesting direction would be to see how the generalization guarantees can be extended to domain generalization Boosting Utility and Fairness of Differential Privacy Using Post-Processing A number of recent post-processing techniques in algorithmic fairness have been used to convert classifiers into those that satisfy equalized odds and multicalibration A differentially private implementation of the equalized odds post-processing was used to improve the fairness of DP learning Further post-processing techniques should be examined as a way to boost the utility of differentially private learning Private Collaborative Learning In this work we investigate the single hospital setting which often means that there is limited data An important direction in the health care setting is to develop private and confidential learning algorithms which allow hospitals to learning a single model while ensuring the privacy of their own data This can be especially helpful in settings of class imbalance if one hospital has more data about a subpopulation that is infrequent at another hospital Practical Deployments of Differential Privacy in Health Care More interdisciplinary work between the differential privacy community biostatisticians and epidemiologists are needed to build differentially private algorithms which provide good utility based on the inherent challenges of health care data CONCLUSION In this work we investigate the feasibility of using DP-SGD to train models for health care prediction tasks We find that DPSGD is not well-suited to health care prediction tasks in its current formulation First we demonstrate that DP-SGD loses important information about minority classes eg dying patients minority ethnicities that lie in the tails of the dat distribution The theoretical robustness guarantees of DP-SGD do not apply to the dataset shifts we evaluated We show that DP learning disparately impacts group fairness when looking at loss of influence for majority groups We show this disparate impact occurs even when standard measures of group fairness show no disparate impact due to poor utility This imposed asymmetric valuation of data by the model requires careful thought because the appropriate use of class membership labels in medical settings in an active topic of discussion and debate Finally we propose open areas of research to improve the usability of DP in health care settings Future work should target modifying DP-SGD or creating novel DP learning algorithms that can learn from data distribution tails effectively without compromising privacy