Evaluating Fairness of Machine Learning Models Under Uncertain and Incomplete Information Training and evaluation of fair classifiers is a challenging problem This is partly due to the fact that most fairness metrics of interest depend on both the sensitive attribute information and label information of the data points In many scenarios it is not possible to collect large datasets with such information An alternate approach that is commonly used is to separately train an attribute classifier on data with sensitive attribute information and then use it later in the ML pipeline to evaluate the bias of a given classifier While such decoupling helps alleviate the problem of demographic scarcity it raises several natural questions such as how should the attribute classifier be trained and how should one use a given attribute classifier for accurate bias estimation In this work we study this question from both theoretical and empirical perspectives We first experimentally demonstrate that the test accuracy of the attribute classifier is not always correlated with its effectiveness in bias estimation for a downstream model In order to further investigate this phenomenon we analyze an idealized theoretical model and characterize the structure of the optimal classifier Our analysis has surprising and counter-intuitive implications where in certain regimes one might want to distribute the error of the attribute classifier as unevenly as possible among the different subgroups Based on our analysis we develop heuristics for both training and using attribute classifiers for bias estimation in the data scarce regime We empirically demonstrate the effectiveness of our approach on real and simulated data CONCEPTS Computing methodologies Machine learning approaches KEYWORDS Model Auditing Bias Estimation Active Learning INTRODUCTION Estimating a machine learning ML models difference in accuracy for different demographic groups is a challenging task in many applications in large part due to imperfect or missing demographic data Demographic information might be unavailable for many reasons users may choose to withhold this information for privacy or other personal preferences or the dataset may have been constructed for use in a setting where collecting the demographic information of the datasets subjects was unnecessary undesirable or even illegal Responsible ML practitioners and auditors may want to evaluate model performance across different demographic groups even if demographic information was not collected explicitly and as researchers of fairness we want to enable this For example a microlender who builds a model to prescreen applicants for a loan may explicitly avoid maintaining demographic records on applicants but may also want to ensure that the rate at which their model approves a candidate is nearly independent of the applicants gender Can one assess this bias of the model even when training and validation data do not contain explicit gender information This presents a common problem how can one audit the performance of a model for predicting Y conditioned on demographic information A when A and Y are observed jointly on little or no data More formally consider two datasets DD where D has features in X and labels in Y and D has features in X and demographic information in A How should we use these datasets to evaluate a model s performance predicting Y conditioned on A for the distribution that generated D With no further assumptions this is impossible if the two datasets are generated from distributions bearing no resemblance to each other correlations existing between X and Y and between X and A may be arbitrarily different for the two distributions Modelers may still wish to understand if their separate datasets can help them with this estimation problem and what properties their two datasets might need to have to result in good estimations One approach particularly natural for enabling many different teams participating in the development of models without demographic information is to use proxy attributes as a replacement for the true sensitive attribute The use of such proxy attributes has been widely used in domains such as health care and finance One can view a particular proxy attribute as a coarse attribute classifier that predicts the sensitive attribute information for a given example For popular fairness metrics such as equalized odds or demographic parity the sensitive attribute information obtained from the classifier can then be used in conjunction with the predicted label information to get an estimate of the bias of a given model The advantage of this approach is that it decouples the data requirement and in particular the attribute classifier can be trained on a separate dataset without any label information present On the other hand such an approach may produce very poor estimates depending upon how the sources of data relate to one another Nonetheless it is interesting to ask what principles should guide the choice of a proxy attribute or more generally the design of a sensitive attribute classifier and similarly what principles should be followed when using such classifiers for bias estimation In this work we aim to understand this question If one wishes to produce the most accurate estimate of the bias term is the optimal approach to train the attribute classifier to have the highest possible accuracy or to have similar error rates for each demographic group It turns out that neither of these criteria alone captures the structure of the optimal attribute classifier To demonstrate this we consider the measurement of violation of equal opportunity or true positive rates for different groups and show that the error in the bias estimates as a result of using a proxy attribute/attribute classifier depends not only on the error rate of the classifiers but also on how these errors and the data more generally are distributed Our analysis has surprising and counter-intuitive implications in certain regimes the optimal attribute classifier is the one that has the most unequal distribution of errors across the two subgroups Our main contributions are summarized below Problem formulation We formalize and study the problem of estimating the bias of a given machine learning model using data with only label information and model output and using a separate attribute classifier to predict the sensitive attributes We call this the demographically scarce regime In particular our setting captures the common practice of using proxy attributes as a replacement for the true sensitive attribute We experimentally demonstrate that the accuracy of the attribute classifier does not solely capture its effectiveness in bias estimation Theoretical analysis For the case of the equal opportunity metric we present a theoretical analysis under a simplified model Our analysis completely characterizes the structure of the optimal attribute classifier and shows that the error in the bias estimates depends on the error rate of the classifier as well as the base error rate a property of the data distribution As a result we show that in certain regimes of the base error rate the optimal attribute classifier surprisingly is the one that has the most unequal distribution of errors among the two subgroups Empirical validation Our theory predicts natural interventions aimed at better bias estimation that can be performed either at the stage of designing the attribute classifier or at the stage of using a given attribute classifier We study their effectiveness in scenarios where our modeling assumptions hold and their sensitivity to violations of the assumptions An active sampling algorithm Finally we propose an active sampling based algorithm for accurately estimating the bias of a model given access to an attribute classifier for predicting the sensitive attribute Our approach makes no assumption about the structure of the attribute classifier or the data distribution We demonstrate via experiments on real data that our sampling scheme performs accurate bias estimation using significantly fewer samples in the demographically scarce regime as compared to other approaches While highlighting the need for careful considerations when designing such attribute classifiers our analysis provides concrete recommendations and algorithms that can be used by ML practitioners Remark Inferring demographic information should be done with care even when used only for auditing a models disparate treatment Any estimation of demographics will invariably have inaccuracies This can be particularly problematic when considering for example gender as misgendering an individual can cause them substantial harm Furthermore building a model to infer demographic information purely for internal evaluation leaves open the possibility that the model may ultimately be used more broadly with possibly unintended consequences That said preliminary evidence of bias may be invaluable in arguing for resources to do a more careful evaluation of a machine learning system For this reason we believe that methods to estimate the difference in predictive performance across demographic groups can serve as a useful first step in understanding and improving the systems treatment of underserved populations RELATEDWORK Various fairness metrics of interest have been proposed for classification ranking unsupervised learning and online learning These notions of fairness are usually categorized as either group fairness or individual fairness In the case of group fairness training a model to satisfy such fairness metrics and evaluating the metrics on a pre-trained model typically requires access to both the sensitive attribute and the label information on the same dataset Only recently ML research has explored the implications of using proxy attributes and the sensitivity of fair machine learning algorithms to this noisy demographic data The most related work to ours also study the problem of estimating the bias of predictor as a function few or no samples from the joint distribution over X Y A In the authors aim to estimate the disparity of a model predicting Y by considering predicting A via a threshold q and then assuming A a whenever PA a X x q This can work well with unlimited data from the joint distribution over AX but can both overestimate and underestimate the bias because of the misclassification of A using said thresholding rules The work of investigates how to perfectly or partially identify the same bias parameter making few or no assumptions about independence between AY and X Given that exact identification is usually impossible the authors describe how to find feasible regions of various disparity measures making very few assumptions about independence between Y X A Similar to our work the authors in study the setting where one has access to a dataset involving and another dataset involving ax Given that exact identification is usually impossible they construct an uncertainty set confidence interval for bias estimates assuming access to the Bayes optimal classifier ie the classifier that predicts sensitive attribute according to Pa x An important contribution of our paper is to show that the Bayes optimal attribute classifier is not the right classifier in the first place We demonstrate this via experiments in Table Theorem and in fact we go deeper and characterize the structure of the optimal attribute classifier in Theorem under assumptions Our work also explains why the confidence intervals in the experiments of are large In particular the work of focuses only on the setting where there is no common data hence their confidence regions will never shrink with dataset size due to inherent uncertainty We however believe that it is practically more relevant to study how we can use a little amount of common data and produce accurate estimates This motivates our active sampling approach as proposed in Section SETUP AND NOTATION We consider the classification setting where X denotes the feature space Y the label space and A the sensitive attribute space For simplicity we assume Y and A Consider two datasets D and D D is drawn from the marginal over of a distribution P over X Y A Similarly D is drawn from the marginal over of a distribution Q over X Y A Thus both datasets contain the same collection of features D contains attribute information and D contains label information In this setting given a classifier X Y we aim to estimate the bias of the difference in the performance of on Q over A In this work we define the bias to be the equal opportunity metric defined as bias Q where x y a x y a We consider the demographically scarce regime where the designer of an attribute classifier has access to D and the user of the label classifier has access toD and additionally has access to the attribute classifier X A trained on D Here no party has access to both datasets We aim to understand how the classifiers can be used in combination for accurate bias estimation From the perspective of the user of an attribute classifier it is easy to see that no matter what the attribute classifier is one cannot get any non-trivial estimate of the bias of without access to some common data drawn from X Y A Here we provide a simple proof of this fact In particular we will construct a scenario where there is a fixed attribute classifier X A We will construct two distributions such that the output distribution of the attribute classifier is the same in both the cases Furthermore any label classifier trained on just x y pairs will have the same output distribution in both the cases However the estimates of the bias when using the attribute classifier will differ wildly Formally we have the following proposition Proposition Consider a fixed attribute classifier X A and a fixed learning algorithm for computing a label classifier from a distribution over X Y Then there exists two distribution over such that have the same marginals over X A b produces the same label classifier given access to Q or Q and c either the label classifier is the Bayes optimal classifier or the ratio of the estimated bias using to the true bias of on either Q or Q is or Proof Given the learning algorithm for the labels and an attribute classifier we will construct two distributions Q over X Y A such that the following holds is the same as thereby ensuring that the distribution of over is the same Y is the same Y thereby ensuring that produces the same classifier On Q the true bias of is zero and on Q the true bias of is one thereby ensuring that the ratio of the estimated will be distorted by either zero or To construct the two distributions fix any marginal distribution over X Y and let be any classifier for predicting y given x that is not Bayes optimal Since is not Bayes optimal we can consider two non-empty regions A Y and B Y Construct Q by setting the conditional distribution conditioned to be uniformly distributed and similarly the distribution of A conditioned on B to be uniformly distributed Construct Q to be such that the conditional distribution of A on A is entirely supported on a and the conditional distribution of A on B is entirely supported on a Now it is easy to see that has zero bias on Q and a bias value of one on Q Since the estimated bias using will result in the same value for Q and Q in at least one of the cases the distortion must be or In the next two sections we study the following questions a how should one design an attribute classifier and b how can one use an attribute classifier along with minimal common data and perform accurate bias estimation We will focus on the case of P Q ie both the designer and the user of an attribute classifier have marginals from the same joint distribution PERSPECTIVE OF THE ATTRIBUTE CLASSIFIER MODELER What properties of the attribute classifier are desirable for bias estimation At first thought training the most accurate classifier seems to be a reasonable strategy However it is easy to construct instances where using the Bayes optimal attribute classifier does not lead to the most accurate bias estimation This is formalized in the Theorem below Theorem Let be binary attributes a be a binary sensitive attribute and y be a binary class label Furthermore let be a label classifier given by x ie x Then there exists a joint distribution Q over tuples such that the bias of is zero ie bias Q On the other hand using the Bayes optimal predictor for the attribute a leads to an estimated bias of one for Proof Let the joint distribution Q be as follows x a y x a y x a y x a y x a y x a y Then we have for x Pf y a y a Similarly Pf y a y a Hence the true bias of is zero Next consider the Bayes optimal predictor for a that makes predictions based on Pa If this probability is half then we consider an arbitrary predictor for the Bayes optimal classifier In this case the Bayes optimal predictions a are shown in the table below Next using a instead of a to estimate x x a y a the bias of we get Pf y a y a Similarly Pf y a y a Hence using a Bayes optimal attribute classifier leads to an estimated bias of one whereas the true bias is zero In this case using a random attribute predictor ie predicting the class label via a coin toss is better than using the Bayes optimal one Next we show that even in practical settings there is little correlation between the accuracy of the attribute classifier and its effectiveness for bias estimation Table contains the result of various attribute classifiers trained on the UCI Adult dataset As can be seen the accuracy of an attribute classifier is not necessarily correlated with its ability to estimate the true bias of a model In order to gain further understanding we consider a simplified model where we aim to theoretically characterize the structure of the optimal attribute classifier In particular we will consider Attribute Classifier Test Accuracy Random Forest Logistic Regression LR LR LR LR SVM -hidden-layer NN Table Distribution of test accuracies of different attribute Gender classifiers and the resulting estimated biases over the UCI Adult dataset is the estimated bias using the attribute classifier and is the true bias without the absolute value The label classifier is a fixed Random Forest Classifier to predict income with test accuracy By default all models are trained with all features except Gender and Income For the Logistic Regression LR model the names within the bracket indicate which features are used R Relationship E Education M Marital Status O Occupation Work Class the setting where the two datasets D and D are drawn from distributions which are marginals of the same distribution For an attribute classifier X A define the conditional errors of with respect to a as a a y a a y Here x a is drawn from the marginal of joint distribution over X Y A from which both D and D are drawn Suppose we use to predict the noisy attributes a on D and then use these estimates to measure the bias of a label classifier To simplify our analysis we make the following key assumption Below we denote y to represent the prediction of the label classifier Assumption I Given y and a y and a are conditionally independent that is for all and aa we have Py y a a y a Py y y a Pa a y a Notice that while the above assumption is restrictive there are practical scenarios where one can expect it to hold Furthermore the assumption has been studied in prior works to understand noise sensitivity of post-processing and in-processing methods for building fair classifiers Two settings where such an assumption would hold are as follows First this would hold if the attribute classifier uses a set of features that are conditionally independent of the features used by the label classifier The assumption will also hold if the attribute classifier makes independent errors with a certain probability This for instance might be the case for certain classifiers based upon crowdsourcing As we will soon see even under the above simplified setting the structure of the optimal attribute classifier can defy convention wisdom Using the attribute classifier we get estimates of the true values see where Pf x y a Pf x y a Under Assumption I we have the following relationship between the true and the estimated values a a Py a Py a a a Py a Py a To see the above we have Pf x y a A B C D where A Pf x y a a Pa y a Py a B Pf x y a a Pa y a Py a C Pa y a Py a D Pa y a Py a The above can be rewritten as Pf x y a x y a a Py a a a x y a a Py a a a with defined as in Under Assumption I we have Pf x y a a and Pf x y a a and hence Pf x y a a a a a Similarly we obtain Pf x y a a a a a It follows that Pf x y a Pf x y a s r r s where r Py a s Py a A simple calculation then shows that the true bias and the estimated bias satisfy where the distortion factor is defined as s r r s with larger values of corresponding to higher accuracy estimates of bias and the quantities r s are defined as r Py a s Py a We refer to as the ratio of base rates and assume that it is bounded in G for some finite G else there will be no good way to estimate the bias from a finite sample Notice that if equals one then is zero In every other case it is not hard to see that Remark Under Assumption I is an underestimate of the bias of and is therefore not an unbiased estimator This is evident from our derivation but may not be obvious for someone using empirical value as a stand-in for the true value of bias Hence even in the simplified setting of Assumption I the distortion in the estimated bias depends on the conditional errors of as well as the ratio of base rates Therefore to accurately estimate the true bias one not only needs the conditional errors of but also an estimate of the ratio of base rates Under Assumption I an optimal attribute classifier should aim to maximize as defined in since Our next theorem quantifies the structure of the optimal attribute classifier Theorem Assume that the distribution P is such that the ratio of base rates equals one ie r s Furthermore denote an attribute classifier as a tuple where and denote the conditional errors as defined in Then under a given error budget U ie ay U the only global maximizers of as defined in are the attribute classifiers U and U r or U and U r Proof In case of r s as defined in simplifies to and the constraint ay Pa y Pa y U is equivalent to U r Our goal is to show that argmax equals max min min max Writing in terms of we get that U r and U r U r The constraint is equivalent to max U r min U r We assume that U r Maximizing is equivalent to maximizing U r and the maximum of the latter function is attained both at max r and min r which correspond to min r and max r respectively The above theorem implies that even under simplifying assumptions the structure of the optimal attribute classifier is counterintuitive to estimate bias as accurately as possible one may want to distribute the errors of the given attribute classifier as unevenly as possible Hence great care must be taken when designing the attribute classifier for the purpose of bias estimation Furthermore while we only consider the case r s Figure shows the distortion factor as a function of for a fixed error budget U also when r s middle and right plot As we can see also then attains its maximum either at the smallest or the largest possible value of which corresponds to distributing the error among the two groups as unevenly as possible PERSPECTIVE OF THE USER OF AN ATTRIBUTE CLASSIFIER If the user of the attribute classifier expects Assumption I as defined in to approximately hold and she has some common data over from which she will estimate the conditional errors and the ratio of base rates exploiting she can try to improve the naive bias estimate by dividing it by an estimate of the distortion factor In this section we present an experiment that illustrates that such an approach indeed yields a better estimate of the true bias In a data scarce regime this approach also compares favorably to using the available data for directly estimating Dataset We use the FIFA player dataset We predict whether a soccer players wage is above y or below y the median wage based on the players age and their Overall attribute For doing so we train a one-hidden-layer NN We restrict the dataset to contain only players of English or German nationality leaving us with players and consider nationality as sensitive attribute We train an LSTM to predict this attribute from a players name In this case we can expect the conditional independence assumption to hold since given a players wage and nationality their name should be conditionally independent of age and Overall attribute Indeed the measure proposed in is small enough see below as to confirm that our expectation holds and the independence assumption is satisfied Experimental setup We randomly split the dataset into three batches of sizes and respectively We use the first batch to train the label and the attribute classifier On the second batch we compute both the true bias and the naive estimate Finally we use n points from the third batch to estimate r s and thus and also to directly estimate We refer to the latter estimate as direct estimate Exploiting we use the estimate of to correct the naive estimate by dividing it by the estimate of We refer to the result as corrected estimate Figure shows the absolute difference between the true bias and the various estimates where for the direct estimate and the corrected estimate we show the results depending on n the amount of data for which we have both y and a The boxplots are obtained from running the experiment times We can see that the corrected estimate significantly improves over the naive estimate eg the mean absolute difference averaged over the runs is for the naive estimate and for the corrected estimate with n Furthermore the corrected estimate consistently outperforms the direct estimate When only n points of common data over are used the mean absolute difference for the corrected estimate is while for the direct estimate it is In this experiment the mean true bias is the mean error of the label and the attribute classifier is and respectively and the mean violation of the independence assumption according to the measure proposed in is An Active Sampling based Algorithm for Bias Estimation in General Settings In some real-world scenarios the conditional independence assumption might be violated and we would like to ask in those cases does the correction as specified in still give a good estimate of the true bias We show in the following that the correction might not always lead to a better bias estimate when Assumption I does not hold Therefore in order to obtain a good bias estimate in general we explore active-sampling strategies that aim to use as little common data over as possible ideally comparable to the setting when the independence assumption holds Dataset We use the UCI Adult dataset which comes divided into a training and a test set On the training set we train Random Forest classifiers for both predicting the label income or not using all features present in the dataset except gender and the attribute gender Male or Female as categorized in the dataset using all features except income We used the measure proposed in to evaluate whether the independence assumption approximately holds and found that the independence assumption clearly fails to hold Inaccurate bias estimation when the conditional independence assumption is violated We use the UCI Adult training set for training our label classifier and attribute classifier For the Adult test set around examples we hold out a set with examples for estimating r s since it requires some common data over and use the rest for estimating the true bias In Figure we show the absolute difference between the estimated bias and the true bias For the estimated bias we use the same setting as the previous experiment with direct estimate corrected estimate and naive estimate We can see the the correction gives a more accurate estimate than direct estimation however the naive estimate remains to be the one with the most accurate estimate Active sampling algorithm We next address the limitations of the approaches explored above In particular we propose a general active-sampling based strategy that the label classifier can use to accurately estimate the bias using as little common data over as possible given the attribute classifier Furthermore we will make no assumption about the conditional independence or the structure of the attribute classifier g ga m m a r s g ga m m a r s g ga m m a r s U U U U Figure The distortion factor as a function of for different and under different error budgets U for the attribute classifier Apparently always attains its maximum either at the smallest or the largest possible value of Figure Experiment on the FIFA player dataset independence assumption holds Absolute difference between the true bias and the estimated bias for direct and corrected Cor estimation when to points of common data over are available and for naive Naive estimation Cor Cor Cor Cor Naive e st b s tr b s Figure Experiment on the UCI Adult dataset independence assumption is violated Absolute difference between the true bias and the estimated bias for direct and corrected Cor estimation when to points of common data over are available and for naive Naive estimation In the general setting the estimated bias and the true bias are related as follows Theorem The true bias can be derived from as sr r s r s s r where Pa x a y Pa x a y and are as defined in Proof To see notice that from the definition of we have Pf x y a Pf x y a Py a Pa x y a Pa x y a Py a a Py a a Sample size e st b s tr b s Uniform Positive sampling Active sampling Sample size e st b s tr b s Direct estimation Uniform Direct estimation Positive Figure The absolute difference between estimated bias and the true bias based on different active sampling strategies left and based on direct estimation over selected samples using uniform/positive sampling right Both are results averaged over runs with random initialization Next we first simplify the two terms in the numerator We can write Pa x y a Pa x a y Pf x a y Pa y Similarly we write Pa x y a Pa x a y Pf x a y Pa y For the denominator we have Py a a Pa y a Py a Similarly we have Py a a Pa y a Py a Substituting into we get Similarly for it is easy to derive Pf y a Hence we have s r s r r s r s Multiplying by r s multiplying by s r and subtracting the latter from the former we get s r r s r s s r Dividing the right hand side by we have Algorithm Active Sampling Input D attribute classifier parameters Output Estimated bias tolerance Sample a set of b examples uniformly at random from D conditioned on y and get their true attributes Get estimates r and s using the sampled examples Initialize t and to be zero for t do Sample a batch unlabeled ie no sensitive attribute information examples uniformly at random from D conditioned on y Sort the examples in ascending order according to the uncertainty of ie Obtain true attribute information the first examples and append to the labeled set of data Compute t t t t on the labeled set using and If t t t t t t t t are all bounded by then break end for Output estimated bias obtained by using and the above estimates While r s can be estimated from a small amount of data we explore sampling strategies for estimating the attribute classifier dependent quantities namely and Two natural sampling schemes are i Uniform sampling where we uniformly sample a set of examples and get their true sensitive attribute information and ii Positive sampling where we perform uniform sampling on the subset of examples with y We expect positive sampling to perform better than uniform sampling since the quantities that we are estimating are conditioned on y We compare the above two approaches with iii an active sampling approach that involves querying for true sensitive attributes conditioned on y of examples on which the attribute classifier is the most uncertain as described in Algorithm For each of the methods in each iteration we compute the estimated bias on the UCI Adult test set by using true values of a over the selected samples and the predicted values a from the attribute classifier for the unselected samples using a sampling batch size of The results are shown in Figure left where we see that the uncertainty based active sampling approach requires significantly less common data to accurately estimate the bias Furthermore we also compare our approach with iv Direct estimation where we directly estimate the bias based on the currently sampled set S ie the small amount of common on the same test set using a uniform sampling strategy and a positive sampling strategy The result is shown in Figure right Note that the magnitude in the y-axis is much larger compared to the left figure indicating that direct estimation using few samples tends to give less reliable estimation of the bias DISCUSSION AND CONCLUSION We formalized and studied a commonly occurring scenario in fairness evaluation and auditing namely the lack of common data involving both the class labels and sensitive attributes Our experiments and theoretical analysis reveal that great care must be taken in designing attribute classifiers for the purpose of bias estimation Furthermore in certain scenarios the structure of the optimal attribute classifier can be contradictory to natural criteria such as accuracy and fairness From the perspective of the designer of the attribute classifier maximizing the distortion factor as in is a challenging optimization problem It would be interesting to explore efficient algorithms for solving this Throughout our analysis we assume that the datasets DD are sampled from the same distribution It would be interesting to extend our theory to the more realistic case of when the dataset distributions are different Finally it would also be interesting to provide theoretical guarantees for the active sampling scheme in Algorithm