The Cost of Fairness in Binary Classification Binary classifiers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive eg race We study the inherent tradeoffs in learning classifiers with a fairness constraint in the form of two questions what is the best accuracy we can expect for a given level of fairness and what is the nature of these optimal fairness-aware classifiers To answer these questions we provide three main contributions First we relate two existing fairness measures to cost-sensitive risks Second we show that for such costsensitive fairness measures the optimal classifier is an instance-dependent thresholding of the class-probability function Third we relate the tradeoff between accuracy and fairness to the alignment between the target and sensitive features class-probabilities A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates INTRODUCTION Suppose we wish to learn a classifier to determine if an applicant will repay a loan That is given various input features about the applicant such as their employment status income and credit history we wish to predict the target feature namely likelihood of repaying the loan Suppose however that one of the input features is deemed sensitive eg their race Then we might be required to constrain the classifier to not be overly discriminative with respect to this sensitive feature subject to this constraint we would of course like our classifier to be as accurate at predicting the target feature as possible This fairness-aware learning problem has received considerable attention in the machine learning community of late Pedreshi et al Kamiran and Calders Calders and Verwer Dwork et al Kamishima et al Fukuchi et al Zafar et al Hardt et al Zafar et al Existing work on fairness-aware learning has largely focussed on two central questions a how does one formally measure the fairness of a classifier and b given such a measure how does one learn a classifier that achieves fairness For the former the challenge is that seemingly sensible definitions of fairness can have subtle undesirable consequences Å½liobaite et al Dwork et al to address this a range of progressively refined measures have been designed Calders and Verwer Dwork et al Hardt et al Zafar et al For the latter the challenge is that merely ignoring the sensitive feature is inadmissible owing to it potentially being predictable by other features Pedreshi et al to address this approaches based on post-hoc correction Calders and Verwer Hardt et al regularisation Kamishima et al and surrogate loss minimisation have been proposed Zafar et al The limits of the possible in fairness-aware learning Despite the impressive advances detailed above some basic theoretical aspects of the fairness-aware problem have received less attention For example before attempting to design an algorithm targetting a particular measure of fairness it is natural to ask Q What is the best we can do There is typically an unavoidable tradeoff between how accurate our classifier is with respect to the target feature and how fair it is with respect to the sensitive feature One may seek to quantify this tradeoff in terms of properties of the data source giving inherent limits of what is possible for any possible algorithm Menon RC Williamson The Cost of Fairness in Binary Classification Q How do we achieve the best Having determined the inherent accuracy-fairness tradeoff one may seek to find what classifiers achieve this limit This is not purely of theoretical import as we may then seek to design methods to approximate these optimal classifiers In machine learning terminology Q concerns the Bayes-optimal classifiers for the fairness-aware learning problem The Bayes-optimal classifier attains the lowest possible average error for a given problem thus no algorithm no matter how clever or sophisticated can attain lower average error than this classifier Such classifiers are foundational in the study of standard binary classification Devroye et al and provide a limit of the possible in a manner similar to what Shannons information theory did for practical problems of telecommunication Gleick or what the science of thermodynamics did for heat engines in the th century Bryant A Mathematics of Morality We should strive for a kind of moral geometry with all the rigor which this name connotes Rawls pg In this paper we present three contributions in the study of Q and Q First we show that two popular fairness measures can be seen as instances of a more general scheme We then show that for this general scheme of fairness measures the Bayes-optimal classifier can be explicitly computed Intuitively this classifier deems an instance to be positive if the probability of the target feature being active is sufficiently higher than the probability of the sensitive feature being active Finally we use the explicit form of this optimal classifier to provide an analytical expression for the fundamental tradeoff curve of accuracy versus fairness This curve is shown to depend on measure of similarity between the target and sensitive features Our approach in answering Q and Q is mathematical in nature but does not purport to provide a complete answer to the probems of fairness It is motivated by statements such as that due to John Rawls quoted above Such an approach provides the ability to make precise statements at a considerable level of generality and is comparable to the formal analysis of problems including fairness in welfare economics Harsanyi Sen In the context of this literature our work follows the precept of Sen Chapter that mere identification of fully just social arrangements is neither necessary nor sufficient We embrace Sens pragmatism by focussing on the quantifiable tradeoffs one might make to approach certain notions of fairness However we do not claim to derive the right tradeoffs neither in the choice of loss functions to be used nor even their relative weights in contrast to Rawls pg ff who not only ackowledges there will be tradeoffs between overall social utility and fairness but tries to argue what the right tradeoff is See Appendix for further relations to the philosophical and economic literature on fairness Formally our main contributions CC are C we reduce two popular fairness measures disparate impact and mean difference to costsensitive risks Lemmas C we show that for cost-sensitive fairness measures the optimal fairness-aware classifier is an instance-dependent thresholding of the class-probability function Propositions C we quantify the intrinsic method-independent impact of the fairness requirement on accuracy via a notion of alignment between the target and sensitive feature Proposition Our results deal with the theoretical limits of what is possible for any fairness-aware learning method for the class of fairness measures we consider given access to the theoretical population distribution This leaves some important questions unanswered such as how one can construct a classifier that is optimal for a given dataset rather than the theoretical population While we do not provide a complete answer to this matter we do provide a practical means of approximating the Bayes-optimal classifier Specifically the form of the optimal fairness-aware classifiers C lets us derive a practically usable algorithm wherein we separately estimate class-probabilities for the target and sensitive features eg by logistic regression and combine them suitably BACKGROUND AND NOTATION We fix notation and review background Table summarises some symbols that we frequently use Standard learning from binary labels Let X be a measurable instance space eg characteristics of an applicant for a loan In standard learning from binary labels we have samples from a distribution D over X with D Here Y is some target feature we would like to predict eg whether to grant a loan Our goal is to output a measurable randomised classifier parametrised by X that distinguishes between positive Y and negative Y instances A randomised classifier predicts any x X to be positive with probability x the quality of any such classifier is assessed by a statistical risk R D X R Often this is some function of the false negative and false positive rates FNR D E X Y X FPR D E X Y X viz the class-conditional error probabilities when classifying x as positive with probability x In the sequel we will drop dependence of quantities on the underlying distribution when it is clear from context eg we will write FNR in place of FNR D Remark A randomised classifier parameterised by X should not be confused with two related objects a deterministic classifier g X and a class-probability estimator X The former produces deterministic nonrandom classifications for each input Consequently eg in contrast to Equation D P X Y The latter emits the confidence that an instance has positive label as per PY X x However one typically uses this to make deterministic classifications eg by constructing the classifier o Thus while has the same type as the resulting predictions and their evaluation are different eg in contrast to Equation D P X Y A canonical risk is the cost-sensitive risk which for cost parameter c and PY is c Randomised classifiers are commonly used in rectifying local non-concavities in ROC curves Fawcett Strictly this is an abuse of terminology as risks conventionally refer to expected losses In our context this corresponds to using functionals that only linearly combine the false positive and negative rates ie cost-sensitive risks Fortunately these are precisely the class of R that we consider in the sequel When c this is a scaled version of the balanced error FNR FPR A Bayes-optimal randomised classifier for a risk is any minimiser Argmin R D For the costsensitive risk with parameter c the Bayes-optimal classifier is x co co where PY X x is the class-probability function nEo if E is true and zero otherwise and is arbitrary Consequently for the loss corresponding to c we classify as deterministically positive those instances whose label is on average more likely to be positive Fairness-aware learning In fairness-aware learning one modifies the standard binary label learning problem in two ways The statistical setup is modified by assuming that in addition to the target feature Y there is some binary sensitive feature Y we would like to treat in a special way eg the race of an applicant The classifier evaluation is modified by assuming that we reward classifiers that are fair in the treatment of Y To make this goal concrete the literature has studied notions of perfect and approximate fairness Perfect fairness We consider two notions of perfect fairness stated in terms of Y Y and classifier prediction Y X Bern X The first is demographic parity Calders and Verwer which requires the predictions to be independent of the sensitive feature PY Y PY Y The second is equality of opportunity Hardt et al which requires the predictions to be independent of the sensitive feature but only for the positive instances PY Y Y PY Y Y Other notions include equalised odds Hardt et al and lack of disparate mistreatment Zafar et al Demographic parity has received the most study but is known to have deficiencies Dwork et al Hardt et al Zafar et al Approximate fairness We consider two notions of approximate fairness via fairness measures that quantify the degree of fairness for a classifier The The sensitive feature may or may not be available during training see even if not available other correlated features may induce discrimination Pedreshi et al Symbol Meaning X Instance Y Target feature Y Sensitive feature Y Prediction Symbol Meaning D Distribution D One of DEO Distribution Y DEO Distribution Y Y Symbol Meaning Randomised classifier PY X x PY X x y PY X y Symbol Meaning Cost-sensitive risk Balanced risk DI Disparate impact MD Mean difference Table Glossary of commonly used symbols first is the disparate impact DI factor Feldman et al which is the ratio of the probabilities appearing in Equation DI PY Y Y The second is the mean difference MD score Calders and Verwer MD PY Y PY Y We refer the reader to Å½liobaite for a survey of other fairness measures While some scenarios may demand perfect fairness having a strong degree of approximate fairness may be acceptable in others for example disparate impact has its roots in the rule of the US Equal Employment Opportunity Commission EEOC Remark The DI factor takes values in the range and the MD score in Demographic parity is achieved when DI or MD thus for both measures it is undesirable for the score to be too small or too large It is often implicitly assumed that is such that DI or MD in which case a large score is desirable Alternately one can work with symmetrised versions of these measures eg DI minDI DI Maximising this ensures that one cannot predict the sensitive feature merely by flipping outputs Formally observe that eg DI and DI DI ie we have perfect fairness see Appendix C Fairness-aware learning and risk difference minimisation We now formalise the fairness-aware learning problem by viewing fairness measures as statistical risks We assume the following statistical setup Let random variables X for some joint distribution over X where X represents the instance Y the target feature and Y the sensitive feature Suppose D refers to the distribution and D to a suitable distribution over X Y concretely in the demographic parity setting D Y while in the equality of opportunity setting D DEO Y Y Existing fairness measures as risks We begin with a simple observation the DI and MD fairness measures are transformations of suitable false positive and negative rates and thus are statistical risks Specifically for any randomised classifier X with predictions Y X Bern X we have FNR PY Y and FPR PY Y Consequently for D DI D FPR FNR D MD D FPR D FNR D Observe that one can equally choose D DEO so as to yield approximate fairness measures for the equality of opportunity setting clearly when eg DI DEO we recover Equation The notion that fairness measures are risks on D is implicit in prior surveys eg Å½liobaite By making this notion explicit we may now succinctly state the fairness-aware learning problem Fairness-aware learning general case Informally fairness-aware learning involves finding a randomised classifier X so that Y is well predicted but Y is not Formally suppose that we measure how well can predict Y and Y via two risks a performance measure D R and fairness measure D R For example we might pick to be a cost-sensitive risk and to be one of DI or MD Then we seek to minimise the difference of two statistical risks as below Problem Fairness-aware learning For tradeoff R minimise the fairness-aware objective D D This is understood to mean a randomised classifier parametrised by The tradeoff parameter determines how we balance the competing goals of accuracy and fairness We do not constrain for a subtle but important reason as per Remark for the DI and MD scores it is undesirable for the fairness measure to be too small or too large Equation effectively considers a constrained version of the problem where and converts it to Lagrangian form corresponds to the difference in Lagrange multipliers for the two bounds which can be negative In we will see that another interpretation for the DI and MD scores is that we constrain their symmetrised versions per Remark to be large Remark In practical settings one only has access to finite samples from the joint distribution By focussing on Equation our analysis is thus asymptotic We emphasise that our motivating questions Q and Q are nonetheless non-trivial even with access to the underlying distributions it is not obvious what the accuracy-fairness tradeoff is nor what the form of the optimal classifier is Further we will show how to approximate the latter given only finite samples Fairness-aware learning cost-sensitive case The generality of Problem has conceptual appeal but presents challenges if we are to proceed with our intended analysis of Q and Q To make progress we assume both the performance and fairness measures are cost-sensitive risks Equation Problem Cost-sensitive fairness-aware learning For tradeoff R and cost parameters c c minimise the fairness-aware cost-sensitive D D D c D c a More precisely this is D D c c we suppress c c as their scope will be clear from context Using a cost-sensitive risk as performance measure is not strongly limiting as it can encompass more complex performance measures with distribution-dependent costs Parambath et al Narasimhan et al However it would be disappointing if this choice makes us unable to accommodate the popular DI and MD fairness measures We now allay this concern A cost-sensitive view of existing fairness measures The previous section cast the DI and MD measures as statistical risks We now show they may be further related to cost-sensitive risks implying that it suffices to analyse the latter In the following recall that for brevity we write eg FPR in place of FPR D Relating DI and MD to cost-sensitive risks Underpinning our results is the balanced costsensitive risk c c FNR c FPR When c we get the balanced error For general c this is simply a scaled and re-parameterised version of the standard cost-sensitive risk it will however prove more convenient in our analysis Our first result relates the disparate impact factor Equation and balanced cost-sensitive risk Lemma Pick any randomised classifier Then for any if DI Lemma does not imply that disparate impact equals a cost-sensitive risk rather it says that a disparate impact constraint is equivalent to a costsensitive constraint ie their super-level sets are equivalent We shall shortly see that this is sufficient to justify learning with a cost-sensitive risk We next show an analogous in fact stronger result for the mean difference score Equation Lemma Pick any randomised classifier Then for any if MD MD Note that Equation implies an equivalence of risks and not just super-level sets Note also that for the MD score the corresponding balanced costsensitive risk has a cost-parameter that does not depend on This proves beneficial for the purposes of learning as we shall see in Implications for learning with DI and MD Lemmas and establish the versatility of costsensitive fairness measures we can reduce learning with the DI and MD scores to the cost-sensitive Problem for a suitable choice of cost c Lemma Pick any distributions D D and fairness measure Pick any c Then R c with min D c D D min D c D c The first objective in Equation constrains that the symmetrised fairness measure in the sense of Remark is large recall that maximising this quantity ensures perfect fairness and thus the objective is sensible The equivalence to the second objective is a consequence of the Lagrangian principle see Appendix E combined with Lemmas and We emphasise that our focus on the cost-sensitive version of the fairness-aware learning problem is owing to Lemma it implies that by analysing this special case we encompass the use of the MD and DI scores as fairness measures Moving beyond costsensitive risks is only beneficial if one is interested in a more exotic fairness measure that is inexpressible as such a risk Remark One subtlety with Lemma is that for the DI we have to tune both and c this is because the cost in Lemma depends on For the MD however we can set c Related work Lemma can be seen a special case of a broader relationship between fractional performance measures and level-finder functions Parambath et al Theorem Narasimhan et al Lemma We are not however aware of prior results relating the DI and MD scores to costsensitive risks The closest analogue is Feldman et al who related for the DI to the balanced error via DI FNR This bound depends on the distribution and classifier while our bound on uses a constant Our result is thus simpler and allows for tractable analysis of optimal classifiers see Remark Although not our primary focus Lemma also lets us address the problem of certifying whether a dataset is free of disparate impact Feldman et al ie for fixed every classifier satisfies DI By Lemma this is equivalent to asking whether for every where See Appendix and G Optimal fairness-aware classifiers Having justified the broad applicability of the costsensitive fairness-aware learning problem Problem we are in a position to examine its inherent tradeoffs We begin by studying the question what are the theoretically ie Bayes- optimal randomised classifiers and how do they differ in the fairness-unaware problem Formally fix some D corresponding to and pick D DEO Then for costs c c and tradeoff R we seek Argmin X D D for the fairness-aware risk of Equation Such Bayesoptimal classifiers represent the gold-standard for the learning problem and computing them is thus of theoretical import Further we will shortly see that their explicit form suggests a simple practical algorithm Our results will be expressed in terms of three sets of quantities the base rates PY PY the class-probability functions for the target and sensitive feature under demographic parity and equality of opportunity PY X x PY X x PY X and the modified Heaviside or step function o o for parameter Computing the Bayes-optimal classifiers Thus far we have not seen any distinction between the demographic parity and equality of opportunity settings however in computing the Bayes-optimal classifiers a separate analysis is necessary Demographic parity We begin with the explicit form of the optimal solutions under demographic parity D Proposition Pick any costs c c and R Then Argmin X D for c c Three comments are in order First as a sanity check when the optimal comprises the familiar Bayes-optimal classifiers for a cost-sensitive risk which thresholds the class-probability around c via x co co Here is arbitrary since for instances at the threshold boundary c the risk is a constant and so any prediction is optimal Second for the optimal modifies the solution with an instance dependent threshold correction which depends on The correction increases the standard threshold of c whenever c intuitively when we are confident in the sensitive feature being active for an instance we are more conservative in classifying the instance as positive for the target feature Third the optimal classifier above is in fact deterministic except when c c in general for a given we expect this to only hold for few or no x X In the above we made no explicit assumption as to whether or not the sensitive feature is provided as input to the classifier If we assume this feature is in fact available the form of the optimal classifier simplifies dramatically Corollary Pick any costs c c and R Suppose D is over where X X Y and y PY X x Y y Then Argmin X D s c c c c Here we simply apply a constant threshold to the class-probabilities for each value of the sensitive feature This is a simple consequence of Proposition as we can simply consider one of the features of X to be perfectly predictive of the sensitive feature which makes y Equality of opportunity We next turn to the optimal classifiers for the equality of opportunity setting D DEO The result here is similar to Proposition but with a multiplicative threshold correction It is surprising that a simple change in fairness setting results in such a non-trivial modification of optimal solutions Proposition Pick any costs c c and R Then Argmin X D DEO s c c When the sensitive feature is available an analogous result to the previous section holds Corollary Pick any costs c c and R Then Argmin X R D DEO s c c c c where y PY X x Y y Related work Computing the Bayes-optimal classifiers as above is not without precedent Hardt et al Corbett-Davies et al considered the same question but in the case of exact fairness measures We are not aware of prior work on computing the optimal classifiers for approximate fairness measures While our results have a similar flavour to the exact fairness case explicating them is important to understand the full tradeoff between accuracy and fairness and also suggests a simple algorithm as we now see A plugin approach to the fairness problem The form of the Bayes-optimal classifiers above is not only of theoretical interest they enable the derivation of practical classifiers suitable which are suitable for learning from finite samples and rely on nothing more than logistic regression As a warm up recall that for standard cost-sensitive learning ie the optimal classifier x co suggests the following plugin approach estimate eg by logistic regression and then threshold the resulting predictions around c This approach is intuitive and provably consistent Narasimhan et al For the cost-sensitive fairness problem the Bayesoptimal classifiers similarly rely on thresholding a suitable combination of the class-probabilities and Thus we can analogously construct a plugin classifier by estimating separately eg by logistic regression and then combining them per Equations When the sensitive feature is available we only need a single model for y which is thresholded separately for each sensitive feature value Equation Algorithm summarises this procedure for the demographic parity setting Appendix G presents some illustrative experiments for this algorithm Algorithm Plugin approach to fairness-aware learning demographic parity setting Input Samples from distribution cost parameters c c tradeoff parameter Output Fairness-aware randomised classifier X Estimate X via logistic regression on Estimate X via logistic regression on Compute s x c c from above estimates Return x for any Strengths and weaknesses of the plugin approach The plugin approach has several salient features a it reduces the problem to two calls of a logistic regression or other class-probability estimation solver and avoids the need for any bespoke algorithms b as a consequence of a it involves a convex optimisation unlike some existing approaches to the problem Kamishima et al c tuning of the tradeoff parameter does not require any retraining we can simply learn once and appropriately change how they are thresholded The same holds true for tuning the cost parameter c when learning with the DI score Despite these appealing properties we caution that in practice the result of Algorithm may be suboptimal This is because our estimates of the class probabilities may be imperfect owing to at least two distinct possible sources of error a there are limits on how accurately we can estimate these probabilities from a finite sample b the true class-probabilities may not be expressible in our chosen class of models These are manifestations of the more general issues of estimation and approximation error Devroye et al that plague any machine learning algorithm even for standard binary classification There are principled means of at least partially mitigating both issues for example by employing suitable regularisation to prevent overfitting to a finite sample and employing nonparameteric estimators to allow modelling of arbitrarily complex functions Quantifying the degradation resulting from using imperfect probabilities is nonetheless an important but non-trivial task indeed until the recent work of Woodworth et al we are not aware of any prior fairness-aware algorithm with finite sample guarantees Related work The idea of correcting outputs to ensure fairness goes back to at least Calders and Verwer who proposed to modify the output of naÃ¯ve Bayes so as to minimise the MD score However their approach does not have any optimality guarantees More recently Hardt et al proposed to post-process the outputs of a classifier trained on the original problem and argued for its optimality They however worked in the exact rather than approximate fairness setting and did not consider separate training procedures for predicting the target and sensitive features Woodworth et al established limits on such post-processing extending this to approximate fairness measures would be of interest Zafar et al proposed to approximately solve Equation by picking convex surrogate losses R R and find s Argmin s E Y E Y for C cC c For nonlinear this objective will be non-convex in s Similar problems plague related approaches based on regularisation Kamishima et al Fukuchi et al Remark We emphasise a conceptual difference between our plugin approach and the surrogate approach of Equation The latter aims to directly design a differentiable approximation to Equation but it is non-trivial task for the resulting objective to be convex By contrast the plugin approach side-steps direct minimisation of this Equation and instead follows a procedure that asymptotically results in the same theoretical optimal solution The accuracy-fairness tradeoff As our final contribution we now study the tradeoff between performance on our base problem and fairness and show it is quantifiable by a measure of alignment of the target and sensitive variables The fairness frontier Our definition of the cost-sensitive fairness-aware learning problem Problem was in terms of a linear tradeoff between the performance and fairness measures To quantify the tradeoff imposed by a fairness constraint we will study the following explicitly constrained problem for let Argmin X D c D c D c D c Equations and are related by the Lagrangian principle see Appendix E The function R R represents the fairness frontier for a given lower bound on fairness it measures the best excess risk over the solution without a fairness constraint Evidently is non-decreasing since the constraints are nested as increases ie demanding more fairness can never improve performance Remark The tradeoff measured here is one inherent to the problem rather than one owing to the specific technique one uses By computing we determine the fundamental limits of what accuracy is achievable by any classifier no matter how sophisticated it may be Intuitively this tradeoff captures the level of contention between utility and fairness in the distribution The frontier and probability alignment The behaviour of summarises the tradeoff between performance and fairness the steeper its growth the more we have to sacrifice per unit increase in fairness Can we relate this behaviour to properties of the underlying distributions D D Note that since cost-sensitive risks are linear in the classifier can be computed empirically via a linear program see Appendix D however we seek a more abstract understanding of its behaviour By analogy we seek something akin to the notion of compatibility functions in semi-supervised learning Balcan and Blum wherein one can guarantee that unlabelled data is useful when there is an alignment of the marginal data distribution with ones function class Intuitively we expect that the tradeoff between fairness and performance depends on how similar the target and sensitive features are in the extreme case where they are one and the same there is an inescapable linear penalty while if they are completely dissimilar we expect the penalty to be milder This idea can be formalised via a notion of disalignment between the features For concreteness the following is for demographic parity Proposition Pick any cost parameters c c For any there is some R and Bayes-optimal randomised classifier Argmin X D so that the frontier is E X c X co If further this is deterministic ie Im the frontier is E X c c c c o Unpacking the above Equation gives a concrete notion of disalignment between and how much they disagree around the respective thresholds c and c and Equation shows that when this disalignment is high the fairness constraint has less of an effect The requirement that be deterministic may be dropped at the expense of an additional term in Equation that depends on the alignment of the non-deterministic component and Appendix G presents some illustrations of this frontier on synthetic datasets Our prior analysis of the cost-sensitive fairness problem is crucial to establishing Proposition we exploit the explicit form of Bayes-optimal classifiers to Problem and existing results about costsensitive risks In particular we use the fact that the frontier Equation is simply the cost-sensitive regret or excess risk of the classifier This quantity can be related to a Bregman divergence Reid and Williamson and combined with our explicit form for the Bayes-optimal classifier we obtain our Equation Relation to existing work That there is in general a tradeoff between accuracy and fairness has been long recognized Kamiran et al Å½liobaite has considered the subtleties of empirically determining trade-offs between fairness and accuracy but did not provide a theoretical analysis as above Some work has empirically studied the impact of varying on problems akin to Equation eg Zafar et al however we are not aware of a result analogous to Proposition that analytically quantifies the performance-fairness tradeoff Conclusion and future work We studied the tradeoffs inherent ie not specific to any algorithm in the problem of learning with a fairness constraint showing that for cost-sensitive approximate fairness measures the optimal classifier is an instance-dependent thresholding of the class probability function and quantifying the degradation in performance by a measure of alignment of the target and sensitive variable We used our analysis to derive a simple plugin approach for the fairness problem An intuitive summary of our results In order to grasp our main results at a more intuitive level let us consider their implications for a concrete task of designing a classifier to determine whether an applicant should be given a loan the target feature while not discriminating based on gender the sensitive feature C says that if we measure fairness using either the DI or MD score then this is equivalent to measuring a particular cost-sensitive error This simply means that we use our classifier to predict each applicants gender based on the available features and look at the error rates amongst both males and females by summing a suitably weighted combination of these error rates we get a quantity that is reflective of the underlying fairness measure C says that given access to the entire population of males and females the classifier which attains the best tradeoff between accuracy and fairness has a simple form one computes for a given applicant the probability of them repaying the loan and determines if this probability is larger than a threshold based on the probability of them being male or female To understand this intuitively assume that women are inherently more likely to repay a loan than men and that the sensitive feature is available as an input Then for a given choice of accuracy-fairness tradeoff the optimal classifier aims to make it easier for men and harder for women to be granted a loan so as to make the proportions of men and women amongst the accepted and rejected pool more commensurate and clearly it does so at some expense in accuracy of predicting the probability of repayment C says that if gender perfectly coincided with loan repayment eg women were guaranteed to repay loans and men guaranteed to default then we can either have maximum accuracy but no fairness or maximum fairness but random-level accuracy At the other extreme if gender were perfectly independent of loan repayment eg both women and men were equally likely to repay loans then we can have maximum accuracy and fairness simultaneously Finally if gender partially correlates with loan repayment then the tradeoff between accuracy and fairness is determined by the strength of this correlation and falls in between the previous two extremes The ability to theoretically compute the tradeoffs between fairness and utility is perhaps the most interesting aspect of our technical results We stress that the tradeoff is intrinsic to the underlying data in fact it is intrinisc to the underlying distributions that generated that data That is any fairness or unfairness is a property of the data not of any particular technique This raises interesting philosophical issues which are briefly touched upon in Appendix The tradeoffs we can theoretically compute precisely quantify what price one has to pay in utility in order to achieve a desired degree of fairness in other words we have computed the cost of fairness Future work There are several possible directions for future work we believe it valuable to study Bayes-optimal scorers for ranking measures such as AUC establish consistency and finite-sample guarantees of the plugin estimators of and extend our analysis to the case of multi-category sensitive features