Interventions over Predictions Reframing the Ethical Debate for Actuarial Risk Assessment Actuarial risk assessments might be unduly perceived as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system from pretrial release to sentencing parole and probation In recent times these assessments have come under increased scrutiny as critics claim that the statistical techniques underlying them might reproduce existing patterns of discrimination and historical biases that are reflected in the data Much of this debate is centered around competing notions of fairness and predictive accuracy resting on the contested use of variables that act as proxies for characteristics legally protected against discrimination such as race and gender We argue that a core ethical debate surrounding the use of regression in risk assessments is not simply one of bias or accuracy Rather its one of purpose If machine learning is operationalized merely in the service of predicting individual future crime then it becomes difficult to break cycles of criminalization that are driven by the iatrogenic effects of the criminal justice system itself We posit that machine learning should not be used for prediction but rather to surface covariates that are fed into a causal model for understanding the social structural and psychological drivers of crime We propose an alternative application of machine learning and causal inference away from predicting risk scores to risk mitigation Keywords causal inference criminal justice interventions risk assessment tools INTRODUCTION In a team of investigative journalists from ProPublica published an article Angwin et al claiming that COMPAS a proprietary risk assessment tool that has been used in the US criminal justice system was racially biased The article sparked a national debate about the expanding use of algorithmic decision-making aids in the courts This debate is emblematic of a much broader conversation around the appropriate use of data and statistical methods in society in spheres as varied as health care housing employment and education For example Selbst Selbst cites media debates on consumer finance Economist employment Wang housing Biggs health care Siwicki and sentencing Tashea While these issues are largely framed in terms of new technology driven by big data or artificial intelligence the methods and tools in question are often incremental iterations on much older actuarial decision-making practices This is certainly the case for risk assessments which have existed in some form in the US criminal justice system since the s By placing the current debate around risk assessment in a broader context we can get a fuller understanding of the way these actuarial tools have evolved to achieve a varied set of social and institutional agendas The current debate around risk assessment tends to characterize or implicitly c C Barabas Dinakar Ito M Virza Zittrain Interventions over Predictions Reframing the Ethical Debate for Actuarial Risk Assessment concede the purpose of these tools as fundamentally predictive in nature The more significant power of data-driven risk assessment can be as a broader diagnostic tool one used to help practitioners address risk as a dynamic intervenable phenomenon When risk assessments are recast in this light we can ask whether or not regression and machine learning methods can help in diagnosis and intervention rather than prediction We make a case for why causal inference a statistical method designed explicitly to establish a causal connection between a treatment and an outcome of interest provides a more desirable framework for developing intervention-driven risk assessments in the criminal justice system CURRENT DEBATES ON THE FAIRNESS OF RISK ASSESSMENTS Risk assessment tools have been adopted to assist with a number of decision points throughout the criminal justice system from pretrial release to post-conviction sentencing probation and parole In spite of the tools growing influence over judicial or administrative decisions defendants rarely have an opportunity to probe or challenge the recommendations the tools generate While defendants are supplied with their end scores they are typically not entitled to access the calculations or input data that were used to calculate the final tabulation Critics have called for increased transparency surrounding the development and administration of these tools arguing that the proprietary interests of assessment developers should not take precedence over a defendants right to due process in the courts Wexler Angwin et al Fennell and Hall In addition to transparency issues related to proprietary software a major focus of recent scholarship has been on developing interpretable machine learning models that enable outside researchers to better scrutinize the underlying logic of actuarial tools which are based on machine learning Zeng et al Vellido et al Others have focused on the particular ethical challenges that arise during the administration and interpretation of risk assessments Although risk assessments can be perceived and sometimes marketed as an objective means of overcoming human bias in decision making there remain many ways that human discretion enters into the formation and interpretation of risk scores Ethnographic work has shown that scores are frequently misapplied and misinterpreted by judges and practitioners Hannah-Moffat Moreover scholars have documented specific strategies that administrators can use to shade or manipulate end scores to reflect their preconceptions of offender risk Hannah-Moffat Hardy There have also been multiple documented instances in which a risk or needs score has been inappropriately referenced for a decision it was not intended to inform for example a needs score that was intended to inform probation decisions being cited in judges sentencing statements Wexler Angwin et al At least one recent court ruling has emphasized the importance of providing judges additional information about the limitations inherent in risk score predictions Roggensack and Abrahamson Others have called for more work to be done in order to understand how risk scores interact with the pre-existing biases and beliefs of the individuals who rely on them Fennell and Hall Amidst all these issues arguably the primary focus of debate regarding the fairness of risk assessments has centered around the specific methods and data used to develop statistically valid predictions Risk assessment is commonly characterized as a predictive technology one whose value is measured by the degree to which they are more accurate than the fallible human decision makers they are designed to advise or supplant Kleinberg et al a This characterization stems in part from the fact that much of the recent attention on risk assessments has centered around their appropriate use in pretrial release decisions Angwin et al Kleinberg et al a Flores et al In contrast to other stages of the criminal justice process pretrial release is framed as a narrow and fundamentally predictive task risk assessments are used to inform decisions about whom to detain and whom to release before a trial date Increasingly pretrial risk assessments are being embraced as part of a larger effort to move away from a resource-based approach of making release decisions based on individuals ability to post cash bail to a risk-based approach that centers on predicting ones likelihood of engaging in a specific set of undesirable behaviors such as failure to appear in court or to engage in new criminal activity Foundation Institute In this context some scholars have framed the value-add of risk assessment strictly in terms of their ability to make predictions more accurately than the judges meting out such decisions by themselves Kleinberg et al a Because the value-add of risk assessments has been framed largely in terms of their predictive power much of the ethical debate surrounding them has focused on how to handle various forms of bias in the face of less-than-accurate tools Scholars have identified numerous ways that statistically driven methods like machine learning can reproduce existing patterns of individual prejudice and institutionalized bias These mechanisms correspond with different stages in the model design process such as identifying the research question collecting training data labeling examples within the training set feature selection and double encoding bias in proxies for protected classes Barocas and Selbst Gong Angwin et al Starr Citron Others have focused specifically on bias in model outcomes Critics of risk assessment have expressed concerns about the distribution of inter-group inaccuracy in risk models They emphasize the importance of predictive parity as an explicit goal that is the systems we use should not only be equally accurate but also have similar accuracy rates over all test groups eg different racial groups or genders to which they are applied These researchers have argued that risk assessments are not that accurate to begin with and for the large percentage of outcomes that is not accurate disparate impact tends to fall disproportionately across the population along racial lines Angwin et al Citron Yet proponents of risk assessment have argued that uneven distributions of false positives and false negatives do not constitute a biased model in a strictly technical sense These individuals contend that the most important consideration regarding bias is whether or not the probability estimates provided by the algorithm are well-calibrated so that the algorithm performs in a way that is equally accurate across all groups Flores et al Skeem et al This debate has led to a broader discussion on the inherent trade-offs in these competing notions of fairness Kleinberg et al Kleinberg et al b argue that in general no mechanism can both achieve optimal accuracy and optimal predictive parity unless the base rates for the groups are equal or the algorithm in fact achieves perfect classification In their excellent survey Berk et al identify six kinds of fairness and show that not only these notions conflict with accuracy but also with one another Berk et al This debate is now evolving to identify specific trade-offs between things like predictive accuracy and predictive parity across groups This makes sense when we frame the value of risk assessments strictly in terms of their utility as a predictive technology whereby accuracy levels are often in direct tension with these competing notions of fairness However this framing of the issues eschews the fact that risk assessments are rarely used just for predictive purposes The majority of risk assessments in use today are based on a risk-needs-responsivity framework to inform a broad range of decisions regarding effective punishment and treatment Hannah-Moffat Such assessments are and should be fundamentally diagnostic in nature whereby risk is conceptualized as a dynamic phenomenon that can be lowered through a range of semi-personalized interventions In the criminal justice system risk assessments play an integral role in shaping how practitioners understand and intervene in the lives of offenders in the service of lowering future crime rates We conceptualize risk assessments as part of an assemblage of practices and tools that give rise to visibilities or ways of viewing individual offenders and complex phenomena like criminal behavior Hardy Risk assessments have evolved over the last several decades to support a diverse set of institutional goals and processes In the following section we give a brief overview of how risk assessments have evolved from a tool used solely for prediction to one that is diagnostic at its core We then call into question whether or not the statistical methods currently underlying risk assessments namely regression and to a limited extent machine learning are the appropriate methods to use in the service of this goal Risk assessments predictive or diagnostic tools The evolution of actuarial risk assessment has been well documented by others and is frequently broken down into four generations of tools Andrews et al Harcourt Monahan and Skeem Desmarais and Singh Hannah-Moffat The first generation of risk assessments emerged in the s and were based largely on semi-structured clinical evaluations that were carried out by skilled professionals as part of an effort to identify rehabilitative treatment options for offenders These assessments were structured around a standardized set of clinical items but did not include a statistical mechanism for validating scores and decisions Andrews et al First generation risk assessment tools fell out of fashion in the s as they were widely criticized for being too subjective and having low levels of predictive accuracy Hannah-Moffat Also during this time a nothing works mindset began to supplant the rehabilitative approach that had dominated the correctional system during the first half of the twentieth century Maurutto and Hannah-Moffat In place of treatment programs approaches based on incapacitation and other law and order measures became the norm A second generation of assessment was optimized and validated for predictive accuracy using a new statistical method regression modeling Regression modeling is particularly well-suited for prediction-oriented assessment because it enables researchers to identify variables that are predictive of an outcome of interest without necessarily having to understand why that factor is significant To this end the second generation of assessment focused almost exclusively on static historical factors such as age and criminal history Such factors were considered desirable because they were perceived as both objective and highly predictive Maurutto and Hannah-Moffat This marked a subtle shift in the values and goals risk assessments were designed to support away from semi-personalized treatment and towards objective prediction As a result risk assessments were reconfigured as a more simplistic tool that relied heavily on prior criminal history to predict future outcomes These changes marked a significant shift in the way the justice system understood and responded to the notion of offender risk While earlier tools were designed to facilitate effective treatment new statistical assessments were based on a static notion of risk one that was not easily changed through intervention These assessments gave rise to what some scholars have dubbed a new penology whereby penal policies shifted away from rehabilitative interventions to more administrative approaches to population management that rely almost exclusively on incapacitation to mitigate risk Feeley and Simon As Maurutto and Hannah-Moffat argue when assessments pivot towards prediction rather than intervention risk is conceived of as a negative strategy that incapacitates and manages but never produces productive transformations Maurutto and Hannah-Moffat Since the s the shift towards standardized decisionmaking and away from individualized treatment accompanied a sharp increase in incarceration rates Harcourt These prediction-oriented risk assessments were a part of a suite of tools and policies introduced in the s and s that relied heavily on prior criminal history to predict and systematize interventions to prevent future crime Other policies included state and federal sentencing guidelines which were ostensibly designed to standardize decision making across jurisdictions and minimize bias in sentencing A growing body of scholarship has linked these colorblind policies to trends in mass incarceration and growing racial disparities in the criminal justice system Van Cleve and Mayes Schlesinger Harcourt Hamilton Scholars have argued that prediction-oriented risk assessments produce a ratchet effect on profiled populations because they fail to take into consideration the criminogenic nature of the criminal justice system itself Harcourt Hamilton Studies have demonstrated diminishing returns on the use of incarceration as a means of lowering crime Zimring In fact the ripple effects of incarceration such as the weakening of family ties and diminishing employment opportunities can exacerbate the likelihood of future recidivism at both the individual and collective level Cullen et al DeFina and Hannon Mitchell et al As Hamilton has argued assessments that rely on a static prediction-oriented notion of risk can act as a self-fulfilling prophecy that justifies and widens the net of social control over marginalized populations Hamilton When risk is constituted as a static phenomenon it can only suggest what should not be done ie release and gives little insight into what can be done to improve outcomes Beck By the s critiques of this prediction oriented approach started to emerge heralding in the next iteration of assessment Proponents of evidence-based justice reform argued that risk assessment should be reconfigured to help in the effort of reducing the risk of recidivism and lowering incarceration rates Risk assessments were reframed as a way of effectively identifying and allotting scarce resources to programs and interventions that were proven to work Andrews and Bonta Andrews et al Cullen and Genderau Etienne To this end a third generation of actuarial tools integrated factors that were conceived as criminogenic needs or intervenable factors that are believed to impact ones risk of recidivating These included a more diverse set of inputs such as employment status and history of substance abuse which effectively reconstituted risk as a dynamic phenomenon that could be intervened upon Andrews et al While many of these dynamic factors held less predictive power than the static attributes included on second generation tools these needs factors were desirable because they enabled criminal justice practitioners to consider a broader set of treatment options aimed at lowering risk Thus predictive risk and intervenable needs factors were combined into a hybrid risk needs model one that used regression to identify statistically significant risk needs factors The key difference between second and third generation assessments then was the incorporation of these less predictive but dynamic risk needs factors that could inform the selection of treatment interventions beyond incapacitation By the s these efforts were further supplemented by fourth generation tools that added responsivity factors such as levels of intelligence self-esteem and psychological disorders in order to improve response outcomes to treatment Andrews and Bonta The risk-needs-responsivity RNR framework has reasserted the authority of a correctional treatment approach one that envisions offenders as individuals capable of reform through intervention Today risk assessments are used for two primary purposes which Monahan and Skeem deem prediction-oriented and reduction-oriented approaches to assessment Monahan and Skeem Prediction-oriented assessments are used to facilitate accurate and efficient prediction of future recidivism while reduction-oriented tools are intended to inform treatment and supervision plans In recent years weve seen a resurgence of public interest in prediction-oriented assessments particularly at the pretrial stage where release decisions are viewed as essentially a forecasting task Yet we argue that even pretrial decisions should be characterized as a moment of intervention rather than mere prediction The push to adopt pretrial assessments like the Public Safety Assessment is tightly coupled with efforts to eliminate cash bail which empirical analysis has demonstrated to be ineffective at lowering near-term risks failure to appear and new criminal activity and long-term recidivism rates For example Gupta etal show that high bail amounts which often result in pretrial detention drive a rise in long-term recidivism rates Gupta et al Bail is an ineffective pretrial intervention that proponents of risk assessment would like to eliminate in the service of achieving better pretrial outcomes When framed in this way the logical next question is not so much whether or not judges make more accurate decisions when they use risk assessments Rather we should be asking whether or not pretrial risk assessments effectively support judges in making better decisions about whom to release and under what conditions ie how should the criminal justice system intervene in an individuals life to mitigate specific relevant risks Thus we argue that the most socially beneficial use of risk assessments should be centered around their utility as a diagnostic tool at all stages of the criminal justice life cycle This is an important point because it informs the goals and characteristics we strive to achieve in the development of a fair and ethical risk assessment tool In recent times there has been a growing enthusiasm around the idea of using machine learning techniques to enhance the performance of modern day assessments However these methods run the risk of swinging the trend of assessment back towards prediction rather than intervention New statistical techniques like machine learning have garnered a lot of enthusiasm and as Kleinberg et al have pointed out this increased enthusiasm and investment might unduly push institutions to reframe their goals in a way that is amenable to machine learning analysis Kleinberg et al Similar to regression machine learning models are well-suited to the task of identifying correlational relationships across an even wider range of variables with the goal being to create assessments that can predict risk with greater accuracy While earlier assessments were based on theories in criminology data mining enables the surfacing of new patterns and trends that are born from the data without necessarily having a theoretical explanation for why they emerge Kitchin As such machine learning is touted as a way of helping us identify patterns that we could not have discovered by ourselves in the service of creating more predictive tools Yet diagnostic practices do not map cleanly to tools that use prediction-oriented methods such as regression and machine learning While the goals of risk assessment have expanded the statistical methods used to identify risks and needs have remained the same since their introduction in second generation tools This regression-based methodology is ill-suited for the purpose of effective diagnosis and intervention of criminogenic needs When we use regression for intervention we run the risk of conflating correlational variables with causal ones which gives rise to a range of challenges In the following section we argue that assessments designed for intervention require a different set of statistical techniques ones that can identify causal relationships between risk factors and future crime We outline key differences between regression machine learning and causal inference in order to make the case for moving away from using regression and machine learning for intervention-oriented assessment Regression vs Machine Learning vs Causal Inference Nothwithstanding the popular discourse on the ethical use of risk assessments the vast majority of these tools do not use new statistical methods frequently associated with artificial intelligence such as machine learning They are overwhelmingly based on regression models Moreover the small number of tools that are currently being developed using machine learning methods should be characterized as an incremental rather than a transformational innovation in the way risk assessments have historically worked Regression analysis is widely used for purposes of forecasting future events The main goal of regression is to identify a set of variables that are predictive of a given outcome variable This is achieved by determining the optimal weights for a given set of covariates ones that are best predictive of the outcome variable of interest This is done through processes called model checking and selection Gelman whereby statistical tests are run on each covariate to see how significantly predictive they are of the outcome variable Covariates that are identified through this process are correlational not causal of the outcome variable For example prior arrest has been found to be an important predictor of future crime but does not shed light on the causal drivers of criminal behavior Regression based models while better suited for predicting risk scores are not well-equipped to answer causal questions on interventional covariates Tacking on interventional covariates to a list of risk covariates and using regression models for risk prediction suffers from three main drawbacks First regression models for risk prediction are blunt instruments that do not always contextualize risk and intervenable factors specific to different subgroups in the data Greiner Critics have pointed out that criminogenic needs in risk assessments are treated as universal even though the theories and data on which they are validated often skew heavily towards a white male population Very little attention has been given to validating these models on more diverse populations in spite of the fact that there is a robust body of academic literature that captures a variety of gendered and racialized pathways to crime Belknap Chesney-Lind Simpson This has significant implications for how we understand the predictive and diagnostic potency of risk assessment tools for diverse populations Reisig et al For example scholars have argued that covariates currently included in many regression models for risk prediction are limited to a narrow set of androcentric theories of criminogenic need to the exclusion of risk and interventional covariates specific to female defendants The decision of which covariates to add into the regression model is largely driven by how accurate the models predictions are not by well-posed questions around how causal the covariates are potentially excluding covariates that are core drivers of criminal behavior for different subgroups in the population For example some analyses of the LSIR risk assessment tool have shown that the covariates used for risk prediction are largely informed by a male offender worldview ignoring well-established pathways to crime for women ie childhood sexual abuse substance addiction and lack of employment as a crucial interventional class of covariates specific to women Reisig et al Second the set of factors considered in RNR assessments are typically constrained to a narrow set of variables ones which are amenable to individualized treatment plans and are informed by particular psychological and normative theories of re-offending Hannah-Moffat Very little consideration has been given to broader social or structural drivers of crime Indeed influential scholars of some of the most widely used risk assessments in use today have explicitly discounted the relevance of sociological factors such as ethnicity and socioeconomic status because those factors are viewed as static and challenging to intervene upon These researchers claim that it is a myth to think that the roots of crime are buried deep in structured inequality arguing that psychological factors such as antisocial cognitive patterns and beliefs have demonstrated much stronger statistical significance when it comes to inter-individual variations in re-arrest within the defendant population Andrews and Bonta Prins and Reich Yet as Prins and Reich have pointed out these scholars often conflate causes of crime with causes of individual differences in crime Prins and Reich Such heavy emphasis on psycho-social factors may limit our ability to understand the underlying drivers of criminal activity at the population level or ask harder questions about why we see persistent disparities in crime and arrest rates across certain populations and geographies Prins and Reich As Hannah-Moffat has argued in RNR assessments an offenders needs are stripped of their broader social context and are framed largely in terms of poor choices or moral and psychological deficiencies that can be treated through re-education at the individual level Hannah-Moffat This approach tends to obfuscate the significance of underlying factors that are highly prevalent for the population of interest As Prins and Reich argue if such structural factors are nearly ubiquitous for individuals who become involved in the criminal justice system we would not expect them to be predictive of inter-individual variation in arrest Prins and Reich The narrow theoretical focus of risk assessments stems in part from the fact that regression is ill-suited to test and differentiate competing models of criminal behavior This has led to a fairly narrow conceptualization of criminogenic need that is limited to attributes which demonstrate a statistically significant relationship to future recidivism rates in a regression model Other common sense needs factors for which a statistically significant relationship could not be established such as mental health have been excluded from these models Within a regression framework one might address this issue by identifying competing theories of criminogenic behavior and running two different regression models to see which one is more predictive But if two different regression models were estimated using different guiding criminogenic theories for different subgroups in the population one has to contend with the real possibility of the models arriving at different risk predictions leaving no principled way to choose the prediction of one model over another Finally it is challenging to differentiate intermediate outcomes from covariates in a regression based model sans a causal inference framework Greiner This is important because without establishing a causal relationship between a given covariate to the risk of recidivism we run the risk of misinterpreting an intermediate outcome as a causal driver of crime This is deeply significant if we are to use risk assessments as the basis for identifying effective interventions to address the underlying drivers of crime Associative relationships between covariates used in a regression model and the outcome variable of interest need not be causal such covariates might be misconstrued as an intermediate outcome that is causal of criminal behavior Prins and Reich underline this important point by means of a compelling example using a directed acyclic graph They show how intensive policing causes more re-arrests and anti law-enforcement resentment antisocial cognition at the same time While antisocial cognition might strongly predict re-arrest it is not typically a causal driver of rearrest Intervening to tackle antisocial cognition without addressing the root cause of excessive and intensive policing will not reduce re-arrests under such a model antisocial cognition is itself caused by increased intensive policing and must not be misinterpreted as a causal driver of crime It is important to distinguish machine learning from causal inference Machine learning leverages concepts of pattern recognition prediction abstraction and generalization while a causal inference framework is based on estimating the effect of an intervention on a outcome variable of interest Machine learning algorithms leverage the same principles as regression for predicting an outcome variable but do so by expanding the set of covariates for prediction In machine learning one wants to obtain a generalized model that appropriately fits the data at hand This is achieved in machine learning using a variety of advanced statistical techniques such as kernel transformations and parameter sweeping which enable complex interactions between covariates to be captured but not necessarily understood for the purposes of increased predictive accuracy In a sense machine learning treats prediction as sacrosanct it is not important why a given set of covariates are predictive so long as they are Machine learning algorithms used for prediction of risk scores should be seen as incremental extensions of regression-based methods ones that will amplify rather than mitigate the challenges we describe above Though the vast majority of risk prediction models are based on regression there has been an increased interest in using contemporary supervised machine learning methods in this setting In supervised machine learning a given set of covariates also referred to as features are used in a model to predict the outcome variable of interest An analyst estimating a model to predict risk scores has many types of algorithms to choose from Many of these algorithms use advanced statistical methods to transform the space of input features into a higher order space that is often difficult to interpret even by the analyst estimating the model In a supervised machine learning driven prediction setting the principal focus is on achieving reliable and accurate prediction The question of what constitutes the input space of features is secondary to achieving the best predictions possible Often a machine learning engineer will simply add or remove individual features reestimate a new model with the changed feature spaces and check for the predictive performance of the model James There is often no other rationale given for including and excluding individual features Similar to regression the set of input features are merely correlational and not causal of the outcome variable Recent work on interpretable supervised learning algorithms for risk score prediction in the criminal justice system attempts to explain the logic used by the model in computing the risk for a new defendant but this should be seen as incremental to current practices as it fails to answer the question of which features are causal drivers of criminal behavior Furthermore such a supervised setting even with better model interpretability Zeng et al is not a principled way of determining how much a given variable needs to change to reduce the risk of future crime For example determining number of hours of cognitive behavioral therapy given to incarcerated individuals to address their criminogenic psychopathology a common intervenable variable is difficult to compute using these methods devoid a causal inference framework In short without a causal inference framework contemporary supervised learning algorithms used for risk prediction are not useful for targeted intervention The increased attention and excitement around machine learning and artificial intelligence coupled with the advanced statistical techniques they make use of has given them the veneer of appearing scientific accompanied by descriptive language that often subtly connotes causality when in reality these models do not even broach causality Towards a Causal Framework for Risk Assessment In contrast to regression and machine learning statistical causal inference Imbens is a framework that is used to establish causal relationships between covariates and the outcome variable of interest This is achieved through the design of experimental conditions in which covariates are altered systematically to see if the alteration produces effect changes in the outcome variable The key difference between causal inference and regression-like methods is the ability for one to change values of a covariate for the purpose of examining whether it causes the outcome variable Causal inference is best suited for the design of interventions that reduce the risk of future crime Yet regression and to a limited extent predictive machine learning dominate the landscape of risk assessments used for both prediction and intervention This gives rise to a series of challenges that limit the justices systems ability to effectively intervene to lower the risk of recidivism A causal inference experiment produces new data to be analyzed for the purpose of establishing causality but in regression and machine learning one is limited to historical data that has already been captured potentially excluding important variables crucial for effective intervention Unlike regression and predictive supervised machine learning models a causal inference framework is better suited to answer questions of what interventions work in the criminal justice system particularly those aimed at decreasing well-specified risks such as recidivism and failure to appear in court For example consider the potential outcomes causal inference framework In this approach causality is inferred by randomly assigning individuals or groups referred to as units to an intervention or treatment Each unit subjected to a treatment may realize an outcome of interest and upon receiving no treatment may realize an alternate outcome also known as the counter-factual Randomly assigning units to both the treatment and the control and comparing the potential outcome after the application of treatment and control gives a measure of the causal effect of the chosen intervention When performed under an experimental setting this is known as a randomized control trial RCT and is considered the gold standard for measuring the causal effects of a given treatment under a set of conditions for a defined unit Imbens For example a randomized-controlled trial in Philadelphia found that community-based low intensity supervision program of low-risk offenders was more effective than high-intensity law-enforcement supervision in reducing new criminal activity of the offenders Barnes et al In the criminal justice system it may not always be possible to practically obtain a potential outcome for both the treatment and the control of the same unit Beyond technical challenges with the data researchers may encounter resistance from the legal profession which has been reticent to adopt rigorous empirical methods to infer the effectiveness of interventions in the criminal justice system Greiner and Matthews Greiner and Matthews point to the fact that the first RCTs in the medical and legal fields were conducted around the same time in the early s Yet today the legal field overwhelmingly lags medical research in the use of scientific causal methods to evaluate the effectiveness of interventions Causal inference in cases where it is not possible to do an RCT is usually derived from applying the same concepts to observational data with donor pooling and other matching methods to fill the counter-factual values for each unit based on identifying similar units receiving the two different interventions Rubin For example a recent observational study evaluated the effectiveness of different forms of electronic monitoring EM technologies for released defendants in the state of Florida Using propensity score matching to group unit defendant covariates the study found that while EM reduced new criminal activity NCA across all demographics the decrease was the lowest for violent offenders Bales et al In addition electronic monitoring was found to reduce NCA more than the use of global-positioning system GPS monitoring The potential outcomes model offers several structural advantages over regression-based methods for evaluating which interventions work in the criminal justice system Greiner First one is able to measure the impact of timing and duration of the applied intervention in a causal inference framework an advantage severely lacking in regression-based methods For example given the limited resources for behavioral therapy in correctional settings research has shown that the timing of the initiation of behavioral therapy has an effect not only on prison conduct of defendants but also on the risk of recidivism In the potential outcomes framework a randomized experiment can test when it might be a good time to initiate behavioral therapy as an intervention Duwe Second random assignment of units to treatments ensures a balance of the covariates thereby isolating the applied treatment as the causal driver or not Third causal inference insists that the analyst carefully separates covariates that are not impacted by the treatment from intermediate outcomes that are indeed impacted by the treatment This has serious implications for using risk assessments as a diagnostic tool especially for high-risk groups whose risk stems from structural inequality This doesnt mean that there is no room for machine learning in the development of diagnostic risk assessments Unsupervised and semisupervised machine learning algorithms are well suited for surfacing important causal questions regarding highly correlational covariates Such causal inference questions are much better addressed by either performing inference on observational data or even better the design of a randomized experiment when possible For example Kim and Duwe show Duwe and Kim how supervised machine learning models outperform classical regression models for predicting recidivism Yet these models are difficult to interpret even though they yield more accurate predictions Rather than using machine learning for prediction these methods could be used to identify features that are highly predictive of recidivism in order to inform hypotheses on interventions and their timing that can then be tested using causal inference Implications and Conclusion In recent times predictive risk assessments have garnered unprecedented interest as individuals from across the political spectrum seek to achieve a diverse set of criminal justice reforms with the help of data-driven actuarial tools The promise of these tools is frequently characterized in terms of the affordances they offer with regard to cutting-edge technological advances in predictive data analysis in fields such as machine learning Risk assessments have been sold as an innovative means of achieving efficiency neutrality and fairness in a system that has been plagued by the implicit bias of individual decision makers ie judges and flawed organizational risk mitigation practices ie money-bail Institute However the reality is that most risk assessments in use today are built using statistical methods that are based on regression Little has changed since the s when the first statistically validated actuarial tools were built using regression algorithms Even in instances where new methods commonly associated with artificial intelligence are being developed for risk assessments these innovations are really incremental extensions of prior statistical methods used for forecasting future criminal events Risk assessment has a long and rich history in the criminal justice system Yet in current debates surrounding the ethical use of these tools this larger historical context is often lost Fairness debates surrounding risk assessment end up in conceptual cul-de-sacs whereby trade-offs regarding things like accuracy equity and predictive parity are weighed in lieu of asking deeper questions about what the purpose of these tools is and should be in the first place Flores et al Kleinberg et al b Gong Those deeper questions include asking how actuarial tools might support or undermine crime prevention interventions We argue that when risk assessments are used primarily as a predictive technology they fuel harmful trends towards mass incarceration and growing inequality in the justice system Predictive risk assessments offer little guidance on how to effectively intervene to lower risk When predictive accuracy is the primary metric along which these technologies are evaluated the system misses opportunities to explore a deeper set of questions surrounding the way its administrators can use data as part of a reflexive practice of testing hypotheses in the service of achieving near and long term goals We argue for a shift away from predictive technologies towards diagnostic methods that will help us to understand the criminogenic effects of the criminal justice system itself as well as evaluate the effectiveness of interventions designed to interrupt cycles of crime In contrast to the current emphasis on machine learning techniques that offer no grounded way of understanding the underlying drivers of crime these methods should be based in a more rigorous approach that incorporates both qualitative and quantitative data analysis To this end we argue that risk assessments should be conceived of as a diagnostic tool that can be used to understand the underlying social economic and psychological drivers of crime We posit that causal inference offers the best framework for pursuing these goals More work should be done to examine how these quantitative methods might be supplemented by more qualitative practices of knowledge production For example Paluck argues that ethnographic methods can enable researchers to move beyond average treatment effects to more deeply understand the underlying mechanisms of a causal effect via causal inference Paluck Moreover Elish and boyd have pointed out that quantitative researchers would benefit immensely from the rich frameworks for reflexivity found in more qualitative data practices such as ethnography Elish and danah boyd This is of paramount importance for data applications in the criminal justice system where researchers should constantly re-examine the way their research practices might influence and warp the outcomes of their work Though causal inference through randomized experiments has been touted as the gold standard for evaluating criminogenic interventions their practical application is much more sparse Weisburd Greiner and Matthews A large scale survey of intervention evaluations in the US criminal justice system found that only of all interventions were evaluated using randomized experiments Sherman Randomized experiments to infer causal relationships between interventions and recidivism are seen as difficult to implement in practice given resource constraints in correctional facilities Others have raised ethical challenges around subjecting a vulnerable population to a control arm of a randomized experiment Weisburd These concerns and challenges mirror challenges in medical research where the testing of medical devices drugs and therapies can often have life and death implications Yet there are well-established and mature frameworks on ethical experimentation and causal inference in clinical medicine It is inconceivable today to imagine the introduction of a new drug without RCTs the Food and Drug Administration mandates a framework for furnishing evidence of a new drugs effectiveness in four phases Guidelines and rules for ethical experimentation on patients from funding bodies such as the National Institutes of Health combined with Institutional Review Board IRB oversight at host institutions are the norm for causal inference in medical research Scholars such as Greiner Greiner and Matthews have cast the lack of causal inference in the law as a problem of political will In the criminal justice system many new interventions such as the timing type and doses of psychological therapy both within and outside of correctional facilities can be tested using randomized experiments In cases where a randomized experiment is either not feasible or is ethically fraught with serious potential risks deriving causal inference from observational data should be considered Greiner There is a growing body of work that deploys observational methods in the field of criminal justice For instance Dobbie Goldin and Yang show via quasi-experimental observational study design that pretrial detention weakens defendants bargaining position in plea negotiations and a conviction also reduces their future participation in the the formal labor market Dobbie et al Just in the last year four other observational causal inference studies exploring the downstream consequences of pretrial detention were released Gupta et al Heaton et al Leslie and Pope Stevenson In addition Lum and Yang Lum and Yang have highlighted structural challenges that also hamper the use of causal inference studies such as the lack of early mentorship available for empirical criminology analysts interested in using these methods as well as a shift of research funding away from randomized experiments to quick statistical analysis The relative ease with which most regression and supervised prediction algorithms can be estimated makes them all the more convenient to use Its no surprise that machine learning has gained momentum over causal inference at a time when convenience is combined with the growing hype around the use of machine learning to solve some of our most intractable social problems Nevertheless careful design of a causal inference setup both experimental and observational offer significant benefits for the process of discovering and validating criminogenic interventions Data-driven tools provide an immense opportunity for us to pursue goals of fair punishment and future crime prevention But this requires us to move away from merely tacking on intervenable variables to risk covariates for predictive models and towards the use of empirically grounded tools to help understand and respond to the underlying drivers of crime both individually and systemically