All The Cool Kids How Do They Fit In Popularity and Demographic Biases in Recommender Evaluation and Effectiveness In the research literature evaluations of recommender system effectiveness typically report results over a given data set providing an aggregate measure of effectiveness over each instance eg user in the data set Recent advances in information retrieval evaluation however demonstrate the importance of considering the distribution of effectiveness across diverse groups of varying sizes For example do users of different ages or genders obtain similar utility from the system particularly if their group is a relatively small subset of the user base We apply this consideration to recommender systems using offline evaluation and a utility-based metric of recommendation effectiveness to explore whether different user demographic groups experience similar recommendation accuracy We find demographic differences in measured recommender effectiveness across two data sets containing different types of feedback in different domains these differences sometimes but not always correlate with the size of the user group in question Demographic effects also have a complex and likely detrimental interaction with popularity bias a known deficiency of recommender evaluation These results demonstrate the need for recommender This paper can be reproduced with scripts available at This paper is an extension of the poster by Ekstrand and Pera system evaluation protocols that explicitly quantify the degree to which the system is meeting the information needs of all its users as well as the need for researchers and operators to move beyond naıve evaluations that favor the needs of larger subsets of the user population while ignoring smaller subsets Keywords recommender systems fair evaluation INTRODUCTION Recommender systems are algorithmic tools for identifying items eg products or services of interest to users Adomavicius and Tuzhilin Ekstrand et al Ricci et al They are usually deployed to help mitigate information overload Resnick et al Internet-scale item spaces offer many more choices than humans can process diminishing the quality of their decision-making abilities Toffler Gross Recommender systems alleviate this problem by allowing users to more quickly focus on items likely to match their particular tastes They are deployed across the modern Internet suggesting products in e-commerce sites movies and music in streaming media platforms new connections on social networks and many more types of items We are concerned with the fairness of recommender systems a surprisingly tricky concept to define In addition to the numerous types and MD Ekstrand M Tian I Madrazo Azpiazu JD Ekstrand O Anuyah D McNeill MS Pera All the Cool Kids of fairness in the research literature recommender fairness must identify which stakeholder groups to consider for fair treatment Burke Both offline Herlocker et al Shani and Gunawardana and online Knijnenburg et al evaluations of recommender systems typically focus on evaluating the systems effectiveness in aggregate over the entire population of users While individual user characteristics are sometimes taken into account as in demographic-informed recommendation Pazzani Ghazanfar and Prugel-Bennett the end evaluation still aggregates over all users Recent developments in human-centered information retrieval have incorporated user demographics and characteristics to evaluate search engines and understand users search behavior Weber and Castillo use light user information augmented with census-based demographics to understand who is using a search engine Mehrotra et al follow this trend by measuring Bings ability to satisfy the information needs of different subgroups of its user population eg assessing whether it meets the needs of grandparents as effectively as those of young professionals This attention is necessary because the largest subgroup of users will tend to dominate overall statistics If other subgroups have different needs their satisfaction will carry less weight in the final analysis This can lead to a misguided perception of the performance of the system and more importantly make it more difficult to identify how to better serve specific demographic groups Our fundamental research question is this Do different demographic groups obtain different utility from the recommender system This is a starting point for many further questions such as whether particular demographic groups need to be better served by recommender systems and if so how they can be identified and supported in their information needs To address this question we present an empirical analysis of the effectiveness of collaborative filtering recommendation strategies stratified by the gender and age of the users in the data set We apply widely-used recommendation techniques across two domains musical artists and movies using publicly-available data We also explore the effect of rebalancing the data set by gender the influence of user profile size on recommendation quality and the interaction of demographic effects with previously documented biases in recommender evaluation all in the context of demographically-distributed differences in effectiveness Our work is inspired by that of Mehrotra et al We translate the concepts of their analysis from search engines to recommender systems While our experiment is less sophisticated than Mehrotra et als and necessarily limited by our offline experimental setting it is fully reproducible using widely-distributed public data sets and can be easily adapted to additional algorithms domains and applications BACKGROUND AND RELATED WORK Recommender systems Adomavicius and Tuzhilin Ekstrand et al are algorithmic tools for helping users find items that they may wish to purchase or consume They have substantial influence the best available public data indicates that recommendation drives of Netflix video viewing GomezUribe and Hunt and of Amazon purchases Linden et al Recommendation Techniques There are a variety of families of recommendation algorithms Collaborative filters Ekstrand et al mine user-item interaction traces such as purchase records click logs or user-provided ratings of items to generate recommendations based on the behavior of other users with similar taste Content-based filters Pazzani and Billsus Lops et al use item content or metadata such as tags and text to recommend items with similar content to items the user has liked in the past Many production systems use a combination of these and other techniques as hybrid strategies to enhance the overall recommendation process Burke Bobadilla et al Recommender System Evaluation Recommender systems are evaluated in offline settings using evaluation protocols derived from information retrieval Herlocker et al Gunawardana and Shani Bellogin These protocols hide a portion of the data and attempt to predict it using the recommendation model measuring either the models ability to predict withheld ratings prediction accuracy evaluation or its ability to recommend withheld items top-N evaluation Top-N evaluation is widely regarded as the preferred setting as it reflects the end goal of the recommender systemto recommend items the user will likemore accurately than predicting ratings Offline top-N evaluation however has significant known problems Among these are popularity bias Bellogin et al where the evaluation protocol gives higher accuracy scores to algorithms that favor popular items irrespective of their ability to meet user information needs and misclassified decoys Ekstrand and Mahant Cremonesi et al where a good recommendation is erroneously penalized because data on user preferences is incomplete Online evaluation commonly using A/B tests Kohavi et al and measuring user response to recommendation is the gold standard for effectiveness and avoids many of the problems of offline evaluation User studies Knijnenburg et al allow even deeper insight into why users respond to recommendations in the way that they do This type of study however is more expensive to conduct in terms of time protocols and resources than its offline counterpart Shani and Gunawardana Fairness in Recommender Systems The recommender system research community has long been interested in examining the social dimension of recommendation the earliest modern recommender systems were developed in a human-computer interaction setting Resnick et al Hill et al and there has been work on how they promote diversity or balkanization van Alstyne and Brynjolfsson Hosanagar et al More recent work has begun to consider questions of fairness in recommendation Proposals for fair recommendation methods include penalizing algorithms for disparate distribution of prediction error Yao and Huang balancing neighborhoods before producing recommendations Burke et al and making recommended items independent from protected information Kamishima and Akaho Burke taxonomizes fairness objectives and methods based on which set of stakeholders in the recommender system are being considered as it is meaningful to consider fairness among many groups in a recommender system In our work we examine the C-fairness of recommender algorithms whether or not they treat their users consumers fairly Demographic-Aware Evaluation Traditionally demographic information have been considered in the past to improve the effectiveness of diverse tasks from text classification Hovy to search Weber and Castillo and recommendation Said et al Unfortunately little is known about the effects of demographic information when it comes to evaluation tasks Langer and Beel Typical evaluations average over all users or data points providing a simple aggregate measurement of the recommenders effectiveness However user satisfaction in a recommender system depends on more than accuracy Herlocker et al Langer and Beel In fact Mehrotra et al demonstrate that this naıve approach to simply aggregate measurements masks important differences in how different groups of users experience the system The system may be delivering high-quality service to one subset of its user group while another smaller group of users receives lower-quality recommendations or search results the overall metric will not reward effort that improves the experience of minority users as much as it rewards efforts that make things better for those already well-served The fundamental thrust of our present work is to translate this idea from the online web search setting employed by Mehrotra et al to offline evaluation of recommender systems and examine whether applying existing algorithms to existing public data sets will provide comparable utility to different groups of users The discussion presented in this paper expands the initial analysis presented by Ekstrand and Pera Data and Methods We used the LensKit recommender toolkit Ekstrand et al to build and evaluate several collaborative filtering algorithms across multiple public data sets with different types of feedback in multiple product domains Data Sets While there are many public records of ratings plays and other common recommender inputs for use in research few of them have the necessary user demographic information to assess bias in recommender effectiveness We have found three that have the necessary data early versions of the MovieLens data Harper and Konstan and the two LastFM data sets collected by Celma Table summarizes these data sets Table Summary of data sets Datasets Users Items Pairs Density The data set contains M records of users playing songs from artists gathered from the LastFM audioscrobbler We aggregated this data at the artist level to produce play counts for user-artist pairs The data set contains the top most-played artists from users along with their play counts covering artists Both data sets contain gender age and sign-up date for many users Figure shows demographic coverage The data set contains M -star ratings of movies by users who joined MovieLens a noncommercial movie recommendation service operated by the University of Minnesota through the year Each user has a self-reported age gender occupation and zip code Some time after releasing the M data set MovieLens stopped collecting demographic data from new users so the larger recent data sets M and M do not contain the data required for our experiment Source Data Distributions Differences in recommender effectiveness need to be understood in the context of the demographic distribution of the underlying data Age Gender LF M LF M M LM NA M NA Demographic P o U se rs Figure User distribution by demographic group Numbers in bars are the number of users in that bin Age Gender LF M M LM NA M NA Demographic M ed n m s C on su m ed Figure Median items consumed by users in each demographic group We omit since it only contains each users top artists Figure shows the distribution of each data set All three data sets exhibit similar distributions of user genders with the majority of users reporting as male is the least imbalanced The largest block of users belong to the group whereas a plurality of users belong to the group most users did not report their age Approximately of users declined to share their gender while close to declined to share their age All user records in the data set contain full demographic information For consistency among the reported results we bin LastFM users into the same age groups used in the data set throughout Figure shows user activity levels as measured by the number of movies rated or artists played in each user group Men are more active than women in both data sets The activity-age relationship in data almost follows the demographic distribution with those groups that have more users also having more active users the small number of users in most age brackets in preclude drawing conclusions from age-activity relationships in that data Experimental Protocol We partitioned each data set with -fold crossvalidation Our primary results use LensKits default user-based sampling strategy select test sets of users and for each user select ratings to be the test ratings the rest of those users ratings along with all ratings from users not in that test set comprise the train set for that test set For we sampled disjoint sets of test users or items for each test set to decrease compute time For and we partitioned the users into disjoint sets We also tested Bellogins UR method Bellogin for neutralizing popularity bias this works exactly like the default except it picks test sets of items instead of users and it generates a different recommendation list for each user-item pair in the test data with that item as the only test item to be found The idea is that by having the same number of test ratings for each item recommenders that favor popular items cant win simply by having popular items be the right answer more often than unpopular ones Performance Metrics We measure recommender effectiveness using Normalized Discounted Cumulative Gain Jarvelin and Kekalainen a widely-accepted measure of the effectiveness of a recommender system measures the utility that a user is expected to obtain from a recommendation list based on that users estimated utility for individual items and the position in the list at which those items were presented The for a recommendation list L generated for user u is computed with Equation is defined by Equation where li is the item in list L and is user us utility for item li and is computed as with a list consisting only of the users rated items in non-increasing order of utility L i quantifies the utility achieved by a recommendation list as a fraction of the total achievable utility if the recommender could perfectly identify the users most-preferred items For the data set we define as the users rating for movie li for the LastFM data sets we use the number of times the user has played the artist Items for which no data is available are assumed to have a utility of Although this has significant conceptual problems Ekstrand and Mahant it is standard practice in recommender systems research and there is no widely-accepted improvement Algorithms We employed several classical and widely-used collaborative filtering algorithms as implemented by LensKit We operated each algorithm in both explicit rating-based and implicit consumption record feedback mode Popular Pop recommending the most frequently-consumed items Mean recommending the items with the highest average rating Item-Item II an item-based collaborative filter Sarwar et al Deshpande and Karypis using neighbors and cosine similarity The explicit-feedback version normalizes ratings by subtracting item means the implicit-feedback version replaces the weighted average with a simple sum of similarities User-User a user-based collaborative filter Resnick et al Herlocker et al configured to use neighbors and cosine similarity The explicit-feedback variant uses user-mean normalization for user rating vectors and the implicit-feedback variant again replaces weighted averages with sums of similarities User-user did not provide effective recommendations on the LastFM data so we exclude it from that data sets results MF the popular gradient descent matrix factorization technique Funk Paterek with latent features and training iterations per feature In the results each algorithm is tagged with its variant Algorithms suffixed with are explicit-feedback recommenders applicable only to ML are implicit-feedback recommenders that only consider whether an item was rated or played irrespective of the number of plays both data sets and are implicit-feedback recommenders that use the number of times an artist was played as repeated feedback and log-normalized prior to recommendation The purpose of this work is not to compare algorithms but to compare recommender performance across demographic groups We have selected these algorithms to provide a representative sample of classical collaborative filtering approaches Results Using the data and methods presented in Section we discuss below the results of the experiments conducted to quantify user satisfaction with presented recommendations among different demographic groups For doing so we consider three different perspectives that guide our assessments i analysis based on raw data ie considering all users in the data sets ii analysis based on user activity levels ie controlled profile size and iii analysis based on gender-balanced data sets Basic Results In order to quantify to what extent demographics affect the overall satisfaction obtained by the users we conducted an experiment that considers the performance of traditional recommendation algorithms for different gender and age groups respectively Figure illustrates the overall satisfaction obtained by each gender group measured by whereas Figure does the same for users grouped by age M LM A ny M A ny M A ny M A ny M A ny M A ny M A ny M A ny M nD C G LF M LF M A ny M N A A ny M N A A ny M N A A ny M N A A ny M N A A ny M N A A ny M N A gender nD C G Figure Algorithm performance by gender Highlighted cell is for the algorithm with the best overall performance on that data set For each data sets best-performing algorithm highlighted we compared the differences in utility for each demographic group and have statistically-significant differences between gender groups and has significant differences between age brackets Kruskal-Wallis p with the Bonferroni correction for multiple comparisons Controlling for Profile Size As seen in Figure different demographic groups have different activity levels as measured by the number of items they have rated or consumed The size of a users profile can be a factor in their recommendation utility given that more items provide a stronger basis for recommendation To control for the effect of profile size on user satisfaction we fitted linear models predicting the using the number of items in the users profile excluding since it only contains each users top artists We used the average achieved by all algorithms for a particular user as the dependent variable so we are only predicting a single metric per user this captures an overall notion of the difficulty of M LM A ny A ny A ny A ny A ny A ny A ny A ny nD C G LF M A ny N A A ny N A A ny N A A ny N A A ny N A A ny N A A ny N A age nD C G Figure Algorithm performance by age Highlighted cell has highest overall accuracy We omit because most users in that data set lack age data effective recommendations for that user Figure shows the fitted models we apply a log transform to the item count and take the square root of the to achieve a better fit Surprisingly there is a negative relationship between user profile size and recommendation accuracy the exact cause is unknown but we suspect that users with more items in their profile have already rated the easy items so recommending for them is a harder problem Consumed Items nD C G Figure Models predicting with profile size Figure shows the for each group after removing the effect of user profile size We see that the demographic effects observed in Section remain after this control indicating a demographic effect of training the models on the data beyond that explained by user profile size II B II C II C S M B M C P B P C II B II E M ea n E M B M E P B U U B U U E Algorithm C on tr le d nD C G Gender M Unknown a Corrected utility by gender II B II E M ea n E M B M E P B U U B U U E Algorithm C on tr le d nD C G b MovieLens corrected utility by age Figure Recommendation utility after controlling for profile size Resampling for Balance As shown in Figure both and data sets include a larger proportion of male users unbalancing the training data As preprocessing data to produce fair training data is one way to train fair models Kamiran and Calders we resampled the and data sets to produce gender-balanced versions of each and re-trained the algorithms We balanced the data sets by identifying users with known gender information and randomly sampling without replacement the same number of female and male users samples each for the data set and samples each for data set Figure shows the experiment results on the gender-balanced data sets and Table shows the numeric change from the unbalanced experiment for the best-performing algorithm on each data set We repeated the Kruskal-Wallis test on both sampled and data sets and it did not find a statistically significant difference between groups on either data set Resampling the data while reducing recommender accuracy slightly did not create new gender differences in performance for and seems to have reduced the difference for We are not sure that it went entirely away as the Kruskal-Wallis test may be overly conservative and does not test directly for the elimination of an effect but it does seem to have diminished Resampling so that each group has the same number of ratings may eliminate the difference M LM G B A ny M A ny M A ny M A ny M A ny M A ny M A ny M A ny M nD C G LF M G B A ny M A ny M A ny M A ny M A ny M A ny M A ny M gender nD C G Figure Algorithm accuracy by gender on balanced data sets Highlighted cell is for the algorithm with the best overall performance on that data set Reducing Popularity Bias To reduce the effect of popularity bias we ran the version of the experiment using Bellogins Table Changes on observed on balanced vs raw data on and data sets Datasets Algorithm Gender Balanced data Relative Difference Female Male Any Female Male Any UR protocol as described in Section Since this protocol partitions items instead of users different users may have different numbers of test items and the distribution of user demographics may differ from the underlying data Figure a shows the distribution of test pairs per user and Figure b shows the demographic distribution of the users in the test data This distribution corresponds well to the underlying user distribution Test Items per User U se r C ou nt a Distribution of test items per user Test Pairs Users LF M M LM M NA M NA Gender ra io n of D at a P nt s b Gender distribution of test pairs and test users Figure Distribution of test data in UR experiment Figure shows accuracy by demographic group for the best algorithm for each data set under the UR protocol The differences on gender are consistent with the basic results in Figure We compared two averaging strategies averaging across all user-item pairs by user gender and averaging each users recommendation results prior to averaging all users with a particular gender and saw no difference Age tells a different story on the we see a different pattern in the distribution of accuracy across ages than we do under the user-based evaluation protocol in Figure It is not clear which provides a more accurate picture but this does demonstrate that correcting for one effect popularity bias can change the results for another effect demographic bias on on M NA M Gender nD C G a Effectiveness by gender on on N A Age nD C G b Effectiveness by age Figure Recommender effectiveness under UR protocol The best overall algorithm for each data set is shown Discussion and Limitations Having observed some differences in recommender performance between demographic groups we now turn to the implications of our results and some of their limitations Implications for Recommender Evaluation The existence of differences in measured recommender performance between demographic groups indicates a need to consider who is obtaining how much benefit from a recommender system If some users are underserved by the recommender it may be indicative of an area for improvement particularly if that group of users represents a market segment in which the recommender operator would like to expand their business Research and production evaluation of recommender systems needs to account for how different subsets of the user population should be weighted There is not necessarily a one-size-fits-all answer to the question of how to structure an evaluation it is a decision that needs to be made based on the values and goals of the business or research program Our methods and results can provide data to understand the ramifications of the decisions made about recommender evaluation Interaction with Popularity Bias Popularity bias Bellogin et al describes the phenomenon in which offline top-N recommender evaluation gives higher scores to algorithms that favor popular items The extent to which this is a defect in the evaluation favoring popularity irrespective of user preference versus an actual measurement of the effectiveness of popular recommendations is unclear it is believed that it represents a significant deviation from true performance but the degree of that deviation is difficult to quantify From first principles we expect popularity bias to exacerbate demographic biases the patterns of the largest group of users will dominate the list of most-popular items so favoring popular recommendations will also favor recommendations that are more likely to match the taste of the dominant group of users at the expense of other groups with different favorite items However our empirical results do not demonstrate that effect in the data we have Some of the demographic differences in recommender accuracy that we see such as the gender difference correlate with the size of the user group others such as gender differences and age differences do not It is difficult to generalize about the causes of the differences we have seen from only three data sets but it is clear that we need to look beyond popularity bias and demographic group size to understand the drivers of demographic differences in recommender performance The consistency of the results across algorithm families however suggests some robustness to these effects Further we have observed that applying one technique for reducing popularity bias can shift our measurements of demographic bias This indicates tradeoffs in the measurement of different biases so that applying the popularity bias reduction method is not a clearly correct decision User Retention One of the goals of recommender systems is to engage users with the systems themselves so that over time users can benefit in terms of personalization given the availability of explicit preference data Age Gender M R et en n R at e Figure Retention rate for each demographic group on the data set with Wilson confidence intervals A way to quantify this engagement is through retention do users continue using the system The data set includes timestamps for each rating allowing us to analyze user activity over time we use this to measure retention and examine its relationship to demographic group We divide user rating activity into sessions by considering the user to be starting a new session whenever there is a gap of at least an hour between two ratings Halfaker et al Figure shows the retention rate the percentage of users who returned for a second session for each demographic group We observe that men have a higher retention rate than women p in the data set the algorithms we tested provide more accurate recommendations to men than women While this by no means demonstrates a causal link for one thing we are not testing the same algorithm and implementation that MovieLens employed when these users were active it suggests room for further exploration The link between recommendation quality and user retention is key to the online testing employed by largescale recommender system operators such as Netflix Limitations of Data Our analysis on the data set was conducted with users explicit feedback the provided ratings While this data set shows that a certain demographic group dominates its counterparts in providing ratings in the system it does not account for implicit feedback or the behavior of users who watched movies without necessarily providing ratings for them To ensure that we accounted for the differences in how demographic groups prefer to provide feedback we also performed an analysis on LastFM based on the number of times a song was played Our results show consistency across the different groups irrespective of the type of user feedback ie implicit or explicit While our results highlight the need to consider disparate demographic groups when evaluating recommender systems to better account for user satisfaction the users of MovieLens and LastFM may not be representative of general recommender system users Both of these systems particularly at the time the LastFM data was collected appeal to experienced users who care deeply about their movies and music Casual users are more likely to use services such as Netflix and Spotify and may exhibit markedly different behavior and experience different recommender utility than the expert users in the data sets we examined Unfortunately data from more widely-used systems with sufficient attributes to look for demographic effects is difficult to find Many widely-used data sets such as Amazoncom and Netflix do not contain user demographics Limitations of Evaluation Protocol The fact that our results are in an entirely offline experimental setting also introduces limitations Our data cannot distinguish whether the differences in measured performance are due to actual differences in the recommenders ability to meet users information needs or differences in the evaluation protocols effectiveness at measuring that ability While we suspect that they do reflect actual differences in recommender utility additional study with online evaluation is needed to complement and calibrate these results as the correlation between offline accuracy and online measures of effectiveness is often weak Rossetti et al A similar concern can be raised for online protocols Mehrotra et al but the closer connection between online measures and long-term customer value and experience improves their external validity However even if our observed differences are due in significant part to limitations of the evaluation protocol the result is still interesting biases in the evaluation protocol for or against groups of users would impede the development of fair recommender systems Even institutions that can carry out online evaluations use offline protocols to pre-screen algorithms prior to live deployment and offline evaluation metrics are the basis for the objective functions in many recommender model-training processes Limitations of Algorithm Selection While we have tested representatives of several key families of collaborative filtering algorithms there are many types of algorithms that we have not considered Two notable omissions are content-based filters which we omitted because only one of our data sets has sufficient data to support them and learning-to-rank recommenders which LensKit does not yet provide Our evaluation methodology and open experimental scripts make it easy to re-run our analyses on additional algorithms as they become available in the underlying software Choice of Metric There are many widely-used metrics that can be used to evaluate recommender systems Gunawardana and Shani For clarity and space we focus our results in Section on because it considers all of a users test items and has a good conceptual mapping to recommendation utility We included several metrics in our experimental runs however and they showed similar result trends Figures and show our key results from Section with the Mean Reciprocal Rank MRR metric Kantor and Voorhees MRR measures recommendation accuracy effectiveness by taking the reciprocal of the position of the first relevant suggestion in each users ranked list recommendations and averages this M LM A ny M A ny M A ny M A ny M A ny M A ny M A ny M A ny M M R R LF M LF M A ny M N A A ny M N A A ny M N A A ny M N A A ny M N A A ny M N A A ny M N A gender M R R Figure Algorithm performance based on Mean Reciprocal Rank for users grouped by gender value over all users in the data set These performance trends match those in Figures and i male users gain better utility from varied recommendation strategies than female users in and ii female users gain better utility on data sets and iii there are age differences that do not map to demographic group size The difference in recommender system satisfaction among users of different genders is more prominent when measured by MRR than We hypothesize that this is due to the fact that MRR penalizes recommendations that move the first relevant item ie highly rated items further down the list more heavily than especially in long lists On the other hand considers the position of all relevant recommendations along with their value to the user instead of only observing the position of the first item Which metric is a better measurement of usefulness depends on the precise recommendation task Ethical Considerations As our work is entirely based on widely-distributed public data and we did not perform any data linking that might expose or deanonymize users in the underlying data sets it does not place MovieLens or LastFM users at any risk to which they have not already been M LM A ny A ny A ny A ny A ny A ny A ny A ny M R R LF M A ny N A A ny N A A ny N A A ny N A A ny N A A ny N A A ny N A age M R R Figure Algorithm performance based on Mean Reciprocal Rank for users grouped by age posed for years by the publication of these data sets Conclusion and Future Work We set out to consider whether recommender systems produced equal utility for users of different demographic groups Using publicly available data sets we compared the utility as measured with for users grouped by age and gender Regardless of the recommender strategy considered we found significant differences for the among demographic groups Selecting the best algorithm from the families we tested and data sets showed statistically significant differences in effectiveness between gender groups while the data set highlighted a significant effect based on user age The demographic effect remains when controlling for the amount of training data available for a user it is diminished but may not entirely disappear when resampling the underlying data to train the recommender on a gender-balanced data set Notably the effects in utility did not exclusively benefit large groups we observed more accuracy for women on the LastFM data despite the lower representation of female users in the respective LastFM data sets Future Work for Research While our analysis focused on whether this effect could be found across a variety of common recommendation algorithms the differences appear to vary from algorithm to algorithm This suggests there is room for considering how different algorithms respond to evaluation and what characteristics contribute to more uniform utility Analysis can also be expanded to include more families of algorithms such as content-based recommendation and learning-to-rank techniques Having found this effect with age and gender we have not yet considered intersectionality how does recommender effectiveness vary with the interaction of multiple demographic data Our analysis did not find that smaller groups were always disadvantaged so more research should be done to understand why groups are unevenly advantaged by recommendation algorithms There is also room for this analysis to be repeated across other item domains How recommender system utility compares across demographics may be especially interesting for domains like real estate housing and job recommendations areas with well-documented historical discrimination Future Work for Industry Given the hazards of publishing data sets which include individual users demographic information there are limits to the advances academia can pursue in this work As it falls to industry to consider whether their own recommender systems provide comparable utility across demographics so does the responsibility for publishing their results We see the work of Mehrotra et al as an exemplary start in this direction although we would like to see additional details provided to ease replication of the results for other system operators Towards Fair Recommendation Research on the fairness of recommender systems is just getting started and there are many important questions to explore We have focused on one small corner of the problem the equity of recommender utility as experienced by different groups of users As Burke shows there are many more dimensions to the problem such as the equitable treatment of content producers as well as the distribution of non-accuracy recommendation value like diversity and serendipity