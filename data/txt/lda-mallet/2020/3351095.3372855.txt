Doctor XAI An ontology-based approach to black-box sequential data classification explanations Several recent advancements in Machine Learning involve blackbox models algorithms that do not provide human-understandable explanations in support of their decisions This limitation hampers the fairness accountability and transparency of these models the field of eXplainable Artificial Intelligence XAI tries to solve this problem providing human-understandable explanations for black-box models However healthcare datasets and the related learning tasks often present peculiar features such as sequential data multi-label predictions and links to structured background knowledge In this paper we introduce Doctor XAI a model-agnostic explainability technique able to deal with multi-labeled sequential ontology-linked data We focus on explaining Doctor AI a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit Furthermore we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations CONCEPTS Computing methodologies Artificial intelligence Machine learning Applied computing Health care information systems KEYWORDS explainable artificial intelligence machine learning healthcare data INTRODUCTION The recent availability of large amounts of electronic health records EHRs provides an opportunity for low-cost access to rich longitudinal clinical data EHRs are usually noisy sparse fragmented have high dimensionality and nonlinear relationships among variables The ability of Deep Learning techniques to model such highly nonlinear relationships enables them to have good predictive performance without the need for feature engineering This has led to many successful applications of such technologies to clinical tasks based on EHR data Deep Learning techniques have been proven useful for information extraction from clinical notes patients and medical concept representation learning outcome prediction and new phenotype discovery Some of these works focus on developing predictive models able to forecast any future diagnosis Most of these models take as input the clinical history of the patient and output the set of future diagnoses This kind of versatile models able to tackle mixed scenarios can be extremely useful in day-to-day clinical practice However the complexity of Deep Learning models hinders the straightforward understanding of the rationale behind their decisions This lack of interpretability prevents the deployment of such models in real-world healthcare scenarios For instance it was proven that biases in the data and adversarial examples can easily mislead such black-boxes Furthermore being able to understand the reasoning behind the models predictions would increase the healthcare professionals trust in such a technology and would increase its acceptance and use Recently being able to explain the reasoning behind machine learning decisions also became a legal requirement prescribed by Art of the GDPR General Data Protection Regulation Indeed GDPR requires the data processor to provide to the data subject meaningful information about the logic involved as well as the significance and the envisaged consequences of such processing for the data subject in case of a decision based solely on automated processing which might produce legal effects concerning him or her In this paper we introduce Doctor XAI a novel explainability technique able to deal with multi-labeled sequential ontology-linked data Doctor XAI is a post-hoc interpretability method that focuses on local explanations ie it explains the rationale behind the classification of a single data point It is also model-agnostic as it produces explanations whose computation is not based on the black-box inner parameters or structure In this regard Doctor XAI is similar to other black-box-agnostic techniques However to the best of our knowledge ours is the first agnostic XAI technique applicable to sequential and ontology-linked data classification The mining of sequential data is of pivotal importance in healthcare since this format is how typically the clinical history of the patient is represented Furthermore the presence of an ontology associated with the data is widespread in the medical and biological fields Given a patient whose clinical history classification needs an explanation Doctor XAI first generates a Art paragraph Art paragraph g Art paragraph local synthetic neighborhood around the selected patient exploiting the semantic information encoded in the ontology and uses the black-box model to label it Then it transforms the clinical history of such synthetic patients into a format suitable to train a decision tree This transformation allows taking the sequential nature of the data into account Finally Doctor XAI trains a decision tree on the labeled synthetic neighborhood and it extracts an explanation in the form of a decision rule We applied Doctor XAI to explain the decisions of Doctor AI a recurrent neural network which takes as input patients sequential EHR data and predicts the next visit set of diagnoses We compared the quality of the explanations provided by Doctor XAI against those of the same technique without the ontological information We show how exploiting the semantic information encoded in the ontology increases the performance of the explainability technique across all the evaluated metrics We want to highlight that even if our system deals by design with sequential multi-labeled ontology-based data none of these features is strictly necessary Doctor XAI can be used with datasets displaying any combination of the three aforementioned features by exploiting only the corresponding specific modules The contribution of this paper is twofold We propose an agnostic explainability technique able to tackle the sequential data classification explanation problem We show how exploiting the semantic information present in the medical ontology increases the quality of explanation The paper is structured as follows Section reviews related work on the topics of explainable artificial intelligence machine learning for sequential healthcare data and ontology use in machine learning Section introduces the algorithmic building blocks of our XAI technique as well as the pipeline as a whole Section presents experimental setups and results Section ends the paper with the conclusions and directions for future work RELATEDWORK Doctor AI Doctor AI is a Recurrent Neural Network RNN with Gated Recurrent Units that predicts the patients next visit time diagnoses and medications order We focus here only on the diagnosis prediction task of the model The authors trained their model on patients of the EHRs database of Sutter Health Palo Alto Medical Foundation The multi-hot input vector representing the diagnoses at each time-step of patient clinical history is first projected in a lower-dimensional space and then received as input by a stack of RNN layers implemented using GRUs Finally a Softmax layer is used to predict the diagnosis codes of the next time-stamp The predictive performance of Doctor AI are evaluated using recall n with n achieving recall Explainable AI In response to the increasing demand for interpretability a vast literature on this matter has been produced Many interpretability approaches related to sequential data modeling focus on adding an attention mechanism to a sequential model and use the attention weights as a form of explanation however recent works have highlighted how this kind of explanation might lack consistency and that attention should not be used as an explanation Other interpretability approaches related to sequential data modeling focus on understanding the internal behavior of the black-box under study conversely our approach is agnostic to the black-box whose outcome we want to explain The agnostic approach to explanations was first introduced in LIME The intuition behind LIME is that even if the decision boundary learned by the black-box in the feature space can be arbitrarily complex it can always be locally approximated by a simpler more interpretable model In the LIME approach the explanation is the set of weights of a sparse linear model Other examples of agnostic approaches that focus on explaining the black-box behavior around a specific instance are SHAP and ANCHORS The SHAP approach evaluates local features importance using a game theory approach while ANCHORS formulate the problem using a multiarmed bandit approach Our work shares some of the features of these approaches for example we mine our explanations using a perturbation-based strategy However unlike any other method we also exploit the domain knowledge encoded in the ontology to generate relevant perturbations for the specific problem under study By doing so we increase the quality of the generated synthetic instances which is of pivotal importance for the quality of the explanation provided Similarly to LIME we train a local surrogate model able to mimic the black-box behavior around the data point and similarly to ANCHORS our approach produces rule-based explanations However we mine our explanations from a multi-label decision tree which allows us to deal with a complex output space the multi-label one in a simple straightforward manner Furthermore none of the aforementioned approaches can be directly applied to a sequential input In our work we introduce a temporal encoding/decoding scheme that allows the user to visualize which events are the most relevant for the instance classification directly on the temporal sequence Ontology use in machine learning In our work we exploit the medical ontology of ICD- International Classification of Diseases Ninth Revision diagnosis codes to increase the fidelity performance of the interpretable model to the black-box The increase in predictive performance thanks to the infusion of knowledge in the learning procedure was adopted in several other works For example in the authors use an attention mechanism that leverages the medical ontology of ICD to learn a code representation that combines the embeddings of its ontology ancestors They then train this attention mechanism together with an RNN with units to improve the classification performance of prediction of the predictive model They show that the performance is increased by with respect to a basic model that does not exploit the medical ontology Furthermore they show that the learned representation of medical codes aligns with medical knowledge Moreover the authors of show how disease classification performance can improve using features based on the ICD- codes semantic similarity To compute the ontological similarity among sets of ICD- codes ie a visit they first calculate the semantic similarity of each pair of terms in the sequences as the importance of their lowest common ancestor in the hierarchy and then take the maximum of these similarities as the similarity of the two sequences This approach over-estimates the similarity of the two sequences since it is sufficient to have one ICD- code in common to have similarity equal to one The importance of the lowest common ancestor is related to the level of the term in the hierarchy according to the authors this feature is related to the rarity of the disease but it just captures how well specified is the disease However even with this basic approach to encoding medical knowledge into the learning process the performance of the algorithms is increased We use a more sophisticated approach to compute patients similarity as detailed in Section METHODS In this section we introduce the components of Doctor XAI and how they form the full explanation pipeline Our technique is based on the idea presented in of learning an interpretable classifier able to mimic the decision boundary of the black-box that is relevant to the decision taken for a particular instance More formally Given an instance x and its black-box outcome y an explanation is extracted for this individual decision from an inherently interpretable model c trained to mimic the local behavior of b For our approach we follow the general pipeline of generating a set of synthetic instances called neighborhood surrounding the instance x we want to explain labeling them utilizing the black-box b training an interpretable model c on the labeled neighborhood and finally extracting an explanation in the form of a symbolic rule However we have developed specific modules in order to deal with the temporal dimension in the data and exploit linked structural knowledge representation Figure illustrates our explanation pipeline The explanation pipeline The starting point is the data point whose black-box prediction we are interested in explaining As the first step we select the data points that are closest to the instance to be explained in the available dataset these points are called the real neighbors of the instance We can either select the closest data points according to a standard distance metric such as the Jaccard one or exploit ontology-base similarities We describe the latter in Subsection In both cases we obtain a set of real neighbors each of which is represented as a sequence We then generate the synthetic neighborhood perturbing the first real neighbors to ensure the locality of the augmented neighborhood The synthetic neighbors sampling is crucial to the purpose of auditing black-box models Ideally the synthetic instances should be drawn from the true underlying local distribution Unfortunately this distribution is generally unknown and how to generate meaningful synthetic patients is still an open question While most state-of-the-art agnostic explainers employ random perturbations we use the domain knowledge encoded in the ICD- ontology to generate more meaningful synthetic instances as explained in Subsection It could be argued that the interpretable model could be trained directly on the closest real neighbors However the rationale behind the generation of synthetic neighbors is that we want to build a dense training set for DATA POINT to be explained Real neighbors sequence Real neighbors flattened Synthetic neighbors flattened Interpretable Model Rules Synthetic neighbors sequence Labels for synthetic neighbors Ontological Perturbation Jaccard Ontological Distance Selection Feeding as input Extraction Temporal Encoding Temporal Encoding Temporal Decoding Normal Perturbation Black-box Labeling Figure The explanation pipeline the interpretable classifier c in order to increase its performance in mimicking the black-box Unlike other explanation techniques we do not perturb directly the features of the instance whose blackbox decision we want to explain By doing so we prevent the case of generating a synthetic neighborhood containing only instances with the same black-box classification a situation that would make the training of any interpretable model impossible In other words we ensure the expressiveness of the synthetic neighborhood ie the black-box classifications are heterogeneous among the synthetic neighbors For the perturbation steps in our pipeline we can follow two alternative paths represented by the red and blue arrows in Figure the two paths share the black arrows The red path is based on the normal perturbation which we describe in Subsection the blue path involves the ontological perturbation as described in Subsection Both paths involve steps of temporal encoding/decoding with the relative algorithms described in Subsection since the black-box model requires a sequential input whereas the interpretable one requires a tabular flat one The red path is based on the normal perturbation first the real neighbors are encoded flattened into sparse vectors Then the normal perturbation is applied in order to obtain a synthetic neighborhood and this kind of data can be fed to an interpretable model In order to obtain the labels for the synthetic data points however we have to decode them back into sequences so that we can feed them to our black-box model for labeling Once we have both the synthetic neighborhood and the corresponding labels we can train the interpretable model and finally extract symbolic rules Similarly to we chose a multi-label decision tree as inherently interpretable classifier c From such decision tree we extract rule-based explanations in the form p y where y The explanations are extracted by including in the rule premise p all the split conditions on the path from the root to the leaf node that is satisfied by the instance x The blue path involves the ontological perturbation In this case we can apply the perturbation directly on sequential data obtain a synthetic neighborhood as a set of sequences and feed them to the black-box model for labeling However as it was for the red path the interpretable model requires a tabular input so we proceed to flatten time-encode the synthetic neighbors in a set of vectors At this point the blue path follows the same final steps as described above training of the interpretable model and extraction of symbolic rules We remark that while we followed a general framework for our model-agnostic explanation pipeline we have extended the framework with novel contributions in order to deal with structured data and sequential data respectively We observe that these components can be independently plugged in an explanation pipeline according to the nature of the data point to be explained Ontological Distances In this section we define a new distance measure that allows us to select the first neighbors of the instance whose decision we want to explain Each patients clinical history is represented as a list of visits which in turn are encoded as lists of ICD- codes Every instance is therefore a list of lists of ICD- codes More formally if we define the set of ICD- codes as C c c c C each patients clinical history is represented by a sequence of visits V such C A simple example of a patient clinical history representation is as follows The patient visited the hospital three times the condition Acute embolism and thrombosis of superficial veins of unspecified upper extremity is chronic condition Occlusion and stenosis of carotid artery without mention of cerebral infarction was observed on the first visit only whereas two new conditions with codes and were diagnosed only in the third visit We observe that multi-hot encoding all occurring ICD- codes is a fairly inefficient representation for visits the obvious drawback being the size of the encoding vector corresponding to the size of the ICD- dictionary Furthermore this positional representation does not encode the semantic distance from ICD- codes a patient with food poisoning one with a broken hand and one with a broken wrist are equally distant from a purely Hamming-based perspective In order to mine the semantically similar data points we introduce an ontology-based distance metric Code-to-code similarity Each ICD- code represents a medical concept in a hierarchical ontology these concepts are the nodes of the graph-representation of such ontology and it is therefore possible to compute distance and similarity scores among any pair of them Several similarity metrics could be selected in this paper we adopt the Wu-Palmer similarity score WuP because it is one of the most commonly used for ICD- ontologies Given two ICD- nodes c and c let L be their lowest common ancestor LCA and R be the root of the ICD- ontology also let y be the number of hops steps required to reach node y from node x following the ontology links The WuP similarity measure between c and c corresponds to c c for any couple of ICD- nodes The lower bound is obtained when that is when the LCA of c and c is the root node Conversely a node has WuP-similarity with itself By relying on the underlying ICD- ontology we can therefore use the WuP similarity to compute pairwise distances between ICD- codes This yields a much more fine-grained analysis compared to a coarse Hamming similarity Visit-to-visit distance Having defined a code-to-code distance the following step is to compute distances at the visit level since visits are defined as lists of occurring ICD- codes We adopted the weighted Levenshtein distance a string metric for measuring the difference between two sequences as the minimum number of single-character edits insertions deletions or substitutions required to change one sequence into the other The weighted version of the Levenshtein distance allows defining custom insertion/deletion/edit costs We have set c as edit cost for modifying c into c and as insertion/deletion indel cost since c c in order to favor edits over indels This gives us a distance metric between pairs of visits which is based on the similarity between the ICD- codes occurring in each of the two visits Patient-to-patient distance The third step is to compute a patient-to-patient distance metric based on how similar the visits of the two patients are In order to do so we adopted the Dynamic Time Warping DTW algorithm again using the pairwise visit distances provided by the weighted Levenshtein algorithm as edit distance The sequences of visits are warped non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension This final step provides us with the pairwise distances for all patients data points in the dataset thus enabling us to select real neighbors with ontologically similar conditions the data point to explain Ontological Perturbation As previously mentioned after selecting the first real neighbors of the instance whose decision we want to explain we perturb them in order to generate synthetic neighbors There are mainly two ways to perform an ontology-based perturbation on an instance by masking or replacing some conditions ICD- codes in the patients clinical history according to their relationships in the ontology We decided to adopt the first type of perturbation in order to limit the amount of noise injected in the training set of the interpretable classifier The idea behind perturbing the patients history in this way is that we want to explore how the black-box label changes if we mask all the semantically-similar items from the sequence We decided to randomly mask all the occurrences of the items with the same least common superconcept By doing so we are exploring how a Figure A branch of the ICD- ontology general condition a higher concept in the ontology is affecting the black-box diagnosis In our case we are dealing with patients clinical history Each patients clinical history is a sequence of visits and each visit is represented by lists of ICD- codes In the ICD ontology all codes are composed of a prefix and a suffix separated by a dot the prefix defines the general condition and the suffix provides increasingly specific information We show an example of the hierarchical structure of the ICD- ontology in Figure Our implementation of the ontological perturbation is the following we first randomly select one ICD- code in the clinical history of the patient we want to perturb a leaf of the ontology then we mask all the ICD- codes in the patients history that share the same prefix the least common superconcept By doing so we generate synthetic patients that lack a specific group of semantically similar conditions Consider for example the following patient P One example of ontological perturbation is the following we randomly select ICD- code which is mixed acid-base balance disorder Starting from this code we create the synthetic patient P by masking all the ICD- codes related to ICD- ie disorders of fluid electrolyte and acid-base balance the least common superconcept Note that without ontological information we have different codes and therefore potential perturbations most of which dont really isolate different conditions Conversely using the ontology we group the occurring ICD- codes in three categories as a consequence we have potential maskings each of which isolates a subset of different conditions Normal Perturbation As an alternative to the ontological perturbation of the first real neighbors of the instance under study we performed a normal perturbation on such features This perturbation applies to a broader Figure Example of temporal encoding for a patient Figure Example of temporal decoding for a patient number of cases since it does not require an ontology to be performed Given the flattened version of the real neighbors the normal perturbation creates the new synthetic instances feature by feature drawing from a normal distribution with mean and standard deviation of the empirical distribution of that feature in the real neighbors This perturbation implies the strong assumption that every feature is independent of the others Temporal encoding and decoding As introduced above the standard data type for longitudinal healthcare data is to represent a patient as a list of visits and in turn each visit as a list of occurring conditions in our case ICD- codes There is no inherently interpretable model able to deal with the multi-label classification of such type of input therefore we need to perform an input transformation that both retains its sequential information and allows to feed it into an interpretable model a decision tree in our case We introduce a pair of encoding-decoding algorithms so that we can flatten the temporal dimension when feeding our synthetic neighborhood to the interpretable model The binary encoder implements a time-based exponential decay rooted at the last item of the sequence Intuitively each code in visit will be given a score of if is the last visit if is the second-to-last visit and so on More formally when encoding a patient P V each code c P will be encoded as follows EN c P n i if c Vi else The encoding is for all items that never occur in that sequence and it tends to for a growing number of elements in the sequence in which that item occurs The encoded flattened representation of a patient is therefore a sparse vector of real numbers and as such it can be fed to multiple interpretable models Conversely we define the decoding from a sparse vector of real numbers to a sequence of visits as t l if X or l t l if X t l otherwise where X is the value to be decoded t is initially set at and l controls the maximum length of the generated sequence we use the average length of the real neighbors The result of the decoding is a list of s and s that indicates the presence/absence of a certain code We show a simple example of temporal encoding in Figure In this example the patient visited the hospital three times Each visit contains a set of ICD- codes for the sake of simplicity here represented as letters As a first step a weight is associated to each visit Then the weight of each ICD- code is computed by adding the weights of the visits where it occurred We also show a simple example of temporal decoding of a flat synthetic patient in Figure In this example we transform the value of the first ICD- code represented by letter A into its occurrence in the sequence In this example we set the maximum length of the generated sequence to l It is important to remark that the decoding algorithm when presented with perturbed data might potentially produce arbitrarily long sequences where progressively small residuals are mapped to the occurrence of the decoded ICD- code in progressively further away visits The l-guard was introduced to prevent this from happening so that flattened synthetic patients match the number of visits of the flattened real neighbors EXPERIMENTS AND RESULTS Dataset We ran our experiments on the Multiparameter Intelligent Monitoring in Intensive Care III MIMIC-III database This database contains de-identified data of over ICU Intensive Care Unit patients of the Beth Israel Deaconess Medical Center data in Boston collected from to We used the information related to the hospital stay dates and diagnosis codes to build the patient clinical history as performed by the pre-processing script available in Doctor AI GitHub repository This operation removes all patients with less than two visits some statistics about the dataset after the pre-processing procedure can be found in Table MIMIC-III n of patients n of visits avg n of visits per patient min n of visits per patient max n of visits per patient n of unique ICD- codes n of unique CSS grouper codes avg n of ICD- codes per visit Table MIMIC-III characteristics for patients with more than one visit The clinical history of each patient is modeled as time-stamped sequence of visits As previously mentioned each visit is represented by a set of ICD- diagnosis codes these codes are assigned to each patient at the end of his or her hospital stay and hospitals use them to bill for care provided They are organized in a is-a hierarchical tree structure that places more general concepts closer to the root of the tree and more fine-grained concepts closer to the leaves of the tree The ICD- taxonomy and occurring ICD- codes in MIMIC are visualized in Figure We used this ontology to measure the similarity between patients clinical history as described in section and to generate the synthetic neighbors of each patient as described in section Figure ICD- ontology The red dots represent codes occurring in the MIMIC dataset the orange ones their parent nodes Black-box classifier We trained Doctor AI on MIMIC-III for epochs using approximately of patients as the training set as the validation and as the test set We built the label for each time step of the sequence by grouping the full-length ICD- codes using single-digit groupers By doing so the dimensionality of the label space shrinks from codes to groups of codes We compare the predictive performance of Doctor AI trained by us on MIMIC-III dataset with the ones reported in the original paper in Table The metric used to evaluate the predictive performance is recall n of true positives in the top n predictions of true positives We also trained a baseline model to imitate one of the benchmarks of the original paper This baseline the Most frequent predicts the top-k most frequent labels observed in visits before the current visit The fact that we trained Doctor AI on a much smaller dataset lowers the algorithms predictive performance compared to the ones of the original paper However they are in line with the performance on the MIMIC-II dataset discussed in the original paper Furthermore having a good predictive performance is not our goal we will use the black-box labels as ground-truth labels for the decision tree In our work we focus on explaining Doctor AI because of the availability of its source code and because the authors results are easily Table Doctor AI performance on different datasets Dataset and algorithm recall n n n n Doctor AI MIMIC-III Most frequent MIMIC-III Doctor AI dataset from Most frequent dataset from reproducible using open-source data However we want to stress that our method is not specific to this black-box Experimental set-up We decided to test our explanation method on a cohort of randomly selected patients from the MIMIC database We put each of these patients through different explanation pipelines and we explained their top- -codes prediction The first two exploit the ontological information encoded into ICD- codes whereas the last one can also be used to explain sequential data classification if an ontology is missing We aim to show that exploiting the ontological information in the data increases the explanation quality Ontological pipeline with ontological perturbation DrXAI This pipeline fully exploits the knowledge encoded into the ICD- ontology to create the synthetic neighborhood Given a patient whose black-box decision we want to explain it selects its first neighbors in the dataset using the ontological distance described in section and then it generates the synthetic neighborhood by perturbing them using the ontological perturbations described in section This pipeline corresponds to the blue path of Figure using the Ontological similarity Ontological pipeline with normal perturbation This pipeline selects the first real neighbors of the instance to explain using the ontological distance but then it creates the synthetic neighborhood by perturbing these instances using the normal perturbation described in section This pipeline corresponds to the red path of Figure using the Ontological similarity Non-ontological pipeline with normal perturbation This pipeline does not use the semantic information encoded in the ICD codes It first selects the real neighbors of the instance to be explained using Jaccard similarity between each patient visit and then it perturbs them by using normal perturbations This pipeline corresponds to the red path of Figure using the Jaccard similarity By comparing the two ontological pipelines we want to show that exploiting the semantic information encoded in the ICD- ontology is also useful to create the synthetic neighbors We developed the non-ontological pipeline as a baseline for explanation quality However this last pipeline is also the most general one because it can be applied to sequential data that does not have an associated ontology Furthermore we wanted to show that increasing the density of the feature space around the instance to be explained by creating the synthetic neighbors actually increases the interpretable models ability to mimic the black-box locally For this reason for each instance to be explained we trained two decision trees One decision tree is trained directly on the real neighbors of that patient from the dataset and the other one is trained on a fraction of the augmented synthetic neighborhood We then compare the performance of these decision trees on an out-of-sample set of synthetic neighbors We utilize the following metrics to evaluate and compare the different explanation pipelines Fidelity to the black-box This metric compares the predictions made by the interpretable model with the predictions made by the black-box on a synthetic neighborhood of the instance It measures the ability of the interpretable classifier to locally mimic the black-box and therefore it is tested on a held-out subset of the synthetic neighborhood Since we are dealing with a multi-label classification task we calculate the fidelity the measure with micro-averaging Hit This metric compares the interpretable classifier prediction and the black-box prediction on the instance to be explained It tells us if the interpretable classifier predicts the same label as the black-box on the instance we want to explain Since the prediction we are trying to explain is a multi-label classification we calculate the hit as hamming-distance b Explanation complexity This metric measures the complexity of the explanation as the number of premises in the rule-based explanation This measure is important since we do not want to approximate the black-box with a model that loses its interpretability because of the high-dimensionality of the explanations it produces Results Figure Fidelity distribution for the ontological pipeline with different perturbation type and training/test set In Figure we show the fidelity sample distributions at different values of for the decision trees trained using the ontological explanation pipelines ie the pipelines that select the first dataset Table Mean values of fidelity Explanation Pipeline Fidelity Ontological pipeline with ontological perturbation Ontological pipeline with normal perturbation Non-ontological pipeline with normal perturbation Table Mean values of hit Explanation Pipeline Hit Ontological pipeline with ontological perturbation Ontological pipeline with normal perturbation Non-ontological pipeline with normal perturbation neighbors of the instance to be explained using the ontological distance The first observation is that the decision trees trained directly on the real neighbors blue and green boxplots generally have a lower fidelity to the black-box compared to the ones trained on the augmented synthetic neighborhood orange and red boxplots This trend is true for all values of and for both the ontological pipeline with ontological perturbation and the ontological pipeline with normal perturbation The fidelity values of each decision tree have been evaluated on an held-out test set of synthetic neighbors This trend confirms that increasing the local density of points in the feature space around the instance to be explained helps the interpretable model to understand the black-box behavior The second observation is that the fidelity of the decision tree trained using the ontological pipeline with ontological perturbation red boxplot is generally higher compared to all the other explanation pipelines This observed tendency confirms that exploiting the ontological information during the synthetic neighborhood creation allows the decision tree to better approximate the local black-box decision boundary Figure Fidelity distribution for the non-ontological pipeline at different values of and training set In Figure we show the fidelity sample distributions at different values of for the decision trees trained using the non-ontological explanation pipeline ie the pipeline that selects the first dataset neighbors of the instance to be explained using the Jaccard similarity between patients visits We developed this explanation pipeline that does not use the semantic information encoded into the ICD codes as a baseline to prove that an approach that does not exploit this information has lower performance This is true if we compare this explanation pipeline with the fully-ontological one the ontological pipeline with ontological perturbation However the fidelity performance of this non-ontological pipeline is comparable to the ones of the ontological pipeline with normal perturbation The high values of fidelity achieved by this pipeline prove that we developed a trustable explainability technique applicable to any black-box that takes as input any sequential data even when there is no ontology associated with the items of the sequence Furthermore it is important to notice that also for this pipeline the values of fidelity to the black-box increase after the synthetic neighborhood augmentation the orange boxplot Figure Explanation complexity for the ontological pipelines In Figure we show the sample distribution of explanation complexity ie the number of premises in the rule-based explanations at different values of for the two ontological explanation pipelines As expected we see how the length of the explanation increases as increases This happens because if we start from a high number of first real dataset neighbors we are trying to approximate a larger portion of the decision boundary of the black-box with the interpretable classifier We could say that we are not restricting ourselves to the local decision boundary close to the instance whose decision we want to explain Therefore since we are trying to approximate a more complex decision boundary the dimensionality/complexity of the decision tree grows and consequentially the length of the rule increases From this plot it is also possible to see that the explanation length of the explanations extracted from the ontological pipeline with ontological perturbation orange boxplot is more variable than the ones extracted using the ontological pipeline with normal perturbation for large values of Aggregated statistics of fidelity and of hit for all the explanation pipelines are shown in Tables and we can observe that the value of hit is consistently high for all explanation pipelines and across all values of Explanation example We show in Figure an explanation example extracted with the ontological pipeline with ontological perturbation with In order to make it more comprehensible for readers not familiar with ICD- codes we enriched the rule-based explanation with the ICD codes semantic The original decision rule extracted from the decision tree can be seen at the top of the figure with the fidelity of the decision tree and its hit value There are several ways to read this rule since it contains many layers of information The decision rule is the decision tree pathway that leads from the root of the tree to the leaf containing the black-box decision for this reason all inequalities are to be considered in conjunction furthermore the ICD- codes occurring in the rule are ranked in order of information gain Each conjunct of the rule follows the pattern ICD-_code observed_value threshold_value The observed value is the value of that ICD- code for the patient whose decision we want to explain Recall that the temporal encoding or flattening procedure described in Section assigns to each ICD- code a weight according to the visit in which it was observed diagnosed The threshold value is the split value assigned by the decision tree to that ICD- code Both these values can be interpreted as the presence of the ICD- code in a set of visits The patient under examination had four visits The ICD- codes describing the diagnoses associated with each visit are represented in the timeline just below the decision rule Recall that we are explaining the top- -codes predicted by Doctor AI The ICD- codes considered meaningful by the black-box have been colored to enhance the readability The explanation of each real and threshold value can be found in the list below the timeline For example the ICD code has an observed value of which means that it was observed in the second-to-last visit visit Its threshold value is whose closest value among those generated in the temporal encoding process is which represents the third-to-last visit visit For this reason even if this ICD- code was observed in the penultimate visit the interpretation of the first rule conjunct is has to have been observed at least once in the last three visits The code to run our experiments as well as our results are available on GitHub CONCLUSIONS AND FUTURE WORK In this paper we presented Doctor XAI the first agnostic explanation technique suitable for any black-box classifier that deals with data having one or more of these characteristics sequential multi-labeled and ontology-linked These features are typical of healthcare data Our technique first generates a set of synthetic instances close to the instance whose black-box decision we need to explain then it trains an interpretable classifier a decision tree on such neighborhood and finally it extracts a rule-based explanation from it We studied the behavior of the interpretable classifier varying the hyper-parameter the number of first neighbors in the real dataset that are considered in the synthetic neighborhood generation In particular we showed that for all values of the synthetic neighborhood generation procedure which exploits the ontological information encoded in the ICD- codes achieves better performance in approximating the local behavior of the black-box if compared to a procedure which does not have access to the ontology Furthermore the synthetic augmentation of the interpretable classifier training set allows it to increase its fidelity to the blackbox We also tested the sequential-only version of our explanation technique showing that it achieves good fidelity to the black-box while also confirming that the ontology-enriched approach achieves a better score Application scenario We believe that doctors and patients would both benefit from such an explanation of the black-box behavior Ideally doctors the target users of our method would be able to have a higher understanding of the decision support system they are using the black-box This means that the ultimate decision would be more informed and ultimately better than the decision that the human decision-maker would have made without the black-box as well as better than the automated decision by the black-box alone Such an informed decision would also benefit the patient because of the increased quality of care provided by the doctor In this paper we focused on the medical domain but since our method is agnostic the black-box the possible applications cover several scenarios where we can identify a sequence of events linked to ontology concepts such as online market basket analysis or Wikipedia user behavior prediction Concerning directions for future work We will focus on studying other kinds of synthetic neighbors generation for sequential data Furthermore we would like to better assess the impact of the random components of the synthetic neighbors generation procedure on the quality of the explanations Right now Doctor XAI can explain only black-box classifiers but with a simple extension we would be able to explain black-box regressors producing continuous outcomes this is another common healthcare task for instance for predicting risk stratification Figure Explanation example