Toward Situated Interventions for Algorithmic Equity Lessons from the Field Research to date aimed at the fairness accountability and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes Researchers have given less attention to methods that account for the social and political contexts of specific situated technical systems at their points of use Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit The main insights we gleaned from our experiences were i many meaningful interventions toward equitable algorithmic systems are non-technical ii community organizations derive the most value from localized materials as opposed to what is scalable beyond a particular policy context iii framing harms around algorithmic bias suggests that more accurate data is the solution at the risk of missing deeper questions about whether some technologies should be used at all More broadly we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts INTRODUCTION Existing scholarship on algorithmic fairness often begins with a recognition of real-world harms yet many contributions to date offer technical solutions that precede application or evaluation of systems in use A situated investigation is one that forefronts the details of the context of specific people and places at particular points in time rather than trying to study a system or question with an abstract approach removed from social context We invoke this use of the term situated from feminist philosopher of science Donna Haraway who describes situated knowledge as the product of embodied vantage points located historically see also Situated investigations offer opportunities to reveal not only the technical features and limitations of a system but also the different ways a system affects different people in its context of use Many recent studies have pointed to the need to incorporate sociotechnical context in design of algorithmic systems Selbst et al call for scholars in this area to mind the traps associated with abstract analysis Green Chen demonstrate that human decision-making interacts with algorithmic processing in the ultimate disparate impacts of algorithmic systems Other scholars have pursued empirical case studies field evaluations context-aware uses of datasets and questions of institutional access to real-world data In each case attending to end-use provides a richer design space and new opportunities for improvements and safeguards In our work we move towards a deeper engagement with systems social and political context We report on a process of using a participatory design and action research approach to craft interventions supporting the agendas of people most affected by the disparate impacts of algorithmic systems We look to the field of human-computer interaction HCI to deepen our commitment to engaging communities As Paul Dourish notes HCI joins distinct scholarly traditions that are often viewed as at odds with each other a positivist engineering approach which seeks to achieve precisely defined objectives through system design and a phenomenological interpretivist approach which understands the meaning of both social action and technical systems as constructed contigent and constantly in flux in defiance of the rigid operationalization of an instrumentalist view At the intersection of these two traditions HCI has fostered a wide array of participatory design methods Here we report on our process drawing on participatory methods and a distinct tradition sharing similar goals known as participatory action research in support of algorithmic accountability and community decision-making Participatory methodologies are valuable complements to the practice of data science more broadly The work of data science has a normative dimension but there is little agreement about which norms or values should be prioritized Research methods that center the people disparately affected by the practice of data science can guide design choices and clarify the implications of these choices by situating the work where the social costs and benefits can be more directly assessed Researchers in critical data studies have explored similar themes in the context of mixed methods data science Our work joins others in understanding algorithmic discrimination as a product of societal inequity rather than as solely a result of inaccurate performance by models or under-representative training data When systems are deployed in contexts historically shaped by discrimination in policy and practice such as policing or medical care data employed as inputs for decision systems contributes to feedback loops and amplified discrimination As a result it is not enough to attend to bias in systems from the perspective of accuracy interventions must acknowledge the historical political and institutional contexts in which systems are used and by whom Our Contributions The imperative to account for situated context in algorithmic accountability requires a move beyond the laboratory and academy In this work we ask What are the opportunities and challenges in using participatory design and action methods for research on fairness in algorithmic systems We approach this question by drawing on insights from our use of these methods in a project aimed at empowering community organizers and community members in advocacy and public comment efforts in state and local technology policy In work described in more detail in a companion paper we engaged in a co-design project that included local community groups advocacy campaigns and policy stakeholders resulting in the Algorithmic Equity Toolkit a set of heuristic tools and an interactive demo that assists users in recognizing algorithmic systems understanding potential algorithmic harms and holding policymakers accountable to public input Our central contributions here are lessons learned from the development of the Algorithmic Equity Toolkit for the practice of situated investigations into fairness accountability and transparency in algorithmic systems Our interactions with community partners re-framed our objectives and highlighted the opportunities and challenges of clearly communicating concepts from artificial intelligence and scholarship on algorithmic bias We also note existing frictions to further incorporation of community input in settings where data science and machine learning research conventionally happens The algorithmic fairness community has a unique opportunity to foster the intersection of data science machine learning and community-based work This shift will require us to learn from other fields as to how to build reciprocal partnerships with community organizations elicit their input and attend to the practical dimension of managing trust and power dynamics in these relationships A more inclusive data science practice will result in novel conventions of work which attend to fairness accountability transparency and equity as an explicit part of research method and practice rather than as topic alone RELATEDWORK Research in human-computer interaction has drawn on and developed methods for participatory design for decades Participatory design is the process of enlisting intended user groups in the development of information technologies in hope of better supporting these users interests and goals Historically it is explicitly concerned with the politics of system design with respect to worker power early participatory design in Scandinavia aimed to support workers self-determination of their local working conditions as well as to affect policy More recent calls for an emancipatory HCI also affirm these value commitments Participatory design approaches can intervene at individual organizational and political scales All participatory methods recognize users as authorities over their own social context and hold that active cooperation with designers can make meaningful input or control possible Participatory approaches to design fall across a spectrum from eliciting information from users to active collaboration with them in the design process this latter approach is known as co-design Co-design describes any collaborative design process undertaken by designers end-users and stakeholders Key strategies to make critiques and alternative visions for sociotechnical systems more accessible include the use of material prompts such as photos and card decks that provide questions and themes for further reflection Other methods that aim to empower stakeholders as designers include the use of metaphors design fiction and futuristic scenarios Participatory action research PAR is a wholly distinct and longstanding tradition of community-embedded research that involves partnering with affected communities to achieve social change The tradition was formalized in the late s by an international network of researchers via a set of principles which specify for example that the approach requires a research problem to be defined analyzed and solved by the community While its historical development defies easy attribution PAR draws on a range of influences including work of Paulo Freire feminist epistemology and praxis and grassroots civil rights and workers movements from the US Latin America and South Asia It has been used in a broad a range of settings and fields including research in health education and social work The resulting tradition is committed to advancing social change in active collaboration with affected communities particularly by restructuring power relationships between researchers and participants to collectively reflect on intermediate findings and co-determine action in an action-reflection cycle Participatory action research operates within a epistemological paradigm that is constructivist recognizing knowledge as partial and situated critical interrogating knowledge as an artifact of power relations and participatory as forming through an interaction between researchers and participants Its ultimate aim is to re-center power in communities outside academia and to support those communities in taking action Both participatory design and action research approaches have informed generations of work in HCI In machine learning and AI scholarship their value is becoming more apparent as a growing contingent holds that AI has ignored the voices opinions and needs of disempowered stakeholders in favor of those with power money and influence Academics interested in addressing such concerns have turned to activist and advocacy groups to learn from them and support their work For example Seeta Peña Gangadharan and Virginia Eubanks Our Data Bodies project is centered on empowering communities to address digital data collection and human rights through a series of workshops and activities A key strength of the project is close collaboration with activists Tamika Lewis Tawana Petty Kim M Reynolds and Mariella Saba who are embedded within networks of grassroots organizations in cities across the US A number of labs research institutes and meetings have also committed their missions to community engagement cf Often such efforts emphasize the importance of equitable process as determinative of equitable outcomes As the principles of the media activism organization Allied Media Projects state The strongest solutions happen through the process not in a moment at the end of the process As part of outlining this new paradigm a notable recent call for greater partnership with and service to communities comes from the Design Justice Network initially formed in by community organizers which includes among its principles to center the voices of those who are directly impacted by the outcomes of the design process In our work we report on lessons learned from our own efforts to draw on participatory and co-design approaches BACKGROUND Our work takes place in the context of Seattle Washington In September the Seattle City Council passed a significant revision of the Seattle Surveillance Ordinance producing landmark municipal legislation that mandated the disclosure of surveillance technologies in use by city departments and subjecting those technologies to a political process that includes community input and oversight As indicated in the declarations that introduce it the Ordinance was explicitly motivated to incorporate racial equity principles to ensure that surveillance technologies do not perpetuate institutionalized racism or race-based disparities Seattle Surveillance Ordinance The law responds in part to a troubled Ali Alkhatib Blog Anthropological/Artificial Intelligence the HAI March Accessed August history of discriminatory policing in Seattle as well as to a consensus among local civil rights activists and key policymakers that surveillance systems disparately harm people of color and other marginalized communities The Seattle Surveillance Ordinance joins a growing cadre of similar ordinances enacted in response to concerns about the expanding use of surveillance technologies by government agencies Civil rights activists have pressed for these ordinances with the intention of making public-sector technologies subject to greater transparency and to political oversight While the laws differ many share common attributes They require the disclosure of systems that meet the definition of a surveillance technology the production of reports about their capabilities and intended uses and authorization for such uses by lawmakers Importantly many of these laws provide opportunities for public engagement and comment prior to lawmaker determinations on the acquisition and use of a particular technology In a case study of Seattles surveillance ordinance three of this papers authors found that although the ordinance was aimed at assessing surveillance systems disparate impact its efficacy was potentially limited by a lack of explicit language directing public officials to consider the algorithmic bias dimension of each technology A key finding of this prior work was that the analytic capabilities inside systems such as automated license plate readers were not recognized by interviewed city employees as relying on machine learning This finding reflects a larger definitional disconnect affecting AI policy oversight efforts between researchers and policymakers in what technologies are considered to be AI or are potential sources of algorithmic bias As a result the potential for algorithmic harm from everyday technologies may be overlooked at least in existing surveillance regulations The incipient effort leading to the Algorithmic Equity Toolkit was inspired by a desire to assist lawmakers in closing this definitional disconnect and to aid the policy implementation process METHODS Participatory Action Research Our work on the Algorithmic Equity Toolkit relied on participatory design methods and a participatory action research PAR approach In the tradition of PAR our work involved partnering with organizations engaged in civil rights advocacy eliciting those partners priorities creating prototypes seeking input and reflecting on what we learned Our main partner organization was the American Civil Liberties Union of Washington ACLU-Washington We also worked closely with two member organizations of a coalition assembled by the ACLU-Washington the Tech Fairness Coalition which advocates for fairness and equity in technology policy The first coalition organization was Densho an organization dedicated to preserving the history of World War II incarceration of Japanese Americans and advocating to prevent state violence in the future The second organization was the Council on American-Islamic Relations of Washington CAIR-Washington a prominent Islamic civil liberties group who defends the rights of American Muslims After a four-month planning process with ACLU-Washington our codesign work kicked off in June and involved meeting several times with all three organizations The project conducted Diverse Voices panels in August presented to the broader Tech Fairness Coalition in September and is planned to be shared for member organizations use in March We also note that ACLU-Washington encouraged early and continuous engagement with community organizations given underrepresentation of historically marginalized communities in surveillance policy decision-making This input was a key takeaway of an early scoping discussion about the Toolkit in June Through prior engagements with ACLU-Washington we learned that their work was motivated by a desire to increase public input on surveillance and automated decision-making system ADS technologies from those who are typically not consulted We recognize that some communities have a greater stake in this than others theyre really not talking about the safety of everyone Were talking about some communities being posed as threats to safety and not talking about the ones were trying to keep safe interview with Shankar Narayan August th Drawing on our partners input and larger motivation we revisited and expanded the co-design dimension of the Toolkit to center the perspectives of communities with the most at stake in decisions about surveillance and ADS technologies Participatory Design Methods The techniques we adopted from participatory design included informal elicitation sessions as well as design methods more formally applied We drew on contextual inquiry to learn more about partners work and needs asking How do you currently advocate in the area of algorithmic decision systems and What would support your work We also used iterative heuristic evaluation to identify specific ways to improve draft prototypes asking Given the goals that we identified in the last meeting how could these drafts better meet those goals What should this do differently We documented each session and revised the Toolkit in turn After an open-ended needs assessment co-development proceeded more quickly once we began sharing draft prototypes as part of eliciting feedback We also brought in additional civil rights organizations and advocates for three panel sessions using the Diverse Voices method to get input from groups focused on particular lived experiences Diverse Voices panels are a means of enlisting experiential experts to identify potential sources of disparate impact in the way technology policy and other artifacts are designed for a specific stakeholder group The purpose of the panels is not to speak on behalf of a particular group nor in a representative way rather like heuristic evaluation to surface specific problems with the draft or artifact-in-development that can be changed to respond to matters of concern for this group In each minute panel session we used minutes to ask for introductions provide context and describe our co-design process to date We planned minutes for open discussion of the topic of automated decision systems as they relate to their impact on the stakeholder group for which the panel had been convened In the remaining minutes we asked for edits and critiques of the Toolkit asking questions like How would these tools be more useful to you in your work What do these tools not do that you wish they did The experiential experts convened for our three panels addressed the lived experiences of three vantage points immigrants race and social justice activists and formerly incarcerated people The panel on immigrants featured representatives from the Northwest Immigrant Rights Project a legal services nonprofit Mijente a grassroots organization for Latinx movement building La Resistencia an organization led by undocumented people to stop deportation and detention and Community Community Development an organization dedicated to food sovereignty and immigrant rights The panel on formerly incarcerated people featured and took place at Pioneer Human Services a nonprofit organization that assists people reentering society from prison or jail as well as those who are overcoming substance use disorders and mental health issues as well as unaffiliated active community members who were formerly incarcerated The panel on race and social justice activists featured representatives from Black Lives Matter Seattle which organizes for the empowerment and liberation of people of color and the City of Seattle Office of Civil Rights a municipal office dedicated to oversight toward equal access to housing employment public accommodations and contracting Panelists were paid honoraria for their time This measure along with deliberateness about holding panel sessions in convenient locations was intended to make participation more accessible and reciprocal to those involved Institutional Context Other major stakeholders in our work included data scientists and administrative staff at the University of Washingtons eScience Institute Much of the work leading to the present state of the Toolkit happened within the eScience Institutes Data Science for Social Good DSSG summer program and was carried out by DSSG fellows This program provided important resources and support to increase the project capacity and provided ready access to technical expertise on data science and machine learning At the same time the programmatic goals of a data science skills training program introduced within the DSSG context also presented at times competing expectations which we explore in our lessons learned CASE STUDY We present the Algorithmic Equity Toolkit project as a case study of the value of participatory design and action research toward greater fairness accountability and transparency in algorithmic systems We embrace a wider design space inclusive of systems laws community organizations and advocacy campaigns that affords a new perspective to achieve these aims In our case we support public efforts to push for greater community control over public-sector technology Specifically the Algorithmic Equity Toolkit assists public understanding of algorithmic systems for more empowered advocacy and decision-making Toolkit Description The co-design project we report on here is an ongoing effort We are aiming toward a final version of the Toolkit in early We provide a detailed description of the present state of the Toolkit in a companion paper documenting the choices we made in rendering concepts from algorithmic bias and fairness for policy advocacy the substance of partners input and the details of the changes we made in response Here we provide a brief summary of these details for the purposes of grounding our lessons learned from this work The intended users of the Algorithmic Equity Toolkit are community members including civil rights advocacy grassroots organizations and members of the general public Through planning conversations with the ACLU-Washington we set out to address power asymmetries between the public and government agencies in determining what technologies should be implemented in communities in Seattle and Washington In order to integrate into ACLU-Washington and their Tech Fairness Coalitions efforts towards technology oversight laws the Toolkit was aimed for use in organizing and outreach participating in public comment sessions and meetings and assessing the impact of particular technologies At the time of writing the Toolkit had three components illustrated in Figure supporting a user to Determine whether a system is artificial intelligence AI A flowchart for assessing whether a given system relies on machine learning or automated decision-making capabilities Ask tough questions A list of questions for advocates to surface the social context of a given system its technical failure modes ie potential for not working correctly such as producing false positives and its social failure modes ie its potential for discriminatory harms when working correctly This tool is intended for communities to demand answers from policymakers and vendors as to salient risks in algorithmic systems and to use those answers as an input to their policy positions Better understand how machine learning ML works and how it fails An interactive demo of facial recognition that allows the user to make adjustments to the match threshold of a facial recognition system in order to demonstrate how false positive identifications are unevenly distributed with respect to race and gender Through conversations with community organizations we iteratively co-designed and refined the Algorithmic Equity Toolkit As a result of community input we came to reflect on the shortcomings of a narrow focus on bias made materials more accessible to community organizers and community members and provided additional context and examples alongside the tools to support users to reach their own assessments Project Action and Reflection We reflected on this work as it unfolded using the norms of participatory design in which the quality of work is evaluated by users This process is akin to member checking in ethnographic investigations wherein members of the community are asked to assess an ethnographers work In our case we evaluated the Toolkit continuously in consultation with community partners by asking them if it would be of practical use to them in their existing advocacy efforts In the areas where the prototype of a tool fell short we responded by asking how it could be made more informative and most importantly useful to their own objectives This form of evaluation is drawn from a tradition of participatory work that derives its validity through long-term robust interactions and relationships it values practical and experiential knowledge as authoritative Our other goals included i supporting community partners stated priorities by reflecting learning through our collaboration and ii foregrounding partners voices We worked with multiple partners who had different priorities and therefore different success criteria For instance one organization was interested in the Toolkit producing ways to amplify messages through social media to other grassroots organizations Meanwhile technical advisors to the project emphasized the importance of translating complex technical details about how machine learning algorithms work to a broader audience To navigate these contrasting priorities we worked to identify clarify and co-articulate shared priorities and forge common understanding Ultimately we found a shared interest in empowering community organizations during public comment meetings and other coordinated campaigns for algorithmic accountability A shared focus on improving draft Toolkit materials also gave a common focal point to integrate different perspectives While we leveraged HCI evaluation techniques such as user evaluations of prototypes to meet these aims the metrics that flow directly from those evaluations eg did a user correctly interpret x only partially capture what is instrumental to project success at this stage Proof of success for our ongoing work is our partners continued willingness and interest to collaborate Diverse Voices Panels As another part of our evaluative strategy and co-design process we presented the Toolkit to three panels using the Diverse Voices method During these panels activists and members of historically marginalized communities evaluated the Toolkit and provided feedback about what would make it more useful and accessible In addition to targeted feedback on draft prototypes members of these panels asked for additional features such as organizational charts specific to the local municipal government departments to support community implementation of the Toolkit in practice As we discuss in our lessons learned this type of evaluative feedback made an important fact salient that the most meaningful measures towards fairness accountability and transparency are not necessarily technical but are instead informed through contextualized understanding about how a given tool or technology is implemented and contested in practice LESSONS LEARNED Our work has provided us key lessons that may be useful to others adopting participatory methods for fairness accountability and transparency work We arrived at these lessons by assessing the Algorithmic Equity Toolkit project to date against two goals i supporting community advocates in algorithmic accountability campaigns by explaining the risks of automated decision-making systems ADS and making them more legible and ii performing this work in a manner that centered the voices and interests of historically marginalized communities particularly those who have been the primary targets of surveillance We report both successes and challenges in service of these goals a AI Flowchart b Question-Asking Tool c Interactive Demo Figure The three primary components of the Algorithmic Equity Toolkit as of an August version consist of a an identification guide for automated decision systems b a questionnaire on potential harms and c an interactive tool demonstrating algorithmic bias in a face recognition algorithm Reproduced from Katell et al Participatory design requires a means of handling competing input We first note the effectiveness of the participatory design approach toward our goals Our work began with a relatively loosely defined project scope which provided room for us to respond to the priorities of community partners who participated in the project Our interactions with partners were overwhelmingly positive Our key partners expressed enthusiasm about the project early on which encouraged us and helped garner institutional support for the project At the same time our partnering organizations sought features that sometimes contradicted other stakeholder input to make lightweight and ready-at-hand tools For example one organization asked for information about ongoing activist campaigns on ADS and for updated developments to be included with the Toolkit Another organization pointed out the importance of legal resources to be included as a tool There needs to be included in the Toolkit not just an understanding of how algorithms work but what to do if you think your rights or the rights of your family members have been violated Because I think frequently the time people come to us for information is when harms already been done So understanding the tool isnt helpful at that moment they need to know what they can do to find justice for the person that was mistreated So having a coordinated effort with civil rights lawyers or whomever who have a background in this kind of law that can give people information about how to get help Panel with immigrant experiential experts August In some cases these ideas presented competing visions and goals such as a trade-off between completeness and lightweight accessibility The difficulty of managing the contributions of design participants is an expected result of adopting a participatory approach but it also created uncertainty during the development process when competing input would be translated into shareable prototypes As a result our attempts to be responsive to input also impacted the development timeline by re-defining design specifications as they emerged Partners valued more local contextualized materials for their work Another tension at play in our work was between our desire to conduct situated research and the potential for a broader usefulness and constituency for the Toolkit in other jurisdictions While we committed to delving deeply into the Seattle context the frame of surveillance ordinances as operating in multiple municipalities shaped our impression in the background that the Toolkit would be useful in other cities While some conversations with partner organizations did highlight the value in having a Toolkit that could be widely useful in many locales other conversations emphasized that advocates would derive the most value from highly localized materials as opposed to what is scalable beyond a particular policy context Experiential experts pointed out that additional localization to Seattle would make it more useful in their advocacy campaigns In particular the idea arose in multiple panels that the Toolkit should come with a list of technologies known to be in use an organizational chart listing personnel within local government who procure and oversee these technologies their phone numbers and their email addresses To make this more concrete I want to see a diagram of who are the key players Whos making the decision like whos determining whos allocating the money for the departments Because then that helps you a little more to target who these folks are ask questions in terms of racialized outcomes Panel with formerly incarcerated experiential experts August Starting with Seattle We are investing all this money who is making that decision If City Council wants to vote on every piece of software or hardware thats being brought in how are they being educated on its racial implications and impacts Panel with race and social justice activist experiential experts August Within this diagram almost like a diagram of who are the power players Who is making the decision Who is allocating the money Who are the departments Who are the agencies It helps you a little bit more to target who these folks are Panel with race and social justice activist experiential experts August Here these panelists point out that localization would streamline the process of identifying the correct point of contact for public questioning and campaigning For the Toolkit to address this need would potentially require additional resources to keep a highly localized version of the Toolkit updated and easily editable In responding to partners and participants suggested changes we find increased responsiveness to community needs and increased likelihood of its adoption for those users would merit reducing its potential to scale Non-technical measures are powerful steps toward algorithmic accountability We also find that many meaningful interventions toward equitable algorithmic systems are non-technical Where many efforts consider the design and development of a specific algorithm or system our work highlights the role of community organizing public engagement and policy oversight in addressing system failures This lens opens challenging problems such as AI safety and explainability to non-technical approaches For example an approach that highlights known risks and failures could prepare the public and government agencies to better address them For example on another component of the Toolkit a panelist suggested resources on the history of automated systems This would maybe be something that accompanies it even thinking about what has been the history of similar technologies Even recently how have these technologies been used in the past to target and harm communities of color Would be helpful And I think part of that too when we ask where does race come in its not just in the increased police presence it is in determining that gunshots are what we are going to spend our money on Why arent we targeting other crimes So its also a question of whats the data we are valuing and thats telling us how these are racialized at every level Panel with race and social justice activists August Here the panelist highlights how race permeates the history and use of public sector technologies going beyond technical failures These considerations further highlight how organizing work itself can be a crucial non-technical contribution to the fields of machine learning and data science as organizations such as AI Now have pointed out Notably a wave of recent scholarship promotes the ban of face recognition systems eg Hartzog Selinger Stark the increasing uptake of this policy position in US municipalities like Somerville MA San Francisco Oakland and Berkeley CA and Portland OR provides a strong example of the power of nontechnical interventions Nor are these approaches limited to policy interventions in a strict sense At its inception our focus had been on how we could develop materials responsive to our previous findings on the definitional gaps we saw in city government for instance by recommending specific changes to existing municipal government technology procurement and reporting processes Whereas initially we had approached ACLU-Washington about the value we saw at the time of promoting the legibility of public-sector ADS to municipal government employees our partners recommended the primary audience instead be community organizers pursuing ongoing work in this space As a result of this meeting we pivoted the project focus to supporting community advocacy At the same time panelists point out that asking the right questions is resource-intensive for community groups to do alone These are very helpful questions They are so helpful that Im like how can we use them even more Its very useful for activist groups to be asking these questions and documenting the results of it But can we offset that work to a routine government work item So there will be a report and an auditing office cares a lot about their credibility so we can wave that report and say Look at what this found It will have a strong impact Community groups have ups and downs they are not always around People are busy and its harder So thats why Im thinking from that angle Panel with race and social justice activists August Here the panelist conceived of asking the right questions for algorithmic accountability as a form of labor which should ultimately be included as part of conventional government reporting or auditing functions Making systems more fair at the expense of making them more just A key lesson learned came out of conversations about one component of the Toolkit the interactive demonstration of facial recognition and its disproportionate false positive rate for people of color This component illustrated a single dimension of algorithmic harm erroneous results Conversations with our partner organizations and participants emphasized that while such demonstrations are important they risk promoting single axis thinking about algorithmic systems in which a focus on technical errors in a problematic technology diverts attention from the social systems that produce both the technologys inequitable effects and the narratives that justify its use Surveillance systems that work as intended still produce undesirable effects and reproduce patterns of discrimination This point is particularly salient for technologies in the context of law enforcement as Moy explains Police technologies are not adopted in a vacuum or by brand new police agencies with no history of racial inequity Rather police technologies are adopted by existing agencies colored by the same inequities that permeate American society Given these issues we were attentive to feedback we received from our colleagues and community members alike who worried that illustrating technical failures and inaccuracies in algorithmic systems has the potential to distract from the fundamental problems with the deployment of sophisticated surveillance technologies by powerful actors such as government officials We recognize that we must take care not to present algorithmic equity as a problem that can be solved with more accurate surveillance systems Machine learning skills gaps did not present a barrier to receiving meaningful feedback As succinctly stated by the common black box metaphor for algorithmic systems it is difficult even for specialists to understand the reasoning driving operations in machine learning and artificial intelligence These domains defy easy translation to a non-specialist audience However a specialist machine learning background was not necessary for providing feedback on the Toolkit Person Im not very clear on predictive policing and what that does What I see here is that the output is a score its troubling to me because it furthers the idea of a good bad good or bad neighborhood and that impacts people who are poor Person When trying to draw the border around the system itself theres also how the system feeds into itself predictive policing leading to certain neighborhoods and more crimes are found there because more eyes are there That seems like its an important piece to capture in your tool for how to recognize these systems the human element of the inputs to these systems belongs with the other inputs in this diagram Panel on race and social justice activist experiential experts August This panelist went on to add that the Toolkit should also highlight what a non-automated alternative would be to the use of a system For example rather than the use of automated gunshot detectors in neighborhoods community members hearing a gun would call key component of the Toolkit is intended to help non-specialists identify algorithmic systems creating this tool required decisions about what aspects of these systems needed to be explained For example at the time of the Diverse Voices panels the prototype of this tool explained regression classification and clustering methods in machine learning algorithms Through community engagement we learned that these distinctions were not as useful as material that would highlight the salient failure modes of these systems and what community organizations could ask for as an alternative On the bottom it says classification clustering regression right You follow this diagram and youre moving through it and that is the thing you are left with Its like okay then what am I to do with Classification I found myself wanting examples of how these different data and systems have been used to make decisions So going from the inputs to This is the racialized outcome Panel with race and social justice activist experiential experts August The same panelist pointed out that the questions in the Toolkit should emphasize the users of the systems Is there something we can add about who gets to interpret the information and make decisions about it Even if this data was accurate for age gender and race who is using it still determines how they use the information right Panel with race and social justice activist experiential experts August Overall these comments helped us recognize that the Toolkit should emphasize sociotechnical context of systems adoption and use rather than a narrow focus on how the algorithms themselves work We also found that technical background is not required for community participation in technical decisions in a policy setting Traditional data science practice benefits from qualitative and participatory methods A substantial portion of our work on the Algorithmic Equity Toolkit took place within the Data Science for Social Good program which brings together student fellows and domain experts for data intensive projects in the public interest As a result we also reflect on our project as an intervention into traditional data science practice This DSSG program was hosted within the University of Washingtons data science studio at the eScience Institute and was supported by hands-on data science mentors During the year of our work the DSSG program was expanding its vision of what constituted data science and hired a mentor to be an on-site Interpretive Data Scientist This colleague became an important resource for our team deepening the qualitative and reflective dimensions of this work and supporting us in enlarging the scope of who should be considered essential to the practice of data science This arrangement provided invaluable insights and essential resources but at times it was attended by competing visions of what data science is or could be Given that the field conventionally valorizes engineering techniques to a greater degree than understanding the social domain in which data arise and are used our approach does not meet conventional data science learning outcomes In the -week summer program that formed a core component of the development timeline our team spent several weeks in conversations doing background research understanding partner needs and revisiting the design specifications of the work Only the demo component of the Toolkit required software development or machine learning model training and while the demo is modest as an attempt at technological innovation which is the aim of many computational approaches it primarily responds to an unmet need for a particular audience The time spent reflecting on how to provide technical communication for advocacy use on complex and hard-to-explain technologies competed with time that might have spent doing conventional data science Nevertheless we ultimately found that the deliberative process undergirding this project allowed for a deeper engagement critique and problematization of data practices that support the pedagogical objectives of the program CONCLUSION Our work presents lessons learned from a case study of participatory design and action research for fairness accountability and transparency in algorithmic systems To create the Algorithmic Equity Toolkit we worked with community groups to produce materials promoting awareness of algorithmic systems and equip users with strategies to advocate for their communities in the face of surveillance and automated decision systems In so doing we navigated competing input from community partners as to what interventions would be most effective The co-design process pointed toward materials responsive to local needs increasing the likelihood the intervention would be used by project partners Where technical literacy and approaches would at first appear to present barriers to meaningful engagement we find that non-technical perspectives and approaches are uniquely valuable contributions to algorithmic accountability outcomes Most importantly the process of developing Toolkit artifacts engendered questions about the ultimate aim of efforts to make systems more fair We argue for future interventions and the practice of data science more broadly to understand algorithmic systems in their situated context A research process centered on the priorities goals and direct feedback of community partners and experiential experts moves toward fairness reciprocity and accountability in research method itself