Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions An intriguing class of explanations is through counterfactuals hypothetical examples that show people how to obtain a different prediction We posit that effective counterfactual explanations should satisfy two properties feasibility of the counterfactual actions given user context and constraints and diversity among the counterfactuals presented To this end we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes To evaluate the actionability of counterfactuals we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries outperforming prior approaches to generating diverse counterfactuals We provide an implementation of the framework at CONCEPTS Applied computing Law social and behavioral sciences INTRODUCTION Consider a person who applied for a loan and was rejected by the loan distribution algorithm of a financial company Typically the company may provide an explanation on why the loan was rejected for example due to poor credit history However such an explanation does not help the person decide what they should do next to improve their chances of being approved in the future Critically the most important feature may not be enough to flip the decision of the algorithm and in practice may not even be changeable such as gender and race Thus it is equally important to show decision outcomes from the algorithm with actionable alternative profiles to help people understand what they could have done to change their loan decision Similar to the loan example this argument is valid for a range of scenarios involving decision-making on an individuals outcome such as deciding admission to a university screening job applicants disbursing government aid and identifying people at high risk of a future disease In all these cases knowing reasons for a bad outcome is not enough it is important to know what to do to obtain a better outcome in the future assuming that the algorithm remains relatively static Counterfactual explanations provide this information by showing feature-perturbed versions of the same person who would have received the loan eg you would have received the loan if your income was higher by In other words they provide what-if explanations for model output Unlike explanation methods that depend on approximating the classifiers decision boundary counterfactual CF explanations have the advantage that they are always truthful the underlying model by giving direct outputs of the algorithm Moreover counterfactual examples may also be human-interpretable by allowing users to explore what-if scenarios similar to how children learn through counterfactual examples However it is difficult to generate CF examples that are actionable for a persons situation Continuing our loan decision example a CF explanation may suggest to change your house rent but it does not say much about alternative counterfactuals or consider the relative ease between different changes a person may need to make Like any example-based decision support system we need a set of counterfactual examples to help a person interpret a complex machine learning model Ideally these examples should balance between a wide range of suggested changes diversity and the relative ease of adopting those changes proximity to the original input and also follow the causal laws of human society eg one can hardly lower their educational degree or change their race Indeed Russell recognizes the importance of diversity and proposes an approach for linear machine learning classifiers based on integer programming In this work we propose a method that generates sets of diverse counterfactual examples for any differentiable machine learning classifier Extending Wachter et al we construct an optimization problem that considers the diversity of the generated CF examples in addition to proximity to the original input Solving the optimization problem requires considering the tradeoff between diversity and proximity and the tradeoff between continuous and categorical features which may differ in their relative scale and ease of change We provide a general solution to this optimization problem that can generate any number of CF examples for a given input To facilitate actionability our solution is flexible enough to support user-provided inputs based on domain knowledge such as custom weights for individual features or constraints on perturbation of features Further we provide quantitative evaluation metrics for evaluating any set of counterfactual examples Due to their inherent subjectivity CF examples are hard to evaluate While we cannot replace behavioral experiments we propose metrics that can help in fine-tuning parameters of the proposed solution to achieve desired properties of validity diversity and proximity We also propose a second evaluation metric that approximates a behavioral experiment on whether people can understand a ML models decision given a set of CF examples assuming that people would rationally extrapolate from the CF examples and guess the local decision boundary of an ML model We evaluate our method on explaining ML models trained on four datasets COMPAS for bail decision Adult-Income for income prediction German-Credit for assessing credit risk and a dataset from Lending Club for loan decisions Compared to prior CF generation methods our proposed solution generates CF examples with substantially higher diversity for these datasets Moreover a simple -nearest neighbor model trained on the generated CF examples obtains comparable accuracy on locally approximating the original ML model to methods like LIME which are directly optimized for estimating the local decision boundary Notably our method obtains higher score on predicting instances in the counterfactual outcome class than LIME in most configurations especially for Adult-Income and COMPAS datasets wherein both precision and recall are higher Qualitative inspection of the generated CF examples illustrates their potential utility for making informed decisions Additionally CF explanations can expose biases in the original ML model as we see when some of the generated explanations suggest changes in sensitive attributes like race or gender The last example illustrates the broad applicability of CF explanations they are not just useful to an end-user but can be equally useful to model builders for debugging biases and for fairness evaluators to discover such biases and other model properties Still CF explanations as generated suffer from lack of any causal knowledge about the input features that they modify Features do not exist in a vacuum they come from a data-generating process which constrains their modification Thus perturbing each input feature independently can lead to infeasible examples such as suggesting someone to obtain a higher degree but reduce their age To ensure feasibility we propose a filtering approach on the generated CF examples based on causal constraints To summarize our work makes the following contributions We propose diversity as an important component for actionable counterfactuals and build a general optimization framework that exposes the importance of necessary tradeoffs causal implications and optimization issues in generating counterfactuals We propose a quantitative evaluation framework for counterfactuals that allows fine-tuning of the proposed method for a particular scenario and enables comparison of CF-based methods to other local explanation methods such as LIME Finally we demonstrate the effectiveness of our framework through empirical experiments on multiple datasets and provide an open-source implementation at BACKGROUND RELATEDWORK Explanations are critical for machine learning especially as machine learning-based systems are being used to inform decisions in societally critical domains such as finance healthcare education and criminal justice Since many machine learning algorithms are black boxes to end users and do not provide guarantees on input-output relationship explanations serve a useful role to inspect these models Besides helping to debug ML models explanations are hypothesized to improve the interpretability and trustworthiness of algorithmic decisions and enhance human decision making Below e focus on approaches that provide post-hoc explanations of machine learning models and discuss why diversity should be an important component for counterfactual explanations There is also an important line of work that focuses on developing intelligible models by assuming that simple models such as linear models or decision trees are interpretable Explanation through Feature Importance An important approach to post-hoc explanations is to determine feature importance for a particular prediction through local approximation Ribeiro et al propose a feature-based approach LIME that fits a sparse linear model to approximate non-linear models locally Guidotti et al extend this approach by fitting a decision-tree classifier to approximate the non-linear model and then tracing the decision-tree paths to generate explanations Similarly Lundberg and Lee present a unified framework that assigns each feature an importance value for a particular prediction Such explanations however lie about the machine learning models There is an inherent tradeoff between truthfulness about the model and human interpretability when explaining a complex model and so explanation methods that use proxy models inevitably approximate the true model to varying degrees Similarly global explanations can be generated by approximating the true surface with a simpler surrogate model and using the simpler model to derive explanations A major problem with these approaches is that since the explanations are sourced from simpler surrogates there is no guarantee that they are faithful to the original model Explanation through Visualization Similar to identifying feature importance visualizing the decision of a model is a common technique for explaining model predictions Such visualizations are commonly used in the computer vision community ranging from highlighting certain parts of an image to activations in convolutional neural networks However these visualizations can be difficult to interpret in scenarios that are not inherently visual such as recidivism prediction and loan approvals which are the cases that our work focuses on Explanation through Examples The most relevant class of explanations to our approach is through examples An example-based explanation framework is MMD-critic proposed by Kim et al which selects both prototypes and criticisms from the original data points More recently counterfactual explanations are proposed as a way to provide alternative perturbations that would have changed the prediction of a model In other words given an input feature x and the corresponding output by ML model The trained model obtained from the training data Original input x The feature vector associated with an instance of interest that receives an unfavorable decision from the ML model Original outcome The prediction of the original input from the trained model usually corresponding to the undesired class Original outcome class The undesired class Counterfactual example c i An instance and its feature vector close to the original input that would have received a favorable decision from the ML model CF class The desired class Table Terminology used throughout the paper a ML model a counterfactual explanation is a perturbation of the input to generate a different output y by the same algorithm Specifically Wachter et al propose the following formulation c argmin c y loss x c where the first part y loss pushes the counterfactual c towards a different prediction than the original instance and the second part keeps the counterfactual close to the original instance Extending their work we provide a method to construct a set of counterfactuals with diversity In other domains of information search such as search engines and recommendation systems multiple studies show the benefits of presenting a diverse set of information items to a user Our hypothesis is that diversity can be similarly beneficial when people are shown counterfactual explanations For linear models a recent paper by Russell develops an efficient algorithm to find diverse counterfactuals using integer programming In this work we examine an alternative formulation that works for any differentiable model investigate multiple practical issues on different datasets and propose a general quantitative evaluation framework for diverse counterfactuals COUNTERFACTUAL GENERATION ENGINE The input of our problem is a trained machine learning model and an instance x We would like to generate a set of counterfactual examples cc such that they all lead to a different decision than x The instance x and all CF examples cc are d-dimensional Throughout the paper we assume that the machine learning model is differentiable and static does not change over time and that the output is binary Table summarizes the main terminologies used in the paper Our goal is to generate an actionable counterfactual set that is the user should be able to find CF examples that they can act upon To do so we need individual CF examples to be feasible with respect to the original input but also need diversity among the generated counterfactuals to provide different ways of changing the outcome class Thus we adapt diversity metrics to generate diverse counterfactuals that can offer users multiple options Section At the same time we incorporate feasibility using the proximity constraint from Wachter et al and introduce other user-defined constraints Finally we point out that counterfactual generation is a post-hoc procedure distinct from the standard machine learning setup and discuss related practical issues Section Diversity and Feasibility Constraints Although diverse CF examples increase the chances that at least one example will be actionable for the user examples may end up changing a large set of features or maximize diversity by considering big changes from the original input This situation could be worsened when features are high-dimensional We thus need a combination of diversity and feasibility as we formulate below Diversity via Determinantal Point Processes We capture diversity by building on determinantal point processes DPP which has been adopted for solving subset selection problems with diversity constraints We use the following metric based on the determinant of the kernel matrix given the counterfactuals dpp_diversity where Ki c i c and c denotes a distance metric between the two counterfactual examples In practice to avoid ill-defined determinants we add small random perturbations to the diagonal elements for computing the determinant Proximity Intuitively CF examples that are closest to the original input can be the most useful to a user We quantify proximity as the negative vector distance between the original input and CF examples features This can be specified by a distance metric such as ℓ-distance optionally weighted by a user-provided custom weight for each feature Proximity of a set of counterfactual examples is the mean proximity over the set Proximity i x Sparsity Closely connected to proximity is the feasibility property of sparsity how many features does a user need to change to transition to the counterfactual class Intuitively a counterfactual example will be more feasible if it makes changes to fewer number of features Since this constraint is non-convex we do not include it in the loss function but rather handle it through modifying the generated counterfactuals as explained in Section User constraints A counterfactual example may be close in feature space but may not be feasible due to real world constraints Thus it makes sense to allow the user to provide constraints on feature manipulation They can be specified in two ways First as box constraints on feasible ranges for each feature within which CF examples need to be searched An example of such a constraint is income cannot increase beyond Alternatively a user may specify the variables that can be changed In general feasibility is a broad issue that encompasses many facets We further examine a novel feasibility constraint derived from causal relationships in Section Optimization Based on the above definitions of diversity and proximity we consider a combined loss function over all generated counterfactuals argmin c c i y loss y i x dpp_diversity c where is a counterfactual example CF is the total number of CFs to be generated is the ML model a black box to end users y loss is a metric that minimizes the distance between s prediction for s and the desired outcome y usually in our experiments d is the total number of input features x is the original input and dpp_diversity is the diversity metric and are hyperparameters that balance the three parts of the loss function Implementation We optimize the above loss function using gradient descent Ideally we can achieve y for every counterfactual but this may not always be possible because the objective is non-convex We run a maximum of steps or until the loss function converges and the generated counterfactual is valid belongs to the desired class We initialize all randomly Practical considerations Important practical considerations need to be made for such counterfactual algorithms to work in practice since they involve multiple tradeoffs in choosing the final set Here we describe four such considerations While these considerations might seem trivial from a technical perspective we believe that they are important for supporting user interaction with counterfactuals Choice of y loss An intuitive choice of y loss may be ℓ-loss y c or ℓ-loss However these loss functions penalize the distance of c from the desired y whereas a valid counterfactual only requires that c be greater or lesser than threshold typically not necessarily the closest to desired y or In fact optimizing for c to be close to either or encourages large changes to x towards the counterfactual class which in turn make the generated counterfactual less feasible for a user Therefore we use a hinge-loss function that ensures zero penalty as long as c is above a fixed threshold above when the desired class is and below a fixed threshold when the desired class is Further it imposes a penalty proportional to difference between c and when the classifier is correct but within the threshold and a heavier penalty when c does not indicate the desired counterfactual class Specifically the hinge-loss is loss max z c where z is when y and when y and c is the unscaled output from the ML model eg final logits that enter a softmax layer for making predictions in a neural network Choice of distance function For continuous features we define as the mean of feature-wise distances between the CF example and the original input Since features can span different ranges we divide each feature-wise distance by the median absolute deviation MAD of the features values in the training set following Wachter et al Deviation from the median provides a robust measure of the variability of a features values and thus dividing by the MAD allows us to capture the relative prevalence of observing the feature at a particular value p where is the number of continuous variables and is the median absolute deviation for the continuous variable For categorical features however it is unclear how to define a notion of distance While there exist metrics based on the relative frequency of different categorical levels for a feature in available data they may not correspond to the difficulty of changing a particular feature For instance irrespective of the relative ratio of different education levels eg high school or bachelors it is quite hard to obtain a new educational degree compared to changes in other categorical features We thus use a simpler metric that assigns a distance of if the CF examples value for any categorical feature differs from the original input otherwise it assigns zero p I where is the number of categorical variables Relative scale of features In general continuous features can have a wide range of possible values while typical encoding for categorical features constrains them to a one-hot binary representation Since the scale of a feature highly influences how much it matters in our objective function we believe that the ideal solution is to provide interactive interfaces to allow users to input their preferences across features As a sensible default however we transform all features to Continuous features are simply scaled between and For categorical features we convert each feature using one-hot encoding and consider it as a continuous variable between and Also to enforce the one-hot encoding in the learned counterfactuals we add a regularization term with high penalty for each categorical feature to force its values for different levels to sum to At the end of the optimization we pick the level with maximum value for each categorical feature Enhancing Sparsity While our loss function minimizes the distance between the input and the generated counterfactuals an ideal counterfactual needs to be sparse in the number of features it changes To encourage sparsity in a generated counterfactual we conduct a post-hoc operation where we restore the value of continuous features back to their values in x greedily until the predicted class c changes For this operation we consider all continuous features c whose difference from x is less than a chosen threshold Although an intuitive threshold is the median absolute distance MAD the MAD can be fairly large for features with large variance Therefore for each feature we choose the minimum of MAD and the bottom percentile of the absolute difference between non-identical values from the median Hyperparameter choice Since counterfactual generation is a post-hoc step after training the ML model it is not necessarily required that we use the same hyperparameter for every original input However since hyperparameters can influence the generated counterfactuals it seems problematic if users are given counterfactuals generated by different hyperparameters In this work therefore we choose and based on a grid-search with different values and evaluating the diversity and proximity of generated CF examples In general whether the explanation algorithm should be uniform is a fundamental issue for providing post-hoc explanations of algorithmic decisions and it likely depends on the nature of such explanations EVALUATING COUNTERFACTUALS Despite recent interest in counterfactual explanations the evaluations are typically only done in a qualitative fashion In this section we present metrics for evaluating the quality of a set of counterfactual examples As stated in Section it is desirable that a method produces diverse and proximal examples and that it can generate valid counterfactual examples for all possible inputs Ultimately however the examples should help a user in understanding the local decision boundary of the ML classifier Thus in addition to diversity and proximity we propose a metric that approximates the notion of a users understanding We do so by constructing a secondary model based on the counterfactual examples that acts as a proxy of a users understanding and compare how well it can mimic the ML classifiers decision boundary Nevertheless it is important to emphasize that CF examples are eventually evaluated by end users The goal of this work is to provide metrics that pave the way towards meaningful human subject experiments and we will offer further discussion in Section Validity Proximity and Diversity First we define quantitative metrics for validity diversity and proximity for a counterfactual set that can be used to evaluate any method for generating counterfactuals We assume that a set C of counterfactual examples are generated for an original input Validity Validity is simply the fraction of examples returned by a method that are actually counterfactuals That is they correspond to a different outcome than the original input Here we consider only unique examples because a method may generate multiple examples that are identical to each other Valid-CFs unique instances in C st c Proximity We define distance-based proximity separately for continuous and categorical features Using the definition metrics from Equations and we define proximity as Continuous-Proximity i x Categorical-Proximity i x That is we define proximity as the mean of feature-wise distances between the CF example and the original input Proximity for a set of examples is simply the average proximity over all the examples Note that the above metric for continuous proximity is slightly different than the one used during CF generation During CF generation we transform continuous features to for reasons discussed in Section but we use the features in their original scale during evaluation for better interpretability of the distances Sparsity While proximity quantifies the average change between a CF example and the original input we also measure another related property sparsity that captures the number of features that are different We define sparsity as the number of changes between the original input and a generated counterfactual Sparsity i d l l i where d is the number of input features For clarity we can also define sparsity separately for continuous and categorical features where Categorical-Proximity is identical to Categorical-Sparsity Note that greater values of sparsity and proximity are desired Diversity Diversity of CF examples can be evaluated in an analogous way to proximity Instead of feature-wise distance from the original input we measure feature-wise distances between each pair of CF examples thus providing a different metric for evaluation than the loss formulation from Equation Diversity for a set of counterfactual examples is the mean of the distances between each pair of examples Similar to proximity we compute separate diversity metrics for categorical and continuous features Diversity i c where is either or In addition we define an analagous sparsity-based diversity metric that measures the fraction of features that are different between any two pair of counterfactual examples Count-Diversity kd i d l l It is important to note that the evaluation metrics used here are intentionally different from Equation so there is no guarantee that our generated counterfactuals would do well on all these metrics especially on the sparsity metric which is not optimized explicitly in CF generation In addition given the trade-off between diversity and proximity no method will be able to maximize both Therefore evaluation of a counterfactual set will depend on the relative merits of diversity versus proximity for a particular application domain Approximating the local decision boundary The above properties are desirable but ideally we would like to evaluate whether the examples help a user in understanding the local decision boundary of the ML model As a tool for explanation counterfactual examples help a user intuitively explore specific points on the other side of the ML models decision boundary which then help the user to guess the workings of the model To construct a metric for the accuracy of such guesses we approximate a users guess with another machine learning model that is trained on the generated counterfactual examples and the original input Given this secondary model we can evaluate the effectiveness of counterfactual examples by comparing how well the secondary model can mimic the original ML model Thus considering the secondary model as a best-case scenario of how a user may rationally extrapolate counterfactual examples we obtain a proxy for how well a user may guess the local decision boundary Specifically given a set of counterfactual examples and the input example we train a -nearest neighbor -NN classifier that predicts the output class of any new input Thus an instance closer to any of the CF examples will be classified as belonging to the desired counterfactual outcome class and instances closer to the original input will be classified as the original outcome class We chose -NN for its simplicity and connections to peoples decisionmaking in the presence of examples We then evaluate the accuracy of this classifier against the original ML model on a dataset of simulated test data To generate the test data we consider samples of increasing distance from the original input Consistent with training we scale distance for continuous features by dividing it by the median absolute deviation MAD for each feature Then we construct a hypersphere centered at the original input that has dimensions equal to the number of continuous features Within this hypersphere we sample feature values uniformly at random For categorical features in the absence of a clear distance metric we uniformly sample across the range of possible levels In our experiments we consider spheres with radiuses as multiples of the MAD r MAD For each original input we sample points at random per sphere to evaluate how well the secondary -NN model approximates the local decision boundary Note that this -NN classifier is trained from a handful of CF examples and we intentionally choose this simple classifier to approximate what a person could have done given these CF examples Datasets To evaluate our method we consider the following four datasets Adult-Income This dataset contains demographic educational and other information based on Census database and is available on the UCI machine learning repository We preprocess the data based on a previous analysis and obtain features namely hours per week education level occupation work class race age marital status and sex The ML models task is to classify whether an individuals income is over LendingClub This dataset contains five years data on loans given by LendingClub an online peer-to-peer lending company We preprocess the data based on previous analyses and obtain features namely employment years annual income number of open credit accounts credit history loan grade as decided by LendingClub home ownership purpose and the state of residence in the United States The ML models task is to decide loan decisions based on a prediction of whether an individual will pay back their loan German-Credit This dataset contains information about individuals who took a loan from a particular bank We use all the features in the data including several demographic attributes and credit history without any preprocessing The ML models task is to determine whether the person has a good or bad credit risk based on their attributes COMPAS This dataset was collected by ProPublica as a part of their analysis on recidivism decisions in the United States We preprocess the data based on previous work and obtain features namely bail applicants age gender race prior count of offenses and degree of criminal charge The ML models task is to decide bail based on predicting which of the bail applicants will recidivate in the next two years These datasets contain different numbers of continuous and categorical features as shown in Table COMPAS dataset has a single continuous feature while Adult-Income LendingClub and German-Credit have and continuous features respectively For all three datasets we transform categorical features by using one-hot-encoding as described in Section Continuous features are scaled between and To obtain an ML model to explain Dataset Linear Non-linear cat Adult-Income LendingClub German-Credit COMPAS Table Model accuracy and feature information we divide each dataset into train and test sets and use cross-validation on the train set to optimize hyperparameters To facilitate comparisons with Russell we use TensorFlow library to train both a linear logistic regression classifier and a non-linear neural network model with a single hidden layer Table shows the test set accuracy on each dataset and the modelling details are in Supplementary Materials Baselines We employ the following baselines for generating CF examples SingleCF We follow Wachter et al and generate a single CF example optimizing for y-loss difference and proximity Mixed Integer CF We use the mixed integer programming method proposed by Russell for generating diverse counterfactual examples This method works only for a linear model RandomInitCF Here we extend SingleCF to generate CF examples by initializing the optimizer independently with random starting points from d Since the optimization loss function is non-convex one might obtain different CF examples NoDiversityCF This method utilizes our proposed loss function that optimizes the set of examples simultaneously Equation but ignores the diversity term by setting To these baselines we compare our proposed method DiverseCF that generates a set of counterfactual examples and optimizes for both diversity and proximity As with RandomInitCF we initialize the optimizer with random starting points In addition we consider a variant DiverseCF-Sparse that performs post-hoc sparsity enhancement on continuous features as described in Section Similarly for RandomInitCF and NoDiversityCF we include results both with and without the sparsity correction For all methods we use the ADAM optimizer implementation in TensorFlow learning rate to minimize the loss and obtain CF examples In addition we compare DiverseCF to one of the major feature-based local explanation methods LIME on how well it can approximate the decision boundary We construct a -NN classifier for each set of CF examples as described in Section For LIME we use the prediction of the linear model for each input instance as a local approximation of the ML models decision surface Note that our -NN classifiers are based on only counterfactuals while LIMEs linear classifiers are based on samples EXPERIMENT RESULTS In this section we show that our approach generates a set of more diverse counterfactuals than the baselines according to the proposed evaluation metrics We further present examples for a qualitative overview and show that the generated counterfactuals can approximate local decision boundaries as well as LIME an explanation method specifically designed for local approximation Ad ul nc om e Le nd in gC b Ge term an re di t M PA S CFs Valid CFs CFs Categorical-Diversity CFs Continuous-Diversity CFs Cont-Count-Diversity CFs Categorical-Proximity Continuous-Proximity CFs Continuous-Sparsity DiverseCF DiverseCF-Sparse NoDiverseCF NoDiverseCF-Sparse RandomInitCF RandomInitCF-Sparse SingleCF Figure Comparisons of DiverseCF with baseline methods on Valid CFs diversity proximity and sparsity SingleCF only shows up for Ally-axes are defined so that higher values are better while the x-axis represents the number of requested counterfactuals DiverseCF finds a greater number of valid unique counterfactuals for each and tends to generate more diverse counterfactuals than the baseline methods For COMPAS dataset none of the baselines could generate CF examples for any original input therefore we only show results for DiverseCF in COMPAS when Quantitative Evaluation We first evaluate DiverseCF based on quantitative metrics of valid CF generation diversity and proximity As described in Section we report results with hyperparameters and from Equation Figure shows the comparison with SingleCF RandomInitCF and NoDiversityCF for explaining the non-linear ML models while Figure compares with MixedIntegerCF for explaining the linear ML models All results are based on random instances from the test set Explaining a non-linear ML model Figure Given that nonlinear ML models are common in real world applications we focus our discussion on explaining non-linear models Validity Across all four datasets we find that DiverseCF generates nearly valid CF examples for all values of the requested number of examples Baseline methods without an explicit diversity objective can generate valid CF examples for but their percentage of unique valid CFs decreases as increases Among the datasets we find that it is easier to generate valid CF examples for LendingClub RandomInitCF also achieves validity while COMPAS is the hardest likely driven by the fact that it has only one continuous feature prior count of offenses As an example at for COMPAS a majority of the CFs generated by the next best method RandomInitCF are either duplicate or invalid Diversity Among the valid CFs DiverseCF also generates more diverse examples than the baseline methods for both continuous and categorical features For all datasets Continuous-Diversity for DiverseCF is the highest and increases as increases reaching up to eleven times the baselines for the LendingClub dataset at Among categorical features average number of different features between CF examples is higher for all datasets than baseline methods especially for Adult-Income and LendingClub datasets where Categorical-Diversity remains close to zero for baseline methods Remarkably DiverseCF has the highest number of continuous features changed too Cont-Count-Diversity even though it was not explicitly optimized for this metric The only exception is on COMPAS data where NoDiversityCF has a slightly higher Cont-Count-Diversity for but is unable to generate any valid CFs for higher Proximity To generate diverse CF examples DiverseCF searches a larger space than proximity-only methods such as RandomInitCF or NoDiversityCF As a result DiverseCF returns examples with lower proximity than other methods indicating an inherent tradeoff between diversity and proximity However for categorical features the difference in proximity compared to baselines is small up to of the baselines proximity Higher proximity over continuous Ad ul nc om e Le nd in gC b M PA S CFs Valid CFs CFs Categorical-Diversity Continuous-Diversity CFs Cont-Count-Diversity CFs Categorical-Proximity Continuous-Proximity CFs Continuous-Sparsity DiverseCF DiverseCF-Sparse MixedIntegerCF Figure Comparisons of DiverseCF with MixedIntegerCF on Valid CFs diversity proximity and sparsity on linear ML models For a fair comparison we compute average metrics only over the original inputs where MixedIntegerCF returned the required number of CF examples Thus we omit results when for COMPAS since MixedIntegerCF could not find more than four CFs for any original input Results for German-Credit are in the Supplementary Materials features can be obtained by adding the post-hoc sparsity enhancement DiverseCF-Sparse which results in higher sparsity than DiverseCF for all datasets but correspondingly lower count-based diversity Thus this method can be used to fine-tune DiverseCF towards more proximity if desired Explaining linear ML models Figure To compare our methods with MixedIntegerCF we explain a linear ML model for each dataset Similar to the results on non-linear models DiverseCF outperforms MixedIntegerCF by finding valid counterfactuals and the gap with MixedIntegerCF increases as increases We also find that DiverseCF has consistently higher diversity among counterfactuals than MixedIntegerCF for all datasets Importantly better diversity in the CFs from DiverseCF does not come at the price of proximity For Adult-Income and LendingClub datasets DiverseCF has better proximity and sparsity than MixedIntegerCF Qualitative evaluation To understand more about the resultant explanations we look at sample CF examples generated by DiverseCF with sparsity in Table In the three datasets the examples capture some intuitive variables and vary them Education in Adult-Income Income in LendingClub dataset and PriorsCount in COMPAS In addition the user also sees other features that can be varied for the desired outcome For example in the COMPAS input instance a person would have been granted bail if they had been a Caucasian or charged with Misdemeanor instead of Felony These features do not really lead to actionable insights because the subject cannot easily change them but nevertheless provide the user an accurate picture of scenarios where they would have been out on bail and also raise We skip German-Credit for space reasons questions about potential racial bias in the ML model itself In practice we expect that a domain expert or the user may provide unmodifiable features which DiverseCF can treat as constants in the counterfactual generation process Similarly in the Adult-Income dataset the set of counterfactuals show that studying for an advanced degree can lead to a higher income but also shows less obvious counterfactuals such as getting married for a higher income in addition to finishing professional school and increasing hours worked per week These counterfactuals are likely generated due to underlying correlations in the dataset married people having higher income To counter such correlational outcomes and preserve known causal relationships we present a post-hoc filtering method in Section These qualitative examples also confirm our observation regarding sparsity and the choice of the y loss function Continuous variables in counterfactual examples eg income in LendingClub never change to their maximum extreme values thanks to the hinge loss which was an issue using other y loss metrics such as loss Furthermore it does not require changing a large number of features to achieve the desired outcome However based on the domain and use case a user may prioritize changing certain variables or desire more sparse or more diverse CF examples As we described in Section these variations can be achieved by appropriately tuning weights on features and the learning rate for optimization Overall these initial set of CF examples help understand the important variations as learned by the algorithm We expect the user to engage their actionability constraints with this initial set to iteratively generate focused CF examples that can help find useful variations In addition these examples can also expose biases or odd edge-cases in the ML model which can be useful for model builders in debugging or for fairness evaluators in discovering bias Adult Education Occupation Work Class Race MaritalStat Sex Original input outcome HS-grad Service Private White Single Female Masters Married Male Counterfactuals Doctorate Self-Employed outcome White-Collar Married Prof-school Married LendingClub Inc Ac Loan Grade HomeOwner Purpose State Original input outcome Default D Mortgage Debt NY B Purchase Counterfactuals A TX outcome Paid A A Rent COMPAS PriorsCount Crime Degree Race Age Sex Original input outcome Will Recidivate Felony African-American Female Caucasian Counterfactuals Male outcome Wont Recidivate Hispanic Misdemeanor Table Examples of generated counterfactuals in Adult-Income LendingClub and COMPAS datasets Ad ul nc om e Le nd in gC b Ge term an re di t CO M PA S CFs MAD CFs MAD CFs MAD DiverseCF CF_class NoDiverseCF CF_class RandomInitCF CF_class LIME CF_class a score of the counterfactual class Ad ul nc om e Le nd in gC b Ge term an re di t CO M PA S CFs MAD DiverseCF CF_class NoDiverseCF CF_class RandomInitCF CF_class LIME CF_class b Precision Ad ul nc om e Le nd in gC b Ge term an re di t CO M PA S CFs MAD DiverseCF CF_class NoDiverseCF CF_class RandomInitCF CF_class LIME CF_class c Recall Figure Performance of -NN classifiers learned from counterfactuals at different distances from the original input DiverseCF outperforms LIME and baseline CF methods in score on correctly predicting the counterfactual class except in LendingClub dataset For Adult-Income and COMPAS datasets both precision and recall is higher for DiverseCF compared to LIME Approximating local decision boundary As a proxy for understanding how well users can guess the local decision boundary of the ML model see Section we compare classifiers based on the proposed DiverseCF method baseline methods and LIME We use precision recall and for the counterfactual outcome class Figure as our main evaluation metrics because of the class imbalance in data points near the original input To evaluate the sensitivity of these metrics to varying distance from the original input we show these metrics for points sampled within varying distance thresholds Even with a handful of training examples generated counterfactuals and the original input we find that -NN classifiers trained on the output of DiverseCF obtain higher score than the LIME classifier in most configurations For instance on the CFs C ou nd All Education Levels CFs Education Masters Prof-school Doctorate Any Change in Education Decrease in Education infeasible Increase in Education feasible Increase in Education infeasible Figure Post-hoc filtering of CF examples based on causal constraints The left figure shows that there are nearly CFs that include any change in education out of which more than one-third are infeasible If we filter to only people with higher degrees almost half of the changes in educational degrees are infeasible Adult-Income dataset at and MAD threshold DiverseCF obtains while LIME obtains This result stays consistent as we increase or the distance from the original input One exception is LendingClub at MAD and where the score for DiverseCF drops below LIME Figures b and c indicate that this drop is due to low recall for DiverseCF at this configuration Still precision remains substantially higher for DiverseCF compared to for LIME This observation is likely because LIME predicts a majority of the instances as the CF class for this dataset whereas DiverseCF has fewer false positives On Adult-Income and COMPAS datasets DiverseCF achieves both higher precision and recall than LIME As for the difference between different methods of generating counterfactuals DiverseCF tend to perform similarly with NoDiversityCF and RandomInitCF in terms of except in LendingClub An advantage of DiverseCF is that it can handle high values of for which NoDiversityCF and RandomInitCF cannot find unique and valid counterfactuals Another intriguing observation is that the performance improves and saturates very quickly as the number of counterfactuals increases which suggests that two counterfactuals may be sufficient for a -NN classifier to get a reasonable idea of the data distribution around x in these four datasets This observation may also be why DiverseCF provides similar score compared to baselines and merits further study on more complex datasets Overall these results show that examples from DiverseCF can approximate the local decision boundary at least as well as local explanation methods like LIME Still the gold-standard test will be to conduct a behavioral study where people evaluate whether CF examples provide better explanation than past approaches which we leave for future work CAUSAL FEASIBILITY OF CF EXAMPLES So far we have generated CF examples by varying each feature independently However this can lead to infeasible examples since many features are causally associated with each other For example in the loan application it can be almost impossible for a person to obtain a higher educational degree without spending time aging Consequently while being valid diverse and proximal such a CF example is not feasible and thus not actionable by the person In this context we argue that incorporating causal models of data generation is important to prevent infeasible counterfactuals Here we present a simple way of incorporating causal knowledge in our proposed method Users can provide their domain knowledge in the form of pairs of features and the direction of the causal edge between them Using this we construct constraints that any counterfactual should follow For instance any counterfactual that changes the cause without changing its outcome is infeasible Given these constraints we apply a filtering step after CF examples are generated to increase the feasibility of the output CF set As an example we consider two infeasible changes based on the causal relationship between educational level and age Education Age and on the practical constraint that educational level of a person cannot be decreased Education As Figure shows over one-third of the obtained counterfactuals that include a change in education level are infeasible and need to be filtered most of them suggest an individual to obtain a higher degree but do not increase their age In fact this fraction increases as we look at CF examples for highly educated people Masters Doctorate and Professional as high as of all CFs suggest to switch to a lower education degree Though post-hoc filtering can ensure feasibility of the resultant CF examples it is more efficient to incorporate causal constraints during CF generation We leave this for future work CONCLUDING DISCUSSION Building upon prior work on counterfactual explanations we proposed a framework for generating and evaluating a diverse and feasible set of counterfactual explanations We demonstrated the benefits of our method compared to past approaches on being able to generate a high number of unique valid and diverse counterfactuals for a given input for any machine learning model Here we note directions for future work First our method assumes knowledge of the gradient of the ML model It is useful to construct methods that can work for fully black-box ML models Second we would like to incorporate causal knowledge during the generation of CF examples rather than as a post-hoc filtering step Third as we saw in it is important to understand peoples preferences with respect to what additional constraints to add to our framework Providing an intuitive interface to select scales of features and add constraints and conducting behavioral experiments to support interactive explorations can greatly enhance the value of CF explanation It will also be interesting to study the tradeoff between diversity and the cognitive cost of making a choice choice overload as the number of CF explanations is increased Finally while we focused on the utility for an end-user who is the subject of a ML-based decision we argue that CF explanations can be useful for different stakeholders in the decision making process including model designers decision-makers such as a judge or a doctor and decision evaluators such as auditors