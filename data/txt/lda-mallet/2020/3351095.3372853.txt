POTs Protective Optimization Technologies Algorithmic fairness aims to address the economic moral social and political impact that digital systems have on populations through solutions that can be applied by service providers Fairness frameworks do so in part by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures Not surprisingly these decisions limit fairness frameworks ability to capture a variety of harms caused by systems We characterize fairness limitations using concepts from requirements engineering and from social sciences We show that the focus on algorithms inputs and outputs misses harms that arise from systems interacting with the world that the focus on bias and discrimination omits broader harms on populations and their environments and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial We propose Protective Optimization Technologies POTs POTs provide means for affected parties to address the negative impacts of systems in the environment expanding avenues for political contestation POTs intervene from outside the system do not require service providers to cooperate and can serve to correct shift or expose harms that systems impose on populations and their environments We illustrate the potential and limitations of POTs in two case studies countering road congestion caused by traffic-beating applications and recalibrating credit scoring for loan applicants CONCEPTS Social and professional topics Socio-technical systems KEYWORDS Fairness and Accountability Protective Optimization Technologies Advances in computational power software engineering and machine learning algorithms have been instrumental in the rise of digital systems Their ubiquity in our everyday activities raises concerns regarding the centralization of decisional power These concerns are amplified by the opaque and complex nature of these systems which results in hard-to-explain outputs and unjust outcomes for historically marginalized populations Computer scientists counter these inequities through frameworks studied under the rubric of fairness using a variety of formal fairness notions Their results have been instrumental in our understanding of the discriminatory effects of algorithmic decisions The frameworks however rely on narrowing the inequity problem to primarily consider the discriminatory impact of algorithms and assuming trustworthy providers The narrow view has enabled valuable breakthroughs centered around the service providers ability to address some inequities but fails to capture broader harms or explore other ways to contest service-provider power In this work we investigate digital systems from a new perspective in order to understand how to address the broader class of harms that they cause To achieve this we characterize the type of systems in which algorithms are integrated and deployed These systems typically build on distributed service architectures and incorporate real-time feedback from both users and third-party service providers This feedback is leveraged for a variety of novel forms of optimization that are geared towards extraction of value through the system Typically optimization is used for technical performance and minimizing costs eg by optimizing cloud usage orchestration or allocation of hardware resources It has also become part and parcel of continuous development strategies based on experimentation that allow developers to define dynamic objective functions and build adaptive systems Businesses can now design for ideal interactions and environments by optimizing feature selection behavioral outcomes and planning that is in line with a business growth strategy We argue that optimization-based systems are developed to capture and manipulate behavior and environments for the extraction of value As such they introduce broader risks and harms for users and environments beyond the outcome of a single algorithm within that system These impacts go beyond the bias and discrimination stemming from algorithmic outputs fairness frameworks consider Borrowing concepts and techniques from software and requirements engineering and from economics and social sciences we characterize the limitations of current fairness approaches in capturing and mitigating harms and risks arising from optimization Among others we show that focusing on the algorithms and their outputs overlooks the many externalities caused by optimizing every aspect of a system that discrimination is only one of the injustices that can arise when systems are designed to maximize gain and that ignoring service providers incentives and capabilities to enforce proposed solutions limits our understanding of their operation and further consolidates the power providers have regarding decisions and behaviors that have profound effects in society Finally we propose Protective Optimization Technologies POTs which aim at addressing risks and harms that cannot be captured from the fairness perspective and cannot be addressed without a cooperative service provider The ultimate goal of POTs is to eliminate the harms or at least mitigate them When these are not possible POTs can shift harms away from affected users or expose abusive or non-social practices by service providers We illustrate the potential of POTs to address externalities of optimization-based systems in two case studies traffic-beating routing applications and credit scoring We also identify numerous techniques developed by researchers in the fields of security and privacy by artists and by others that though not necessarily designed to counter externalities can be framed as POTs FAIRNESS FROM A SYSTEMS PERSPECTIVE We introduce Michael A Jacksons theory of requirements engineering to discuss the focus goals and assumptions behind fairness framework from a systems perspective This theory argues that computer scientists and engineers are concerned both with the world in which the machine serves a useful purpose and with the machine itself The purpose of the machine is located in the world in which the machine is to be installed and used In other words our objective is to design systems that fulfill requirements in the world where the system is to be deployed More precisely a portion of the world becomes the environment or the application domain of a machine In this world there are phenomena in the application domain eg events behavior of people activities and in the machine eg data algorithms state The machines inputs and outputs ie the things the machine can sense or affect are shared phenomena they are observable both to the application domain and the machine We introduce machines into existing application domains to effect change in these domains To achieve the desired change we need descriptions of the phenomena before the machine is introduced known as domain assumptions and statements about the desired situation once the machine is introduced to the domain known as requirements A specification is a set of requirements providing enough information for engineers to implement the machine typically describes phenomena shared between the machine and application domain A program derived from the specification is a description of a machine If implemented correctly programs satisfy the specification If the specification is derived correctly programs generate phenomena that attain the desired effects in the application domain ie they fulfill the requirements Jacksons explicit treatment of the application domain and its interaction with the machine helps us to project known problems with algorithms to a systems view It enables us to distinguish problems due to badly derived requirements description of the problem from those due to badly derived specifications description of the solution and those due to badly implemented programs how solutions are implemented Application domain does not refer to a class of applications like health or banking domain but to actors and things in the environment where the machine is introduced The focus on algorithms is insufficient for addressing inequitable outcomes of systems In the light of Jacksons theory we claim that the focus on algorithms leaves out the systems view Algorithms are often a part of a larger technical system which is deployed in an environment Fairness proposals rarely evaluate the systems environmental conditions thereby leaving out the possibility that even when a fairness metric is satisfied by the algorithm the system environment plus the machine could still be unfair or have other negative side effects Such focus on the specification of the machine also promotes the idea that the difficulty in addressing unfairness lies in devising a solution and not necessarily in rethinking the world Most fairness frameworks focus on describing independent of and and then guarantee that states of the machine and its input and outputs have certain properties However a machine that has fair inputs and outputs and fulfills a specification fair does not guarantee the fulfillment of requirements of fairness in the application domain This would require evaluating establishing what phenomena the machine is expected to change in the application domain and articulating requirements fair with fairness as a goal Only then could we evaluate whether an fair fulfills fair Focusing on fair has a number of repercussions First it does not reflect how harms manifest themselves in the environment Without an understanding and fair a specification fair may simply not lead to a fair outcome Imagine a hypothetical fair predictive policing algorithm that can fairly distribute police officers to different neighborhoods If the algorithm does not consider that the policing institution is already configured to control minorities and that interactions with police pose greater risk of harms for minorities a fair allocation can still disparately impact those minorities Second focusing on achieving fairness for users might leave out the impact of the system on phenomena in the application domain that is not shared with the machine Unjust outcomes could arise due to the optimization of certain behavior in the application domain and not because was unfair For instance self-regulated housing markets such as Airbnb may not actively discriminate against their users but reports have shown that they can disrupt neighborhoods by changing rent dynamics and neighborhood composition Third the focus on algorithms abstracts away potential harms of phenomena in the machine Much of the fairness literature focuses on ways in which algorithms can be biased or on harms caused by decision-making algorithms This overlooks that when the system hosting the algorithms optimizes its operation it may gather more inputs and outputs than those of the algorithm Therefore focusing on the algorithm may miss effects on the world that can go beyond those generated by the outputs of the algorithm actions When phenomena in the machine domain are subject to optimization unfairness can arise from optimization programs fulfilling the specification fair but not the requirements fair in the application domain For instance prediction techniques to optimize targeted advertising can create discriminatory effects and exploration strategies to optimize fair may gather inputs from the application domain that put some users unfairly at risk Discriminatory effects are not the only concern for building just systems In Jacksons terms considering only discriminatory effects constrains the requirements fair to a particular class of harms This approach risks missing other harms caused by the system when evaluating the performance of the specification fair in the environment We assume the introduction of a machine in an environment aims to improve specific phenomena The fact that this machine follows a specification fair that does not discriminate according to fair does not guarantee that this machine will not induce other harms to the environment in any applications domain Take as examples unsafe housing or bad working conditions If we use a machine to distribute these resources more efficiently even if it does so fairly all users are harmed Bad housing conditions and lack of labor protection are problems in and of themselves so users will be badly served regardless of fairness conditions When the system is by design unjust or when the phenomena are structurally unjust or harmful claims on fair are meaningless In Jacksons terms the requirements are incomplete with respect to just outcomes Sometimes however injustices are tightly tied to the machine is not the solution but the source of problems The way requirements are optimized might lead to externalities for a subpopulation of users When a specification optimizes an asocial outcome such as excessive user engagement in social networks it can expose users to harms like addiction Solving fairness in this system will not resolve the underlying problem the system is harmful Striving to fulfill the requirements fair itself might bring new harms to the application domain Consider a fairness solution that alleviates distributional shift based on increasing diversity in the training set If its specification fair requires collecting data from more individuals or collects new attributes to implement the fairness measure it will exacerbate privacy issues These issues might result in many other harms in the application domain This ontology assumes that is built to solve problems and improve phenomena in the world Thus it does not provide the conceptual tools to address adverse situations This positive valence hinders the consideration of cases in which a machine amplifies existing injustices or introduce new ones Neither this ontology nor fairness frameworks account for power imbalances or economic incentives and how they impact how machine requirements are considered and prioritized We consider these matters in the next section by augmenting our systems view to consider socioeconomic aspects of the application domain FAIRNESS INCENTIVES AND POWER In this section we extend our analysis to problems related to the political economy of systems To model harms we borrow the term negative externalities from the economics literature A system causes negative externalities when its consumption production and investment decisions cause significant repercussions to users non-users or the environment The introduction of a machine might cause externalities in the application domain independent of the completeness or correctness of its requirements and specification For example the heavy use of traffic-beating apps such as Waze can worsen congestion for all drivers in the application domain We argue that validating the specification against the requirements is not enough To build just systems one must consider externalities of the machine in the application domain Congruent with models in fair optimization and economics to express externalities and incentives we introduce two utility functions that capture the machines impact on the application domain the service providers utility which measures how much value the provider extracts from introducing the machine and the social utility which measures the machines utility for the environment and people We define two versions of social utility one with a gods view of the application domain and one from the specifications perspective These utility functions enable us to capture injustices due to the introduction of the machine into the environment Idealized Fair-by-Design Service Provider We first consider an idealized fair-by-design service provider that is willing to address the externalities of the machine That is this provider aims to maximize both their own utility and the social utility Using this setting we show the ways in which fairness models fail to address a broad class of systems externalities We consider that a system is parametrized by a vector of internal parameters for some convex set of possible parameters Let be a population a set of individuals or other environmental entities that might be affected by the system Let R be the utility function of the provider when they use parameters Let R denote a hypothetical social-utility function or benefit defined in the requirements Let be the social utility in the providers specification The provider optimizes its operation by solving the following multi-objective optimization problem max This problem is considered in fair learning literature in its scalarized form or constraint-form with the social utility modeling a notion of fairness We assume an ideal situation in which the chosen parameter vector is a Pareto-optimal solution that is it cannot improve any of the objectives without hurting at least one of them Pareto-optimality however is not sufficient to guarantee fairness or equity as different trade-offs between the objectives are possible We assume that out of the possible Pareto solutions the provider chooses one that maximizes social utility argmax where is the set of all Pareto-optimal solutions Limitations in the Face of Externalities Consider the system obtained when using the gods view values of social utility argmax being the set of the corresponding Pareto-optimal solutions when using in Let We say that a system with parameters induces externalities on its environment when the social utility of is not equal to that of the system This definition is analogous to the neoclassical economic interpretation of externalities due to an inefficiency a partial optimum is achieved that is different from the optimum if no inefficiencies were present One can also parallel the divergence between social-utility values in to the Pigouvian divergence of social and private costs Because we are analyzing harms we focus on negative externalities Figure Pareto frontiers with real and providers version Misspecification of results in externalities difference in values of the benefit function between providers system and Clearly when If does not precisely match however there will likely be externalities present as illustrated in Fig In general it is not known how exactly is impacted by deviations of from In Appendix A for a class of strictly concave utility functions we show that the sensitivity of to infinitisemal error of is approximately quadratic in the magnitude of the error Intuitively grows quadratically fast as diverges Incomplete Information of Social Utilities We first study the case in which the providers view of the social utility does not fully reflect the requirements and the context of the application domain Social benefit is often modeled as a function incorporating individual models of social utility A common assumption in neoclassical economics is that social utility is the sum of individual utilities where R is a utility of individual In practice however the fair-by-design provider has incomplete information they are aware of the users yet lack the full knowledge of their needs similar to imperfect knowledge in economics and game theory That is they could misspecify for some user inadvertently ignoring the needs and well-being of this individual Consider a hypothetical fair-by-design predictive policing application that assumes that is maximized when there is equality of false positives in patrol dispatches across regions The resulting dispatching rates might be fair for this definition but the overall social benefit might be unchanged as minorities could still be over-policed in terms of the number of dispatches Omitting Impact on Non-Users and Environmental Impact Another case is when the fair-by-design provider has structural lack of knowledge of the application domain eg knows the utility of their users but not the utility of anything else ie of phenomena not shared with the machine Thus a fair solution for the users could harm non-users as their benefits were never specified in the model This is exemplified in the case of self-regulating housing markets which could hypothetically be optimized for fairness in acceptance rates for guests but disrupt neighborhoods impacting housing conditions especially for people with low incomes Limitations Under Complete Knowledge Even in the presence of complete information regarding and there can exist externalities that the provider cannot mitigate First the provider can maximize and yet cause externalities if the systems goal itself is harmful For example a facial recognition surveillance system might be completely fair with respect to skin color but it still causes risks and harms to the population as a whole in terms of privacy loss Second the fair-by-design model has as premise that there exists a solution that if not maximizes then at least satisfies minimum standards of everyones benefit However this may not be the case Recall the examples about unsafe housing or bad jobs in Sec In those cases the fair outcome might still be harmful for all users Limitations on Ideals Mitigating externalities becomes a greater issue when incentives capacities and power structures are not aligned One way this manifests itself is in the modeling of social benefit as a function Indeed a function cannot encode all the nuance regarding human needs Moreover neither the provider nor the users utility functions model the political context or the power asymmetries in the environment of the machine These can heavily influence or skew what we assume to be the ground truth Thus power and politics may come to render the system specification unfair even when it was designed considering the perfect benefit function for users Furthermore the fairness-by-design approach inherently assumes that the provider always has enough resources to implement the fair solution that maximizes social utility However it is unreasonable to believe that such an assumption will hold in practice For instance even if Waze was cognizant of the needs of all individuals and all the peculiarities in their streets it is unlikely that they could afford an operations in which all of those constraints are taken into account or be capable of mitigating the impact of externalities due to the interaction of multiple traffic beating applications Finally not all service providers have the incentives to implement fair-by-design solutions The provider might not be incentivized to care about social utility or could benefit from externalizing certain costs In such cases the magnitude of the externalities is likely to be more pronounced than in the case of incomplete knowledge PROTECTIVE OPTIMIZATION TECHNOLOGIES The systems view on algorithms in previous sections enables us to systematically explore a problem space that had not been formalized before A major source of problems is the use of optimization techniques that help to capture and manipulate phenomena in the application domains for the extraction of value This practice causes intentional or unforeseen changes to the environment which result in often neglected harms to the environment We showed that existing fairness frameworks produce solutions that can only address a subset of these harms at the discretion of a service provider These frameworks have limited capability to mitigate harms arising from inequities inherent to the application domain and from harmful impacts introduced by the machine By focusing solely on actions that can be taken by service providers who have opposite incentives fairness frameworks narrow both politics and contestation to the re-design of the algorithm which may not always be the site of either the problem or the solution Markets and Regulators Optimization System Uber Algorithm Algorithm Non-users Online data cookies pixels online maps Drivers locations Riders Hails Online Status Affect Users Offline data municipality information Online environment Offline environment Non-users Users Figure Uber a complex optimization system Inputs to Uber optimization black effects on the environment blue and alternatives to deploy POTs red In this section we discuss means to address these issues Although these means could be of socio-legal nature we focus on technological approaches that we call Protective Optimization Technologies POTs Their goal is to shape the application domain in order to reconfigure the phenomena shared with the machine to address harms in the environment POTs are designed to be deployed by actors affected by the optimization system As these actors directly experience the externalities a they have intimate knowledge of the systems negative effects b they are in position to have a better view of their social utility than a system provider can model because it is their own utility Lastly c POTs do not rely on the incentives of the provider POTs seek to equip individuals and collectives with tools to counter or contest the externalities of the system Their goal is not to maximize benefits for both users and service providers nor to find the best strategy to enable optimization systems to extract value at minimum damage POTs are intended to eliminate the harms induced by the optimization system or at least mitigate them In other cases POTs may shift the harms to another part of the problem space where the harm can be less damaging for the environment or can be easier to deal with Finally when service providers react to reduce the effectiveness of POTs this very action exposes the service providers need to maintain the power relationship and their capability to manipulate the environment to their own benefit POTs and Optimization Systems To extract value through optimization service providers obtain inputs from their environments that help them make decisions We consider three kinds of inputs i inputs that users generate when interacting with the system ii inputs about individuals and environments received from third parties and iii inputs from regulations and markets that define the political and economic context in which the system operates Note that third parties can be public or private and the data they provide can be gathered online or offline We use Uber ride-hailing service as an example to showcase this complexity Fig In order to maximize its profit Uber optimizes the prices offered to riders and the wages offered to drivers Uber uses the following inputs black arrows First it uses the direct inputs it receives from both riders and drivers Second it uses data from other sources such as online service providers eg Google for maps and data that they might collect using cookies or pixels on other sites their users visit Uber also receives offline data from parties like municipalities interested in promoting the use of Uber to reduce costs of public transport Lastly Uber uses inputs from the market and regulators to evaluate the economic context in order to adjust wages and ride prices Uber ultimately uses these inputs and all the political and economic context in a combination of managerial and mathematical optimization to deliver outcomes to the environment match riders to drivers and set ride prices Reports and studies demonstrate that these outcomes cause externalities Ubers activity increases congestion and misuse of bike lanes increases pollution and decreases public support for transit In the previous sections vocabulary Uber and its optimization techniques are the machine and the application domain comprises Uber drivers and riders non-users the online and offline environments and the market and regulatory frameworks in which Uber operates Roughly speaking Ubers requirements are to match drivers to riders with associated dynamic pricing The specification defines what Uber applications should do to fulfill Uber is known to have unfairness problems For example Rosenblat et al show that customer-based reviews are biased against minority drivers As getting blocked from the system depends on these reviews even if the Ubers algorithms do not discriminate drivers on their attributes per se the rules of the system result in disparate impact on blocked drivers Further even when Uber algorithms are fair eg they reward all drivers the same irrespective of their protected attributes the optimization processes underlying Ubers operation result in unjust outcomes low wages The former is an externality stemming from structural biases in the application domain thus a hypothetical fair does not result on fair whereas the latter is a problem of incentives misalignment In the Uber scenario POTs can be deployed by users and non-users with the goal of changing the phenomena captured by Uber These can come in three forms red lines in Fig by changing the inputs of the users to the system eg the surge-induction POT as we describe shortly in Sec by changing the online or offline signals gathered by Uber eg mayors changing the city urban planning or by affecting the market eg by changing regulations or mandating salary increases Examples of POTs Our vision for POTs systematizes the use of technologies as tools to both explore the effects that algorithms and optimization systems have on our society and the design of countermeasures to contest their negative effects in the absence of regulatory or other accountability measures We now revisit recent academic technologies artistic interventions and deployed tools that can be reframed as POTs Table provides a summary POTs formalize such technologies and interventions enabling the systematic study and design of such solutions We illustrate this in Sec where we design two POTs from scratch These technologies have different origins First we observe that there are technologies proposed in the academia that can be repurposed as POTs For instance in the field of computer security Table Repurposing technologies as POTs We detail the origin of the technology the externality it addresses the optimization system causing the externality and the desired outcome of the POT deployment requirements individual or cooperative action and the underlying design techniques counter-optimization or heuristics to decide how to shape the environment Origin Optimization System Externality POT Desired Outcome Deployment Technique Academic Face Recognition Privacy discrimination Wear printed eyeglasses Evade face detection Individual Optimization Academic Copyright Infringement Detection Fair use takedowns Adversarial examples Avoid a fair use takedown Individual Optimization Academic Psychometric Profiling Privacy manipulations Text style transfer Prevent from attribute inference Individual Optimization Academic YouTube Recommendations Manipulation Poisoning Breaking out of content bubbles Individual Optimization Academic Waze Routing Local traffic congestion Sybil devices simulate traffic Prevent routing into towns Individual Heuristic Academic GRE Scorer Biased grading system Generate essay to pass GRE Higher test score Individual Heuristic Deployed Ad Network Privacy manipulations Click on all ads Ad Network destroyed Collective Heuristic Deployed Uber Pricing System Low wages Shut off app turn it back on Induce surge Collective Heuristic Deployed Instacart Pricing Low wages Tip in app cash at door Fair pay for jobs Collective Heuristic Deployed Automated Hiring Bias discrimination Edit resume Flip automated hiring decision Individual Optimization Deployed Pokemon Go Resource Spawn Unfairness Edit Open Street Maps Encourage resources to spawn Individual Heuristic Deployed FitBit for Insurance Premium Privacy surveillance Spoof device location Get insurance benefits Individual Heuristic Deployed Pharma Optimizing Patents End of humanity Find potential drugs using ML Get drugs in the public domain Individual Mixed Deployed Insurance Coverage Optimization High costs of treatment Doctors changing claim codes Get higher reimbursements Individual Heuristic Artistic Face Recognition Privacy surveillance Scarf that is classified as a face Evade face detection Individual Optimization Artistic Face Recognition Privacy surveillance Camouflage to cover features Evade face detection Individual Heuristic Artistic Autonomous Cars Exploration risks Ground markings Trap autonomous cars Individual Heuristic research that aims to protect against attackers gaming the YouTube algorithm can be repurposed by users to fight against filter bubbles in the field of adversarial machine learning tools developed to evade copyright detection originally developed to strengthen DRM can be reframed as a way to prevent fair-use takedowns Second we draw from works produced by artists looking into the impact of technology on society For instance counter-surveillance fashion that tricks facial recognition technologies can be repurposed to evade discriminatory facial recognition algorithms or discriminatory uses of facial recognition Finally we look to deployed technologies that are already countering optimization either intentionally or as a side effect For instance Jobscan assists job applicants in getting past the automated sorting implemented by large companies and job-posting sites This tool could be repurposed to reduce the gender or racial bias reported for these tools These examples can also be categorized based on the means used to design the POT those based relying on adversarial machine learning and those that use heuristics to exploit the target optimization process For instance as a response to low wages Uber drivers have developed heuristics for inducing surges Finally some technologies can be deployed individually and others require collective action AdNauseam for example reduces the utility of ad networks by clicking all ads served to the user flooding the network with false information Without a critical mass of users however AdNauseams effect would not be observable DESIGNING PROTECTIVE OPTIMIZATION TECHNOLOGIES CASE STUDIES In this section we show how given an optimization system and a negative externality one can design new POTs from scratch Like many of the technologies and interventions in Table we make use of optimization techniques to design the operation of POTs Starting from the model introduced in Sec we model the optimization systems objective as a function x R where denotes the space of environment inputs to the system ie phenomena that are sensed by the machine and the vector x is a set of inputs coming to the system Each concrete input can come from a different source users non-users or other actors in the environment For simplicity we model the time dimension through discrete time steps and we assume that there are two possible time steps and To maximize its gain the optimization system strives to solve a mathematical optimization problem argmax for a set of inputs at each point in time Given this system the goal of a POT is finding actionable feasible and inexpensive modifications to the inputs of the optimization system so as to maximize social utility that we denote as pot Note that this definition of social utility needs not to correspond to the social utility considered by the optimization system in Section We consider that each input has an associated modification cost X X R that represents how hard it is to modify it The cost of changing a set of inputs x x can be any function of the individual costs In this model we define the POT design as a bi-level multiobjective optimization problem min x x st argmax x where x is the vector of inputs if no intervention happened and x are possible vectors of modified inputs represents the systems state in the next step after the POT has been deployed through the modified inputs x We now instantiate this problem for two different use cases one where the POT can be deployed by an individual and one where effective deployment requires a collective Thwarting Traffic from Routing Apps In our first case study we look at Waze a crowdsourced mobile routing application that optimizes the routes of its users based on information about traffic road closures etc Waze causes negative externalities for residents of towns and neighbourhoods that are adjacent to busy routes For example take the town of Leonia New Jersey USA which lies just outside one of the bridges into New York City As Waze rose in popularity and directed an increasing amount of users through the town when the highway was busy the town became crowded during rush hours To prevent Waze traffic the town was briefly closed off to non-local traffic which was determined illegal In this section we propose a solution for discouraging Waze from selecting routes through the town while minimizing the impact on its inhabitants Problem Setup We set up this problem as a planning problem in which the towns traffic network is modeled as a weighted directed graph and the goal is to increase the cost of paths between the highway ramps We define Wazes utility as the capability to provide fastest routes for its users Routing through town can increase this utility when it takes less time than traveling via the highway The POT is designed to increase the minimum time through town so that Waze users are not routed through it We define the social utility pot as a binary variable that takes value when no vehicle is routed through the town ie the cost in time of traversing the town is greater than traversing the highway and otherwise Let G VE be a weighted directed graph representing the traffic network within the town Each edge E V V represents a road segment and each vertex represents a junction of segments The edges have associated time to pass the segment given by the function define the time cost of traversing a path in the graph as the total time to pass its edges e e Let V be the source and sink vertices that represent the entry to the town from the highway and the exit to return to the highway Let the time to travel from point to via the highway be While we do not know the routing algorithm used by Waze we assume that Waze will send users through the town when the path through town is quicker than the highway That is if there is a path e from to inG with cost e Waze routes users through the town Avoiding Routing via Planning We aim to transform the graph G into a graph G such that the time cost of any path e from to in G is e We focus on what the town can control time to traverse a road segment We express these changes as the increase in time that it takes to traverse a segment We abstract the exact method for adding time to the segment which could be changes to speed limits traffic lights stop signs speed bumps etc We construct G by modifying the time values in the original graph where is a function representing the edge time in G We acknowledge that some roads are more costly to change than others by associating a modification cost to every road In practice this cost will be specified by the town We use length of a road segment as such a cost capturing that changing longer roads will likely have more impact on the town Let be binary interdiction variables that represent whether we are modifying the edge in graph G To express the cost of modifying a graph into G we use the following modification-cost function E where E R represents the cost of modifying the edge We now formalize the POT through posing the multi-objective optimization problem in in a constraint form minimize subject to the pot constraint given that Waze is maximizing min E st e for any path e from to This is equivalent to the problem known as shortest-path network interdiction In the form equivalent to ours it can be solved as a mixed-integer linear program MILP We refer to Appendix B for the exact formulation of the MILP in our context We use or tools for specifying this MILP in Python and the CBC open-source solver for solving Empirical Evaluation We apply this POT to three towns all of which reported issues with Waze Leonia NJ USA Lieusaint France and Fremont CA USA We retrieve the map data for each via the Open Street Maps API To assign to each segment we estimate the time it takes to traverse the segment using its length and type eg we consider residential streets to have a speed of mph and add extra time for traversing the intersection We infer the intersection time using the travel time from Google Maps for accuracy For each city we run the solver for different values corresponding to different travel times on the highway Larger values of require more changes to roads in the town Figure Solutions for Leonia NJ left and Lieusaint France right when the time of road segments is allowed to be increased by Town streets are marked in blue highways in orange and surroundings in grey The red dots signify and the closest points to the highway in the town The roads marked by thick black lines are the optimal set of segments in which the time should be increased For simplicity of implementation we choose two points at the edge of the town red dots in Fig and not the sink/source points which would be on the highway for the solver We then add the time that it takes to travel from the actual source point on highway to the first point and from the second point back to the sink point in order the calculate the results For Leonia for instance we approximated this as seconds on each side of the town We consider scenarios in which the town is able to increase the time that it takes to traverse each road segment by and For each town we found the value of for which Waze begins sending cars through the town and the value of in which no further changes to the road can prevent Waze from sending its users through the town The graph for Leonia NJ contains vertices and edges Given the parameters we choose without any changes to the town Waze begins to send its users through Leonia when That is it normally takes minutes to travel through Leonia so when it takes longer than minutes to traverse the highway Waze routes users through town This corresponds to traffic traveling at about mph on the highway If we limit the amount of time that can be added to traverse a road segment to of the original time we can prevent cars from being routed through Leonia until the average highway speed has reached mph That is we can find a solution that prevents routing through town as long as the highway speed is greater than mph For time increase we can change the town such that traffic will not be routed through Leonia until the average speed on the highway has fallen to mph This solution for Leonia is shown in Figure Lieusaint is larger than Leonia with vertices and edges Given the parameters we choose Waze routes its users through Lieusaint when which corresponds to the speed on the highway dropping to mph Allowing the road segments to be lowered by we can prevent traffic from being routed through Lieusaint until the highway speed has dropped below mph See Lieusaint in Figure We report the solution for Fremont CA a significantly larger town in Appendix B Finally we measure the cost of implementing these solutions Fig For each town we consider the impact to the town to be how much longer on average it takes to travel between any two points in town We compute the shortest path between every pair of points in the town and average these times before and after the POT solution We then compute the percentage increase from the initial average time to the post-POT average time The higher impact solutions are those which will tolerate a lower highway speed That is they will prevent cars from being routed through the town at lower highway speeds We see that even though allowing road segments to take longer to traverse can prevent cars from entering the town at a lower highway speed the impact to the town is much higher The inhabitants of Lieusaint also suffer more from the changes than the residents of the much smaller Leonia POT Impact and Limitations The intervention we propose in this section focuses on alleviating the problems routing applications cause in one town The POT would support this towns government to decide which changes should be implemented in the city layout and traffic rules so that the cost of traversing the city becomes undesirable for pass through drivers While this intervention indeed mitigates the effect of external traffic on the target town it is likely that then vehicles are routed elsewhere ie the POT shifts the harms from this particular town to other regions Moreover we acknowledge that our POT can only help in cases when vehicles can still circulate on the highway The moment the congestion forces vehicles to halt the town becomes the better option regardless of any modification on its road network Reducing False Negatives in Credit Scoring In this case study we explore solutions for countering harmful effects of a machine-learning credit scoring system A system that a bank uses to decide how to price a loan application according to the predicted risk The underlying algorithms that support such P er ce n t In cr ea se o A ve ra g e In ow n T ra ve l T im e Leonia Percentage Decrease of Segment Time P er ce n t In cr ea se o A ve ra g e In ow n T ra ve l T im e Lieusaint Percentage Decrease of Segment Time Figure Effect of Changes on In-Town Travel decisions are designed to maximize banks profits and minimize their risks These algorithms can be discriminatory or cause feedback loops for populations disadvantaged by the financial system These harms are often caused by inputs that represent unjust realities which are propagated to the models decisions Problem Setup We model the credit scoring system as a classifier that takes as input the information about the loan applicant and the loan details and outputs a confidence score for whether the applicant would repay the loan or not This function optimizes the banks utility that we model here as the negative empirical loss over historical data argmin where is the parameters of the classifier is the loss function and is the banks dataset of historical loans and their repayment or default outcomes We assume that the classifier is retrained as new data points arrive In our case study we model the harms of the system as the rate of false negatives wrong default predictions for economically disadvantaged populations In this POT we counter this problem using adversarial machine learning techniques deployed by a collective of individuals pot with the means to take and repay loans as explained shortly This POT aims to increase the social utility defined as the loss for a target group pot is the disadvantaged subset of applicants in this case study we define disadvantaged as having little funds in the bank account who were wrongfully denied a loan This POT can be thought as promoting equality of false-negative rates between the target group and everyone else with the difference that we do not limit our view of externalities to a commonly protected subgroup ie economical disadvantage is not commonly considered as protected Reducing False Negatives with Adversarial Machine Learning We first identify what inputs x in the abstract model can be modified by the collective deploying the POT First the deployers can only add new inputs to the dataset by taking and repaying loans Second the demographic attributes of these added loan applications have to be similar to those of individuals in Thus the POT must inform the collective pot about who and for which loans they should apply for and repay in such a way that they reduce the false-negative rate on the target group after retraining This POT is idealistic in that it assumes that this collective will include applicants of diverse backgrounds to be able to provide different inputs to the classifier However in the absence of other means of feedback communication and accountability it represents the only means to influence an unjust system It is also consistent with the existing practices people resort to in this setting Finding which inputs to inject into a training dataset to modify its outputs is known as poisoning in adversarial machine learning Typically poisoning attacks aim to increase the average error of the classifier or increase the error on specific inputs We note that our use of poisoning is different First we poison to decrease the error for a given target group Second our use of poisoning is not adversarial On the contrary we use it to protect users against harmful effects of the model With this in mind we design the POT using the following bi-level optimization problem min st argmin accept for all pot pot pool pot where is the current parameters of the classifier pot is the set of poisoned applications is the maximum number of poisoned applications and pool is a set of feasible loan applications That is we minimize the classifiers loss for the target group where the classifier is trained using the poisoned examples pot In our evaluation below we additionally make use of a regularizer to minimize the effect of poisoning on any other applications This formulation makes two assumptions First we assume that when designing the POT we have access to the training dataset of the provider and we can obtain retrained loan-approval models We consider this assumption reasonable as poisoning attacks tend to transfer even if the dataset and model do not match the real ones especially when the models have low complexity Second the added loan applications must be feasible there has to exist a person in the pot with demographics required for this loan application We solve this problem by scoring each example in pool according to the value of our optimization objective retraining for each example and then employing a greedy algorithm to assemble pot that satisfies the constraints We refer to the Appendix C for the details of the algorithm Empirical Evaluation We create a simulated loan approval system using the German credit risk dataset from the UCI Machine C ha ng e in N Number of Poisoned Applications C ha ng e in P Group Target Everyone else Setting Noisy Noisy Clean Figure Effects of the POT on the error rates of the classifier for the target group Top decrease in the number of false negatives wrongly denied loans Bottom increase in the number of false positives wrongly granted loans Learning Repository This dataset contains feature vectors representing loan applications including applicants job type bank account details gender etc and loan details amount duration and purpose Each has a binary label encoding whether the loan was repaid or not We implement the loan approval system as a logistic regression classifier For our simulation we split the dataset into the banks training dataset examples and the test set The classifier achieves test accuracy We obtained the best results with the use of as the trade-off parameter for the regularization term out of in We simulate the set of feasible applications pool using a subset of successfully repaid applications from users that are not in the target group by generating all possible changes of modifiable attributes We evaluate our POT in two settings a clean setting in which the only applications received by the bank are those from the collective pot and a noisy setting in which other people take loans between the time when the POT is deployed and the time when the classifier is retrained We report the results in the noisy setting for and additional loans and of the original dataset respectively We repeat the noisy experiments times in which we draw the noise loan applications at random We present the effect of poisoning on the target group and everyone else in Fig top Unsurprisingly the more poisoning applications there are the more significant the effect With poisoning applications of the training dataset our algorithm reduces the number of false negatives in the target group by in the presence of noise this decreases to The false negatives in the rest of the dataset are on average not impacted by our POT POT Impact and Limitations Unfortunately the POT increases the false positives Fig bottom That is it shifts the harm to the bank which would give more loans to people who cannot repay increasing its risk This effect sharply increases with the seventh application out of This is due to the poisoning inputs starting to change the model parameter corresponding to the loan purpose In turn this increase in false positives could lead to adverse social impact over time if banks try to counter the POT effect The POT deployers could adjust the trade-off parameter to control for the side-effects Yet such a reaction would expose the incentives and motivations behind the banks choice of parameters To be consistent with the fairness literature we assume that the target group is interested in getting higher credit ratings through decrease in false-negatives This model that postulates the access to credit as beneficial however is naïve Even though the loan-making process that relies on risk-based pricing has economic backing by definition it implies that the less financially advantaged people will get more expensive loans Hence an intervention that aims at increasing inclusion of disadvantaged populations in the lending process can be seen as an instance of predatory inclusion Even if it results in lower loan prices in the short term it can lead to dispossession in the long run When harms are viewed through this lens It is not clear if any technological intervention is capable of counteracting such systems DISCUSSION Our inspiration to provide a more holistic view on optimization systems and their harms comes from works that point to the logic and potential impact of optimization systems In particular Poon has drawn attention to the ways in which optimization systems are driven by outcomes as exemplified in our utility functions in Sec This allows for techniques like operational control and statistical management to be the primary mode with which machines interact with phenomena in the world As a result these techniques function both as a means for engineering and as a mathematical state that poses as a solution to political contention Optimization is a technique long established in for example resource allocation However its increasing supremacy in systems that reconfigure every aspect of life education care health love work etc for extraction of value is new and refashions all social political and governance questions into economic ones This shift allows companies to commodify aspects of life in a way that conflates hard questions around resource allocation with maximization of profit and management of risk The impact is subtle but fundamental as evident in the way even we start framing complex and historical questions of justice in terms of utility functions To ground ourselves in the world we used the requirements engineering model of Michael A Jackson aligning it with calls for decentering technology and centering communities in the design of systems We extended the model to ensure that we are not suggesting that problems and solutions are neatly separable and void of power and political economy However many elements that might be crucial for conceptualizing optimization systems are still missing The ontology offers few concepts to capture matters around data machine learning or services and it does not provide deeper insights into addressing issues like fairness or justice It is however a nod to the importance of ontological work for systematizing reflections on our frameworks To capture the political economy of optimization systems we turned to utilitarian models and calculations of externalities Such models are commonly used both in mathematical and managerial forms of optimization and are the cornerstone of neoclassical economics However utilitarian models have been thoroughly critiqued for among others perpetuating inequalities Most prominently Sen has highlighted the limitations of assessing value through consequences assessing value through subjective utility maximizing welfare without regard for its distribution and fetishizing resources over relations between resources and people Overall utilitarian approaches are weak in capturing collective interests social well-being forms of power and subjugation Given these critiques the central role that these models play in designing large-scale optimization systems is a problem in and of itself One possible way forward is to consider alternative economic models for the design and evaluation of systems eg POTs depend and build on the existence of such alternative economic models and the availability of collectivity altruism and reciprocity They assume there is something to be gained both individually and collectively dismissing the selfish agents presumed in utilitarian approaches In fact we struggled to express POTs in the utilitarian logic if we optimize for the utility of the service provider it is hard to justify any POT that may reduce the utility of the service provider Beyond economic gains POTs strategically support peoples agency Optimization systems offer little agency to effectively contest their value proposition and offer more optimization as solutions for externalities POTs can be used to exercise some agency towards an unaccountable and authoritative system Nevertheless service providers may argue that POTs are gaming the system Our focus is on social-justice contexts in which POTs can be cast as weapons of the geek for the least equipped to deal with the consequences of optimization POTs can also serve to expose systems injustices achieving transparency and accountability goals In that sense they also can come to act like rhetorical software that turn the logic of a system against itself to demonstrate its pathology This includes making apparent the damaging results of utilitarian forms of governance prominent in optimization systems Despite their positive potential designing and deploying POTs is not trivial By virtue of modifying subverting or sabotaging an optimization system POTs may elicit transitions in the system state that result in externalities If several POTs are deployed and enter in an arms race those agents with the most knowledge and resources are likely to deploy the most aggressive and effective POTs and have the most leverage This undermines the ability of less powerful populations who may need POTs the most to have any effect on the system This signals that well-thought POTs must be built to provide less powerful actors with the means to respond to the potential abuse of power by those that have more capabilities Just as much the multi-input form of most optimization systems poses a serious challenge when optimization is based on continuous tracking across many channels POTs cannot be built short of creating optimized doubles of entities in the environments The fact that a whole infrastructure for optimizing populations and environments is built in a way that minimizes their ability to object to or contest the use of their inputs for optimization is of great concern to the author sand should be also to anyone who believes in choice subjectivity and democratic forms of governance