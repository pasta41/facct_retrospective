Towards Fairer Datasets Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy Computer vision technology is being used by many but remains representative of only a few People have reported misbehavior of computer vision models including offensive prediction results and lower performance for underrepresented groups Current computer vision models are typically developed using datasets consisting of manually annotated images or videos the data and label distributions in these datasets are critical to the models behavior In this paper we examine ImageNet a large-scale ontology of images that has spurred the development of many modern computer vision methods We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology the ImageNet stagnant concept vocabulary of WordNet the attempt at exhaustive illustration of all categories with images and the inequality of representation in the images within concepts We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively CONCEPTS Social and professional topics Gender Race and ethnicity Age Computing methodologies Computer vision KEYWORDS computer vision fairness dataset construction representative datasets INTRODUCTION As computer vision technology becomes widespread in peoples Internet experience and daily lives it is increasingly important for computer vision models to produce results that are appropriate and fair However there are notorious and persistent issues For example face recognition systems have been demonstrated to have disproportionate error rates across race groups in part attributed to the underrepresentation of some skin tones in face recognition datasets Models for recognizing human activities perpetuate gender biases after seeing the strong correlations between gender and activity in the data The downstream effects range from perpetuating harmful stereotypes to increasing the likelihood of being unfairly suspected of a crime eg when face recognition models are used in surveillance cameras Many of these concerns can be traced back to the datasets used to train the computer vision models Thus questions of fairness and representation in datasets have come to the forefront In this work we focus on one dataset ImageNet which has arguably been the most influential dataset of the modern era of deep learning in computer vision ImageNet is a large-scale image ontology collected to enable the development of robust visual recognition models The dataset spearheaded multiple breakthroughs in object recognition In addition the feature representation learned on ImageNet images has been used as a backbone for a variety of computer vision tasks such as object detection human activity understanding image captioning and recovering depth from a single RGB image to name a few Works such as Huh et al have analyzed the factors that contributed to ImageNets wide adoption Despite remaining a free education dataset released for non-commercial use only the dataset has had profound impact on both academic and industrial research With ImageNets large scale and diverse use cases we examine the potential social concerns or biases that may be relected or amplified in its data It is important to note here that references to ImageNet typically imply a subset of categories selected for the image classification task in the ImageNet Large Scale Visual Recognition Challenge ILSVRC of and much of the research has focused on this subset of the data So far Dulhanty and Wong studied the demographics of people in ILSVRC data by using computer vision models to predict the gender and Please refer to ImageNet terms and conditions at image-netorg/download-faq age of depicted people and demonstrated that eg males aged to make up the largest subgroup Stock and Cisse did not explicitly analyze the dataset but demonstrate that models trained on ILSVRC exhibit misclassifications consistent with racial stereotypes Shankar et al and DeVries et al showed that most images come from Europe and the United States and the resulting models have difficulty generalizing to images from other places Overall these studies identify a small handful of protected attributes and analyze their distribution and/or impact within the ILSVRC dataset with the goal of illuminating the existing bias Goals and contributions There are two key distinctions of our work First we look beyond ILSVRC to the broader ImageNet dataset As model accuracy on the challenge benchmark is now near-perfect it is time to examine the larger setting of ImageNet The categories selected for the challenge contain only people categories scuba diver bridegroom and baseball player while the full ImageNet contains people categories under the person subtree accounting for roughly of the total images Their use can be problematic and raises important questions about fairness and representation In this work we focus on the person subtree of the full ImageNet hierarchy Second in contrast to prior work our goal is to do a deeper analysis of the root causes of bias and misrepresentation and to propose concrete steps towards mitigating them We identify three key factors that may lead to problematic behavior in downstream technology the stagnant concept vocabulary from WordNet the attempt at exhaustive illustration of all categories with images and the inequality of demographic representation in the images For each factor we seek to illuminate the root causes of the concern take the first steps to mitigate them through carefully designed annotation procedures and identify future paths towards comprehensively addressing the underlying issues Concretely we thoroughly analyze the person subtree of ImageNet and plan to modify it along several dimensions First in Sec we examine the people categories that are annotated within the subtree and determine that of them are potentially offensive labels that should not be used in the context of an image recognition dataset We plan to remove all of these from ImageNet Second in Sec out of the remaining categories we find that only of them are visual with the remaining categories simply demonstrating annotators bias We recommend further filtering the person subtree down to only these categories when training visual recognition models Finally in Sec we run a large-scale crowdsourcing study to manually annotate the gender skin color and age of the people depicted in ImageNet images corresponding to these remaining categories While the individual annotations may be imperfect despite our best efforts eg the annotated gender expression may not correspond to the depicted persons gender identity we can nevertheless compute the approximate demographic breakdown We believe that releasing these sensitive attribute annotations directly is not the right step for ethical reasons and instead plan to release a Web interface As noted by the authors themselves this approach poses a chicken-or-egg problem as trained models are likely to exhibit gender or age biases thus limiting their ability to accurately benchmark dataset bias that allows an interested user to filter the images within a category to achieve a new target demographic distribution We are working on incorporating these suggestions into the dataset We will additionally release our annotation interfaces to allow for similar cleanup of other computer vision benchmarks RELATED WORK ON FAIRNESS IN MACHINE LEARNING We begin with a more general look at the literature that identified or attempted to mitigate bias in modern artificial intelligence systems In short datasets often have biased distributions of demographics gender race age etc machine learning models are trained to exploit whatever correlations exist in the data leading to discriminatory behavior against underrepresented groups A great overview of the history of machine learning fairness can be found in Hutchinson and Mitchell The approaches to address fairness concerns fall loosely into two categories identifying and correcting issues in datasets or studying and encouraging responsible algorithmic development and deployment Identifying and addressing bias in datasets There are several issues raised in the conversation around dataset bias The first common issue is the lack of transparency around dataset design and collection procedures Datasheets for Datasets and relatedly for models have been proposed as solutions encouraging dataset creators to release detailed and standardized information on the collection protocol which can be used by downstream users to assess the suitability of the dataset Throughout this work we dive deep into understanding the data collection pipelines of ImageNet and related datasets and consider their implications The second issue is the presence of ethically questionable concepts or annotations within datasets Examples range from quantifying beauty to predicting sexual orientation to arguably annotating gender In Sec and to a lesser extent in Sec we consider the underlying cause for such concepts to appear in large-scale datasets and propose the first steps of a solution A related consideration is the ethics and privacy of the subjects depicted in these computer vision datasets Here we refer the reader to eg Whittaker et al for a recent detailed discussion as this is outside the scope of our work The final and perhaps best-known source of dataset bias is the imbalance of representation eg the underrepresentation of demographic groups within the dataset as a whole or within individual classes In the context of computer vision this issue has been brought up at least in face recognition activity recognition facial emotion recognition face attribute detection and image captioning as well as more generally in pointing out the imaging bias of datasets This is not surprising as many of the images used in computer vision datasets come from Internet image search engines which have been shown to exhibit similar biased behavior In some rare cases a dataset has been collected explicitly to avoid such influences eg the Pilot Parliaments Benchmark PPB by Buolamwini and Gebru In Sec we propose a strategy for balancing ImageNet across several protected attributes while considering the implications of such design image-netorg/iltering-and-balancing namely the concerns with annotating pictures of individual people according to these protected attributes Responsible algorithmic development Beyond efforts around better dataset construction there is a large body of work focusing on the development of fair and responsible algorithms that aim to counteract the issues which may be present in the datasets Researchers have proposed multiple fairness metrics including statistical parity disparate impact equalized odds and individual fairness and analyzed the relationship between them Algorithmic solutions have ranged from removing undesired bias by preprocessing the data striking a tradeoff between performance and fairness by posing additional regularization during training or inference or designing application-specific interventions such as of Burns et al for reducing gender bias in image captioning models However statistical machine learning models have three fundamental limitations that need to be considered First the accuracy of a machine learning model is strongly influenced by the number of training examples underrepresented categories in datasets will be inherently more challenging for the model to learn Second machine learning models are statistical systems that aim to make accurate predictions on the majority of examples this focus on common-case reasoning encourages the models to ignore some of the diversity of the data and make simplifying assumptions that may amplify the bias present in the data Finally learning with constraints is a di cult open problem frequently resulting in satisfying fairness constraints at the expense of overall model accuracy Thus algorithmic interventions alone are unlikely to be the most effective path toward fair machine learning and dataset interventions are necessary Even more so most algorithmic approaches are supervised and require the protected attributes to be explicitly annotated again bringing us back to the need for intervention at the dataset level Datasets algorithms and intention Finally we note that prior work in this space underscores a single important point any technical fairness intervention will only be effective when done in the context of the broader awareness intentionality and thoughtfulness in building applications Poorly constructed datasets may introduce unnoticed bias into models Poorly designed algorithms may exploit even well-constructed datasets Accurate datasets and models may be used with malicious intent The responsibility for downstream fair systems lies at all steps of the development pipeline BACKGROUND THE IMAGENET DATA COLLECTION PIPELINE To lay the groundwork for the rest of the paper we begin by summarizing the data collection and annotation pipeline used in ImageNet as originally described in This section can be safely skipped for readers closely familiar with ImageNet and related computer vision datasets but we provide it here for completeness The goal of ImageNet is to illustrate English nouns with a large number of high-resolution carefully curated images as to foster more sophisticated and robust models and algorithms to index retrieve organize and interact with images and multimedia data We consider the entire ImageNet dataset consisting of images illustrating concepts rather than just the images illustrating concepts within the ImageNet challenge which are most commonly used There are three steps to the ImageNet data collection pipeline selecting the concept vocabulary to illustrate selecting the candidate images to consider for each concept and cleaning up the candidates to ensure that the images in fact correspond to the target concept We describe each step and its similarities to the steps in other vision datasets Concept vocabulary When building a visual recognition benchmark the first decision is settling on a concept vocabulary and decide which real-world concepts should be included WordNet emerges as a natural answer It is a language ontology in which English nouns are grouped into sets of synonyms synsets that represent distinct semantic concepts The synsets are then organized into a hierarchy according to the relation such as coffee table is a table WordNet serves as the concept vocabulary for ImageNet which provides images for grounding the synsets visually Similarly subsets of the WordNet backbone have been used in datasets like Places Visual Genome and ShapeNet Candidate images The natural and easiest-to-access source of visual data is the Internet For every concept in WordNet the ImageNet creators queried image search engines and aimed to increase the variety of retrieved images through using multiple search engines employing query expansion and translating the search terms into multiple languages Similarly the vast majority of vision datasets rely on images collected from the Internet with many collected by first defining the set of target concepts and then obtaining the associated images using query expansion eg PASCAL COCO Places Manual cleanup As noted in Torralba et al image search engines were only about accurate and thus a manual cleanup of the candidate images is needed The cleanup phase of ImageNet consists of a set of manual annotation tasks deployed on the Amazon Mechanical Turk AMT marketplace The workers are provided with a single target concept eg Burmese cat its definitions from WordNet a link to Wikipedia and a collection of candidate images They are instructed to click on all images that contain the target concept irrespective of any occlusion scene clutter or the presence of other concepts Images that reach a desired level of positive consensus among workers are added to ImageNet Similarly most computer vision datasets rely on manual annotation although the details change eg PASCAL was annotated in-house rather than using crowdsourcing Places relies on both positive and negative verification COCO favors very detailed annotation per image and Open Images and Places both use a computer-assisted annotation approach Outline In the following sections we consider the fairness issues that arise as a result of this pipeline and propose the first steps to mitigate these concerns We use the words concept and synset interchangeably throughout the paper PROBLEM STAGNANT CONCEPT VOCABULARY The backbone of WordNet provides a list of synsets for ImageNet to annotate with images However born in the past century some synsets in WordNet are no longer appropriate in the modern context People have reported abusive synsets in the WordNet hierarchy including racial and sexual slurs eg synsets like n and n This is especially problematic within the person subtree of the concept hierarchy ie synset n and its descendants During the construction of ImageNet in the research team removed any synset explicitly denoted as offensive derogatory pejorative or slur in its gloss yet this filtering was imperfect and still resulted in inclusion of a number of synsets that are offensive or contain offensive synonyms Going further some synsets may not be inherently offensive but may be inappropriate for inclusion in a visual recognition dataset This filtering of the concept vocabulary is the first problem that needs to be addressed Prior work on annotating offensiveness Sociolinguistic research has explored the problem of offensiveness largely focusing on studying profanity Ofcom and Sapolsky et al rate the offensiveness of words in TV programs Dewaele demonstrates offensiveness to be dependent on language proficiency by studying the ratings from English native speakers and non-native speakers Beers Fägersten designs two questionnaires to study the role of context in the level of offensiveness of profanity In one questionnaire subjects see a word together with a short dialogue in which the word appears They are asked to rate the word on a scale from not offensive to very offensive In another questionnaire the subjects see only the words without any context The findings highlight the importance of context as the perceived offensiveness depends heavily on the dialogue and on the gender and race of the subjects Methodology for filtering the unsafe synsets We ask annotators to lag a synset as unsafe when it is either inherently offensive eg containing profanity or racial or gender slurs or sensitive ie not inherently offensive but may cause offense when applied inappropriately such as the classification of people based on sexual orientation and religion In contrast to prior work we do not attempt to quantify the level of offensiveness of a concept but rather exclude all potentially inappropriate synsets Thus we do not adopt a or scale for offensiveness ratings Instead we instruct workers to lag any synset that may be potentially unsafe essentially condensing the rating or on the scale down to a single unsafe label Results and impact on ImageNet after removing the unsafe synsets We conduct the initial annotation using in-house workers who are graduate students in the department and represent countries of Throughout the paper we refrain from explicitly listing the offensive concepts associated with synsets and instead report only their synset IDs For a conversion please see wordnetprincetonedu/documentation/wndbwn origin male and female genders and a handful of racial groups The instructions are available in the appendix So far out of synsets within the person subtree we have identified unsafe synsets The remaining synsets are temporarily deemed safe Table gives some examples of the annotation results with the actual content of offensive synsets obscured The full list of synset IDs can be found in the appendix The unsafe synsets are associated with images in ImageNet Removing them would leave images in the safe synsets of the person subtree of ImageNet Limitations of the offensiveness annotation and future work First it is important to note that a safe synset only means that the label itself is not deemed offensive It does not mean that it is possible useful or ethical to infer such a label from visual cues Second despite the preliminary results we have offensiveness is subjective and also constantly evolving as terms develop new cultural context Thus we are opening up this question to the community We are in the process of updating the ImageNet website to allow users to report additional synsets as unsafe While the dataset may be large scale the number of remaining concepts is relatively small here in the low thousands and further reduced in the next section making this approach feasible PROBLEM NON-VISUAL CONCEPTS ImageNet attempts to depict each WordNet synset with a set of images However not all synsets can be characterized visually For example is it possible to tell whether a person is a philanthropist from images This issue has been partially addressed in ImageNets annotation pipeline Sec where candidate images returned by search engines were verified by human annotators It was observed that different synsets need different levels of consensus among annotators and a simple adaptive algorithm was devised to determine the number of agreements required for images from each synset Synsets harder to characterize visually would require more agreements which led to fewer or no verified images Despite the adaptive algorithm we find a considerable number of the synsets in the person subtree of ImageNet to be non-imageable hard to characterize accurately using images There are several reasons One reason for them sneaking into ImageNet could be the large-scale annotation Although non-imageable synsets require stronger consensus and have fewer verified images they remain in ImageNet as long as there are some images successfully verified which is likely given the large number of images Another reason could be positive bias annotators are inclined to answer yes when asked the question there a concept in the image As a result some images with weak visual evidence of the corresponding synset may be successfully verified The final and perhaps most compelling reason for non-imageable synsets to have been annotated in ImageNet is that search engines will surface the most distinctive images for a concept even if the concept itself is not imageable For example identifying whether someone is Bahamian from a photograph is not always be possible but there will be some distinctive images eg pictures of a people wearing traditional Bahamian costumes and those will be the ones Table Examples of synsets in the person subtree annotated as unsafe offensive unsafe sensitive safe but non-imageable and simultaneously safe and imageable For unsafe offensive synsets we only show their synset IDs The annotation procedure for distinguishing between unsafe and safe synsets is described in Sec the procedure for non-imageable vs imageable is in Sec We recommend removing the synsets of the first two columns from ImageNet entirely and refrain from using synsets from the third column when training visual recognition models Unsafe offensive Unsafe sensitive Safe non-imageable Safe imageable n sexual slur n Anglo-Saxon n demographer n Queen of England n profanity n taxi dancer n epidemiologist n basketball player n sexual slur n orphan n piano maker n bridegroom n gendered slur n camp follower n folk dancer n beekeeper n criminative n separatist n mover n gymnast n criminative n crossover voter n policyholder n ropewalker n obscene n theist n great-niece n rider n pejorative n Zen Buddhist n vegetarian n trumpeter returned by the search engine This issue is amplified by the presence of stock photography on the web which contributes to and perpetuates stereotypes as discussed at length in eg Overall this results in an undoubtedly biased visual representation of the categories and while the issue affects all synsets it becomes particularly blatant for categories that are inherently non-imageable Thus in an effort to reduce the visual bias we explicitly determine the imageability of the synsets in the person subtree and recommend that the community refrain from using those with low imageability when training visual recognition models Annotating imageability Extensive research in psycholinguistics has studied the imageability aka imagery of words which is defined as the ease with which the word arouses imagery For annotating imageability most prior works follow a simple procedure proposed by Paivio et al The workers see a list of words and rate each word on a scale from low imagery to high imagery For each word the answers are averaged to establish the final score We adopt this definitions of imageability and adapt the existing procedure to annotate the imageability of synsets in the ImageNet person subtree However unlike prior works that use in-house workers to annotate imageability we rely on crowdsourcing This allows us to scale the annotation and obtain ratings from a diverse pool of workers but also poses challenges in simplifying the instructions and in implementing robust quality control In our crowdsourcing interface in the appendix we present the human subject with a list of concepts and ask them to identify how easy it is to form a mental image of each To reduce the cognitive load on the workers we provide a few examples to better explain the task include the synonyms and the definitions of each concept from WordNet and change the rating to be a simpler point rather than point scale from very hard to very easy The final imageability score of a synset is an average of the ratings For quality control we manually select synsets as gold standard questions in the appendix half of them are obviously imageable should be rated and the other half are obviously non-imageable should be rated They are used to evaluate the quality of workers If a worker has a high error on the gold standard questions we remove all the ratings from this worker We also devise a el io ni st ne ut ra lis t ne co m er th st la a ge nt na na lis t al er ex pe rim en te r tr es m an ho st er g m us te ac he r nu rs e nn er tr c tr ai ne r ai tr es s synset im a g e a b ty Figure The distribution of raw imageability ratings for selected synsets irreligionist and nurse have more well-accepted imageability than host and waltzer Imageability S y n s e Figure The distribution of the final imageability scores for all of the safe synsets The median is heuristic algorithm to determine the number of ratings to collect for each synset Please refer to the appendix for details Results and impact on ImageNet after removing the non-imageable synsets We annotate the imageability of synsets in the person subtree which have been marked as safe synsets in the previous task Fig shows the imageability ratings for a selected set of synsets Synsets such as irreligionist and nurse have well-accepted imageability irreligionist is deemed to be decidedly non-imageable nurse is deemed to be clearly imageable In contrast it is much harder to reach a consensus on the imageability of host and waltzer Fig shows the distribution of the final imageability scores for all of the safe synsets The median is only synsets have imageability greater than or equal to Table shows some examples of non-imageable synsets The complete list is in the appendix After manually examining the results we suggest that all synsets in the person subtree with imageability less than be considered non-imageable and not be used for training models There would be images and synsets lagged including hobbyist job candidate and bookworm there would be images and synsets remaining including rock star skier and cashier More examples are in Table Future researchers are free to adjust this threshold as needed Limitations of the imageability annotation By manually examining a subset of the synsets we find the imageability results to be reasonable overall but we also observe a few interesting exceptions Some synsets with high imageability are actually hard to characterize visually eg daughter and sister they should not have any additional visual cues besides being female Their high imageability scores could be a result of the mismatch between ease to arouse imagery and ease to characterize using images Daughter and sister are hard to characterize visually but they easily arouse imagery if the annotator has a daughter or a sister The definitions based on ease of characterization with visual cues is more relevant to computer vision datasets but we adopt the former definitions as a surrogate since it is well-accepted in the literature and there are mature procedures for annotating it using human subjects Another interesting observation is that workers tend to assign low imageability to unfamiliar words For example cotter a peasant in the Scottish Highlands is scored while the generic peasant is scored Prior works have demonstrated a strong correlation between familiarity and imageability which explains the low imageability of the less frequent cotter However low familiarity with a concept is anyway an important factor to consider in crowdsourcing dataset annotation as unfamiliar terms are more likely to be misclassified by workers This suggests that removing synsets identified as less imageable by our metric may also have the additional benefit of yielding a more accurate dataset When analyzing Table we further wonder whether even the synsets that are both safe and imageable should remain in ImageNet For example is the Queen of England an acceptable category for visual recognition Would basketball player be better replaced with person interacting with a basketball and captured as a human-object-interaction annotation Would bridegroom be rife with cultural assumptions and biases As always we urge downstream users to exercise caution when training on the dataset And finally we observe that even the remaining imageable synsets may contain biased depictions as a result of search engine Further unfamiliar terms are also likely to be less common and thus less relevant to the downstream computer vision task making their inclusion in the dataset arguably less important artifacts For example the synset mother imageability score primarily contains women holding children similarly the synset beekeeper imageability score predominantly contains pictures of people with bees Even though one remains a mother when not around children or a beekeeper when not around bees those images would rarely be surfaced by a search engine and to be fair would be di cult for workers to classify even if they had been Despite these concerns about the imageability annotations and about the lingering search engine bias one thing that is clear is that at least the non-imageable synsets are problematic Our annotation of imageability is not perfect and not the final solution but an important step toward a more reasonable distribution of synsets in the ImageNet person subtree Relationship between imageability and visual recognition models To conclude the discussion of imageability we ask one final question What is the relationship between the imageability of synset and the accuracy of a corresponding visual recognition model Concretely are the imageable synsets actually easier to recognize because they correspond to visual concepts Or on the lip side is it perhaps always the case that non-imageable synsets contain an overly-simplified stereotyped representation of the concept and thus are easy for models to classify If so this would present additional evidence about the dangers of including such categories in a dataset since their depicted stereotypes are easily learned and perpetuated by the models Computer vision experiment setup To evaluate this we run a simple experiment to study the relationship between the imageability of a synset and the ability of a modern deep learning-based image classifier to recognize it We pick a subset of synsets from the safe synsets so that each synset has at least images The selected synsets are leaf nodes in the WordNet hierarchy meaning that they cannot be ancestors of each other and they represent disjoint concepts We randomly sample images from each synset for training for validation and for testing We use a standard ResNet network to classify the images as belonging to one of the synsets During training the images are randomly cropped and resized to we also apply random horizontal lips During validation and testing we take crops at the center The network is trained from scratch for epochs which takes two days using a single GeForce GTX GPU We minimize the cross-entropy loss using stochastic gradient descent the learning rate starts at and decreases by a factor of every epochs We also use a batch size of a momentum of and a weight decay of Computer vision experiment results The network has an overall testing accuracy of We are more interested in the breakdown accuracies for each synset and how they correlate with the imageability The networks testing accuracy on the easily imageable synsets score is which is higher than the accuracy of on the synsets deemed non-imageable score Overall there is a positive correlation between imageability and accuracy Pearson correlation coefficient r with a p-value of as depicted in Fig left To better understand this we analyze four representative examples also depicted in Fig right which highlight the different aspects at play here Imageable easy to classify A category such as black belt is both deemed imageable score of and is easy to classify accuracy of The retrieved images contain visually similar results that are easy to learn by the model and easy to distinguish from other people categories Non-imageable hard to classify On the other end of the spectrum conversational partner is deemed non-imageable score of only as it doesnt evoke a prototypical visual example The images retrieved from search engines contain groups of people engaged in conversations so the annotators verifying these images in the ImageNet pipeline correctly labeled these images as containing a conversation partner However the resulting set of images is too diverse and the visual cues are too weak to be learned by the model accuracy only Imageable hard to classify Bridegroom synset ID n is an example of a category with a mismatch between imageability and accuracy It is annotated as imageable perfect score of because it easily arouses imagery albeit highly culturally-biased imagery The retrieved search result images are as expected culturally biased but correctly verified for inclusion in ImageNet However the accuracy of the classifier in this case is low only partially because of the visual diversity of the composition of images but primarily because of confusion with a closely related synset n which also corresponds to the term bridegroom but with a slightly different definitions n a man who has recently been married versus n a man participant in his own marriage ceremony This highlights the fact that classification accuracy is not a perfect proxy for visual distinctiveness as it depends not only on the intra-synset visual cues but also on the inter-synset variability Non-imageable easy to classify Finally Ancient person who lived in ancient times is deemed non-imageable score of because the imageability annotators have never seen such a person so it is di cult to properly imagine what they might look like However the image search results are highly biased to ancient artifacts including images that are not even people The annotators agreed that these images correspond to the word ancient at times making mistakes in failing to read the definitions of the synset and annotating ancient artifacts as well In the resulting set of images visual classifiers would have no difficulty distinguishing this set of images with distinctive color patterns and unusual objects from the other people categories accuracy The findings highlight the intricacies of image search engine results of the ImageNet annotation pipeline of the imageability annotations and of evaluating visual distinctiveness using visual classifiers A deeper analysis is needed to understand the level of impact of each factor and we leave that to future research Until then we suggest that the community refrain from using synsets deemed non-imageable when training visual recognition models and we will update ImageNet to highlight that PROBLEM LACK OF IMAGE DIVERSITY So far we have considered two problems the inclusion of potentially offensive concepts which we will remove and the illustration of non-imageable concepts with images which we will clearly identify in the dataset The last problem we consider is insufficient representation among ImageNet images ImageNet consists of Internet images collected by querying image search engines which have been demonstrated to retrieve biased results in terms of race and gender Taking gender as an example Kay et al find that when using occupations eg banker as keywords the image search results exhibit exaggerated gender ratios compared to the real-world ratios In addition bias can also be introduced during the manual cleanup phase when constructing ImageNet as people are inclined to give positive responses when the given example is consistent with stereotypes ImageNet has taken measures to diversify the images such as keywords expansion searching in multiple languages and combining multiple search engines Filtering out non-imageable synsets also mitigates the issue with stronger visual evidence the workers may be less prone to stereotypes Despite these efforts the bias in protected attributes remains in many synsets in the person subtree It is necessary to study how this type of bias affects models trained for downstream vision tasks which would not be possible without high-quality annotation of image-level demographics Prior work on annotating demographics Image-level annotation of demographics is valuable for research in machine learning fairness However it is di cult to come up with a categorization of demographics especially for gender and race Buolamwini and Gebru adopt a binary gender classification and the Fitzpatrick skin type classification system Zhao et al and Kay et al also adopt a binary gender classification Besides Male and Female Burns et al add another category Neutral to include people falling out of the binary gender classification et al do not explicitly name the gender and race categories but they have discrete categories nevertheless ive race categories S S S S Other and three gender categories G G Other Methodology for annotating demographics Annotated attributes To evaluate the demographics within ImageNet and propose a more representative subset of images we annotate a set of protected attributes on images in the person subtree We consider US anti-discrimination laws which name race color national origin religion sex gender sexual orientation disability age military history and family status as protected attributes Of these the only imageable attributes are color gender and age so we proceed to annotate these Gender We annotate perceived gender rather than gender identity as someones gender identity may differ from their gender expression and thus not be visually prominent It is debatable what a proper categorization of gender is and whether gender can be categorized at all Rather than addressing the full complexity of this question we follow prior work and use a set of discrete categories Male Female and Unsure in which Unsure is used to both ancient black belt conversational partner groom Imageability C la s s if a ti o n a c c u ra c y Figure Left The computer vision models classification accuracy vs synset imageability for safe synsets which contain at least images More imageable synsets are not necessarily easier for models to recognize with Pearson correlation coefficient r Right Example images from synsets that are non-imageable and hard to classify conversational partner non-imageable but easy to classify ancient imageable but hard to classify groom imageable and easy to classify black belt Synsets P e rc e n ta g e Unsure Female Male Gender Synsets P e rc e n ta g e Dark Medium Light Skin Color Synsets P e rc e n ta g e Child or Minor Adult Over Adult Adult Age Figure The distribution of demographic categories across the safe and imageable synsets which contain at least images The size of the different color areas reveal the underrepresentation of certain groups handle ambiguous visual cues as well as to include people with diverse gender expression Skin color We annotate skin color according to an established dermatological metric individual typology angle ITA It divides the spectrum of skin color into groups which is too fine-grained for our purpose Instead we combine the groups into Light Medium and Dark Melanin index is another metric for skin color which is used by the Fitzpatrick skin type classification system However we opt to use the more modern ITA system Similar to prior work skin color is used as a surrogate for race membership because it is more visually salient Age We annotate perceived age groups according to discrimination laws which led to the categories of Child or Minor under years old Adult Over Adult and Over Adult Annotation instructions We use crowdsourcing to annotate the attributes on Amazon Mechanical Turk We downloaded all ImageNet images from safe synsets in the person subtree whose imageability score is or higher An image and the corresponding synset form a task for the workers and consists of two parts First the worker sees the image the synset including all words in it and its definitions in WordNet and is asked to identify all persons in the image who look like members of the synset If at least one person is identified the worker proceeds to annotate their gender skin color and age The labels are image-level rather than specific to individual persons There can be multiple labels for each attribute For example if the worker identified two persons in the first phase they may check up to two labels when annotating the gender The user interface is in the appendix The task is less well-defined when multiple persons are in the image It can be di cult to tell which person the synset refers to or whether the person exists in the image at all We have tried to use automatic methods eg face detectors to detect people before manually annotating their demographics However the face detector is a trained computer vision model and thus also subject to dataset bias If the face detector is only good at detecting people from a particular group the annotation we get will not be representative of the demographic distribution in ImageNet Therefore we opt to let workers specify the persons they annotate explicitly Quality control For quality control we have pre-annotated a set of gold-standard questions in the appendix for measuring the quality of workers The workers accuracy on a gold standard question i is measured by intersection-over-union IOU Ai Gi Ai Gi where Ai is the set of categories annotated by the worker and Gi is the set of ground truth categories For example for an image containing a black female adult and a white female child Gi dark light Female Adult Child If a worker mistakenly take the child to be an adult and annotates Ai dark light Female Adult the annotation quality is computed as We exclude all responses from workers whose average IOU is less than After removing high-error workers we aggregate the annotated categories of the same image from independent workers Each image is annotated by at least two workers For any specific category eg Adult we require consensus from max workers where ni is the number of workers for this image For any image we keep collecting annotations from independent workers until the consensus is reached In the annotation results the consensus is reached with only two workers for of the images and workers are enough for images Results of the demographic analysis We annotated demographics on the synsets that are considered both safe Sec and imageable Sec and that contain at least images We annotated randomly sampled images from each synset summing up to images Due to the presence of multiple people in an image each image may have more than one category for each attribute We ended up with attribute categories annotated annotations for gender annotations for skin and annotations for age This was the result of obtaining and consolidating worker judgments Fig shows the distribution of categories for different synsets which mirrors real-world biases For gender there are both male-dominated synsets and female-dominated synsets but the overall pattern across all synsets reveals underrepresentation of female as the blue area in Fig Left is significantly larger than the green area Relatively few images are annotated with the Unsure category except a few interesting outliers birth images labeled Unsure and scuba diver The gender cues in these synsets obscured because birth contain images of newborn babies and scuba diver contains people wearing diving suits and helmets The figure for skin color Fig Middle also presents a biased distribution highlighting the underrepresentation of people with dark skin The average percentage of the Dark category across all synsets is only and the synsets with significant portion of Dark align with stereotypes rapper images labeled Dark and basketball player An exception is first lady as most images in this synset are photos of Michelle Obama the First Lady of the United States when ImageNet was being constructed Limitations of demographic annotation Given the demographic analysis it is desired to have a constructive solution to improve the diversity in ImageNet images Publicly releasing the collected attribute annotations would be a natural next step This would allow the research community to train and benchmark machine learning algorithms on different demographic subsets of ImageNet furthering the work on machine fairness However we have to consider that the potential mistakes in demographics annotations are harmful not just for the downstream visual recognition models as all annotation mistakes are but to the people depicted in the photos Mis-annotating gender skin color or age can all cause significant distress to the photographed subject Gender identity and gender expression may not be aligned similarly for skin color or age and thus some annotations may be incorrect despite our best quality control efforts So releasing the image-level annotations may not be appropriate in this context Methodology for increasing image diversity We aim for an alternative constructive solution one that strikes a balance between advancing the communitys efforts and preventing additional harm to the people in the photos One option we considered is internally using the demographics for targeted data collection where we would find and annotate additional images to re-balance each synset However with the known issues of bias in search engine results and the care already taken by the ImageNet team to diversify the images for each synset Sec this may not be the most fruitful route Instead we propose to release a Web interface that automatically re-balances the image distribution within each synset aiming for a target distribution of a single attribute eg gender by removing the images of the overrepresented categories There are two questions to consider first what is an appropriate target distribution and second what are the privacy implications of such balancing First identifying the appropriate target distribution is challenging and we leave that to the end users of the database For example for some applications it might make sense to produce a uniform gender distribution for example if the goal is to train an activity recognition model with approximately equal error rates across genders In other cases the goal might be to re-balance the data to better mimic the real-world distribution of gender race or age in the category as recorded by census data for example instead of using the distribution exaggerated by search engines Note that any type of balancing is only feasible on synsets with sufficient representation within each attribute category For example the synset baby naturally does not contain a balanced age distribution Thus we allow the user to request a subset of the categories to be balanced for example the user can impose equal representation of the three adult categories while eliminating the Child category Second with regard to privacy there is a concern that the user may be able to use this interface to infer the demographics of the removed images For example it would be possible to visually analyze a synset note that the majority of people within the synset appear to be female and thus infer that any image removed during the gender-balancing process are annotated as female To mitigate this concern we always only include of images from the minority category in the balanced images and discard the other Further we only return a balanced distribution of images if at least attribute categories are requested eg the user cannot request a female-only gender distribution and if there are at least images within each requested category While we only balance the distribution of a single attribute eg gender it is desirable to balance across multiple attributes However it will result in too few images per synset after re-balancing For example if we attempt to balance both skin color and gender we will end up with very few images This creates potential privacy concerns with regard to being able to infer the demographic information of the people in the individual photos Results and estimated impact of the demographic balancing on ImageNet Fig provides one example of the effect of our proposed demographic balancing procedure on the synset programmer Based on Original Balancing gender Balancing skin color Balancing age Figure The distribution of images in the ImageNet synset programmer before and after balancing to a uniform distribution our analysis and statistics so far and under the restrictions described in Sec we could offer such a balancing on synsets for gender ignoring the highly skewed Unsure category and posing uniform distribution among Male and Female synsets for skin color uniform distribution for the three categories and synsets for age removing the Child category and posing a uniform distribution for the other three age categories Users can create customized balancing results for each synset by choosing the attribute categories to balance on Limitations of the balancing solution The downside of this solution is that balancing the dataset instead of releasing the image-level attribute annotations makes it impossible to evaluate the error rates of machine learning algorithms on demographic subsets of the data as is common in the literature Nevertheless this strategy is a better alternative than using the existing ImageNet person subtree strong bias releasing the image-level annotations ethically problematic or collecting additional images technically impractical DISCUSSION We took the first steps towards filtering and balancing the distribution of the person subtree in the ImageNet hierarchy The task was daunting as with each further step of annotation and exploration we discovered deeper issues that remain unaddressed However we feel that this is a significant leap forward from the current state of ImageNet We demonstrate that at most out of the existing synsets should remain in the person subtree as others are inappropriate categories for visual recognition and should be filtered out Of the remaining synsets have sufficient data at least images to warrant further exploration On those we provide a detailed analysis of the gender skin color and age distribution of the corresponding images and recommend procedures for better balancing this distribution While categories may seem small in comparison to the current set it is nevertheless sufficiently large-scale to remain interesting to the computer vision community eg the PASCAL dataset has only classes CelebA has attributes COCO has object categories the fine-grained CUB- dataset has bird species Further note that the most commonly used subset of ImageNet is the set of categories in the ImageNet Large Scale Visual Recognition Challenge ILSVRC which remains unaffected by our filtering the three ILSVRC synsets of the person subtree are bridegroom n safe imageability ballplayer n safe imageability and scuba diver n safe imageability There is still much remaining to be done outside the person subtree as incidental people occur in photographs in other ImageNet synsets as well eg in synsets of pets household objects or sports It is likely that the density and scope of the problem is smaller in other subtrees than within this one so the filtering process should be simpler and more efficient We are releasing our annotation interfaces to allow the community to continue this work