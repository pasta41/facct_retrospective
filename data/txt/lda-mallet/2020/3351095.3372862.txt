Garbage In Garbage Out Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose from hiring crowdworkers to the papers authors labeling the data themselves Such a task is quite similar to or a form of structured content analysis which is a longstanding methodology in the social sciences and humanities with many established best practices In this paper we investigate to what extent a sample of machine learning application papers in social computing specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data give specific details about whether such best practices were followed Our team conducted multiple rounds of structured content analysis of each paper making determinations such as Does the paper report who the labelers were what their qualifications were whether they independently labeled the same items whether inter-rater reliability metrics were disclosed what level of training and/or instructions were given to labelers whether compensation for crowdworkers is disclosed and if the training data is publicly available We find a wide divergence in whether such practices were followed and documented Much of machine learning research and education focuses on what is done once a gold standard of training data is available but we discuss issues around the equally-important aspect of whether such data is reliable in the first place CONCEPTS Information systems Content analysis and feature selection Computing methodologies Supervised learning by classification Social and professional topics Project and people management KEYWORDS machine learning data labeling human annotation content analysis training data research integrity meta-research INTRODUCTION Garbage In Garbage Out Machine learning ML has become widely used in many academic fields as well as across the private and public sector Supervised machine learning is particularly prevalent in which training data is collected for a set of entities with known properties a ground truth or gold standard then used to create a classifier that will make predictions about new entities of the same type One of the earliest applications of supervised ML is in spam detection eg Humans would label e-mails as spam or non-spam then classifiers trained on these data would be used to predict if future e-mails were spam or non-spam As Brunton details in his history of spam spam is a messy concept especially when trying to define it globally at scale Like most problems of classification we typically rely on an I know it when I see it standard which often breaks down at edge cases One challenge in spam detection is in operationalization not just defining spam but also creating a systematic procedure to measure it Then assuming a fuzzy social concept like spam is sufficiently defined and operationalized humans are typically still needed to label cases according to this definition and procedure Garbage In Garbage Out is a longstanding aphorism in computing about how flawed input data or instructions will produce flawed outputs Supervised ML requires high-quality training data to produce high-quality classifiers However contemporary ML research and education tends to focus less on obtaining and validating such a training dataset with such considerations often passed over in major textbooks eg The predominant focus is typically on what is done with the training data to produce a classifier with heavy emphasis on mathematical foundations and routine use of clean and tidy toy datasets The process of creating a gold standard or ground truth dataset is routinely black-boxed Many papers in ML venues are expected to use a standard public training dataset with authors comparing various performance metrics on the same dataset While such a focus on what is done to a training dataset may be appropriate for theoretically-oriented basic research in ML this is not the case for supervised ML applications Study overview All approaches of producing a training dataset involve some form of human judgment albeit at varying levels of granularity In this paper we investigate and discuss a wide range of issues and concerns around the curation of human-labeled or human-annotated data in which one or more individuals make discrete assessments of items We report from a study in which a team of six labelers systematically examined a corpus of supervised machine learning application papers in social computing specifically those that classified tweets from Twitter for various purposes For each paper we recorded what the paper does or does not state about the training data used to produce the classifier presented in the paper The bulk of the papers we examined were a sample of preprints or postprints published on ArXiVorg plus a smaller set of published papers sampled from Scopus We determined whether such papers involved an original classification task using supervised ML whether the training data labels were produced from human annotation and if so the source of the human-labeled dataset eg the papers authors Mechanical Turk recruited experts no information given etc For all papers in which an original human-labeled dataset was produced we then made a series of further determinations including if definitions and/or examples were given to labelers if labelers independently labeled the same items if inter-rater reliability metrics were presented if compensation details for crowdworkers were reported if a public link to the dataset was available and more As our research project was a human-labeling project studying other human-labeling projects we took care in our own practices We only have access to the paper reporting about the study and not the actual study itself and many papers either do not discuss such details at all or without sufficient detail to make a determinations For example many papers did note that the study involved the creation of an original human-labeled dataset but did not specify who labeled it Other papers specified such details about who labeled the cases but did not specify whether labelers independently labeled the same items For some of our items one of the most common labels we gave was no information which is a concerning issue given how crucial such information is in understanding the validity of the training dataset and by extension the validity of the classifier LITERATURE REVIEW AND MOTIVATION A different kind of black-boxing in machine learning In the introduction we noted training data is frequently black-boxed in machine learning research and applications We use the term black-boxed in a different way than it is typically invoked in and beyond the FAT community where often refers to interpretability In that sense black-boxing means that even for experts who have access to the training data and code which created the classifier it is difficult to understand why the classifier made each decision In social science and humanities work on black-boxing of ML and other algorithmic systems there is often much elision between issues of interpretability and intentional concealment as Burrell notes A major focus is on public accountability eg where many problematic issues can occur behind closed doors This is even the case with relatively simple forms of analytics and automation such as if-then statements linear regressions or rule-based expert systems In contrast the issue we refer to around training data being black-boxed is more about what is and is not taken for granted in the practice of developing a classifier This use of the term is closer to how it was used by Latour and Woolgar in an ethnographic study of scientific research laboratories Their use of the term was about scientific lab equipment where a mass spectrometer would typically be implicitly trusted to turn samples into signals However when the results showed something drastically unexpected it could either be a problem with the machine or a fundamental scientific breakthrough Scientists and technicians would have to open up the black box changing their relationship to the equipment and run various tests and diagnostics to determine if the problem was with the equipment or the prevailing theory In this view black-boxing is a relational concept rather than an objective property It is a particular orientation that people may or may not have to the same social-technical systems they routinely work with and rely upon Correspondingly opening up the black box is not so much about digging into technical or internal details per se but more of a gestalt shift in whether the output of a system is implicitly taken for granted or open for further investigation In this view black-boxing is not inherently problematic in fact it is a necessary aspect of modern life and especially of globally-distributed big science However the question is about who is obligated to suspend disbelief and who gets to be more skeptical about the inputs and outputs questions that are also being raised in conversations about scientific reproducibility and replicability Scholarship in this same tradition around Science and Technology Studies ethnomethodology anthropology and related qualitative fields has also extensively discussed and analyzed processes of knowledge production that involve human judgment particularly discussing standardization and formal operationalization Content analysis Creating human-labeled training datasets for machine learning often looks like content analysis a well-established methodology in the humanities and the social sciences particularly literature communication studies and linguistics which also has versions used in the life ecological and medical sciences Content analysis has taken many forms over the past century from more positivist methods that formally establish structural ways of evaluating content to more interpretivist methods that embrace ambiguity and multiple interpretations such as grounded theory The intersection of ML and interpretivist approaches is outside of the scope of this article but it is an emerging area of interest eg Today structured content analysis also called closed coding is used to turn qualitative or unstructured data of all kinds into structured and/or quantitative data including media texts freeform survey responses interview transcripts and video recordings Projects usually involve teams of coders also called annotators labelers or reviewers with human labor required to code annotate or label a corpus of items Note that we use such terms interchangeably in this paper In one textbook content analysis is described as a systematic and replicable p method with several best practices A coding scheme is defined which is a set of labels annotations or codes that items in the corpus may have Schemes include formal definitions or procedures and often include examples particularly for borderline cases Next coders are trained with the coding scheme which typically involves interactive feedback Training sometimes results in changes to the coding scheme in which the first round becomes a pilot test Then annotators independently review at least a portion of the same items throughout the entire process with a calculation of inter-annotator agreement or inter-rater reliability Finally there is a process of reconciliation for disagreements which is sometimes by majority vote without discussion and other times discussion-based Structured content analysis is a difficult complicated and labor-intensive process requiring many different forms of expertise on the part of both the coders and those who manage them Historically teams of students have often performed such work With the rise of crowdwork platforms like Amazon Mechanical Turk crowdworkers are often used for content analysis tasks which are often similar to other kinds of common crowdworking tasks Googles reCAPTCHA is a Turing test in which users perform annotation tasks to prove their humanness which initially involved transcribing scanned phrases from books but now involves image labeling for autonomous vehicles There are major qualitative data analysis software tools that scaffold the content analysis process to varying degrees such as MAXQDA or NVivo which have support for inter-annotator agreement metrics There have also been many new software platforms developed to support more micro-level annotation or labeling at scale including in citizen science linguistics content moderation and more general-purpose use cases For example the Zooniverse provides a common platform for citizen science projects across different domain application areas which let volunteers make judgements about items which are aggregated and reconciled in various ways Meta-research and methods papers in linguistics and crowdsourcing Our paper is also in conversation with various meta-research and standardization efforts in linguistics crowdsourcing and other related disciplines Linguistics and Natural Language Processing have long struggled with issues around standardization and reliability of linguistic tagging Linguistics researchers have long developed best practices for corpus annotation eg including recent work about using crowdworkers Annotated corpus projects often release guidelines and reflections about their process For example the Linguistic Data Consortiums guidelines for annotation of English-language entities version is single-spaced pages A universal problem of standardization is that there are often too many standards and not enough enforcement As notes of linguistics/NLP papers in various venues do not even mention the name of the language being studied usually English A meta-research study found only in qualitative papers in Human-Computer Interaction reported inter-rater reliability metrics Another related area are meta-research and methods papers focused on identifying or preventing low-effort responses from crowdworkers sometimes called spam or random responses or alternatively fraudsters or cheaters Rates of self-agreement are often used determining if the same person labels the same item differently at a later stage One paper examined crowdsourced datasets for sentiment analysis and found none had self-agreement rates Krippendorfs alpha above with some lower than Another paper recommends the self-agreement strategy in conjunction with asking crowdworkers to give a short explanation of their response even if the response is never actually examined One highly-cited paper proposes a strategy in which crowdworkers are given some items with known labels a gold/ground truth and those who answer incorrectly are successively given more items with known labels with a Bayesian approach to identifying those who are answering randomly The data documentation movements Our paper is also in conversation with two related movements in computationally-supported knowledge production that have surfaced issues around documentation First we see connections with the broader open science and reproducibility movements Open science is focused on a range of strategies including open access research publications educational materials software tools datasets and analysis code The reproducibility movement is deeply linked to the open science movement focusing on getting researchers to release everything that is necessary for others to perform the same tasks needed to get the exact same results This increasingly includes pushing for high standards for releasing protocols datasets and analysis code As more funders and journals are requiring releasing data the issue of good documentation for data and protocols is rising There are also intersecting literatures on systems for capturing information in ML data flows and supply chains as well as supporting data cleaning These issues have long been discussed in the fields of library and information science particularly in Research Data Management A major related movement is in and around the FATML field with many recent papers proposing training data documentation in the context of ML Various approaches analogies and metaphors have been taken in this area including datasheets for datasets model cards data statements nutrition labels a bill of materials data labels and supplier declarations of conformity Many go far beyond the concerns we have raised around human-labeled training data as some are also or primarily concerned with documenting other forms of training data model performance and accuracy bias considerations of ethics and potential impacts and more We discuss how our findings relate to this broader emerging area more in the concluding discussion DATA AND METHODS Data machine learning papers performing classification tasks on Twitter data Our goal was to find a corpus of papers that were using original human annotation or labeling to produce a new training dataset for supervised ML We restricted our corpus to papers whose classifiers were trained on data from Twitter for various reasons First we did attempt to produce a broader corpus of supervised ML application papers but found our search queries in academic search engines would either be so broad that most papers were non-applied theoretical papers or papers re-using public pre-labeled datasets or that the results were so narrow they excluded many canonical papers in this area which made us suspect that they were nonrepresentative samples Sampling to papers using Twitter data has strategic benefits for this kind of initial study Data from Twitter is of interest to scholars from a variety of disciplines and topical interest areas in addition to those who have an inherent interest in Twitter as a social media site As we detail in appendix section the papers represented political science public health NLP sentiment analysis cybersecurity content moderation hate speech information quality demographic profiling and more We drew the main corpus of ML application papers from ArXiV the oldest and most established preprint repositories originally for researchers to share papers prior to peer review Today ArXiV is widely used to share both drafts of papers that have not yet passed peer review preprints and final versions of papers that have passed peer review often called postprints Users submit to any number of disciplinary categories and subcategories Subcategory moderators perform a cursory review to catch spam blatant hoaxes and miscategorized papers but do not review papers for soundness or validity We sampled all papers published in the Computer Science subcategories of Artificial Intelligence csAI Machine Learning csLG Social and Information Networks csSI Computational Linguistics csCL Computers and Society csCY Information Retrieval csIR and Computer Vision CSCV the Statistics subcategory of Machine Learning statML and Social Physics physics-soc-ph We filtered for papers in which the title or abstract included at least one of the words machine learning or case insensitive We then filtered to papers in which the title or abstract included at least twitter or tweet case insensitive which resulted in papers We used the same query on Elseviers Scopus database of peer-reviewed articles selecting randomly sampled articles which mostly selected from conference proceedings One paper from the Scopus sample was corrupted so only papers were examined ArXiV is likely not a representative sample of all ML publications However we chose it because ArXiV papers are widely accessible to the public indexed in Google Scholar and other scholarly databases and are generally considered citeable publications The fact that many ArXiV papers are not peer-reviewed and that papers posted are not likely representative samples of ML research is worth considering when reflecting on the generalizability of our findings However given that such papers are routinely discussed in both academic literature and the popular press means that issues with their reporting of training data is just as crucial Sampling from ArXiv also lets us examine papers at various stages in the peer-review cycle breaking out preprints not yet published preprints of later published papers and postprints of published works The appendix details both corpora including an analysis of the topics and fields of papers in an analysis of the publishers and publication types eg an early preprint of a journal article a final postprint of a conference proceeding a preprint never published and publishers in and The final dataset can be found on GitHub and Zenodo Labeling team training and workflow Our labeling team included one research scientist who led the project RSG and undergraduate research assistants who worked for course credit as part of an university-sponsored research experience program KY MD JQ and JH The project began with five students for one semester four of whom continued on the project for the second semester A sixth student replaced the student who did not continue All students had some coursework and in computer science and/or data science with a range of prior experience in machine learning in both a classroom and applied setting Students majors and minors included Electrical Engineering Computer Science Data Science Statistics and Linguistics The labeling workflow was that each week a set of papers were randomly sampled each week from the unlabeled set of ArXiV papers in the corpus For two weeks the sampled papers from Scopus were selected The five students independently reviewed and labeled the same papers each week using a different web-based spreadsheet to record labels The team leader synthesized labels and identified disagreement The team met in person each week to discuss cases of disagreement working to build a consensus about the proper label as opposed to purely majority vote The team leader facilitated these discussions and had the final say when a consensus could not be reached The papers labeled for the first two weeks were in a training period in which the team worked on a different set of papers not included in the dataset In these initial weeks the team learned the coding schema and the reconciliation process which were further refined Second round verification and reconciliation After papers were labeled by five annotators we conducted a second round of verification This was necessary both because there were some disagreements in labeling and changes made to the coding schema discussed in appendix All labels for all papers were independently re-examined by at least two of the six team members Annotators were given a summary of the original labels in the first round and were instructed to review all papers being mindful of how the schema and instructions had changed We then aggregated reconciled and verified labels in the same way as in the first round For papers where there was no substantive disagreement on any question between those who re-examined it in the second round the papers labels were considered to be final For papers where there was any substantive disagreement on any question the paper was either discussed to consensus in the same manner as in the first round or decided by the team leader The final schema and instructions are in the appendix section Finally we cleaned up issues with labels around implicit or blank values using rule-based scripts We learned our process involved some ambiguities around whether a subsequent value needed to be filled in For example if a paper was not using crowdworkers then the instructions for our schema were that the question about crowdworker compensation was to remain blank However we found we had cases where reported crowdworker compensation was no for papers that did not use crowdworkers This would be concerning had we had a yes for such a variable but found no such cases We recoded questions about pre-screening for crowdwork platforms implied by using crowdworkers in original human annotation source and the number of human annotators We measured interrater reliability metrics using mean percent total agreement or the proportion of cases where all labelers initially gave the same label This is a more stringent metric than Fleisss kappa and Krippendorfs alpha and our data does not fit the assumptions for those widely-used metrics IRR rates for round one were relatively low across all questions the mean percent total agreement was with the lowest question having a rate of IRR rates for round two were quite higher the mean percent total agreement across all questions was and the lowest agreement score was for used external human annotation which we discuss later We are confident about our labeling process especially because these individual ratings were followed by an expert-adjudicated discussion-based reconciliation process rather than simply counting majority votes We detail more information and reflection about interrater reliability in appendix section Raw and normalized information scores We quantified the information about training data in papers developing a raw and normalized information score as different studies demanded different levels of information For example our question about whether inter-annotator agreement metrics were reported is only applicable for papers involving multiple annotators Our questions about whether prescreening was used for crowdwork platforms or whether crowdworker compensation was reported is only relevant for projects using crowdworkers However some kinds of information are relevant to all papers that involve original human annotation who the annotators are annotation source annotator training formal instructions or definitions were given the number of annotators involved whether multiple annotators examined the same items or a link to a publicly-available dataset For raw scores papers involving original human annotation received one point each for reporting the six items mentioned above In addition they received one point per question if they included information for each of the two questions about crowdworkers if the project used crowdworkers and one point if they reported inter-annotator metrics if the project used multiple annotators per item For the normalized score the raw score was divided by the highest possible raw score We only calculated scores for papers involving original human annotation Finally we conducted an analysis of information scores by various bibliometric factors which required determining such factors for all papers For all ArXiV papers we determined whether the PDF was a pre-print not yet published in another venue a post-print identical in content to a published version or a pre-print version of a paper published elsewhere with different content For all Scopus papers and ArXiV post-prints we also determined the publisher We detail these in appendix FINDINGS Original classification task The first question was whether the paper was conducting an original classification task using supervised machine learning Our keyword-based process of generating the corpus included many papers not in this scope However defining the boundaries of supervised ML and classification tasks is difficult particularly for papers that are long complex and ambiguously worded We found that some papers claimed to be using ML but when we examined the details these did not fall under our definition We defined machine learning broadly using a common working definition in which machine learning includes any automated process that does not exclusively rely on explicit rules in which the performance of a By if neither crowdworkers nor multiple annotators were used by if multiple annotators were used by if crowdworkers were used and by if both were used task increases with additional data This includes simple linear regressions for example and there is much debate about if and when simple linear regressions are a form of ML However as we were also looking for classification tasks linear regressions were only included if it is used to make a prediction in a set of defined classes We defined an original classifier to mean a classifier the authors made based on new or old data which excludes the exclusive use of pre-trained classifiers or models Table Original classification task Count Proportion Yes No Unsure As table shows the overwhelming majority of papers in our dataset were involved in an original classification task We placed papers in the unsure category meaning they did not give enough detail for us to make this determination or that they were complex boundary cases One of the unsure cases clearly used labels from human annotation and so we answered the subsequent questions which is why the counts in Table add up to as well as some other seeming disparities in later questions Labels from human annotation One of the major issues we had to come to a consensus around was whether a paper used labels from human annotation We observed a wide range of cases in which human judgment was brought to bear on the curation of training data Our final definition required that the classifier was at least in part trained on labeled data that humans made for the purpose of the classification problem We decided on a working definition that excluded many clever uses of metadata from this category but did allow some cases of self-annotation from social media which were typically the most borderline cases on the other side For example one case from our examples we decided was human annotation used specific politically-inflected hashtags to automatically label tweets as for or against a position for use in stance detection eg ProChoice versus ProLife However these cases of self-annotation would all be considered external human annotation rather than original human annotation and so the subsequent questions about the annotation process would be not applicable Another set of borderline cases involved papers where no human annotation was involved in the curation of the training dataset that was used to build the classifier but human annotation was used for validation purposes We did not consider these to involve human annotation as we originally defined it in our schema even though the same issues arise with equal significance for the validity of such research Table Labels from human annotation Count Proportion Yes No Unsure Used original human annotation and external human annotation Our next two questions were about whether papers that used human annotation used original human annotation which we defined as a process in which the papers authors obtained new labels from humans for items It is common in ML research to re-use public datasets and many of papers in our corpus did so We also found papers in which external and original human annotation was combined to create a new training dataset For these reasons we modified our schema to ask separate questions for original and external human annotation data to capture all three cases using only original only external or both Tables and show the breakdown for both questions We only answered the subsequent questions about the human annotation process for the papers producing an original human annotated dataset Table Used original human annotation Count Proportion Yes No Unsure Table Used external human annotation data Count Proportion No Yes Unsure Original human annotation source Our next question asked who the annotators were for the papers that used original human annotation The possible options were the papers authors Amazon Mechanical Turk other crowdworking platforms experts/professionals other and no information We took phrases like we labeled with no other details to be an implicit declaration that the papers authors did the labeling If the paper discussed labelers qualifications for the task beyond an average person we labeled it as experts professionals For example some of our boundary cases involved recruiting students to label sentiment One study involved labeling tweets with both English and Hindi text and noted that the students were fluent in both languages which we considered to be in the experts professionals category Another paper we included in this category recruited students to label tweets with emojis noting that the recruited students are knowledgeable with the context of use of emojis As table shows we found a diversity of approaches to the recruitment of human annotators The plurality of papers involved the papers authors doing the annotation work themselves The next highest category was no information which was found in almost a quarter of the papers using original human annotation Experts professionals was far higher than we expected although we took any claim of expertise for granted Crowdworkers constituted a far smaller proportion than we expected with Amazon Mechanical Turk and other platforms collectively comprising about of papers Almost all of the other crowdworking platforms specified were CrowdFlower/FigureEight with one paper using oDesk Table Original human annotation source Count Proportion Papers authors No information Experts professionals Amazon Mechanical Turk Other crowdwork Other Number of human annotators Our instructions for the question about the number of human annotators was not precise and had one of the lower levels of inter-rater reliability If the paper included information about the number of human annotators the instructions were to put such a number leaving the field blank for no information Most of the disagreement was from differences around how papers report the number of annotators used For example some papers specified the total number of humans who worked on the project annotating items while others only specified how many annotators were used per item particularly for those using crowdworkers and a few reported both Some involved a closed set of annotators who all examined the same set of items similar to how our team operated Other papers involved an open set of annotators particularly drawn from crowdworking platforms but had a consistent number of annotators who reviewed each item Due to these inconsistencies we computationally re-coded responses into the presence of information about the number of human annotators These are both important aspects to discuss although it is arguably more important to discuss the number of annotators who reviewed each item In general having more annotators review each item provides a more robust way of determining the validity of the entire process although this also requires caluclating inter-annotator agreement metrics Table Number of annotators specified Count Proportion Yes No As table shows a slim majority of papers using original human annotation specified the number of annotators involved in some way Based on our experiences we typically noticed that papers discussing the number of annotators often fell into two categories a small closed team more often sometimes that were either the papers authors or recruited directly by the authors who tended to perform the same amount of work for the duration of the project or a medium to large open set of annotators typically but not necessarily recruited through a crowdworking platform who each performed highly variable amounts of work Formal definitions and instructions Our next question was about whether instructions or guidelines with formal definitions or examples are reportedly given to annotators Formal definitions and concrete examples are both important as they help annotators understand how the researchers have operationalized the concept in question and determine edge cases With no or ambiguous definitions/examples there could be fundamental misunderstandings that are not captured by inter-annotator agreement metrics if all annotators make the same misunderstandings We defined two levels giving no instructions beyond the text of a question then giving definitions for each label and/or concrete examples The paper must describe or refer to instructions given or include them in supplemental materials otherwise we categorized it No Information Some borderline cases involved authors labeling the dataset themselves where the paper presented a formal definition but only implied that it informed the labeling which we took to be a formal definition As table shows the plurality of papers did not provide enough information to make a determination it is rare for authors to say they did not do something but provided definitions or examples Table Formal instructions Count Proportion No information Instructions w/ formal definitions/examples No instructions beyond question text Training for human annotators We defined training for human annotators to involve some kind of interactive process in which the annotators have the opportunity to receive some kind of feedback and/or dialogue about the annotation process We identified this as a distinct category from both the qualifications of the annotators and the instructions given to annotators which are examined in other questions Training typically involved some kind of live session or ongoing meeting in which annotators progress was evaluated and/or discussed where annotators had the chance to ask questions or receive feedback on why certain determinations did or did not match definitions or a schema We used our own teams process as an example of this and found several papers that used a similar roundtable process which went into detail about interactions between team members Cases in which the paper only specified that annotators were given a video or a detailed schema to review were not considered training details as this was a one-way process and counted as definitions/instructions Table Training for human annotators Count Proportion No information Some training details The overwhelming majority of papers did not discuss such issues as table shows with of papers involving a training session Because we had a quite strict definition for what constitutes training versus what many may think of around trained annotators this is expected We also are not all that concerned with this low number as there are many tasks that likely do not require specialized training unlike our project which required both specific expertise in an area and with our complicated schema Pre-screening for crowdwork platforms Crowdwork platforms let employers pre-screen or test for traits skills or performance metrics which significantly narrows the pool of crowdworkers For example project-specific pre-screening involves offering a sample task with known outcomes if the crowdworker passed they would be invited to annotate more items of the papers using crowdworkers reported using this approach Platforms also often have location-based screening eg US-only which papers reported using Some crowdwork platforms have a qualification for workers who have a positive track record based on total employer ratings eg AMT Master Platforms also offer generic skills-based tests for certain kinds of work eg CrowdFlowers Skill Tests These last two qualifications were in our coding schema but no papers reported using them Table Prescreening for crowdwork platforms Count Proportion Project-specific prescreening Location qualification No information Multiple annotator overlap and reporting inter-annotator agreement Our next two questions were about using multiple annotators to review the same items multiple annotator overlap and whether inter-annotator agreement metrics were reported Having multiple independent annotators is typically a foundational best practice in structured content analysis so that the integrity of the annotations and the schema can be evaluated although see For multiple annotator overlap our definitions required papers state whether all or some of the items were labeled by multiple labelers otherwise no information was recorded Then for papers that did multiple annotator overlap we examined whether any inter-annotator agreement metric was reported We did find one paper that did not explicitly state that multiple labelers overlapped but did report inter-annotator agreement metrics This implicitly means that at least some of the items were labeled by multiple labelers but for consistency we keep the no information label for this case We did not record what kind of inter-annotator metric was used such as Cohens kappa or Krippendorffs alpha but many different metrics were used We also did not record what the exact statistic was although we did notice a wide variation in what was considered an acceptable or unacceptable score for inter-annotator agreement Table Multiple annotator overlap Count Proportion No information Yes for all items Yes for some items No Table Reported inter-annotator agreement Count Proportion Yes No For multiple annotator overlap table shows that just under half of all papers that involved an original human annotation task did not provide explicit information one way or the other about whether multiple annotators reviewed each item This includes the one paper that reported inter-annotator agreement metrics but did not specify whether overlap was for all items or some items Only three papers explicitly stated that there was no overlap among annotators and so it is quite likely that the papers that did not specify such information did not engage in such a practice For the papers that did involve some kind of multiple annotator overlap the overwhelming majority of this subsample involved multiple annotation of all items rather than only some items We also found that for papers that did involve some kind of multiple overlap the large majority of them did report some metric of inter-annotator agreement as table indicates Reported crowdworker compensation Crowdworking is often used because of the low cost which can be far below minimum wage in certain countries Researchers and crowdworkers have been organizing around issues related to the exploitation of crowdworkers in research advocating ethical practices including fair pay We examined all papers involving crowdworkers for any indication of compensation and found zero mentioned compensation We did find that some papers using other sources of human annotation eg students discussed compensation for annotators but this was not in our original schema Link to dataset available Our final question was about whether the paper contained a link to the dataset containing the original human annotated training dataset Note that this question was only answered for papers involving some kind of original or novel human annotation and papers that were exclusively re-using an existing open or public dataset were left blank to avoid double-counting We did not follow such links or verify that such data was actually available As table shows the overwhelming majority of papers did not include such a link with papers using original human-annotated training datasets linking to such data Given the time labor expertise and funding in creating original human annotated datasets authors may be hesitant to release such data until they feel they have published as many papers as they can Table Link to dataset available Count Proportion No Yes PAPER INFORMATION SCORES The raw and normalized information scores see section for methodology were calculated for all papers that involved original human annotation As previously discussed our corpora represent a likely non-representative sample of ML research even if bounded to social computing Our relatively small sample sizes combined with the number of multiple comparisons would mean that thresholds for statistical significance would need to be quite high Instead we present these results to help provide an initial framework and limited results on this issue intended to help inform a broader and more systematic evaluation the ML literature We do observe quite varying ranges and distributions of information scores which does give evidence to the claim that there is substantial and wide variation in the practices around human annotation training data curation and research documentation Overall distributions of information scores Figure shows histograms for raw and normalized information scores which both suggest a bimodal distribution with fewer papers at the both extremes and the median This suggests that there are roughly two populations of researchers with one centered around raw scores of and normalized scores of and one centered around raw scores of and normalized scores of The normalized information score ranged from to with papers having a normalized score of and only paper with a score of The raw information score ranged from to with no paper receiving a full score of or which would have required a study involving crowdworkers multiple overlap and open datasets Overall the mean normalized information score was with a median of and a standard deviation of The mean raw score was with a median of and a standard deviation of Figure Histograms of raw and normalized information scores for all papers involving original human annotation Information scores by corpus and publication type Figure shows two boxplots of normalized information scores that are based on different intersecting categories of publication type and status The left figure compares scores in four categories all papers in the Scopus sample non-ArXived ArXiv preprints that were never or are not yet published and ArXiv preprints that were either postprints or preprints of a traditional publication The category with the lowest median score are papers from the Scopus sample which is followed closely by ArXiv preprints never published although preprints never published had a much larger IQR and standard deviation Postprints of publications had a similar IQR and standard deviation as preprints never published but a much higher median score Preprints of publications had a similar median score as postprints but with a much smaller IQR and standard deviation The righthand figure plots publication types for the combined corpora Conference proceedings and ArXiv preprints never published have somewhat similar medians and IQRs with journal articles having a higher median of and a much narrower IQR While we hesitate to draw generalizable conclusions we see these findings indicating a wide range of factors potentially at play The main box is the inter-quartile range IQR or the th th percentiles The middle red line is the median the green triangle is the mean and the outer whiskers are th th percentiles Figure Boxplots of normalized information scores by type of paper Top scores by corpus and preprint/postprint status Bottom scores from both corpora by publication type Information scores by publisher Figure shows boxplots for normalized information scores by publisher split between papers sampled from ArXiv and Scopus The boxplots are ordered by the median score per publisher In papers in the ArXiv corpus those that were pre- or post-prints of papers published by the professional societies Association for Computing Machinery ACM or Association of Computational Linguistics ACL tied for the highest median scores of with similar IQRs These were followed by Springer and Elsevier with respective medians and and narrower IQRs ArXiv preprints not published elsewhere had a median score of and the highest IQR and standard deviation suggesting that it represents a wide range of papers The publishers at the lower end of the scale included AAAI with a median of and a narrower IQR and IEEE with a median of and the second-highest IQR and standard deviation Curiously papers from the Scopus corpus show different results per-publisher with the median scores of all publishers lower in the Scopus corpus than in the ArXiv corpus However given the small number of papers in the Scopus sample we hesitate to draw general conclusions from this finding and leave this for future research Figure Boxplots of normalized information scores by publisher and corpus ordered by median score CONCLUDING DISCUSSION Findings In the sample of ML application publications using Twitter data we examined we found a wide range in levels of documentation about methodological practices in human annotation While we hesitate to overly generalize our findings to ML at large these findings do indicate concern given how crucial the quality of training data is and the difficulty of standardizing human judgment Yet they also give us hope as we found a number of papers we considered to be excellent cases of reporting the processes behind their datasets About half of the papers using original human annotation engaged in some form of multiple overlap and about of the papers that did multiple overlap reported metrics of inter-annotator agreement The distribution of annotation information scores was roughly bimodal suggesting two distinct populations of those who provide substantially more and less information about training data in their papers We do see preliminary evidence that papers in our sample published by certain publishers/venues tended to have papers with far more information than others eg ACM and ACL at the top end followed closely by journal publishers Springer and Elsevier with IEEE and AAAI proceedings at the lower end Preprints exclusively published on ArXiv also had the widest range of scores Implications Based on our findings and experiences in this project we believe human annotation should be considered a core aspect of the research process with as much attention care and concern placed on the annotation process as is currently placed on performance-based metrics like scores Our findings while preliminary descriptive and limited in scope tell us that there is much room for improvement This paper also makes steps towards more largescale and systematic analyses of the research landscape as well as towards standards and best practices for researchers and reviewers Institutions like journals funders and disciplinary societies have a major role to play in solutions to these issues Most publications have strict length maximums and many papers we scored highly spent at least a page or more describing their process Reviewer expectations are crucial in any discussion of the reporting of methodological details in research publications It could be that some authors did include such details but were asked to take it out and add other material instead Authors have incentives to be less open about the messiness inherent in research as this may open them up to additional criticism We see many parallels here to issues around reproducibility and open science which are increasingly being tackled by universal requirements from journals and funders rather than relying on individuals to change norms Such research guidelines are common including the COREQ standard for qualitative data analysis reporting a requirement by some journals A number of proposed standards have been created around datasets for ML which are often framed as potential ways to mitigate bias and improve transparency and accountability Several of these are broader proposals around reporting information about ML classifiers and models which include various aspects beyond our study In fact given the recent explosion of proposals for structured disclosure or transparency documents around ML the Partnership on AI has recently created the ABOUT ML working group to arrive at a common format or standard It is important to frame this issue as one of research validity and integrity what kind of information about training data is needed for researchers reviewers and readers to have confidence in the model or classifier As we observed in our discussions we became skeptical about papers that did not adequately describe their human annotation processes However human annotation is a broad and diverse category of analytical activity encompassing a wide range of structured human judgment brought to bear on items some far more straightforward or complex We saw the wide range papers that were engaged in various forms of annotation or labeling even though we bounded our study to papers using data from Twitter One important distinguishing factor is the difficulty of the task and the level of specific knowledge needed to complete it which can vary significantly Another key distinction may be between when there is expected to be only one right answer and when there might be many valid answers Most importantly we would not want a straightforward checklist to over-determine issues of research integrity A number of papers we read were missing details we thought were crucial for understanding that study but would not make sense for a majority of papers we examined If a checklist was created it should not be seen as an end in itself The classic principle of scientific replicability could be a useful heuristic does the paper provide enough information about the labeling process such that any reader could with sufficient resources and access to the same kind of human annotators conduct a substantively identical human annotation process on their own We also see a role for technical solutions to help scaffold adherence to these best practices For example major qualitative data analysis platforms like MAXQDA or NVivo have built-in support for inter-annotator agreement metrics Several crowdsourcing and citizen science platforms for data labeling are built to support reconciliation for disagreements Automated workflow pipeline and provenance tracking is an increasing topic in ML although these can focus more on model building and tuning taking data as given We recommend such projects include human annotation as a first-class element with customization as needed Finally our own experience in this human annotation project studying human annotation projects has shown us the costs and benefits of taking an intensive detailed collaborative and multistage approach to human annotation On one side we believe that after going through such a long process we have not only better data but also a much better contextual understanding of our object of study Yet on the other hand even though struggling over the labels and labeling process is an opportunity our time- and labor-intensive process did have a direct tradeoff with the number of items we were able to annotate These issues and tradeoffs are important for ML researchers to discuss when designing their own projects and evaluating others Limitations and future work Our study has limitations as we only examined a sample of publications in the ML application space First we only examined papers that performing a classification task on tweets which is likely not a representative sample of ML application publications We would expect to find different results in different domain application areas Papers in medicine and health may have substantially different practices around reporting training data due to strict reporting standards in clinical trials and related areas We also generally examined papers that are posted on ArXiV in addition to papers sampled from Scopus and ArXiV is likely to not be a representative sample of academic publications ArXiV papers are self-submitted and represent a range of publication stages from drafts not submitted to review preprints in peer review and postprints that have passed peer review Future work should examine different kinds of stratified random samples to examine differences between various publishers publication types disciplines topics and other factors Our study only examined a set of the kinds of issues that scholars and practitioners in ML are examining when they call for greater transparency and accountability through documentation of datasets and models We have not recorded information about what exactly the rates of inter-annotator agreement are In particular we did not record information about the reconciliation or adjudication process for projects which involve multiple overlap eg majority rule talking to consensus which we have personally found to be a crucial and difficult process Other questions we considered but did not include were the demographics of the labelers the number of labelers total and per item compensation beyond crowdworkers whether instructions or screenshot of the labeling interface was included and whether labelers had the option to choose unsure vs being forced to choose a label We leave this for future work but also found that each additional question made it more difficult for labelers We also considered but did not have our team give a holistic score indicating their confidence in the paper eg a score like those used in some peer reviewing processes Our study also has limitations that any human annotation project has and we gained much empathy around the difficulties of human annotation Our process is not perfect and as we have analyzed our data we have identified cases that make us want to change our schema even further or reclassify boundary cases In future work we would also recommend using a more structured and constrained system for annotation to capture the text that annotators use to justify their answers to various questions ML papers are very long and complex such that our reconciliation and adjudication process was very time-consuming Finally we only have access to what the publications say about the work they did and not the work itself Future work could improve on this through other methods such as ethnographic studies of ML practitioners APPENDIX See the supplementary materials for the appendix