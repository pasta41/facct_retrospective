Multi-layered Explanations from Algorithmic Impact Assessments in the GDPR Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability The aim of this paper is to address how Data Protection Impact Assessments DPIAs Art in the European Union EUs General Data Protection Regulation GDPR link the GDPRs two approaches to algorithmic accountability individual rights and systemic governance and potentially lead to more accountable and explainable algorithms We argue that algorithmic explanation should not be understood as a static statement but as a circular and multi-layered transparency process based on several layers general information about an algorithm group-based explanations and legal justification of individual decisions taken We argue that the impact assessment process plays a crucial role in connecting internal company heuristics and risk mitigation to outward-facing rights and in forming the substance of several kinds of explanations CONCEPTS Applied computing Law social and behavioral sciences Computing methodologies Machine learning KEYWORDS Law General Data Protection Regulation Impact Assessments Introduction The discussion of the GDPR General Data Protection Regulation and algorithmic accountability has largely focused on whether there is an individual right to explanation of an algorithmic decision Only more recently have legal scholars begun to focus on the GDPRs systemic accountability tools Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability The aim of this paper is to address how Data Protection Impact Assessments DPIAs Art link the GDPRs two approaches to algorithmic accountability individual rights and systemic governance and potentially lead to more accountable and explainable algorithms Indeed we argue that algorithmic explanation should not be understood as a static statement but as a circular and multi-layered transparency process based on several layers general information about an algorithm group-based explanations and legal justification of individual decisions taken We argue that the impact assessment process plays a crucial role both in connecting internal company heuristics and risk mitigation to outward-facing individual rights and in forming the substance of several kinds of explanations Section introduces the algorithmic accountability tools in the GDPR while Section explores the individual rights of data subjects Section develops the idea of collaborative governance of algorithms in the GDPR The subsequent sections address the concept of an Algorithmic Impact Assessment and how the DPIA in the GDPR can serve as an AIA The final section explains how the combination of Algorithmic DPIA and individual rights can create a virtuous circle of multi-layered explanations Algorithmic Accountability in the GDPR The GDPR has significant implications for algorithmic decision-making At first the legal debate focused on whether the GDPR created an individual right to an explanation of an individual algorithmic decision Subsequent legal analysis however began to focus instead on other accountability tools either required in the text of the GDPR or recommended in interpretative guidelines These tools include third-party auditing the appointment of Data Protection Officers DPOs Art and the requirement of Data Protection Impact Assessments DPIAs Art As one of us has argued the GDPR combines a series of individual rights Arts with a systemic governance regime overseen by regulators Arts throughout These two systems interact and overlap An individual right is often also a companys duty But even if individuals data subjects fail to invoke their rights companies data controllers have significant obligations both procedural and substantive under the GDPR For example a data subject has a right to contest an individual algorithmic decision Art to receive notice of solely automated decision-making Art and to request access to meaningful information about the logic involved Art Should she fail to invoke any of these rights however the GDPR still puts in place significant obligations on data controllers using automated decision-making whether solely automated or not The GDPR requires an array of systemic accountability tools including third-party auditing the appointment of Data Protection Officers DPOs Art and Data Protection Impact Assessments DPIAs Art These obligations arise from the text of the law but also in accompanying Recitals and in the Guidelines on Automated Individual Decision-making and Profiling Guidelines on ADM released in October and revised in February by the Article Working Party It is also crucial to understand the mode through which the GDPR governs The GDPR largely governs both in the sense of coming up with the substance of data controllers duties and in the sense of monitoring compliance with them through an approach known in legal literature as collaborative governance the use of public-private partnerships Because the GDPR often effectively outsources governance decisions to single data controllers accountability takes on added significance Accountability in the GDPR is not just about protecting individual rights It is about ensuring that this process of co-governing with private parties receives appropriate oversight from the public and both expert and interested third parties With this background in mind the next two sections of this paper go into more detail on both the individual rights and systemic governance elements of the GDPRs approach to algorithmic accountability before turning to the role the Data Protection Impact Assessment DPIA plays in linking the two Individual Rights in the GDPR and the multi-layered Explanation The GDPR gives data subjects several important rights with respect to algorithmic decision-making The GDPR contains both general data protection rights and rights specific to profiling that also apply to algorithmic decision-making On top of this it establishes rights specific to algorithmic decision-making which include a right to be notified of solely automated decision-making Arts a right of both notification and access to meaningful information about the logic involved Arts a right to be informed of the significance of and envisaged effects of solely automated decision-making Arts and a right not to be subject to solely automated decision making Art with safeguards for the limited cases in which automated decision-making is permitted Those safeguards include but are not limited to a right to contest a decision to express ones point of view and to human intervention Art We do not intend to revisit the legal debate over these rights in detail but an overview may be useful As mentioned discussion of these individual rights has largely focused on whether or not or really how solely automated decisionmaking must be explained to data subjects As Selbst Powles point out it is disingenuous to say that there is no right to an explanation in the GDPR the GDPRs text clearly requires companies to explain at least meaningful information about the logic involved in automated decision-making in addition to its significance and envisioned effects Arts What this information constitutes in practice however has been the subject of hot debate including whether it is a system-wide model-wide explanation or is an explanation specific to individual decisions and what depth of explanation is required The core debate has primarily focused on whether or not Article creates an ex post right to explanation of an individual decision made by an automated system Our view explored at length elsewhere is that it does Such decisions must be made legible to data subjects in the sense that data subjects must be able to understand enough about the decisionmaking process to be able to invoke their other rights under the GDPR Several of the Member States implementing Article b of the GDPR have outlined such Article explanation duties in greater detail As we discuss below our view is that the GDPRs transparency rights are best discussed together as a system The GDPR that is is best understood as establishing a system of multi-layered explanations Data subjects have a right to both a system-wide but detailed description of the logic of an algorithm Arts and more specific insights on individual decisions taken Art and recital The level of granularity of explanations provided depends on several factors and is not clearly dictated in the GDPR It is important to remark that different data controllers have different accountability duties Article of the GDPR states that taking into account the nature scope context and risks of data processing the controller shall implement appropriate technical and organisational measures to ensure compliance with the GDPR Accordingly algorithmic decision-making involving bigger risks for data subjects considering the consequences and implications of the decision or the predictive power of the data controller should entail more safeguards As we will explain in Section data controllers to a certain extent choose their own algorithmic accountability safeguards some are required in the GDPR both at Article and at recital algorithmic auditing ensuring the right to context to have a new decision and to a human in the loop but also the right to explanation but these are not closed lists and the Guidelines on ADM suggest additional techniques Accordingly in case of more intrusive and riskier automated decision-making processes the data controller should implement all possible safeguards including the right to explanation of an individual decision However there have been legitimate concerns voiced in the legal literature about the capacity of data subjects to both invoke their rights and execute oversight over algorithmic decision-making These range from concerns about access to justice to concerns about individual capacity and expertise Consequently most policy proposals call either for a dual regime like the GDPR that mixes individual rights with other more systemic forms of accountability or for foregoing individual accountability in favour of expert and external oversight Foregoing individual accountability ignores the dignitary and legitimizing value of such rights Individual rights allow data subjects to exhibit autonomy and exert control and to protest or reject their objectification by profiling or decisionmaking machines Individualized explanations also serve to establish the legitimacy or illegitimacy of a decisionmaking system by subjecting its logics and performance to inspection and assessment as to whether they are socially acceptable or even illegal Rejecting individual rights as we discuss below also ignores the symbiosis between the GDPRs two regimes Individual rights can play a crucial role in the GDPRs system of collaborative governance The GDPRs dual approach to algorithmic accountability has the potential to answer important questions in the literature about the value in practice of individual rights in algorithmic governance Collaborative Governance in the GDPR The other side of algorithmic governance in the GDPR is its systemic governance regime It aims largely to address instrumental goals preventing algorithmic error bias and discrimination This governance regime as one of us discusses at length elsewhere is largely constituted through collaborative governance or public-private cooperation We here illustrate two examples of how this works in the GDPR Article s suitable safeguards on automated decisionmaking are one example of this in practice The GDPRs text does not comprehensively dictate what data controllers must do to protect individual rights when they use solely automated decision-making Art Instead the GDPR lists examples of safeguards contestation expression human intervention and leaves to both data controllers and regulators to determine what additional safeguards are necessary The accompanying Recital famously adds a right to individual explanation The Guidelines too fill in this gap proposing a list of best practices These include but are not limited to regular quality assurance checks algorithmic auditing independent auditing establishing data minimisation and clear retention periods using pseudonymisation techniques certification mechanisms ethical review boards and more All of these are attempts at systemic accountability and oversight and notably attempt to establish that oversight in a comprehensive and ongoing manner Another example is the GDPRs approach to preventing bias and discrimination in algorithmic decision-making Recital tasks data controllers with preventing discriminatory effects on natural persons on the basis of racial or ethnic origin political opinion religion or beliefs trade union membership genetic or health status or sexual orientation in profiling and algorithmic decision-making The GDPR does not however lay out how to do this Instead the Guidelines suggest that data controllers check data sets for bias regularly review the accuracy and relevance of decisions deploy systems that audit algorithms and use appropriate procedures and measures to prevent errors inaccuracies or discrimination on the basis of sensitive data such as race religion or health information deployed on a cyclical basis The GDPR does not tell data controllers precisely what to do It identifies the problem provides suggestions of what regulators might consider adequate but also tasks data controllers with cooperatively coming up with solutions Such company-created solutions may then feed back into what regulators ultimately require Roig explains that the requirement of data protection impact assessment DPIA could compile all the relevant safeguards for specific technologies and automatic processing and turn into a data generator for policy purposes Algorithmic Impact Assessments Within this dual system of algorithmic accountability individual rights accompanied by extensive but collaborative governance of data controllers behaviour the Data Protection Impact Assessment DPIA serves a special role We claim here that the DPIA is best understood as a nexus between the GDPRs two approaches to algorithmic accountability Understanding it in this way allows us to better understand what is or might be required and to observe the tools shortcomings as implemented in the GDPR We turn here to both the general discussion that has arisen recently over Algorithmic Impact Assessments and to the specific role of the DPIA in the GDPR We are not the first to focus on Algorithmic Impact Assessments or impact assessments in closely related fields We are however the first to discuss Algorithmic Impact Assessments not in isolation but as a central component among many components of the GDPRs two-prong approach to algorithmic accountability This changes the nature of the conversation Instead of examining impact assessments in isolation from other accountability tools it situates them within an overarching system of data governance Our GDPR-specific analysis then may have implications for proposals for algorithmic impact assessments in other legal systems It suggests that impact assessments best serve a role in conversation with other accountability tools as part of overarching governance design We suggest that impact assessments play a central role both as a source of and mediator between the multi-layered explanations indicated in the GDPR Proposals for Algorithmic Impact Assessments Algorithmic Impact Assessments have received a good deal of attention on both sides of the Atlantic as possible tools to address problems of algorithmic discrimination bias and unfairness including in at least one proposed US federal law We here briefly discuss several important precursors to the AIA Human Rights Impact Assessments HRIA Privacy Impact Assessments PIA Ethical Impact Assessments EIA and Surveillance Impact Assessments SIA The Human Rights Impact Assessment HRIA process outlined by the United Nations is a comparatively time- and resource-intensive process conducted on a business by third-party assessors who collect data and interview stakeholders experts and management Mantelero for example draws partially on the HRIA model as does Katyal Wright and Friedewald look to Ethical Impact Assessments EIA voluntary assessments that go beyond legal compliance to assess the ethical implications of new technologies and involve consultation with a wide number of stakeholders and publication of the assessment However the most direct precursor for the notion of the AIA especially in the context of the GDPR is the Privacy Impact Assessment As Clarke explained in PIAs originated in the s around the world with multiple regulators issuing guidance in the early s While Privacy Impact Assessments as conducted in the United States have been widely decried as toothless elsewhere they are more substantial It is interesting to note that in at least several European countries the PIA may have originated in part in the system of prior checking under earlier data protection regimes which was effectively a system of licensing prior to data processing In order to receive a license from a national authority a company had to assess whether it was in compliance with national data protection law Clarke characterizes the characteristics of the ideal PIA as being performed on a project rather than an organization being anticipatory in nature rather than retrospective being broad in scope with respect to individual group community and other dimensions of privacy taking into account the perspectives of affected segments of the population being broader than mere legal compliance being oriented towards surfacing solutions not just problems emphasizing process over product and requiring engagement from executives and managers In Wright and Raab proposed the concept of a Surveillance Impact Assessment SIA wider in scope than a PIA but consisting of a similar process of engaging stakeholders in order to identify the impacts on privacy and other values of a new project technology service or other initiative in order to take remedial action to minimise avoid or overcome the risks But this is the ideal Mantelero observes that in practice DPIAs in the European context have tended to focus on data quality and data security leaving out broader social and legal impact despite aspirational language to the contrary We now turn to recent proposals for Algorithmic Impact Assessments which draw to varying degrees on these precursors and others We find both common threads and significant differences in the proposals We find too a significant gap in this literature that our perspective on the GDPR helps to fill Selbst proposes the use of an Algorithmic Impact Statement modelled after the Environmental Impact Statement EIS established in the United States in in the National Environmental Policy Act NEPA with some modifications His AIS would apply narrowly to police departments looking to acquire and use predictive policing technologies An AIS would in Selbsts proposal be performed prior to using such technology First the AIS would like an EIS require explain the various design choices measure the resulting efficacy using the best available audit methods and evaluate the resulting disparate impact for the various systems and configurations Second a police department would have to devote substantial treatment to each alternative including the alternative of no action And finally police must include proposed mitigation measures in the AIS To address various concerns about the EIS model Selbst emphasizes the importance of public disclosure and comment and judicial oversight with not just procedural but substantive bite Katyal incorporates elements of Selbsts proposal into her suggestion of a Human Impact Statement in Algorithmic Decision-making She recommends as a backstop a substantive rather than purely procedural commitment to algorithmic accountability and antidiscrimination And she adds that companies should also identify potentially impacted populations and determine their status-based categories identify the effect of uncertainty or error on those groups and study whether the decision will have an adverse impact on a particular subpopulation The single biggest difference however between Katyals and Selbsts proposals is that Katyal recommends the HIA in AI as a voluntary measure undertaken by private industry rather than required by law The AI Now Institute issued a report also building on Selbsts proposal The reports authors call for a pre-procurement Algorithmic Impact Assessment before public agencies commit to the use of an automated decision-making system Like Selbsts proposal the AI Now proposal is limited to the public sector Like Selbsts proposal it would be mandatory rather than voluntary Unlike Selbsts proposal it goes beyond the policing context The proposed AIA requires agency disclosure and a public comment period including both meaningful access for researchers and auditors once systems are deployed and individual due process for those affected by the systems decisions The AIA is envisioned as being renewed every two years Finally we return to the context of the GDPR Mantelero discusses the idea of a Human Rights Social and Ethical Impact Assessment HRSEIA in the AI context A hybrid between a Human Rights Impact Assessment and a Privacy Impact Assessment the HRSEIA suggests that businesses voluntarily take into account ethical and social impact in addition to human rights Mantelero emphasizes the role of such an impact assessment in addressing the collective dimensions of data harms At its core the HRSEIA has three features it is participatory it is transparent and it is circular in nature Practically it consists of a self-assessment questionnaire sometimes leading to evaluation by an ad hoc committee of experts Stakeholder engagement is encouraged but not required Similarly public disclosure is encouraged Mantelero explains that while this proposal is in line with the declared intent of the GDPR he does not understand the HRSEIA to be required by the GDPR Several other commentators have recently discussed the DPIA and the role it plays in the context of algorithmic accountability more generally Most of these proposals for Impact Assessments centrally depend on release of information to the public This is necessary both to obtain external input into how a system is developed trained or monitored and to gain public legitimacy and acceptance for the use of a system The kind of information released to the public can be more in the nature of a summary or an overview it is not necessarily source code Some suggest a tiered release of information with summaries released to the public and detailed or sensitive information released only to regulators or experts Thus more recent proposals also call for expert input and oversight by suggesting that companies or government agencies use Impact Assessments to come up with and stick to a plan for third-party expert oversight over a systems development and eventual use The Data Protection Impact Assessment DPIA as an Algorithmic Impact Assessment A version of an Algorithmic Impact Assessment might be derived from the GDPRs Data Protection Impact Assessment DPIA Article a requires a DPIA in case of inter alia a systematic and extensive evaluation of personal aspects relating to natural persons which is based on automated processing including profiling and on which decisions are based that produce legal effects concerning the natural person or similarly significantly affect the natural person Interpreting this provision the Article Working Party Guidelines on DPIAs DPIA Guidelines mandate DPIAs for any automated decision-making creating a categorical requirement that applies in the case of decision-making including profiling with legal or similarly significant effects that is not wholly automated as well as solely automated decisionmaking defined in Article Moreover as Casey Farhangi Vogl have noted demonstrating that a DPIA is not necessary will in many instances itself require a DPIA We note too that at least one Member State see the Slovenian Data Protection Act that implements the GDPR requires algorithmic impact assessments as a specific safeguard in case of automated decision-making under Article of the GDPR In this section we address what the purpose of the DPIA is in the GDPR and what it must include Understanding the DPIAs purpose clarifies what the content should be and points to several shortcomings in the current conception of the DPIA What is Required in a DPIA The GDPR describes a DPIA as an assessment of the impact of the envisaged processing operations on the protection of personal data Art That assessment per the text of the GDPR must include a description of the processing operations in this case the algorithm and the purpose of the processing an assessment of the necessity of processing in relation to the purpose an assessment of the risks to individual rights and freedoms and importantly the measures a company will use to address these risks and demonstrate GDPR compliance including security measures The GDPRs version of a DPIA must take place before a company implements a system That is a company must assess a system and propose risk-mitigation measures before data processing takes place Art But the GDPR also envisions iteration If the risk posed by a system changes a company must assess whether it is complying with its own Impact Assessment Art It should also review the DPIA itself The DPIA Guidelines suggest an even more dynamic view of DPIAs They suggest that DPIAs should as a matter of good practice actually be continuous updated throughout the lifecycle of the project and that they should be re-assessed or revised at least every years Carrying out a DPIA is a continual process not a one-time exercise state the DPIA Guidelines This continual process involves assessing risk deploying risk-mitigation measures documenting their efficacy through monitoring and feeding that information back into the risk assessment and ongoing process The DPIA Guidelines envision this process as running multiple times The GDPR also lays out procedural requirements for the DPIA Differing from the Impact Assessments imagined in the literature DPIAs do not involve a period of public comment or input They do require consultation with an internal but independent Data Protection Officer if a company has one Many data controllers that are required to perform impact assessments will have Data Protection Officers in place Art In lieu of public or formal stakeholder consultation the GDPR requires consultation where appropriate with impacted data subjects Art In the original proposal of the Commission consultation with data subjects was mandatory Article The Parliaments text argued that this represents a disproportionate burden on data controllers amendment Accordingly the approved Article requires consultation only where appropriate and without prejudice to the protection of commercial or public interests or the security of the processing operations This puts in place one method for external input from impacted data subjects rather than external experts or the public The DPIA Guidelines envision that this input could be for example in the form of surveys crafted by data controllers and sent to future customers which would make it less meaningful than say deep consultation with a board of representative of civil society members or chosen community representatives envisioned in the literature The DPIA Guidelines explain that if data controllers do not seek these external views they have an obligation to justify this decision In addition if data controllers do seek these views and then disregard them they must document why they have chosen to disregard this input As for other forms of external oversight the DPIA Guidelines recommend but do not require seeking advice from independent experts ranging from lawyers and sociologists to data security experts The GDPR does not generally require most DPIAs to be overseen by a public authority the Data Protection Authority But if a risk assessment indicates that processing would result in high risk in the absence of measures taken by the controller to mitigate the risk then a company must consult with the regulator before processing Art In the biggest departure from the Impact Assessment proposals discussed above DPIAs are not legally required to be released to the public even when finalized As the Guidelines explain publishing a DPIA is not a legal requirement of the GDPR however data controllers should consider publishing their DPIA or perhaps part of their DPIA The Guidelines caution that it is a good practice to publish DPIAs especially where members of the public are impacted But data controllers need not publish the entire assessment the published DPIA could even consist of just a summary of the DPIAs main findings There are of course cases in which full disclosure of the assessment results may be limited by the legitimate interests of the data controller such as confidentiality security and competition The GDPR text and DPIA Guidelines thus give an overview but little specific guidance on what exactly a company must put in a DPIA report Unlike the Impact Assessments proposed in the legal literature they do not require public input or public disclosure though they suggest both as best practices This has led one proposal to dismiss the GDPRs DPIAs as not shared with the public and having no built-in external researcher review or other individualized due process mechanisms As we discuss below this is not entirely correct What is the Purpose of a DPIA in the context of the GDPRs Algorithmic Governance We posit that in the context of the GDPRs algorithmic governance regime the DPIA should be understood as a nexus between the GPDRs two approaches to governing algorithmic decision-making The DPIA links the GDPRs system of individual rights to systemic governance Understanding the DPIA in this way both clarifies its potential content and leads us to observations about how implementing the DPIA as an Algorithmic Impact Assessment might be improved DPIAs are not the perfect Algorithmic Impact Assessment As a tool in the GDPRs overall algorithmic governance regime however they have more potential than might initially meet the eye How Understanding the DPIAs Dual Role Helps Clarify Its Content The DPIA has two roles as a tool in the GDPRs systemic collaborative governance regime and as an element of the GDPRs approach to protecting and enabling individual rights Understanding the DPIA in this way as a connection between the two systems lets us better understand how it is meant to function as an Algorithmic Impact Assessment even to the extent of further clarifying its content When understood as part of the GDPRs collaborative governance system the DPIA is a form of monitored self-regulation Collaborative governance is centrally concerned with affecting management culture and creating meaningful changes within a company Monitored self-regulation attempts to change both company decisionmaking processes and decision-making heuristics The DPIA in the context of algorithmic decision-making tasks data controllers with considering risks of unfairness error bias and discrimination and with coming up with concrete ways of mitigating those risks This affects firms decisional heuristics by dictating through Recitals and the Guidelines what values a company must consider and what harms it must prevent The process of conducting the DPIA-taking input from impacted data subjects consulting with an independent Data Protection Officer consulting with a regulator where required and involving both internal and external experts is meant to change internal company processes As others have noted baking in a compliance culture can be valuable even where public oversight and input is not sought The DPIA can also be understood in this context as a documentation or even a reporting requirement creating records that can later be sought and inspected by regulators The DPIA also however has an unexplored role in the GDPR system of individual rights First the DPIA can serve as a source of material for the much-discussed disclosures to data subjects about algorithmic decision-making the individual notification and access rights Remember data subjects have a right to receive meaningful information about the logic involved as well as the significance and the envisaged consequences of automated decision-making Arts A DPIA must contain as mentioned above a systematic description of the envisaged processing operations and the purposes of the processing Art If data controllers are already internally describing automated decision-making at a systematic level as part of the DPIA process those internal descriptions could be disclosed to data subjects or at least serve as the basis for these disclosures In addition they might be released to the public in the form of summaries Similarly a DPIA must include an assessment of the risks to the rights and freedoms of data subjects and data subjects have a right in the context of automated decision-making to be informed of the significance and envisaged consequences of such decision-making Arts Again as a company conducting automated decision-making must conduct a DPIA it should consider how the information it produces in that process might also feed into or even satisfy the individual rights requirements under the GDPR Second despite other commentators dismissal of DPIAs as failing to put in place individual due process the Guidelines explicitly envision the DPIA as an essential part of establishing suitable measures to safeguard individual rights including a version of individual due process The GDPR requires data controllers that fall under the exceptions to its ban on solely automated decision-making to still implement suitable measures to protect individual rights Art The Guidelines counsel that data controllers should use DPIAs to identify what measures they will introduce to address the risks involved As discussed suggested measures include a number of individual rights informing data subjects about the logic involved explaining the significance and envisaged consequences of algorithmic decision-making providing a way to contest a decision and providing a way to express ones point of view This list effectively imports the various individual rights laid out in the GDPR that are restricted to solely automated decision-making into DPIAs that the Guidelines say go beyond the solely automated context that is algorithmic decisions that involve a human decision-maker In other words the GDPR or really the interpreting Guidelines envisions DPIAs in the context of algorithmic decision-making as serving as a form of commitment-making to protecting or even enabling individual due process rights By characterizing these individual rights as risk mitigation measures it both provides a substantive backstop as to what must be included in a DPIA and tasks data controllers with constituting through the process of performing a DPIA what these individual rights will look like in practice Thus the DPIA serves as a collaborative governance mechanism used to constitute the substance of individual due process rights It also serves as a means of expanding company commitments changing company decision-making heuristics to include an assessment of individual due process rights Finally the DPIA has a role in linking the GPDRs system of collaborative governance to its individual rights regime through the imposition of systemic accountability measures such as audits or external review Remember the general DPIA Guidelines only suggest and do not mandate consultation with external experts In the context of algorithmic decision-making however external expert involvement and oversight is more like a requirement The use of external experts is framed as a necessary risk-mitigation measure for algorithmic decisionmaking functionally changing the required content of a DPIA in that context The reasoning goes as follows Recital requires in the context of algorithmic decision-making the use of technical and organisational measures appropriate to ensure in particular that factors which result in inaccuracies in personal data are corrected and the risk of errors is minimised and that prevents inter alia discriminatory effects Malgieri and Comand√© observe that this requirement effectively expands the suitable safeguards to protect individual rights from the harms of algorithmic decision-making from the series of individual due-process-like protections enumerated in the GDPR text to a far broader set of systemic accountability measures including third-party auditing Art The Guidelines list of best practices for suitable safeguards over algorithmic decision-making supports this interpretation including recommendations that data controllers use both audits and external review boards When a company that deploys algorithmic decision-making conducts its DPIA it will refer to the Guidelines list of best practices in establishing risk mitigation measures that are already regulator-approved This means that in practice a company running through the cyclical DPIA process discussed above will likely incorporate external oversight and input at the risk mitigation stage bringing external input into the cycle despite the fact that it is not a formal procedural requirement for DPIAs in general Conceptually the implications of this are even broader By characterizing third-party and expert oversight as a suitable safeguard or suitable measure to protect individual rights the Guidelines link individual rights protection with collaborative governance techniques Data controllers are tasked with coming up with ways to prevent error bias discrimination and other harms to individual rights and external oversight is imposed over how they choose to address these problems That external oversight itself is simultaneously conceptualized as a crucial aspect of individual rights in the GDPR standing in for data subjects to ensure that they are not subjected to an unfair arbitrary discriminatory or erroneous system A simpler way to say this is that expert oversight in the DPIA process serves two roles it watches the data controllers as they come up with ways of addressing problems with algorithmic decision-making and it reassures data subjects that their dignity and individual rights are being respected by a fair system As the mechanism through which this external oversight is implemented the DPIA thus connects the two approaches to algorithmic governance in the GDPR Shortcomings of the DPIA The biggest shortcoming of the DPIA is that it does not include a mechanism for mandatory disclosure to the public Public disclosure is understood by many to be an essential element of Impact Assessments as a policy tool of collaborative governance Public-facing disclosure enables public feedback both in the form of market feedback and regulatory feedback over the longer term By failing to mandate public disclosure the GPDRs DPIA fails to trigger both of these mechanisms This failure could be drastic The GDPR puts a lot of faith in the behaviour of data controllers As discussed it often tasks data controllers with coming up with the substance of a how individual rights will be implemented and b how to address unfairness biases and discrimination concerns about algorithms The counterbalance is regulatory oversight of DPAs Data Protection Agencies But the GDPRs enforcers have not historically been well-resourced compared to the data controllers they regulate Tasking regulators with extensive monitoring also forgoes some of the touted benefits of governing through public-private partnerships including lowered costs By failing to require public disclosure of Impact Assessments the GDPR fails to activate necessary third parties in its governance regime such as civil society actors or civic-minded experts who might not be recruited for auditing purposes Similarly the DPIA fails to involve serious stakeholder input unless data controllers understand the Guidelines emphasis on expert boards and third-party audits to include impacted stakeholders and to be mandatory As an alternative individual notification and access rights could do some work If data controllers indeed link their DPIA content to what they disclose to data subjects for example disclosing the logic involved in a decision-making system then it is likely that these disclosures will make their way to other third parties including civil society and the press This is a more attenuated way of getting at the same outcome as direct public disclosure however and risks failing if data controllers significantly disaggregate the DPIA process from individual disclosure rights Lessons for Calls for Algorithmic Impact Assessments Generally Our GDPR-specific analysis has implications for proposals for algorithmic impact assessments generally Our research into the GDPRs version of AIAs suggests that the proposals discussed above have missed several important observations First AIAs are not best understood as a stand-alone mechanism In the context of the GDPR they are one part of a much larger system of governance Only one author among the above Katyal considers how impact assessments interact with other tools in the regulatory toolkit discussing the concurrent need for whistleblower protection and exemptions from trade secrecy law We found only one author Binns discussing DPIAs generally who identified the GDPRs version of impact assessments as a kind of collaborative governance with the private sector or what he identifies as meta-regulation But this led Binns to critique the GDPRs version of the DPIA for inadequate public disclosure and stakeholder involvement not to look to how it connects to the broader system of both collaborative governance tools and individual rights in the GDPR Second as part of a larger system of governance there are unexplored connections between the GDPRs potential AIA and its underlying substantive individual rights and substantive principles It is true that many of those rights and principles are articulated in broad sometimes aspirational terms The GDPR version of the AIA has a substantive backstop The oddity is the GDPRs circularity the DPIA helps not just to implement but to constitute those individual rights Third impact assessments can serve as a connection between collaborative governance and individual rights only one other proposal to our knowledge suggests using Impact Assessments to establish something resembling individual rights a system of enhanced due process mechanisms for affected individuals The information a company creates during the Impact Assessment process can feed into what it provides to data subjects and to the public at large The procedures an Impact Assessment puts in place can serve not just to prevent error bias and discrimination but also to legitimize a system or even respect an individuals dignity within it This dual role is exemplified by the GDPRs DPIA Fourth because the DPIA links individual and systemic governance we understand the GDPRs version of the AIA to be both the potential source of and the mediator between what we refer to as multi-layered explanations contemplated in the GDPR Several of the above scholars including both Mantelero and Wright Raab emphasize the often collective dimensions of surveillance and data processing The GDPRs system of individual rights threatens by itself to miss the impact of surveillance or in this case automated decision-making on groups locations and society at large A recent AI Now report provides an illustrative example of the problem providing an individualized explanation for a single stop and frisk incident in New York City would have failed to reveal that over of those subjected to stop and frisk by the NYPD were Black or Latino men The DPIA with its systemic approach to risk assessment and risk mitigation requires data controllers to analyze how the system impacts not just individuals but groups We believe that systemic and group-based explanations uncovered during a DPIA can and should be communicated to outside stakeholders Comparing DPIA content with Algorithmic Accountability requirements under the GDPR Thus far few commentators have linked the Guidelines on Automated Decision-Making to the DPIA process Here we connect the GDPRs text to these Guidelines to show how the required content of DPIAs in fact easily serves as the basis for disclosures controllers must make to data subjects Article GDPR requires that a DPIA should contain a systematic description of the envisaged processing operations and the purposes of the processing an assessment of the necessity and proportionality of the processing operations in relation to the purposes an assessment of the risks to the rights and freedoms of data subjects and the measures envisaged to address the risks The Guidelines on DPIAs clarify that a systematic description of processing should include the nature scope context and purposes of the processing categories of personal data recipients and storage a functional description of the processing operation and the assets on which personal data rely If we compare algorithm accountability requirements with DPIA content there are multiple interesting similarities In particular the data controllers duty to systematically describe the processing operations in a DPIA is similar to the algorithmic transparency duty to clarify the categories of personal data used in automated decision-making and how the algorithmic profiling is built Analogously the controllers duty to assess necessity and proportionality of the processing operations in the DPIA is similar to the algorithmic transparency duty to explain the pertinency of personal data used and the relevance of the profiling The controllers duty to assess the data processing risks and the impacts on individuals is similar to the transparency duty to explain the impact of the profiling use in automated decision-making Lastly the controllers duty to find safeguards of individual rights in case of automated decision-making under Article and GDPR is similar to the duty to find and describe measures envisaged to address the risks in DPIA In other words in the case of automated decision-making the DPIA steps might correspond to transparency duties as interpreted by Article Working Party in Annex I Towards Multi-layered Explanations from Algorithmic DPIA The DPIA process combined with the GDPRs extensive individual transparency rights and Guidelines on suitable safeguards of individual rights suggests what we call multi-layered explanations for automated decision-making Edwards Veale have similarly suggested that data subjects should be given what they call model-centric and subject-centric explanations Model-centric explanations they suggest should include the family of model input data performance metrics and how a model was tested Subject-centric explanations should include counterfactuals that is what changes would change the outcome of a decision the characteristics of similarly classified individuals and the confidence a system has in an outcome We understand these as really being three layers individual explanations group explanations and systemic explanations And unlike Edwards Veale we have more optimism that these multi-layered explanations can be found either in the text or subtext of the GDPR As a first layer there should be the right to individual explanation at least in riskier data processing operations as required at Article GDPR and explained supra in Section This is an individual right to an explanation of an individual decision Even this individualized form of disclosure can be informed by information revealed during a DPIA as discussed further below As a second layer we understand the GDPR to suggest a connection between required DPIA analysis of systemic risks of unfairness and discrimination and the individual rights to contestation to express ones view and to human intervention see Article That is for these series of individual rights to be meaningful data subjects need to know not just information about a particular stand-alone decision but information about the algorithms treatment of groups and tendency towards bias This group-based explanation which we argue can be at least implied from if not required by the rights to contestation etc could be created based on information on bias and discrimination uncovered during a DPIA As a third layer of explanation the GDPR requires disclosure of meaningful information about the logic involved in automated decision-making on a systemic level Arts and The DPIA must include an assessment of necessity proportionality risks and safeguards A DPIA is not required to be made public but its public disclosure is highly recommended at least in the form of meaningful summaries If we compare DPIA requirements to the safeguards in automated decision-making as we do below in Table it seems reasonable and efficient to suggest that a relevant summary of the Algorithmic DPIA could be used as meaningful information about the overall system of decisionmaking that must be provided to data subjects Further levels of details could be tailored from this systemic layer Indeed the description of algorithmic processing including rationales and criteria might well be specified on both a group level and on individual level the description of data categories and their pertinence of the profile-building procedure and its relevance and use of effects and safeguards can be based on groups of similar individuals affected by that automated decision or even on a single specific data subject who is making an explanation request Combining these two different tasks the algorithmic DPIA and the duty to disclose meaningful information about algorithmic decisions in coordinated actions would be highly beneficial for data controllers Combining these tasks would benefit data controllers because if conducted with an adequate degree of rigor it could help controllers comply with both their transparency duties imposed by Article and of the GDPR and with DPIA duties Article GDPR optimizing efforts that would otherwise be spent on two different tasks publicly disclosing at least some parts of the DPIA as a basis for explaining automated decisions is considered a best practice in line with the data protection by design principle Article GDPR disclosing information about algorithmic data processing to data subjects and collecting their reactions through eg the right to contest to have a new decision to have human involvement etc see recital could be considered compliant with the duty to seek the view of impacted data subjects Article GDPR in the continuous cycle of the DPIA framework see also recital merging an algorithmic DPIA and multi-layered explanation might serve as a suitable safeguard to protect fundamental rights and freedoms of individuals both under Article and under Article d of the GDPR developing an algorithmic DPIA and explanation safeguards in parallel intrinsically related to the right to contest a decision right to human-in-the-loop etc might be the best way to enrich transparency with accountability safeguards and overcome the transparency fallacy through a virtuous cycle of algorithmic auditing and continuous detection/mitigation of unfair effects The idea of merging at least partially algorithmic accountability duties and DPIAs also seems useful considering the most advanced literature on explanation Effective explanation is not only the result of an analysis but also a two-stage process both cognitive and social Multi-layered and multi-step explanations would be a continuous process not merely a static product Some scholars have remarked that what is needed is not merely an explanation but a legal justification of automated decisions taken Connecting an algorithmic DPIA to individual transparency rights might address this the information duties about the pertinence and relevance of decision-making processing required by the Guidelines may reflect the duty to assess the proportionality and necessity of data processing required at Article d GDPR If the data controller must prove the legal proportionality and necessity of automated decisions taken they would be creating not merely an explanation but a justification of both data used and profiling mechanisms Conclusion There is a growing literature suggesting that Algorithmic Impact Assessments are a crucial tool in establishing algorithmic accountability This paper addresses that tool as implemented in the GDPR We find that the GDPRs version of Impact Assessments serves as a central connection between its two approaches to regulating algorithms individual rights and systemic governance That framing allowed us to identify both value in and shortcomings of the GDPRs Impact Assessment regime as applied to algorithmic governance This analysis we hope will have value for other discussions of Algorithmic Impact Assessments beyond the GDPR In particular moving from individual transparency rights and governance accountability duties in the field of automated decision-making we suggest a model of Multi-layered Explanations drawn from Algorithmic Impact Assessments Since there are several layers of algorithmic explanation required by the GDPR we recommend that data controllers disclose a relevant summary of a system produced in the DPIA process as a first layer of algorithmic explanation to be followed by group explanations and more granular individualized explanations More research is needed in this field in particular about how different layers of explanations systemic explanations group explanations and individual explanations can interact each other and how technical tools can help in developing an efficient Algorithmic Impact Assessment that can be re-used as GDPR-complying explanations and disclosures