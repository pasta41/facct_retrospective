Bias in Word Embeddings Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers These vectors are used to improve the quality of generative and predictive models Recent studies demonstrate that word embeddings contain and amplify biases present in data such as stereotypes and prejudice In this study we provide a complete overview of bias in word embeddings We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data We investigate bias diffusion and prove that existing biases are transferred to further machine learning models We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient Finally we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data Given that word embeddings are widely used by commercial companies we discuss the challenges and required actions towards fair algorithmic implementations and applications CONCEPTS Human-centered computing HCI design and evaluation methods Computing methodologies Machine learning Information systems Data mining KEYWORDS word embeddings bias detection diffusion mitigation fairness sexism racism homophobia INTRODUCTION The growing ubiquity of algorithms in society poses questions about their social political and ethical consequences One of the issues research focuses on is algorithmic bias which denotes the deviation of the algorithmic results from specific social expectations based on epistemic or normative reasons Prior research has shown that algorithmic bias might result in unfair or discriminative decisions and statements initiating a multilevel debate on the ethical use of algorithms Under that framework researchers decision makers and institutions try to answer the following questions What definitions of fairness and discrimination are appropriate and under what conditions At which part of an algorithm does bias emerge and in what form What are the actual consequences of biased algorithms and who is accountable for them How can researchers and decision makers mitigate the detected bias Problem Statement This study investigates bias in word embeddings a set of natural language processing techniques for the mapping of words into numerical vectors These vectors can then be used for the improvement of the predictions and inferences of other machine learning models Previous work has proven that word embeddings contain bias and researchers have already developed methodologies for tracing quantifying and mitigating it Recently researchers have also started to develop methods for comparing biases existing in different datasets Despite recent scientific findings computer scientists in the industry widely use word embeddings for the development of highly accurate models that perform text generation translation classification and regression without taking into consideration the impact of their inherent biases Similarly researchers have not yet investigated the diffusion and impact of biased word embeddings on further machine learning algorithms Therefore we want to provide a complete overview of bias in word embeddings its detection in the embeddings its diffusion in algorithms using the embeddings and its mitigation at the embeddings level and at the level of the algorithm that uses them We also investigate whether the employment of biased word embeddings contributes to the location of the bias in new data The study raises additional awareness about a technique whose implementation can lead to unfair algorithmic decisions and inferences We achieve this by seeking the answer to the following research questions How can we evaluate and mitigate the word embeddings bias diffusion in further machine learning algorithms Can we employ bias in word embeddings for tracing bias in new data Original Contribution We train state-of-the-art word embeddings based on the German version of Wikipedia and on unique social media data in the German language For that we gather over million tweets and Facebook comments related to German politics We develop a new method for locating biases in gendered languages trace niches of sexist xenophobic and homophobic prejudice and stereotypes on the two sets of vectors and quantify the overall bias for each dataset We transform and compare the vector spaces without distorting the immanent bias by borrowing techniques from embeddings translation We then compare the spaces and prove that the social media data contained a higher level of intergroup prejudice while Wikipedia data contain a stronger bias in terms of stereotypes We create a sentiment classifier based on the two embedding datasets and show how the model replicates bias immanent in the embeddings We compare methodologies to mitigate bias without distorting the accuracy of the classifier We compare debiasing at the embeddings level and at the level of the classifier We illustrate that the standard technique for mitigating bias at the embeddings level is insufficient for removing biases completely We develop a new sexism dataset by labeling Facebook comments as sexist or neutral and illustrate that embeddings with bias similar to the one in the target data perform better on the classification task Finally we discuss the issues possibilities and challenges that accompany the use of biased word embeddings Paper Organization The paper is organized as follows Section presents the theoretic background and related work Section describes the data and methodology we followed Section presents the results Section discusses the results demonstrates the implications of the study and concludes the analysis BACKGROUND AND RELATED WORK Algorithms Bias Fairness A prerequisite for understanding bias in word embeddings is to evaluate how the methods results deviate from given social expectations To do so we adopt the Friedman et al proposed framework for analyzing algorithmic bias They state that algorithms might face three types of bias preexisting technical and emergent Preexisting bias is related to the input data Social or personal attitudes integrated in the input dataset might lead to the deviation of algorithmic inferences from a hypothesized social objective For example white hosts on online lodging marketplaces charge more than their non-white counterpart hosts Algorithms might replicate the asymmetry valuing an apartment as more expensive only because the owner is white Technical bias emerges when there are software hardware or mathematical constraints An overfitted algorithm is biased because its inferences are perfect on the training data but non-generalizable for new cases Different mathematical models trained on the same data have different prediction accuracies and consequently different amounts of bias because of the different cost function that they optimize A computer with RAM limitations will not allow the development of a model on the full dataset leading to the creation of model predictions that might miss important information Emergent bias appears at the evaluation of results and the context of their application Forming a decision based on algorithmic results might pose an ethical problem when the decision or inference proposed contradicts existing normative values in the society Research and policy makers are investigating and trying to define when the emergent bias is transformed to unfair or discriminative decisions Among others Goodman refers to algorithms as unfair when a specific group or individual receive unfavorable treatment as a result of algorithmic decision-making Cowgill and Tucker argue that algorithmic results should always be compared to a counterfactual ideal case in relation to which it will be decided when and how an algorithm discriminates Overall no unique definition of fairness is available making each algorithmic application a distinct case to be studied Word embeddings might face all three types of bias It is proven that social attitudes such as sexism and ethnic stereotypes in the initial dataset are transferred to the embeddings denoting the presence of a preexisting bias Technical bias also appears Word embeddings trained by different models yield different results on benchmark tests Word embeddings might also result in emergent biases Generalizations on social relations based on the distance of words immanent in an embedding space or by inserting the embeddings in another model for prediction or inference might result in the formation of decisions that deviate from given social imperatives Word embeddings are used widely in commercial systems inter alia for ad generation music and hospitality recommender systems and by tech companies who use them to develop models and offer tools they constitute decisions that influence multiple social groups and individuals It is important to understand the appearance of bias in them the related dangers and possible reactions to them This will not only contribute towards fairness but will provide the foundations for creating applications that respect the rights of social groups and individuals Given that the prominent bias form in word embeddings is related to the input dataset we investigate preexisting biases and their connection to emergent biases in related applications Text Social Discrimination The reason why preexisting biases are imprinted in word embeddings is related to the nature of text Because text is a medium for communicating and projecting human interactions it carries features that constitute the social world In human history text has not only been used to organize and comprehend sociopolitical events but also to shape the way these events are perceived and interpreted Therefore power relationships social discrimination and social asymmetries are always imprinted in text In this study we narrow the investigation to bias related to social discrimination We investigate how existing forms of social discrimination in text are diffused and influence word embeddings and further models Social discrimination refers to discrimination emerging from members of one social group towards members of another thus forming a self-other duality By the time the distinction of people into groups takes place group members automatically start to assign different properties to in-group members and other properties to members of the competing group Social theory states that attitudes of dominant social groups are imprinted in the use of language Consequently the bases of social discrimination are diffused through statements of prejudice and stereotypes in text directly and indirectly Social discrimination can be not only hostile but also benevolent Depending social conditions and group relations stereotypes and prejudice might be both positive or negative in nature Regardless of their polarity they are always a result of group antagonism The understanding of how social discrimination is projected into text is not a trivial task For a thorough understanding of the process text must be analyzed and so should the conditions of its production its context and use To achieve that researchers have developed extensive qualitative frameworks that take into consideration the sociopolitical conditions that lead to the emergence and formulation of lingual symbols By taking into consideration political and social structures ethical values biases predispositions and social group perceptions relations and intentions of speakers and receivers in a social situation researchers have studied language to understand sexism racism and other forms of social discrimination Because of the complexity and types of social discrimination the detection and quantification of social discrimination is not always possible by the use of formal mathematical techniques Therefore for the analysis of bias in word embeddings and further models we restrict our study to the detection of forms of social discrimination that are detectable by concept comparison tests eg the adjective check list Implicit association test Sex-Role Inventory polarity tests These methods locate regularities such as stereotypes or prejudices rather than explaining why they emerged An explanation would require additional qualitative analysis which is beyond the scope of this paper Social Discrimination Word Embeddings Researchers have proven that word embeddings contain forms of prejudice and stereotypes related to sexism and racism Based on that we study how and when biases result in further socially discriminative algorithmic behavior To do that we develop methodologies for tracing biases in gendered languages Existing methods for detecting biases in word embeddings are grounded in qualitative techniques of concept associations on which we also rely They analyze qualities of groups and their relation to other concepts assuming that in an ideal society these concepts would have been either equally assigned to these groups or not at all For example ideally an occupation should not be connected more to one sex than the other nor should one sex be treated more positively or negatively than the other These assumptions might hide the actual reasons and conditions for the emergence of the specific associations and their straightforward connection to social discrimination but are the same assumptions used in standard models for measuring social discrimination based on qualitative techniques Because existing methods are developed primarily for the English language we develop a new model that can account for gendered versions of words Until now researchers have investigated various dimensions of bias in word embeddings Garg et al show how prejudice evolving over time is imprinted in word embeddings Kozlowski et al use the positional change of word embeddings to describe semantic transformation Dev et al show that names in word embeddings function as proxies of bias against social groups Arora et al prove that different meanings of words are encoded in word embeddings and can be retrieved Zhao et al propose a methodology to train word embeddings without sexist bias in them Brunet et al develop a technique to trace the origin of bias in embeddings back to the original text Caliscan et al introduce a general methodology to trace bias in word embeddings while Drozd et al automatize the process Drawing from previous research we want to provide a complete picture of bias in the embeddings its diffusion and mitigation Bias Prediction Another objective of the study is to test whether bias in word embeddings can be used constructively To that end we also investigate whether biased word embeddings can contribute toward detecting bias in new text Research shows that bias detection especially in cases of social discrimination such as sexism or racism is very complicated Park et al have attempted to create classifiers that detect abusive language Dahou et al developed models for sentiment analysis Kathrik et al tried to automatically trace cyberbulling in Youtube while Levy tried to detect sexism in newspaper covers Overall the performed attempts yield moderate results especially when using only text as classification inputs because of the complex nature of human language It is a challenge to test if bias in word embeddings would lead to the improvement of classifiers predicting social discrimination DATA AND METHODS Word Embeddings To be able to investigate bias in word embeddings trained on different datasets we collected data from Facebook Twitter and Wikipedia For Facebook and Twitter we used the application programming interfaces APIs of each platform From the social media channels we collected data for the six main political parties in Germany CDU Germanys main conservative party CSU the sister party of the CDU in Bavaria Bundnis/Die Grünen the green party in Germany FDP a neo-liberal party SPD Germanys social-democratic party Die Linke the radical left party and Alternative für Deutschland the extreme right populist party For Facebook we retrieved the posts from the political parties and their comments during the period between January and May For Twitter we collected the tweets from political parties Twitter accounts between January and October We also included tweets from users that mentioned or retweeted the political parties as well as tweets that included the names of the political parties for the same period Overall we gathered million posts comments and tweets which comprised our social media political dataset mil tokens For Wikipedia we collected the complete German wikipedia as bulk file from the official repository The Wikipedia dataset consisted of million articles mil tokens All texts were originally written in German Therefore related biases existed in the original text and were not added to it by further textual processing eg translation from other languages to German We present our results in English for readability purposes For training the embeddings on the two datasets we used GloVe developed by Pennington et al The model creates vectors of words by taking into consideration the word co-occurrence frequencies in the dataset and optimizing V i Xi T i bi bj where V is the vocabulary i and two the word vector of word i the context vector of word and bj their biases the co-occurrence number of the words for a given window x a if x lower than a chosen otherwise x with a a hyper-parameter Following the authors recommendations we tokenized the texts using the nltk tokenizer our window size was a and Overall the datasets for Wikipedia and social media contained and word vectors respectively Vector Space Transformation Optimizing the GloVe cost function results in the nonlinear map N CV where C is the corpus V the vocabulary and the word embeddings vector space Given that the corpora for Wikipedia and social media vary as well as the two vocabularies and the generated vector are not comparable to each other A comparison presupposes the projection of the one space on the other given a Transformation matrix T that preserves the bias in the vector spaces Both Smith et al in embeddings translation and Hamilton et al in measuring semantic change obtain the transformation matrix by solving the Orthogonal Procrustes problem L argmin BF subject to I where A and B two word embeddings vector spaces and the transformation matrix The problem is solvable by applying a singular value decomposition algorithm as proposed by Schönman The specific transformation places all words from vector space A as close as possible to their corresponding words in vector space B As transformation is linear the normalized distance between words does not change thereby preserving bias in the embeddings Bias Detection For detecting bias in word embeddings we must develop a generally applicable formula The method proposed by Bolukbasi et al defines an inter-group direction Then it quantifies the bias of a random word by the cosine distance between the word vector and For example the vector of the word nurse should be independent of the inter-group direction between man and woman Usually it is not since society stereotypically sees nursing as a female profession Nevertheless this definition does not cover cases that words ought to have an inter-group component For example words in German are sex-dependent There is a male and a female version denoting that mathematically the vectors of the words and of the sex direction should be dependent To overcome that we develop an alternate methodology First we define pairs of theory specific words for each type of discrimination Then we introduce a list of concepts for which we want to measure the bias If concepts change based on the social groups eg they have male and female versions they are represented by word-pairs We calculate the general bias in the embeddings by the equation N i where N is the number of concepts the number of theory specific the embeddings for the pair of theory specific words and and the embeddings for nth concept pair in the list When ie when we investigate concepts that are not variable with respect to the social groups under investigation we use the equation N i The general bias equation compares the magnitude of dependence between a concept and the two groups If the concept vector has a higher cosine distance to one group vector than to the other then the concept is biased in that direction We apply the above equation to two tasks We create a list containing professions for a profession-related bias task We also use the sentiment list developed by Remus et al that contains words of positive and negative polarity for a sentiment bias task Bias Diffusion In order to measure bias diffusion we need to capture whether a model that takes biased word embeddings as input will also give biased output The bias of the model needs to be theoretically comparable to the bias in the embeddings Therefore we used the sentiment dictionary of Remus et al which contains a set of positive-laden and negative-laden words We trained a linear support vector machine classifier The classifier took as input the Man-Woman German-Foreigners Straight-Gay for sexist xenophobic and homophobic prejudice respectively Both gender and sexuality are spectra We did not analyze here biases related to the rest social groups for simplicity reasons as the methodology deals with group dualities embedding vector of a word and predicted if it had a positive or negative sentiment We modified the output of the classifier by transforming the class probabilities to a sentiment score by applying the equation positive where is the sentiment score for positive and negative the models assigned probability that the word is positive and negative respectively Then we designed an experimental setting by which we could measure the level of sexist and xenophobic prejudice that the classifier learned We used the first names list developed by Winkelmann and acquired male and female stereotypical first names for nine population groups German Turkish Polish Italian Greek French US American Russian and Arabic We then fed the embeddings of the words into the algorithm and measured the sentiment score across different sexes and populations We claim that ideally a name should have a sentiment score equal to zero because it should be polarity-independent We then defined the sentiment bias c of the algorithm being equal to the classifiers sentiment score and the classifiers social discrimination bias for a specific social discrimination concept as Bc N N i c where N and are the number of names for each of the two investigated social groups and and c the sentiment bias of the classifier for a word in each group respectively This metric quantifies the difference in the assigned sentiment of the classifier for the names of each group For investigating the statistical significance of our results we apply Mann-Whitney U and Kruskal-Wallis tests to compare biases among two or more groups Bias Mitigation A sentiment analysis algorithm has no social discrimination bias when it predicts equal sentiment for names of different sexes or populations In order to achieve that we try two approaches In the first case we adopt and extend the methodology proposed by Bolukbasi et al As the classifier assigns a sentiment polarity value for each input word we define a sentiment direction s Rd where d is the dimension of a word vector The direction is calculated by forming pairs of theory specific dualities eg good bad positive negative etc see Table that are theory specific and taking the difference of their word vectors Afterwards we apply PCA with the resulting first component being the sentiment direction s We also define the set N corresponding to the vectors of theory neutral words Then we hard neutralize these words by applying i wi wi s s s s where i is the debiased non-normalized vector for word wi By doing this we make the vectors of theory neutral words orthogonal to the sentiment vector We then feed the neutralized embeddings into the classifier and calculate the sentiment for the different groups This methodology tries to mitigate bias at the word embeddings level As non-neutral words are not debiased the accuracy of the classifier does not change Table Word pairs used for the calculation of the sentiment direction translated from German Positive Negative good bad positive negative happy sad peace war cheap expensive love hate In the second case we try to mitigate the bias at the level of the classifier The linear SVM classifier learns to split the classes given a linear hyperplane which is defined by a normal vector p This vector actually corresponds to the sentiment direction as learned by the classifier Therefore we hard-neutralize the theory neutral vectors given vector p by applying the same formula as above Bias Prediction The last part of our study focuses on understanding whether biased word embeddings can help detecting bias in new text For this scope we manually labeled user comments from German political parties Facebook pages and created a sexism dataset We categorized each comment as sexist or neutral based on the following criteria the existence of a sexist buzzword the formulation of sex-related compliments the expression of statements against the equality of sexes and the assignment of stereotypical roles to persons based on their sex Each of the four categories denoted a different label in the dataset and its formation was based on previous theoretic work We traced sexist buzzwords under the notions of traditional sexism while we defined and searched sex related compliments given theories of benevolent sexism We located statements against sex equality in comments that the users explicitly argued about the topic and we defined stereotypical roles of the sexes based on the works of Eckes Tilegea and Benokraitis et al To efficiently code the dataset we created sound recordings of each comment because it has been shown that hearing a sentence rather than just reading it improves content understanding Two coders reviewed the sound corpus assigning to each comment one or more of the four labels and giving a concrete reason for their decision In cases of coders disagreement the comments were reviewed by one additional coder For these comments we accepted labels assigned by more than one reviewer Comments that were not assigned a label at all were then classified as non-sexist while comments having at least one of the four labels as sexist Overall we detected sexist comments We then sampled an equal number of neutral comments creating a balanced dataset which we split into a train and a test set We evaluated the biased word embeddings on the classification test We created models that included long-short-memory network LSTM and attention based architectures and investigated their accuracy on the test set with random word embeddings the embeddings from the Wikipedia data the embeddings from the social media data and embeddings trained on the sexism dataset Furthermore we investigate which properties of the word embeddings are responsible for accuracy improvement For that we transformed and compared the word embeddings from the sexism dataset to the other embeddings by calculating their mean weighted cosine similarities as given by the equation i N n N n with n N s i where s is the word embeddings trained on the sexism dataset i is another word embeddings dataset N is the number of common words in the two datasets and is the frequency of appearance of a common word n in the sexism dataset We also perform the sentiment task with the sexism dataset embeddings and calculate the level and type of sexist prejudice within them RESULTS The results are split into three parts First we present our findings on bias within the Wikipedia and social media word embeddings Second we analyze how the bias was diffused and how we mitigated it We also illustrate the efficiency of biased word embeddings when used as sexism detection models In the last part of the section we evaluate bias in word embeddings Bias in Word Embeddings The word embeddings generated on the Wikipedia and social media corpora contained and vectors respectively In both cases the profession and sentiment task revealed intensive stereotypical features assigned to each examined social group In both Wikipedia and social media spaces women were mostly associated with professions like nurses and secretaries On the other hand men were associated with stereotypical male roles like policemen and commanders The aforementioned assigned professions highly correlate with the actual profession distribution in society denoting that the actual social asymmetry is imprinted in the vectors For Wikipedia women were strongly associated with concepts related to marriage while men were linked to concepts related to war and power This could be because Wikipedia extensively includes biographies of historical figures in which women are typically associated with marriage and familial relations while men are associated with concepts such as war and governance In social media the female sex was closer to positive feelings such as love and maturity but also to negative ones like stubbornness and agitation Men were closer to concepts related to aggression and fighting with most of them being negative The stereotypes found in the social media dataset comply with previous research findings which found the existence of power related stereotypes for men and sentiment related stereotypes for women In both Wikipedia and social media Germans were intensively associated with jobs related to governance and journalism while foreigners either to blue collar jobs or to professionals dealing with foreign populations such as aid officials politicians or tour guides Foreigners were generally linked to sentiment concepts related to immigration law and crime while Germans to positive feelings such as charm and passion social media as well as to cooperation and union Wikipedia The association of foreigners to immigration related concepts and professions can be traced back to the refugee crisis taking place in Europe over the last few years which has a prominent position in the public agenda Similarly researchers have proven the existence of biased slants related to immigration issues on wikipedia Given that both German Wikipedia and the German social media discussions are primarily produced by Germans we can attribute the inherent positivity and negativity on Germans and foreigners on the intergroup prejudice existing in the society Table Extreme words for each task and group using the embeddings from Wikipedia data Wikipedia Sexist prejudice Profession Sentiment Woman Man Woman Man Nurse Secretary Teacher Saleswoman Actress Officer Hunter Commander Guard Cameraman Wedding Divorce Anulment Engagement Marry Reinforcement Attack Combat Power Decrease Population Prejudice Profession Sentiment Foreigners German Foreigners German Aid official Craftsman Bank Assistant Tour guide Foreman Author Journalist Historian Director Painter Refugee Unauthorized Lawful Tax Accumulate Champion Cooperation Union New Assignment Sexual Orientation Prejudice Profession Sentiment Homosexuality Heterosexuality Homosexuality Heterosexuality Artist Art dealer Actress Cook Shoemaker Singing teacher Copywriter Forest manager Track driver Carpenter Corruption Violence Adultery Known Prohibited Unserious Nice Fantastic Smart Fair The stereotypes were equally intensive for sexual orientation Homosexuals were related to stereotypical roles such as artists Wikipedia and hairdressers social media while persons of heterosexual orientation were related to blue collar professions or positions in science Strikingly homosexuality was related in both datasets with very negative concepts from violence prohibition and adultery Wikipedia to death sentencing abuse and harassment social media On the complete opposite side heterosexuality was closely positioned to inherently positive sentiments such as fantastic and smart Wikipedia and to concepts like friendship and deliberation social media These findings comply with historic negative social attitudes against homosexuality where conservative groups state that it is abnormal and that should be prohibited by law Regarding positive concept relations to homosexuality researchers have found similar associations in concept association tests illustrating that biases in social media and wikipedia Table Extreme words for each task and group using the embeddings from social media data Social Media Sexist prejudice Profession Sentiment Woman Man Woman Man Nurse Secretary Pharmacist Religion teacher Correspondent Policeman Musician Priest Coach Paramedic Agitation Mature Love Increase Stubbornness Robber Attacker Injured Fascist Overwhelmed Population Prejudice Profession Sentiment Foreigners German Foreigners German Newspaper Skilled worker Politician Consultant Teacher Government Official Correspondent Notary Butler Reporter Criminal Exclude Refugee Increase Frustration Mature Beauty Charm Passion Love Sexual Orientation Prejudice Profession Sentiment Homosexuality Heterosexuality Homosexuality Heterosexuality Artist Scrap dealer Hairdresser Interviewer Consultant Streetworker Political scientist Political economist Mediator Biologist Death sentence Discrimination Abuse Harassment Violence Friendly Moving Deliberation Increasing Unnecessary correspond to the ones found offline An overview of the most extreme concept associations for all groups can be found in tables and The results demonstrate strong stereotypical associations for all groups Overall the calculated general bias was higher for almost all categories and tasks for the Wikipedia dataset table denoting that Wikipedia introduces more severe stereotypes for each social group than the examined social media content The calculated scores are of similar magnitude to those calculated by Bolukbasi et al who calculated a general bias of on the profession task for the two sexes on an English Google news corpus Table General bias for each intergroup comparison bias task and embeddings dataset Wikipedia Social Media Profession Sentiment Profession Sentiment Sex Population Sex orientation The presented associations only reveal partial bias in the embeddings Indeed stereotypes are a base of social discrimination and someone can qualitatively evaluate how specific social groups are presented in the datasets by checking the mostly associated concepts Nevertheless this does not per se signify that a specific group is generally favored over another which would provide evidence of prejudice To achieve that we calculated the mean polarity score for the sentiment concepts being closer to each social group and then extracted the difference for each intergroup comparison The results are given in Figure For both Wikipedia and social media Germans were depicted much more positively than foreigners The Woman Foreigners Homosexuality rc en ta ge o d er en ce Man German Heterosexuality Wikipedia Social Media Sexism Dataset Figure Intergroup positive sentiment difference in the embeddings same applies for heterosexuals in comparison to homosexuals Both results are in accordance to the sentiment task results as Germans and heterosexuals were associated with much more positive feelings and concepts confirming the existence of biases that favor privileged social groups In German Wikipedia men were generally depicted more positively On the other hand in the social media dataset women were associated with more positive words One explanation is that in Wikipedia men were described by stereotypical concepts like power attack and reinforcement which are labeled as positive in the polarity dictionary In contrast the social media data also related men to concepts like fascism and robbery ie words with highly negative sentiment That could also be rooted in the nature of German language which uses the male plural when making colloquial general claims Because negative statements about groups on social media were generated in a male form this bias could have been replicated by the model Furthermore the sentiment difference does not fully replicate bias in text For example in social media data women are often associated with the term mother for which the sentiment lexicon assigns a positive score Nevertheless the actual combination of words in a political context corresponds to sexist speech as numerous users refer to female politicians as mothers in order to undermine their political abilities The above results illustrate that word embeddings contain a high level of bias in them in terms of group stereotypes and prejudice The intergroup comparison between sexes populations and sexual orientations revealed the existence of strong stereotypes and unbalanced evaluations of groups Although Wikipedia contained stronger bias in terms of stereotypes social media contained a higher bias in terms of group prejudice Bias Diffusion Mitigation Prediction Our analysis shows that the above bias was diffused further into the trained sentiment classifiers We trained one classifier for each embedding dataset with both having a test set accuracy of around TR ARAB GR IT PL FR USA DE wikipedia IT ARAB USA FR GR TR PL DE social media Figure Predicted score of the sentiment classifier for stereotypical names of different populations The classification task for stereotypical names of different communities illustrated a preference for German names Figure In both embedding datasets German names were assigned the highest average sentiment score In contrast most of the foreign names were assigned negative sentiment values Arabic and Russian names were negatively associated in both datasets which can be grounded both on existing social stereotypes against Russians and Arabs in the society as well as mainstream media representations of the ethnicities Greek Polish and Turkish names were seen much more positively by the social media classifier This comes in contrast to what someone would actually expect since a large part of contemporary German public opinion holds strong negative stereotypes against Greek Polish and Turkish populations due to economic and migration issues French and US-American stereotypical names were classified much more positively by the Wikipedia classifier The result related to French names was not intuitive given the historical conflicts between Germany and France that are extensively covered in Wikipedia In contrary researchers illustrate that non-English Wikipedia pages on US-American persons generally contain positive cues explaining also the favoritism of the classifier for US-American names Overall the classifiers social discrimination biases for the models trained on the Wikipedia and the social media data were Bc wiki and Bc sm respectively The bias of the classifier was similar to the bias in the embeddings as in both cases German concepts were evaluated much more positively For both classifiers the Kruskal-Wallis tests were significant sm classifier p-value wiki classifier p-value denoting that the mean bias for each ethnicity varies significantly from the others We concluded with similar findings when predicting the sentiment of male and female names The classifiers exactly replicated the prejudice as measured in the word embeddings Figure The Wikipedia classifier predicted a higher average sentiment score for male names In contrast the social media classifier assigned a much more positive overall score to female names This complies with ARAB GR USA TR FR DE PL IT wikipedia embeddings classifier PL DE USA FR ARAB TR IT GR social media embeddings classifier Bias after mitigation Figure Bias in the sentiment classifier for stereotypical names of various populations after mitigation at a the embeddings level b the level of the classifier the results from the intergroup positive sentiment difference in the embeddings where women were associated with more positive concepts than men in the social media dataset while the opposite happened in the Wikipedia embeddings Hence we proved that classifiers trained in biased word embeddings replicate the bias existing in the vectors Overall the classifiers social discrimination biases were Bc wiki and Bc sm respectively The Mann-Whitney U test was significant for the social media classifier U p-value but not for the Wikipedia classifier U p-value This does not mean that there is no bias between sexes in the second case By breaking down names by ethnicity and comparing them we get significant results for German U p-value Polish U p-value Greek U p-value and US-American U p-value names The study proves that the diffused bias can be mitigated Both methodologies for bias mitigation reduced bias significantly Mitigation at the embeddings level resulted in social discrimination biases of the classifiers of Bc wiki and Bc sm for the population comparison Similarly when predicting the sentiment of male and female names the bias of the classifiers after mitigation was Bc wiki and Bc sm respectively Mitigation at the level of the classifier was by far more efficient In all possible tasks the overall social discrimination bias vanished Figure presents an overview of bias before and after mitigation for each case In order to understand why the second methodology provides better results we calculated the cosine distance between the sentiment vectors of the embeddings and the classifier which were used for de-biasing The value was close to denoting that the classifier actually learns a significantly different sentiment direction than the one defined by the methodology proposed by Bolukbasi et al Actually the classifier learns further associations between the vectors which are not taken into consideration when debiasing at the embeddings level Debiasing at the embeddings level female male ip ed before mitigation female male ip ed after mitigation embeddings classifier female male so al m ed female male so al m ed embeddings classifier Bias of classifier Figure Predicted score of the sentiment classifier for male and female names before and after mitigation by applying two different methods results in the diffusion of a different bias in the classifier As Figure shows although bias related to the favored group was highly reduced remaining patterns in the data resulted in a totally different bias diffusion This bias was not universally distributed in all cases but resulted in asymmetries in certain cases For example for the classifier trained on the Wikipedia embeddings the mean bias difference between German and Russian U p-value Arabic U p-value and Italian U p-value names remained statistically significant This was not the case for the sex names comparison in either classifier sm U p-value wiki U p-value or ethnicity names comparison for the classifier trained on the social media embeddings Kruscal-Wallis p-value Hence we show that debiasing at the classifier level is a much better and safer methodology to follow Because of the mathematical definition of the linear support vector classifier it was straightforward to mitigate the bias in it For other cases where non-linearity prevails more sophisticated methodologies are needed Our last finding states that biased word embeddings can be useful for bias prediction tasks We trained and deployed various models on the sexism prediction task with and without the trained biased word embeddings On the first test we created a simple LSTM model which had as inputs either a random dataset Wikipedia social media or the sexism dataset word embeddings We restricted the embeddings from being trainable in order to evaluate their actual influence on the results In addition we only inserted the values of the word embeddings for the words that were common in the datasets In this way we could assure that if an embedding dataset had more impact on the results that it would be because of the type of information encoded into the vectors and not the amount of words existing in the dataset The models with the trained embeddings provided higher test accuracy and scores The model with the sexism dataset vectors yielded the best results The social media embeddings provided better results than the Wikipedia vectors The calculated weighted mean cosine similarity between the sexism dataset vectors and the social media and the Wikipedia datasets was and respectively This denotes that social media vectors are more similar to the vectors of the sexism classifier which in turn signifies that more similar meanings and consequently biases were encoded in them This is also proven by the sentiment task for which the sexism dataset vectors had similar prejudice with the social media vectors Figure Thus the more similar the bias in the embeddings with the target data the higher the ability of the classifier to detect the bias On the second task we used additional architectures for the prediction task We allowed the embeddings to be freely trainable and used all the available vectors to predict sexism The best model contained an attention layer and provided an accuracy of Then we removed all test observations that contained words that did not appear in the training process and recalculated the accuracy We obtained an overall score of on the test data Given the general difficulty in the detection of sexism and hate-speech by machine learning models the results are more than satisfactory The models input was text without any punctuation nor any other metadata that generally help in detecting social discrimination Therefore we showed that biased word embeddings can substantially help in sexism detection while attention based networks can provide really high accuracy in detecting sexism An overview of all models can be found in table Table Classification results for the sexism task Model Embeddings Trainable Accuracy sexist neutral LSTM Random False LSTM Wiki common False LSTM SM common False LSTM Sexism common False Attention Sexism all True Attention Sexism all filtered True Evaluating biased word embeddings The analysis provided a thorough description of bias in word embeddings We proved that the technique replicates biases related to sexism homophobia and xenophobia immanent in the original text We showed that Wikipedia data mediates to the word embeddings stronger stereotypes while political social media data imprints stronger forms of group favoritism into the vectors The study illustrated that the use of biased word embeddings results in the creation of biased machine learning classifiers Models trained on the embeddings replicate the preexisting bias Bias diffusion was proven both for sexism and xenophobia with sentiment classifiers assigning positive sentiments to Germans and negative sentiments to foreigners In addition the amount of polarity for men and women in the embeddings was diffused unaltered into the models We used two methods for bias mitigation one at the level of the embeddings and one at the level of the classifier In both cases we lowered the bias while mitigation at the level of the classifier was the optimal one The analysis also showed that biased word embeddings can be beneficial for bias prediction Embeddings containing bias similar to the one in the investigated dataset can help in the classification task We showed that text-only models for bias prediction can provide more than satisfactory results by using embeddings Among the various models developed we found that simple attention-based neural networks yielded the best results Of course the developed models are in the position to detect forms of sexism similar to that defined by the inter-subjective coding process and its theoretical assumptions The models are not generalizable to other forms of sexism that were not taken into consideration at the development of the dataset Nevertheless the study provides promising findings for the detection of biases in text by the use of word embeddings and deep neural architectures Overall the study provided a full evaluation of biased word embeddings It showed how bias can be detected its diffusion and how it can be mitigated It also proved that different forms of bias influence further models differently In addition we showed positive aspects of word embeddings Not only can they be used for bias detection but most importantly they can help understand and evaluate sociopolitical relations immanent in text DISCUSSION The findings of the study provide a complete picture of the issues limits and possibilities of biased word embeddings at the algorithmic level In the discussion we go one step further and analyze the societal importance of the aforementioned findings We illustrate the emerging opportunities for the use of biased word embeddings while we explain their negative properties Last but not least we describe the related challenges that researchers and decision makers need to deal with in order to assure a just application of algorithmic systems based on word vectors On the positive side the ability of word embeddings to absorb semantic relations of the social world prevails as their main advantage Being able to quantify bias existing in the society latent political relations and properties of language and text has always been a scientific challenge and until now a privilege of qualitative social science Word embeddings constitute a way to mathematically grasp and describe sociopolitical relations through the analysis of text allowing the quantification of phenomena as racism sexism and social discrimination in general Based on the vectors it is possible to evaluate social phenomena compare and measure their magnitude for different conditions and context A systematic analysis of word embeddings can result in the creation of new scientific knowledge about the social world redefining and developing further existing theories Furthermore developing models for bias detection by using biased word embeddings can be beneficial Word embeddings generally improve the accuracy of machine learning models and we proved that this was also the case in bias prediction a task which is highly difficult On the negative side the dependence of word embeddings on the nature of the input data is an open methodological issue There is no such thing as naturally developed neutral text because the semantic content of words is always bound with the sociopolitical relations of a society The study illustrates that even text generated in a formal and controlled environment like Wikipedia results in biased word embeddings Furthermore the preexisted bias becomes even more graspable when evaluating the vectors and using them in further algorithms The algorithms associate stereotypes and concepts to specific social groups while containing latent prejudice These associations are usually not directly perceivable in the initial text nor are they uniformly distributed within it Nevertheless the projection of words in a mathematical space by the embeddings consolidates stereotyping and prejudice assigning static properties to social groups and individuals Relations are no longer context-dependent and dynamic and embeddings become deterministic projections of the bias of the social world This bias is diffused into further algorithms unchanged resulting in socially discriminative decisions Word embeddings are a valuable tool for improving machine learning models and for understanding the social world Managing their bias prevails as an open challenge for ethical and fair algorithmic applications Until now researchers and commercial companies train and integrate word embeddings uncontrollably in their models without taking into consideration the potential impact and societal implications The study showed that bias in word embeddings can result in algorithmic social discrimination yielding negative inferences on specific social groups and individuals Therefore it is necessary not only to reflect on the related issues but also to develop frameworks of action for the just use of word embeddings To achieve that it is necessary to develop frameworks that detect bias in concrete algorithmic applications of the embeddings and quantify their impact on individuals and the society This presupposes commercial companies becoming more transparent regarding the exact algorithms and data they use in their products and decisions Only through detailed auditing can it be possible to fully understand the issues and start implementing measures that assure algorithmic justice These measures include the hard mitigation of the bias at the level of the end product in such a way that no individual is negatively influenced or discriminated against It also includes the development of artificial datasets that comply with certain social expectations on which the embeddings can be trained on Until now word embeddings are either trained on text related to a specific algorithmic application or context or on huge freely accessible corpora In both cases bias in the text is always imprinted in the embeddings and therefore also diffused in further models It is necessary to search for alternatives in order to remove preexisting bias in an optimal way Our study provides a complete overview on the issue of bias in word embeddings Not only does it describe the problems and possible solutions but also initiates an important discussion on the implementation of the vectors in commercial applications The presented results denote the need for more transparency in the use of word embeddings in order to ensure their ethical algorithmic implementation The mathematical tools for model evaluations are already provided actions from the related stakeholders need to follow