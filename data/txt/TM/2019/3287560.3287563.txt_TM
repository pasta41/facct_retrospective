
Despite vigorous debates about the technical characteristics of risk
assessments being deployed in the U.S. criminal justice system,
remarkably little research has studied how these tools a￿ect actual
decision-making processes. After all, risk assessments do not make
de￿nitive decisions—they inform judges, who are the ￿nal arbiters.
It is therefore essential that considerations of risk assessments be
informed by rigorous studies of how judges actually interpret and
use them. This paper takes a ￿rst step toward such research on
human interactions with risk assessments through a controlled
experimental study on Amazon Mechanical Turk. We found several
behaviors that call into question the supposed e￿cacy and fair-
ness of risk assessments: our study participants 1) underperformed
the risk assessment even when presented with its predictions, 2)
could not e￿ectively evaluate the accuracy of their own or the risk
assessment’s predictions, and 3) exhibited behaviors fraught with
“disparate interactions,” whereby the use of risk assessments led
to higher risk predictions about black defendants and lower risk
predictions about white defendants. These results suggest the need
for a new “algorithm-in-the-loop” framework that places machine
learning decision-making aids into the sociotechnical context of
improving human decisions rather than the technical context of
generating the best prediction in the abstract. If risk assessments
are to be used at all, they must be grounded in rigorous evaluations
of their real-world impacts instead of in their theoretical potential.
CCS CONCEPTS
• Human-centered computing → Human computer interac-
tion (HCI); • Applied computing→ Law.
KEYWORDS
fairness, risk assessment, behavioral experiment, Mechanical Turk
ACM Reference Format:
Ben Green and Yiling Chen. 2019. Disparate Interactions: An Algorithm-in-
the-Loop Analysis of Fairness in Risk Assessments. In FAT* ’19: Conference
on Fairness, Accountability, and Transparency (FAT* ’19), January 29–31, 2019,
Atlanta, GA, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.
1145/3287560.3287563
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro￿t or commercial advantage and that copies bear this notice and the full citation
on the ￿rst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6125-5/19/01.
https://doi.org/10.1145/3287560.3287563
1 INTRODUCTION
Across the United States, courts are increasingly using risk as-
sessments to estimate the likelihood that criminal defendants will
engage in unlawful behavior in the future.1 These tools are being
deployed during several stages of criminal justice adjudication, in-
cluding at bail hearings (to predict the risk that the defendant, if
released, will be rearrested before trial or not appear for trial) and
at sentencing (to predict the risk that the defendant will recidivate).
Because risk assessments rely on data and a standardized process,
many proponents believe that they can mitigate judicial biases and
make “objective” decisions about defendants [9, 12, 34]. Risk assess-
ments have therefore gained widespread support as a tool to reduce
incarceration rates and spur criminal justice reform [9, 27, 34].
Yet many are concerned that risk assessments make biased de-
cisions due to the historical discrimination embedded in training
data. For example, the widely-used COMPAS risk assessment tool
wrongly labels black defendants as future criminals at twice the
rate it does for white defendants [3]. Prompted by these concerns,
machine learning researchers have developed a rapidly-growing
body of technical work focused on topics such as characterizing the
incompatibility of di￿erent fairness metrics [6, 44] and developing
new algorithms to reduce bias [24, 33].
Despite these e￿orts, current research into fair machine learning
fails to capture an essential aspect of how risk assessments impact
the criminal justice system: their in￿uence on judges. After all,
risk assessments do not make de￿nitive decisions about pretrial
release and sentencing—they merely aid judges, who must decide
whom to release before trial and how to sentence defendants after
trial. In other words, algorithmic outputs act as decision-making
aids rather than ￿nal arbiters. Thus, whether a risk assessment
itself is accurate and fair is of only indirect concern—the primary
considerations are how it a￿ects decision-making processes and
whether it makes judgesmore accurate and fair. No matter how well
we characterize the technical speci￿cations of risk assessments, we
will not fully understand their impacts unless we also study how
judges interpret and use them.
This study sheds new light on how risk assessments in￿uence
human decisions in the context of criminal justice adjudication.
We ran experiments using Amazon Mechanical Turk to study how
people make predictions about risk, both with and without the
aid of a risk assessment. We focus on pretrial release, which in
many respects resembles a typical prediction problem.