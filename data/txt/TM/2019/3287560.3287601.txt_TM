
Personalization is pervasive in the online space as it leads to higher
e￿ciency for the user and higher revenue for the platform by in-
dividualizing the most relevant content for each user. However,
recent studies suggest that such personalization can learn and prop-
agate systemic biases and polarize opinions; this has led to calls
for regulatory mechanisms and algorithms that are constrained to
combat bias and the resulting echo-chamber e￿ect. We propose a
versatile framework that allows for the possibility to reduce polar-
ization in personalized systems by allowing the user to constrain
the distribution from which content is selected. We then present a
scalable algorithm with provable guarantees that satis￿es the given
constraints on the types of the content that can be displayed to a
user, but – subject to these constraints – will continue to learn and
personalize the content in order to maximize utility. We illustrate
this framework on a curated dataset of online news articles that
are conservative or liberal, show that it can control polarization,
and examine the trade-o￿ between decreasing polarization and the
resulting loss to revenue. We further exhibit the￿ exibility and scal-
ability of our approach by framing the problem in terms of the more
general diverse content selection problem and test it empirically
on both a News dataset and the MovieLens dataset.
CCS CONCEPTS
• Information systems→ Personalization; • Theory of compu-
tation → Online learning algorithms.
KEYWORDS
Personalization, recommender systems, polarization, bandit opti-
mization, group fairness, diversi￿cation
ACM Reference Format:
L. Elisa Celis, Sayash Kapoor, Farnood Salehi, and Nisheeth Vishnoi. 2019.
Controlling Polarization in Personalization: An Algorithmic Framework. In
FAT* ’19: Conference on Fairness, Accountability, and Transparency, January
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro￿t or commercial advantage and that copies bear this notice and the full citation
on the￿ rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01.
https://doi.org/10.1145/3287560.3287601
29–31, 2019, Atlanta, GA, USA. ACM, Atlanta, GA, USA, 13 pages. https:
//doi.org/10.1145/3287560.3287601
1 INTRODUCTION
News and social media feeds, product recommendation, online ad-
vertising and other media that pervades the internet is increasingly
personalized. Content selection algorithms consider a user’s prop-
erties and past behavior in order to produce a personalized list of
content to display [21, 27]. This personalization leads to higher util-
ity and e￿ciency both for the platform, and for the user, who sees
content more directly related to their interests [18, 19]. However, it
is now known that such personalization may result in propagating
or even creating biases that can in￿uence decisions and opinions. In
an important study, [17] showed that user opinions about political
candidates, and hence elections, can be manipulated by changing
the personalized rankings of search results. Other studies show that
allowing for personalization of news and other sources of informa-
tion can result in a “￿lter bubble” [29] which results in a type of
tunnel vision, e￿ectively isolating people into their own cultural or
ideological bubbles; e.g., enabled by polarized information, many
people did not expect a Brexit vote or Trump election [10]. This
phenomenon has been observed on many social media platforms
(see, e.g., [13, 23, 39]), and studies have shown that over the past
eight years polarization has increased by 20% [20].
Polarization, and the need to combat it, was raised as a problem
in [32], where it was shown that Google search results di￿er sig-
ni￿cantly based on political preferences in the month following
the 2016 elections in the United States. In a di￿erent setting, the
ease with which algorithmic bias can be introduced and the need
for solutions was highlighted in [35] where it was shown that it
is very easy to target people on platforms such as Facebook in a
discriminatory fashion. Several approaches to quantify bias and
polarization of online media have now been developed [31], and
interventions for￿ ghting polarization have been proposed [11].
One approach to counter such polarization would be to hide certain
user properties so that they cannot be used for personalization.
However, this could come at a loss to the utility for both the user
and the platform – the content displayed would be less relevant
and result in decreased attention from the user and less revenue
for the platform (see, e.g., [34]).
Can we design personalization algorithms that allow
us to avoid polarization yet still optimize individual
utility?
1.1 Groups and Polarization
Often, content is classi￿ed into di￿erent groups which are de￿ned
by one or more multi-valued sensitive attributes; for instance, news
stories can have a political leaning (e.g., conservative or liberal), and
a topic (e.g., politics, business or entertainment). More generally,
search engines and other platforms and applications maintain topic
models over their content (see e.g., [8]). At every time-step, the algo-
rithm must select a piece of content to display to a given user,1 and
feedback is obtained in the form of whether they click on, purchase
or hover over the item. The goal of the content selection algorithm
is to select content for each user in order to maximize the positive
feedback (and hence revenue) received; to do so, it must learn about
the topics or groups the user is most interested in. Thus, as this
optimal topic is a-priori unknown, the process is often modeled
as an online learning problem in which a user-speci￿c probability
distribution (from which one selects content) is maintained and
updated according to feedback given [28]. As the content selection
algorithm learns more about a user, the corresponding probabil-
ity distribution begins to concentrate the mass on a small subset
of topics; this results in polarization where the feed is primarily
composed of a single type of content.
1.