
Quantitative definitions of what is unfair and what is fair have been
introduced in multiple disciplines for well over 50 years, including
in education, hiring, and machine learning. We trace how the no-
tion of fairness has been defined within the testing communities of
education and hiring over the past half century, exploring the cul-
tural and social context in which different fairness definitions have
emerged. In some cases, earlier definitions of fairness are similar
or identical to definitions of fairness in current machine learning
research, and foreshadow current formal work. In other cases, in-
sights into what fairness means and how to measure it have largely
gone overlooked. We compare past and current notions of fairness
along several dimensions, including the fairness criteria, the focus
of the criteria (e.g., a test, a model, or its use), the relationship of fair-
ness to individuals, groups, and subgroups, and the mathematical
method for measuring fairness (e.g., classification, regression). This
work points the way towards future research and measurement of
(un)fairness that builds from our modern understanding of fairness
while incorporating insights from the past.
CCS CONCEPTS
•General and reference→ Surveys and overviews;Metrics; •
Social and professional topics→History of computing;His-
torical people; History of computing theory; Socio-technical sys-
tems; User characteristics; •Mathematics of computing→ Prob-
ability and statistics; Probabilistic algorithms; • Theory of com-
putation→ Probabilistic computation; • Computing methodolo-
gies→ Algebraic algorithms; • Software and its engineering→
Model checking;
KEYWORDS
history, fairness, ML fairness, test fairness, psychometrics
ACM Reference Format:
Ben Hutchinson and Margaret Mitchell. 2019. 50 Years of Test (Un)fairness:
Lessons for Machine Learning. In FAT* ’19: Conference on Fairness, Account-
ability, and Transparency (FAT* ’19), January 29–31, 2019, Atlanta, GA, USA.
ACM,NewYork, NY, USA, 10 pages. https://doi.org/10.1145/3287560.3287600
1 INTRODUCTION
The United States Civil Rights Act of 1964 effectively outlawed
discrimination on the basis of of an individual’s race, color, religion,
sex, or national origin. The Act contained two important provisions
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287600
that would fundamentally shape the public’s understanding of what
it meant to be unfair, with lasting impact into modern day: Title VI,
which prevented government agencies that receive federal funds
(including universities) from discriminating on the basis of race,
color or national origin; and Title VII, which prevented employers
with 15 or more employees from discriminating on the basis of race,
color, religion, sex or national origin.
Assessment tests used in public and private industry immedi-
ately came under public scrutiny. The question posed by many
at the time was whether the tests used to assess ability and fit in
education and employment were discriminating on bases forbidden
by the new law [2]. This stimulated a wealth of research into how
to mathematically measure unfair bias and discrimination within
the educational and employment testing communities, often with a
focus on race. The period of time from 1966 to 1976 in particular
gave rise to fairness research with striking parallels to ML fair-
ness research from 2011 until today, including formal notions of
fairness based on population subgroups, the realization that some
fairness criteria are incompatible with one another, and pushback
on quantitative definitions of fairness due to their limitations.
Into the 1970s, there was a shift in perspective, with researchers
moving from defining how a test may be unfair to how a test may
be fair. It is during this time that we see the introduction of mathe-
matical criteria for fairness identical to the mathematical criteria
of modern day. Unfortunately, this fairness movement largely dis-
appeared by the end of the 1970s, as the different and sometimes
competing notions of fairness left little room for clarity on when
one notion of fairness may be preferable to another. Following
the retrospective analysis of Nancy Cole [15], who introduced the
equivalent of Hardt et al.’s 2016 equality of opportunity [32] in 1973:
The spurt of research on fairness issues that began in the
late 1960s had results that were ultimately disappointing.
No generally agreed uponmethod to determine whether
or not a test is fair was developed. No statistic that could
unambiguously indicate whether or not an item is fair
was identified. There were no broad technical solutions
to the issues involved in fairness.
By learning from this past, we hope to avoid such a fate.
Before further diving in to the history of testing fairness, it is
useful to briefly consider the structural correspondences between
tests and ML models. Test items (questions) are analogous to model
features, and item responses analogous to specific activations of
those features. Scoring a test is typically a simple linearmodel which
produces a (possibly weighted) sum of the item scores. Sometimes
test scores are normalized or standardized so that scores fit a desired
range or distribution. Because of this correspondence, much of the
math is directly comparable; and many of the underlying ideas in
earlier fairness work trivially map on to modern day ML fairness.
“History doesn’t repeat itself, but it often rhymes”; and by hearing
this rhyme, we hope to gain insight into the future of ML fairness.
Following terminology of the social sciences, applied statistics,
and the notation of [4], we use “demographic variable” to refer
to an attribute of individuals such as race, age or gender, denoted
by the symbol A. We use “subgroup” to denote a group of indi-
viduals defined by a shared value of a demographic variable, e.g.,
A = a. Y indicates the ground truth or target variable, R denotes a
score output by a model or a test, and D denotes a binary decision
made using that score. We occasionally make exceptions when
referencing original material.
