
This paper proposes new search algorithms for counterfactual expla-
nations based upon mixed integer programming. We are concerned
with complex data in which variables may take any value from a
contiguous range or an additional set of discrete states. We propose
a novel set of constraints that we refer to as a “mixed polytope” and
show how this can be used with an integer programming solver to
efficiently find coherent counterfactual explanations i.e. solutions
that are guaranteed to map back onto the underlying data structure,
while avoiding the need for brute-force enumeration. We also look
at the problem of diverse explanations and show how these can be
generated within our framework.
CCS CONCEPTS
• Human-centered computing → Human computer interaction
(HCI); • Theory of computation → Integer programming; Inte-
ger programming; • Computing methodologies → Supervised
learning;
KEYWORDS
Counterfactual Explanation, Machine Learning, Linear Program
ACM Reference Format:
Chris Russell. 2019. Efficient Search for Diverse Coherent Explanations. In
FAT* ’19: Conference on Fairness, Accountability, and Transparency (FAT* ’19),
January 29–31, 2019, Atlanta, GA, USA. ACM, New York, NY, USA, 9 pages.
1 INTRODUCTION
A fundamental tension exists between the high performance of
machine learning algorithms and the notion of transparency (Lip-
ton, 2016). The large complex models of machine learning are cre-
ated by researchers and system builders looking to maximise their
performance on real-world data, and it is precisely their size and
complexity that allows them to fit to the data giving them such
high-performance. At the same time such models are simply too
complex to fit in their builders minds; and even the people that
created the systems need not understand why they make particular
decisions.
This tension becomes more apparent as we start using machine
learning to make decisions that substantially alter people’s lives. As
algorithms are used to make loan decision; to recommend whether
or not some one should be released on parole; or to detect cancer, it
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
FAT*’19, January 2019, Atlanta, Georgia USA
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6125-5/19/01.
is vital that not only are the algorithms used as accurate as possible,
but also that they justify themselves in some way, allowing the
subject of the decisions to verify the data used to make decisions
about them, and to challenge inappropriate decisions
A common remedy to avoid this trade-off is to learn the complex
function, and then fit simple models about datapoints providing
human comprehensible approximations of the underlying function.
While popular in the machine learning community, there are many
challenges in conveying the quality of the approximation and the
domain over which it is valid to a lay audience.
Another promising approach to explaining the incomprehensi-
ble models of machine learning lies in counterfactual explanations
(Lewis, 1973; Wachter et al., 2018). This recent approach to explain-
ablity bypasses the problem of describing how a function works
and instead focuses on the data. Instead, counterfactual explana-
tions attempt to answer the question “How would my data need to
be changed to get a different outcome?”. Wachter et al. make the
argument that there are three important use cases for explanation:
(1) to inform and help the individual understand why a particu-
lar decision was reached,
(2) to provide grounds to contest the decision if the outcome is
undesired, and
(3) to understand what would need to change in order to receive
a desired result in the future, based on the current decision-
making model.
and that counterfactual explanations satisfy all three.
Although making the case for the use of counterfactuals and
showing how they could be effectively calculated for common clas-
sifiers, Wachter et al. left many technical questions unanswered.
Of particular concern is the issue of how should we generate coun-
terfactuals efficiently and reliably for standard classifiers.
This paper focuses on the technical aspects needed to generate co-
herent counterfactual explanations. Keeping the existing definition
of counterfactual explanations intact, we look at how explanations
can be reliably generated. We make two contributions:
(1) Focusing on primarily the important problem of explaining fi-
nancial decisions, we look at the most common case in which
the classifier is linear (i.e. linear/logistic regression, SVM etc.)
but the data has been transformed via a mix-encoding based
upon 1-hot or dummy variable encoding. We present a novel
integer program based upon a “mixed polytope” that is guar-
anteed to generate coherent counterfactuals that map back
into the same form as the original data.
(2) We provide a novel set of criteria for generating diverse
counterfactuals and integrate them with our mixed polytope
method.
Previously, Wachter et al. strongly made the case that diverse coun-
terfactuals are important to informing a lay audience about the
DOI: 10.1145/3287560.3287569
FAT*’19, January 2019, Atlanta, Georgia USA Chris Russell
decisions that have been made, writing that: “...individual coun-
terfactuals may be overly restrictive. A single counterfactual may
show how a decision is based on certain data that is both correct
and unable to be altered by the data subject before future decisions,
even if other data exist that could be amended for a favourable out-
come. This problem could be resolved by offering multiple diverse
counterfactual explanations to the data subject.” but to date no one
has proposed a concrete method for generating them.
We evaluate our new for mixed data approach on standard ex-
plainability problems, and the new FICO explainability dataset,
where we show our fully automatic approach generates coherent
and informative diverse explanations for a range of sample inputs.
