
We present a large-scale study of gender bias in occupation clas-
sification, a task where the use of machine learning may lead to
negative outcomes on peoples’ lives. We analyze the potential allo-
cation harms that can result from semantic representation bias. To
do so, we study the impact on occupation classification of including
explicit gender indicators—such as first names and pronouns—in
different semantic representations of online biographies. Addition-
ally, we quantify the bias that remains when these indicators are
“scrubbed,” and describe proxy behavior that occurs in the absence
of explicit gender indicators. As we demonstrate, differences in true
positive rates between genders are correlated with existing gender
imbalances in occupations, which may compound these imbalances.
CCS CONCEPTS
• Computing methodologies→Machine learning; • Applied
computing → Document management and text processing.
KEYWORDS
Supervised learning, algorithmic fairness, gender bias, online re-
cruiting, automated hiring, compounding injustices.
ACM Reference Format:
Maria De-Arteaga
Christian Borgs
Kenthapadi
1Carnegie Mellon University, 2University
of Massachusetts Lowell, 3Microsoft Research, 4LinkedIn. 2019. Bias in Bios:
A Case Study of Semantic Representation Bias in a High-Stakes Setting. In
FAT* ’19: Conference on Fairness, Accountability, and Transparency, January
29–31, 2019, Atlanta, GA, USA. ACM, New York, NY, USA, 9 pages. https:
//doi.org/10.1145/3287560.3287572
1 INTRODUCTION
The presence of automated decision-making systems in our daily
lives is growing. As a result these systems play an increasingly
active role in shaping our future. Far from being passive players
that consume information, automated decision-making systems are
participating actors: their predictions today affect the world we live
in tomorrow. In particular, they determine many aspects of how
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287572
we experience the world, from the news we read and the products
we shop for to the job postings we see. The increased prevalence
of machine learning has therefore been accompanied by a growing
concern regarding the circumstances and mechanisms by which
such systems may reproduce and augment the various forms of
discrimination and injustices that are present in today’s society.
One domain in which the use of machine learning is increasingly
popular—and in which unfair practices can lead to particularly
negative consequences—is that of online recruiting and automated
hiring. Maintaining an online professional presence has become
increasingly important for people’s careers, and this information
is often used as input to automated decision-making systems that
advertise open positions and recruit candidates for jobs and other
professional opportunities. In order to perform these tasks, a sys-
tem must be able to accurately assess people’s current occupations,
skills, interests, and “potential.” However, even the simplest of these
tasks—determining someone’s current occupation—can be non-
trivial. Although this information may be provided in a structured
form on some professional networking platforms, this is not always
the case. As a result, recruiters often browse candidates’ websites
in an attempt to manually determine their current occupations. Ma-
chine learning promises to reduce this burden; however, as we will
explain in this paper, occupation classification is susceptible to gen-
der bias, stemming from existing gender imbalances in occupations.
To study gender bias in occupation classification, we created a
new dataset of hundreds of thousands of online biographies, written
in English, from the Common Crawl corpus. Because biographies
are typically written in the third person by their subjects (or people
familiar with their subjects) and because pronouns are gendered
in English, we were able to extract (likely) self-identified binary
gender from the biographies. We note, though, that this binary
model is a simplification that fails to capture important aspects of
gender and erases people who do not fit within its assumptions.
Using this dataset, we predicted people’s occupations by per-
forming multi-class classification using three different semantic
representations: bag-of-words, word embeddings, and deep recur-
rent neural networks. For each representation, we considered two
scenarios: (1) where explicit gender indicators are available to the
classifier, (2) where explicit gender indicators are “scrubbed” to
promote fairness or to comply with regulations or laws. We define
explicit gender indicators to be information, such as first names and
gendered pronouns, that make it possible to determine gender. We
note that the practice of “scrubbing” explicit gender indicators and
other sensitive attributes is not unique to machine learning, and is
often used as a way to mitigate the effects of implicit and explicit
bias on decisions made by humans. For example, gender diversity
in orchestras was significantly improved by the introduction of
“blind” auditions, where candidates play behind a curtain [24].
To quantify gender bias, we compute the true positive rate (TPR)
gender gap—i.e., the difference in TPRs between genders—for each
occupation. The TPR for a given gender and occupation is defined
as the proportion of people with that gender and occupation that
are correctly predicted as having that occupation. We also compute
the correlation between these TPR gender gaps and existing gen-
der imbalances in occupations, and show how this may compound
these imbalances; we connect this finding with an existing notion of
indirect discrimination in political philosophy.We show that “scrub-
bing” explicit gender indicators reduces the TPR gender gaps, while
maintaining overall classifier accuracy. However, we also show
that significant TPR gender gaps remain in the absence of explicit
gender indicators, and that these gaps are correlated with existing
gender imbalances. For orchestra auditions, the sounds made by
candidates’ shoes mean that a curtain is not sufficient to make an
audition “blind.” It is therefore common practice to additionally roll
out a carpet or to ask candidates to remove their shoes [24]. By
analogy, “scrubbing” explicit gender indicators is like introducing
a curtain—the sounds made by the candidates’ shoes remain.
Our paper has two main takeaways: First, “scrubbing” explicit
gender indicators is not sufficient to remove gender bias from an
occupation classifier. Second, even in the absence of such indicators,
TPR gender gaps are correlated with existing gender imbalances in
occupations; occupation classifiers may therefore compound exist-
ing gender imbalances. Although we focus on gender bias, we note
that other biases, such as those involving race or socioeconomic sta-
tus, may also be present in occupation classification or in other tasks
related to online recruiting and automated hiring. We structure our
analysis so as to inform discussions about these biases as well.
In the next section, we provide a brief overview of related work.
We then describe our data collection process in Section 3 and outline
our methodology in Section 4, before presenting our analysis and
results in Section 5. We conclude with a discussion in Section 6.
