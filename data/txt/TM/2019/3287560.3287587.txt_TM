
Powered by machine learning techniques, social media provides
an unobtrusive lens into individual behaviors, emotions, and psy-
chological states. Recent research has successfully employed social
media data to predict mental health states of individuals, ranging
from the presence and severity of mental disorders like depres-
sion to the risk of suicide. These algorithmic inferences hold great
potential in supporting early detection and treatment of mental
disorders and in the design of interventions. At the same time, the
outcomes of this research can pose great risks to individuals, such
as issues of incorrect, opaque algorithmic predictions, involvement
of bad or unaccountable actors, and potential biases from inten-
tional or inadvertent misuse of insights. Amplifying these tensions,
there are also divergent and sometimes inconsistent methodologi-
cal gaps and under-explored ethics and privacy dimensions. This
paper presents a taxonomy of these concerns and ethical challenges,
drawing from existing literature, and poses questions to be resolved
as this research gains traction. We identify three areas of tension:
ethics committees and the gap of social media research; questions
of validity, data, and machine learning; and implications of this
research for key stakeholders. We conclude with calls to action to
begin resolving these interdisciplinary dilemmas.
CCS CONCEPTS
• Human-centered computing → Collaborative and social
computing; Social media; •Applied computing→ Psychology;
KEYWORDS
mental health; ethics; machine learning; algorithms; social media
ACM Reference Format:
Stevie Chancellor, Michael L Birnbaum, Eric D. Caine, Vincent M. B. Silenzio,
and Munmun De Choudhury. 2019. A Taxonomy of Ethical Tensions in
Inferring Mental Health States from Social Media. In Proceedings of FAT* ’19:
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287587
Conference on Fairness, Accountability, and Transparency (FAT* ’19). ACM,
New York, NY, USA, 10 pages. https://doi.org/10.1145/3287560.3287587
1 INTRODUCTION
Last year, Facebook unveiled automated tools to identify individuals
contemplating suicide or self-injury [75, 62]. The company claims
that they “use pattern recognition technology to help identify posts
and live streams as likely to be expressing thoughts of suicide,”
which then can deploy resources to assist the person in crisis [75].
Reactions to Facebook’s suicide prevention artificial intelligence
(AI) are mixed, with some concerned about the use of AI to detect
suicidal ideation as well as potential privacy violations [86]. Other
suicide preventionAIs, however, have beenmetwith stronger public
backlash. Samaritan’s Radar, an app that scanned a person’s friends
for concerning Twitter posts, was pulled from production, citing
concerns for data collection without user permission [54], as well as
enabling harassers to intervene when someone was vulnerable [4].
Since 2013, a new area of research has incorporated techniques
from machine learning, natural language processing, and clini-
cal psychology to categorize individuals’ moods and expressed
well-being from social media data. These algorithms are powerful
enough to infer with high accuracy whether an individual might
be suffering from disorders such as major depression [28, 19, 84,
73, 78], postpartum depression [26, 27], post-traumatic stress [21],
schizophrenia [60, 6], and suicidality [15, 22]. These algorithms
can also reveal symptomatology linked to psychiatric challenges,
such as self-harm [89], severity of distress [13], or cognitive distor-
tions [82]. Together, we use the term predictingmental health status
to describe these mental disorders and related symptomatology.
Computer Science (CS) researchers and clinicians are now poised
to learn more about the earliest manifestations of psychiatric dis-
orders through social media data. New insights could prevent the
development of latent conditions, mitigate the impact of emerging
disorders, or as exemplified by Facebook’s new suicide AI, new
opportunities to intervene with life-saving assistance. With the
rising prevalence of mental disorders [67], many researchers see
the benefits of better screening, identification, and intervention
assisting to promote better health and well-being worldwide.
However, the examples of suicide prevention AIs demonstrate
major concerns for algorithmic development and their implica-
tions. This includes new concerns about consent into monitoring
or intervention systems and privacy and data management ques-
tions. Ethics boards do not have standards for managing social
media research, and the prediction of mental health status raises
new questions about consent, vulnerable populations, and online
communities. There are also methodological concerns of data col-
lection and bias, validity of these results for clinical assessment, and
the application of machine learning methods to predicting mental
health status. Furthermore, the lack of consistency with methods
across this research space makes this problem more troubling. For
implications, actors with many motivations can misuse data and
predictions, and amplify the harms of algorithms in reproducing
unfair stereotypes and discrimination of individuals with mental
disorders. Caused in part by the interdisciplinary intersection of
data science, machine learning, psychology, and human-centered
computing, unanswered questions emerge around the role of the in-
dividual in predictions and managing implications of this research.
As these technologies are developed to detect mental health sta-
tus, these concerns will grow unless we rectify these problems. We
stand to gain much from this research – in better understanding
and making interventions in mental health. Addressing these con-
cerns will resolve questions around rigorous science in the area,
benefit clinical research, and safeguard well-being for individuals
and society. Many of these concerns are not limited to just mental
health and social media and apply to other application domains
of these technologies that touch on sensitive issues. In answering
these questions, we offer insight into questions on how to ethically
and rigorously apply machine learning and AI to sensitive domains
such as mental health, and we provide this analysis as a case study
for ethics in applied and fair AI.
This work presents a first taxonomy of issues in algorithmic
prediction of mental health status on social media data. First, we
discuss the gap between ethics committees and participants in such
research, on what can be sensitive and sometimes stigmatizing data.
Second, we identify tensions in methods and analysis, such as con-
struct validity and bias, interpretability of algorithmic output, and
privacy. Finally, we examine implications of this research in bene-
fiting mental health research, challenges faced by key stakeholders,
and the risks of designing interventions.
We contextualize these three areas by drawing from prior work
in this domain, ethics research around these technological advances,
and our experiences conducting this research. In our analysis of
each of our three areas, we look to prior work and standards across
fields: machine learning (ML), natural language processing (NLP),
human computer interaction (HCI), clinical psychiatry, and data
science for guidance. We conclude with calls for interdisciplinary
action to resolve these dilemmas.
