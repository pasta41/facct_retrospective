 
Formulating data science problems is an uncertain and 
difficult process. It requires various forms of discretionary 
work to translate high-level objectives or strategic goals into 
tractable problems, necessitating, among other things, the 
identification of appropriate target variables and proxies. 
While these choices are rarely self-evident, normative 
assessments of data science projects often take them for 
granted, even though different translations can raise 
profoundly different ethical concerns. Whether we consider a 
data science project fair often has as much to do with the 
formulation of the problem as any property of the resulting 
model. Building on six months of ethnographic fieldwork with 
a corporate data science team—and channeling ideas from 
sociology and history of science, critical data studies, and early 
writing on knowledge discovery in databases—we describe the 
complex set of actors and activities involved in problem 
formulation. Our research demonstrates that the specification 
and operationalization of the problem are always negotiated 
and elastic, and rarely worked out with explicit normative 
considerations in mind. In so doing, we show that careful 
accounts of everyday data science work can help us better 
understand how and why data science problems are posed in 
certain ways—and why specific formulations prevail in 
practice, even in the face of what might seem like normatively 
preferable alternatives. We conclude by discussing the 
implications of our findings, arguing that effective normative 
interventions will require attending to the practical work of 
problem formulation. 
CCS CONCEPTS 
• Information systems → Data mining; • Computing 
methodologies → Machine learning; • Human-centered 
computing → Ethnographic studies 
KEYWORDS 
Data Science; Machine Learning; Problem Formulation; 
Fairness; Target Variable 
                                                             
Permission to make digital or hard copies of all or part of this work for personal 
or classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice 
and the full citation on the first page. Copyrights for components of this work 
owned by others than ACM must be honored. Abstracting with credit is 
permitted. To copy otherwise, or republish, to post on servers or to redistribute 
to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions@acm.org. FAT* '19, January 29–31, 2019, Atlanta, GA, USA © 2019 
Association for Computing Machinery. ACM ISBN 978-1-4503-6125-5/19/01 
$15.00 https://doi.org/10.1145/3287560.3287567 
ACM Reference Format: 
Samir Passi and Solon Barocas. 2019. Problem Formulation and 
Fairness. In Proceedings of the ACM Conference on Fairness, 
Accountability, and Transparency (FAT* '19), January 29-31, 
2019. ACM, Atlanta, GA. 10 pages. 
https://doi.org/10.1145/3287560.3287567 
1 INTRODUCTION 
Undertaking a data science1 project involves a series of 
difficult translations. As Provost and Fawcett point out, 
“[b]usiness problems rarely are classification problems, or 
regression problems or clustering problems” [38:293]. They 
must be made into questions that data science can answer. 
Practitioners are frequently charged with turning amorphous 
goals into well-specified problems—that is, problems faithful 
to the original business objectives, but also problems that can 
be addressed by predicting the value of a variable. Often, the 
outcome or quality that practitioners want to predict—the 
‘target variable’—is not one that has been well observed or 
measured in the past. In such cases, practitioners look to other 
variables that can act as suitable stand-ins—‘proxies’. 
This process is challenging and far from linear. As Hand 
argues, “establishing the mapping from the client’s domain to 
a statistical question is one of the most difficult parts of 
statistical analysis,” [22:317] and data scientists frequently 
devise ways of providing answers to problems that differ from 
those that seemed to motivate the analysis. In most normative 
assessments of data science, this work of translation drops out 
entirely, treating the critical task as one of interrogating 
properties of the resulting model. However, ethical concerns 
can extend to the formulation of the problem that a model 
aims to address, not merely to whether the model exhibits 
discriminatory effects. 
To aid in hiring decisions, for example, machine learning 
needs to predict a specific outcome or quality of interest. One 
might want to use machine learning to find “good” employees 
to hire, but the meaning of “good” is not self-evident. Machine 
learning requires specific and explicit definitions, demanding 
that those definitions refer to something measurable. While an 
employer might want to find personable applicants to join its 
1 The term ‘data science,’ as used in this paper, refers to the practice of 
assembling, organizing, processing, modeling, and analyzing data using 
computational and statistical techniques. In this paper, we focus specifically on 
supervised machine learning, but the analysis applies more broadly. 
 
 
 
 
 
PROBLEM FORMULATION AND FAIRNESS 
 
 
sales staff, such a quality can be difficult to specify or measure. 
What counts as personable? And how would employers 
measure it? Given the challenge of answering these questions, 
employers might favor a definition focused on sales figures, 
which they may find easier to monitor. In other words, they 
might define a “good” employee as the person with the highest 
predicted sales figures. In so doing, the problem of hiring is 
formulated as one of predicting applicants’ sales figures, not 
simply identifying “good” employees. 
As Barocas and Selbst [3] demonstrate, choosing among 
competing target variables can affect whether a model used in 
hiring decisions ultimately exhibits a disparate impact. There 
are three reasons why this might happen. First, the target 
variable might be correlated with protected characteristics. In 
other words, an employer might focus on a quality that is 
distributed unevenly across the population. This alone would 
not constitute illegal discrimination, as the quality upon which 
the employer hinges its hiring decisions could be rational and 
defensible. But, the employer could just as well choose a target 
variable that is a purposeful proxy for race, gender, or other 
protected characteristics. This would amount to a form of 
disparate treatment, but one that might be difficult to establish 
if the decision rests on a seemingly neutral target variable. The 
employer could also choose a target variable that seems to 
serve its rational business interests but happens to generate an 
avoidable disparate impact—for instance, the employer could 
choose a different target variable that serves its business 
objective at least as well as the original choice while also 
reducing the disparate impact. 
Second, the chosen target variable might be measured less 
accurately for certain groups. For example, arrests are often 
used as a proxy for crime in applications of machine learning 
to policing and criminal justice, even though arrests are a 
racially biased representation of the true incidence of crime 
[28]. In treating arrests as a reliable proxy for crime, the model 
learns to replicate the biased labels in its predictions. This is a 
particularly pernicious problem because the labeled examples 
in the training data serve as ground truth for the model. 
Specifically, the model will learn to assign labels to cases 
similar to those that received the label in the training data, 
whether or not the labels in the training data are accurate. 
Worse, evaluations of the model will likely rely on test data 
that were labeled using the same process, resulting in 
misleading reports about the model’s real-world performance: 
these metrics would reflect the model’s ability to predict the 
label, not the true outcome. Indeed, when the training and test 
data have been mislabeled in the same way, there is simply no 
way to know when the model is making mistakes. Choosing a 
target variable is therefore often a choice between outcomes 
of interest that are labeled more or less accurately. When these 
outcomes are systematically mismeasured by race, gender, or 
some other protected characteristic, a model designed to 
predict them will invariably exhibit a discriminatory bias that 
does not show up in performance metrics. 
Finally, different target variables might be more difficult to 
predict than others depending on the available training data 
and features. If the ability to predict the target variable varies 
by population, then the model might subject certain groups to 
greater errors than others. 
Across all three cases, we find that whether a model 
ultimately violates a specific notion of fairness is often 
contingent on what the model is designed to predict. Which 
suggests that we should be paying far greater attention to the 
choice of the target variable, both because it can be a source of 
unfairness and a mechanism to avoid unfairness. 
The non-obvious origins of obvious problems. This 
might not be surprising because some problem formulations 
may strike us as obviously unfair. Consider the case of 
‘financial-aid leveraging’ in college admissions—the process 
by which universities calculate the best possible return for 
financial aid packages: the brightest students for the least 
amount of financial aid. To achieve this bargain, the university 
must predict how much each student is willing to pay to attend 
the university and how much of a discount would sway an 
applicant from competitors. In economic terms, ‘financial-aid 
leveraging’ calculates each applicant’s responsiveness to price, 
which enables the university to make tailored offers that 
maximize the likely impact of financial aid on individual 
enrollment decisions. As Quirk [39] explains: “Take a $20,000 
scholarship—the full tuition for a needy student at some 
schools. Break it into four scholarships each for wealthier 
students who would probably go elsewhere without the 
discount but will pay the outstanding tuition if they can be 
lured to your school. Over four years the school will reap an 
extra $240,000, which can be used to buy more rich students—
or gifted students who will improve the school’s profile and 
thus its desirability and revenue.” Such strategies are in effect 
in schools throughout the United States, and the impact has 
been an increase in support for wealthier applicants at the 
expense of their equally qualified, but poorer peers [42,43]. 
One might, therefore, conclude, as Danielson [11:44] does, 
that “data mining technology increasingly structures 
recruiting to many U.S. colleges and universities,” and that the 
technology poses a threat to such important values as equality 
and meritocracy. Alternatively, one could find, like Cook [9] 
in a similar thought experiment, that “[t]he results would have 
been different if the goal were to find the most diverse student 
population that achieved a certain graduation rate after five 
years. In this case, the process was flawed fundamentally and 
ethically from the beginning.” For Cook, agency and ethics are 
front-loaded: a poorly formed question returns undesirable, if 
correct, answers. Data science might be the enabling device, 
but the ethical issue precedes the analysis and 
implementation. The objective was suspect from the start. For 
Danielson, however, certain ethics seem to flow from data 
mining itself. Data science is not merely the enabling device, 
but the impetus for posing certain questions. Its introduction 
affords new, and perhaps objectionable, ways of devising 
admissions strategies. 
Though they are quite different, these positions are not 
necessarily incompatible: data science might invite certain 
kinds of questions, and ‘financial-aid leveraging’ could be one 
such example. One might say that data science promotes the 
formulation of questions that would be better left unasked. 
PROBLEM FORMULATION AND FAIRNESS 
 
 
But, this is a strangely unhelpful synthesis: while according 
agency to the person or people who might formulate the 
problem, it simultaneously imparts overwhelming influence to 
the affordances of data science. The effort of getting the 
question to work as a data science problem drops out entirely, 
even though this process is where the question actually and 
ultimately takes shape. The issues of genuine concern—how 
universities arrive at a workable notion of student quality, 
how they decide on optimizing for competing variables 
(student quality, financial burden, diversity, etc.), how the 
results are put to work in one of many possible ways—are left 
largely out of view. The indeterminacy of the process, where 
many of the ethical issues are actually resolved, disappears. 
Problem formulation in practice. While a focus on the 
work of problem formulation in real-world applied settings 
has the potential to make visible the plethora of actors and 
activities involved in data science work, it has not been the 
focus of much empirical inquiry to date. We still know very 
little about the everyday practice of problem formulation. In 
this paper, we attempt to fill this gap. How and why are 
specific questions posed? What challenges arise and how are 
they resolved in everyday practice? How do actors’ choices 
and decisions shape data science problem formulations? 
Answers to these questions, we argue, can help us to better 
understand data science as a practice, but also the origin of the 
qualities of a data science project that raise normative 
concerns. As researchers work to unpack the normative values 
at stake in the uses of data science, we offer an ethnographic 
account of a special financing project for auto lending to make 
visible the work of problem formulation in applied contexts. 
In so doing, we show how to trace the ethical implications of 
these systems back to the everyday challenges and routine 
negotiations of data science. 
In the following sections, we first situate the paper in a 
longer history attending to the practical dimensions of data 
science, specifically the task of problem formulation. We then 
describe our research site and methodology, before moving to 
the empirical case-study. We conclude by discussing the 
implications of our findings, positioning the practical work of 
problem formulation as an important site for normative 
investigation and intervention. 
