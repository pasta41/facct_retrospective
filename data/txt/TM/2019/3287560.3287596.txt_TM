
Trained machine learning models are increasingly used to perform
high-impact tasks in areas such as law enforcement, medicine, edu-
cation, and employment. In order to clarify the intended use cases
of machine learning models and minimize their usage in contexts
for which they are not well suited, we recommend that released
models be accompanied by documentation detailing their perfor-
mance characteristics. In this paper, we propose a framework that
we call model cards, to encourage such transparent model reporting.
Model cards are short documents accompanying trained machine
learning models that provide benchmarked evaluation in a variety
of conditions, such as across different cultural, demographic, or phe-
notypic groups (e.g., race, geographic location, sex, Fitzpatrick skin
type [15]) and intersectional groups (e.g., age and race, or sex and
Fitzpatrick skin type) that are relevant to the intended application
domains. Model cards also disclose the context in which models
are intended to be used, details of the performance evaluation pro-
cedures, and other relevant information. While we focus primarily
on human-centered machine learning models in the application
fields of computer vision and natural language processing, this
framework can be used to document any trained machine learning
model. To solidify the concept, we provide cards for two super-
vised models: One trained to detect smiling faces in images, and
one trained to detect toxic comments in text. We propose model
cards as a step towards the responsible democratization of machine
learning and related artificial intelligence technology, increasing
transparency into how well artificial intelligence technology works.
We hope this work encourages those releasing trained machine
learning models to accompany model releases with similar detailed
evaluation numbers and other relevant documentation.
CCS CONCEPTS
• General and reference → Evaluation; • Social and profes-
sional topics → User characteristics; • Software and its engi-
neering→Use cases;Documentation; Software evolution; •Human-
centered computing →Walkthrough evaluations;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01.
https://doi.org/10.1145/3287560.3287596
KEYWORDS
datasheets, model cards, documentation, disaggregated evaluation,
fairness evaluation, ML model evaluation, ethical considerations
ACM Reference Format:
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy
Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit
Gebru. 2019. Model Cards for Model Reporting. In FAT* ’19: Conference on
Fairness, Accountability, and Transparency, January 29–31, 2019, Atlanta, GA,
USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3287560.
Currently, there are no standardized documentation procedures to
communicate the performance characteristics of trained machine
learning (ML) and artificial intelligence (AI) models. This lack of
documentation is especially problematic when models are used in
applications that have serious impacts on people’s lives, such as in
health care [14, 42, 44], employment [1, 13, 29], education [23, 45]
and law enforcement [2, 7, 20, 34].
Researchers have discovered systematic biases in commercial ma-
chine learning models used for face detection and tracking [4, 9, 49],
attribute detection [5], criminal justice [10], toxic comment detec-
tion [11], and other applications. However, these systematic errors
were only exposed after models were put into use, and negatively
affected users reported their experiences. For example, after MIT
Media Lab graduate student Joy Buolamwini found that commercial
face recognition systems failed to detect her face [4], she collabo-
rated with other researchers to demonstrate the disproportionate
errors of computer vision systems on historically marginalized
groups in the United States, such as darker-skinned women [5, 41].
In spite of the potential negative effects of such reported biases,
documentation accompanying trained machine learning models (if
supplied) provide very little information regarding model perfor-
mance characteristics, intended use cases, potential pitfalls, or other
information to help users evaluate the suitability of these systems
to their context. This highlights the need to have detailed documen-
tation accompanying trained machine learning models, including
metrics that capture bias, fairness and inclusion considerations.
As a step towards this goal, we propose that released machine
learning models be accompanied by short (one to two page) records
we call model cards. Model cards (for model reporting) are com-
plements to “Datasheets for Datasets” [21] and similar recently
proposed documentation paradigms [3, 28] that report details of
the datasets used to train and test machine learning models. Model
cards are also similar to the tripod statement proposal in medicine
[25]. We provide two example model cards in Section 5: A smiling
detection model trained on the CelebA dataset [36] (Figure 2), and
a public toxicity detection model [32] (Figure 3). Where Datasheets
highlight characteristics of the data feeding into the model, we
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji,
Timnit Gebru
focus on trained model characteristics such as the type of model,
intended use cases, information about attributes for which model
performance may vary, and measures of model performance.
We advocate for measures of model performance that contain
quantitative evaluation results to be broken down by individual
cultural, demographic, or phenotypic groups, domain-relevant con-
ditions, and intersectional analysis combining two (or more) groups
and conditions. In addition to model evaluation results, model
cards should detail the motivation behind chosen performance
metrics, group definitions, and other relevant factors. Each model
card could be accompanied with Datasheets [21], Nutrition Labels
[28], Data Statements [3], or Factsheets [27], describing datasets
that the model was trained and evaluated on. Model cards provide a
way to inform users about what machine learning systems can and
cannot do, the types of errors they make, and additional steps that
could create more fair and inclusive outcomes with the technology.
