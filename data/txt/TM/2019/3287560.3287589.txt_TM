
Computers are increasingly used to make decisions that have sig-
nificant impact on people’s lives. Often, these predictions can affect
different population subgroups disproportionately. As a result, the
issue of fairness has received much recent interest, and a number of
fairness-enhanced classifiers have appeared in the literature. This
paper seeks to study the following questions: how do these different
techniques fundamentally compare to one another, and what ac-
counts for the differences? Specifically, we seek to bring attention to
many under-appreciated aspects of such fairness-enhancing inter-
ventions that require investigation for these algorithms to receive
broad adoption.
We present the results of an open benchmark we have devel-
oped that lets us compare a number of different algorithms under
a variety of fairness measures and existing datasets. We find that
although different algorithms tend to prefer specific formulations
of fairness preservations, many of these measures strongly corre-
late with one another. In addition, we find that fairness-preserving
algorithms tend to be sensitive to fluctuations in dataset composi-
tion (simulated in our benchmark by varying training-test splits)
and to different forms of preprocessing, indicating that fairness
interventions might be more brittle than previously thought.
CCS CONCEPTS
•Computingmethodologies→Machine learning; • Software
and its engineering → Software libraries and repositories;
KEYWORDS
Fairness-aware machine learning, benchmarks
∗This work was partially supported by National Science Foundation under grants
IIS-1633387, IIS-1513651, and IIS-1633724, as well as by a grant from the Ethics and Gov-
ernance of AI Initiative. Source code, including instructions for adding your own algo-
rithm or dataset, can be found at https://github.com/algofairness/fairness-comparison
and installed via pip3 install fairness.
†Work done while the author was a student at the University of Utah.
‡ Work done while the author was a student at Haverford College.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
FAT* ’19, January 2019, Atlanta, Ga
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6125-5/19/01.
https://doi.org/10.1145/3287560.3287589
ACM Reference Format:
Sorelle A. Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam
Choudhary, Evan P. Hamilton, and Derek Roth‡. 2019. A comparative study
of fairness-enhancing interventions in machine learning. In Proceedings of
Conference on Fairness, Accountability, and Transparency (FAT* ’19). ACM,
New York, NY, USA, 10 pages. https://doi.org/10.1145/3287560.3287589
1 INTRODUCTION
As the use of machine learning to make decisions about people has
increased, so has the drive to make fairness-aware machine learning
algorithms. A considerable body of research over the past ten years
has produced algorithms for accurate yet fair decisions, under vary-
ing definitions of fair, for goals such as non-discriminatory hiring,
risk assessment for sentencing guidance, and loan allocation. And
yet we have not yet seen extensive deployment of these algorithms
in the pertinent domains. The primary technical obstacle appears
to be our ability to compare methods effectively across different
evaluation measures and different data sets with consistent data
preprocessing and testing methodologies. Such comparisons would
not just reveal “best-in-class” methods; they would also suggest
which measures are robust and how different algorithms are sensi-
tive to different kinds of preprocessing. As pointed out by Lehr and
Ohm [24], such considerations of the data processing pipeline are
not just important for efficient implementation but also have legal
ramifications for the resulting automated decision-making process.
In this paper, we present a test-bed to facilitate direct compar-
isons of algorithms with respect tomeasures on a variety of datasets.
Our open-source framework allows for the easy addition of new
methods, measures and data for the purpose of evaluation. We show
how to use our test-bed for determining not only which specific
algorithm has the best performance under a fairness or accuracy
measure, but what types of algorithmic interventions1 tend to be
the most effective. In addition to the impact of these algorithmic
choices, we examine the impact of different preprocessing tech-
niques and different measures for accuracy and fairness that have
an important, and previously obscured, impact on the results of
these algorithms. Our goal is to provide a comprehensive compara-
tive analysis of existing approaches that is currently lacking in the
literature.
1In this paper, we use the term ’intervention’ to refer to how the choice of algorithm
used impacts the fairness of the overall system. We are not studying causal definitions
of fairness.
1.1 Our results
Our evaluation yields the following major findings.
Fairness-accuracy tradeoffs depend on preprocessing (Section 5).
Different algorithms tend to have slightly different requirements in
terms of input: how are sensitive attributes encoded? Are multiple
sensitive attributes supported? Does the algorithm directly support
categorical attributes or are attribute transformations required?
Choices for these requirements directly affect the accuracy and
fairness of a fairness-aware classifier. This is significant because
prior formal studies of fairness-accuracy tradeoffs typically focused
on hyperparameter tuning, rather than preprocessing.
Measures of discrimination correlate with each other (Section 6).
Even though there has been a proliferation of measures designed to
highlight discrimination instances by machine learning algorithms,
we find that a large number of these measures tend to strongly cor-
relate with one another. As a result, techniques optimizing for one
measure could perform well for a different measure (and similarly
for poor performance).
Algorithms make significantly different fairness-accuracy trade-
offs (Section 7). The specific mechanisms that different algorithms
employ to increase fairness are quite varied, but surprisingly, the ac-
tual predictions made by these algorithms tend to vary significantly
as well. As a result, no algorithm’s performance (as of the latest
state of our benchmark) appears to dominate, either in accuracy or
fairness measures.
Algorithms are fragile: they are sensitive to variations in the input
(Section 7). We find surprising variability in fairness measures aris-
ing from variations in training-test splits; this appears to not have
been previously mentioned in the literature.
