
Computer vision and other biometrics data science applications
have commenced a new project of pro￿ling people. Rather than
using ’transaction generated information’, these systems measure
the ’real world’ and produce an assessment of the ’world state’ - in
this case an assessment of some individual trait. Instead of using
proxies or scores to evaluate people, they increasingly deploy a
logic of revealing the truth about reality and the people within it.
While these pro￿ling knowledge claims are sometimes tentative,
they increasingly suggest that only through computation can these
excesses of reality be captured and understood. This article explores
the bases of those claims in the systems of measurement, repre-
sentation, and classi￿cation deployed in computer vision. It asks if
there is something new in this type of knowledge claim, sketches
an account of a new form of computational empiricism being op-
erationalised, and questions what kind of human subject is being
constructed by these technological systems and practices. Finally,
the article explores legal mechanisms for contesting the emergence
of computational empiricism as the dominant knowledge platform
for understanding the world and the people within it.
CCS CONCEPTS
• Social and professional topics → Computing / technology
policy; Surveillance; •Computingmethodologies→Computer
vision; Machine learning; • Applied computing → Law;
KEYWORDS
Computer Vision, Data Science, Biometrics, Law and Policy, Com-
putational Empiricism
ACM Reference Format:
Jake Goldenfein. 2019. The Pro￿ling Potential of Computer Vision and
the Challenge of Computational Empiricism. In FAT* ’19: Conference on
Fairness, Accountability, and Transparency (FAT* ’19), January 29–31, 2019,
Atlanta, GA, USA. ACM, New York, NY, USA, Article 4, 10 pages. https:
//doi.org/10.1145/3287560.3287568
∗Dr Jake Goldenfein is a Postdoctoral Research Fellow at the Digital Life Initiative,
Cornell Tech, Cornell University and a Lecturer in Law at Swinburne University of
Technology.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro￿t or commercial advantage and that copies bear this notice and the full citation
on the ￿rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287568
1 INTRODUCTION
“Pro￿ling” means di￿erent things in di￿erent traditions. The prolif-
eration of machine learning and data science in pro￿ling however,
make the European General Data Protection Regulation (GDPR)
de￿nition especially meaningful [19, 24, 58]. Under that regime,
“pro￿ling” includes the automated evaluation of certain traits about
a person [2], reminding us of the legal signi￿cance of “classi￿cation”
even without subsequent discrimination. This de￿nition also ap-
plies to an emerging class of automated image-based pro￿ling tech-
nologies that evaluate “traits” about individuals. ‘Computer vision’
techniques ‘make sense’ of image or video data by measuring, using
machine learning to transform those measurements into “represen-
tations”, and subsequently into decisions and classi￿cations. The
‘online pro￿ling’ discussed over the past several decades, typically
concerned knowledge production from ‘transaction generated in-
formation’ created by interacting with informational environments
[41], framing individuals in terms of proxies and scores [32, 45].
Computer vision (and other biometric) techniques however, use
extremely granular measurements of “the real world” and increas-
ingly deploy a logic of exposing or revealing the truth of reality and
people within it. This article explores the nature of those knowl-
edge claims and what might be the appropriate register of legal
intervention to address data science’s dominance as a paradigm for
knowing people.
Computer vision generates classi￿cations and knowledge about
scenes, objects, events and people. Computer vision is not the only
domain of data science articulating this new epistemological stance.
Similar ideas are evident in numerous data science applications,
often those involving biometrics. Computer vision does however,
provides a useful entry point into what is qualitatively new about
the empirical and epistemological position being developed. When
applied to people, computer vision has clear legal and social sig-
ni￿cance. Facial recognition for instance, one such notorious ap-
plication, links a person’s portrait to some form of institutional
identity like a driver’s licence [15, 53, 73]. By connecting an individ-
ual to their behaviour in physical space, facial recognition couples
identity across physical and computational registers, enabling au-
tomated decisions to be articulated into physical environments.
(Another example is vehicle registration and automated number
plate recognition [12].) Both the intrinsic limitations and problem-
atic applications of these technologies are well documented [49, 62],
with calls for regulation not far behind [77]. However, there is al-
ready much more that computer vision can do when ‘looking at
people’.1 Beyond “identi￿cation”, computer vision can also “classify”
people [41]. As well as linking your image and spatial location to
1The term ‘looking at people’ is an umbrella for research on computer vision applied
to persons, see e.g. [23].
an institutional identity, computer vision can make decisions about
non-visual attributes such as what type of person you are, what
you are doing, what you are feeling, and how you are likely to act
in the future. These decisions may be about identity, gender, emo-
tional state or future behaviours. But more controversial classi￿ers
have also addressed questions of sexuality,wangkosinski criminal
propensity, political position, IQ, workplace suitability, and pae-
dophilic tendencies. That type of ‘computational physiognomy’ is
one element of an emerging ￿eld of ‘personality computation’, of-
ten described as Apparent Personality Analysis (APA) or Apparent
Personality Recognition (APR) [42].
Personality computation uses faces, postures, movements, ac-
tions, gestures, interactions, and emotions (as well as whether those
emotions and expressions are real or fake) to infer personality traits
[42]. Di￿erent analytic techniques operate on di￿erent data inputs.
Some use dynamic information such as what a person does, or the
way a physical morphology changes.