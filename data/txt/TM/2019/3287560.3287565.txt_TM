
A/B/n testing has been adopted by many technology companies as
a data-driven approach to product design and optimization. These
tests are often run on their websites without explicit consent from
users. In this paper, we investigate such online A/B/n tests by using
Optimizely as a lens. First, we provide measurement results of
575 websites that use Optimizely drawn from the Alexa Top-1M,
and analyze the distributions of their audiences and experiments.
Then, we use three case studies to discuss potential ethical pitfalls
of such experiments, including involvement of political content,
price discrimination, and advertising campaigns. We conclude with
a suggestion for greater awareness of ethical concerns inherent
in human experimentation and a call for increased transparency
among A/B/n test operators.
CCS CONCEPTS
· Security andprivacy→Privacy protections; ·Human-centered
computing→User studies; Empirical studies in HCI; · Social and
professional topics →Codes of ethics;
KEYWORDS
online controlled experiments; A/B/n testing; personalization
ACM Reference Format:
Shan Jiang, John Martin, and Christo Wilson. 2019. Who’s the Guinea Pig?,
Investigating Online A/B/n Tests in-the-Wild. In FAT* ’19: Conference on
Fairness, Accountability, and Transparency (FAT* ’19), January 29ś31, 2019,
Atlanta, GA, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.
1145/3287560.3287565
1 INTRODUCTION
It has been almost a century since Sir Ronald Fisher developed the
theory and practice of controlled experiments. Since then, these
techniques have been widely adopted by businesses and marketers
to optimize everything from the layout of assembly lines, to the
messaging and targeting of advertising campaigns.
A/B/n testing (a.k.a. split testing or bucket testing) in particular
has been eagerly adopted by technology companies as a data-driven
approach to product design and optimization. An A/B/n test is a
straightforward between-subjects experimental design where the
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’19, January 29ś31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287565
users of a website are split into n groups, with one serving as the
control and the other n − 1 as treatments. The control group is
shown the product as-is, while the treatments groups are shown
variations. After some time has passed, the users’ behaviors are
analyzed to determine which treatment, if any, had a desirable effect
(e.g., more clicks, higher conversion rates, etc.).
Many large technology companies are open about their advocacy
of Online Controlled Experiments (OCEs) like A/B/n testing. In 2000,
Google famously usedA/B tests to experimentwith the presentation
of results in Google Search, and by 2011 Google engineers claimed to
be running over 7,000 A/B tests per year [21]. Google and other large
companies like Facebook, Microsoft, and LinkedIn have published
papers describing their infrastructure for supporting and scaling
OCEs on their platforms [6, 34, 40, 43, 66, 67, 71, 72].
Although it is known that OCEs are widely used by the biggest
technology companies, critical questions remain unanswered about
their use in practice. First, are OCEs used by websites beyond the
largest platforms, and if so, at what scale (e.g., howmany treatments
and experiments)? Second, what is the substance of the treatments
that users are subject to? This second question is especially perti-
nent, because OCEs are a form of human experimentation. Thus,
we must consider the ethics of these experiments, especially be-
cause they (1) may be conducted without explicit consent,1 and
(2) may cause a variety of harms, depending on the design being
tested. Indeed, companies are not bound by frameworks such as
the Belmont Report [13], and evidence suggests that the executives,
engineers, and marketers who conduct OCEs may not be versed in,
or even aware of, experimental ethics [50, 58].
In this study, we take a first step towards filling this knowledge
gap by using Optimizely [53] as a lens. Optimizely is a service
that enables website operators to build, manage, and analyze OCEs
(such as A/B and multivariate tests) on their websites. It is the most
popular of several services that offer similar functionality, as of mid-
2018 [22]. Crucially for us, Optimizely is implemented using a client-
side JavaScript library: when a user visits a website, the library
dynamically determines what audiences (i.e., treatment groups)
the user is part of, and executes any experiments (i.e., treatments)
associated with those audiences. This design enables us to record
all audiences and experiments that are available on a website
that uses Optimizely.
We crawled 10,584 sites drawn from the Alexa Top-1M that were
previously observed including resources from Optimizely [9]. Of
these sites, 575 were running experiments at the time of our crawls
from January 29 to March 19, 2018. Using this dataset, we examine:
what kinds of websites conducts OCEs, how many audiences do
1We are not aware of any major website that prominently discloses the existence of
Optimizely experiments to users or asks for affirmative consent, although it is possible
that websites may disclose this practice in their terms of use or privacy policy.
experiments do they run? In total, our analysis considers 1,143
audiences and 2,001 experiments.
Measurement Results. We found that the usage of Optimizely
is heavily distributed over top-ranked websites. Most of these web-
sites were conducting ≤ 5 experiments on ≤ 5 audiences, while
a small number of websites were running dozens of experiments
with complicated audience structure (e.g., Optimizely itself, The
New York Times, and AirAsia). We analyze how websites segment
audiences overall, and present detailed results for popular attributes
such as location, device, and browser. Furthermore, we analyze the
experiments captured in our dataset, and find that most were łdum-
miesž (i.e., no variations) or were A/B tests (i.e., a single variation)
that targeted łeveryonež or audiences from a single group.
Case Studies. In addition, we also qualitatively investigate
the substance of a subset of experiments (since Optimizely experi-
ments can make arbitrary changes to web pages, there is no way
to analyze them at scale). We focus on three case studies: political
content, e-commerce, and advertising. Each case study is motivated
by specific ethical concerns. For example, prior work has shown
that changing the partisan valence of online content can influence
voting behavior [28, 29]; if an OCE were to manipulate the valence
of political content (e.g., news headlines), this could increase po-
litical polarization. Similarly, prior work has uncovered numerous
instances of online price discrimination, including A/B testing on
Expedia [35]; OCEs could also be used to manipulate consumer
behavior by altering prices, changing the presentation of products
to emphasize more or less expensive options, or tailor discounts to
specific users.
Our hope is that this study raise awareness of the scope of OCEs
in-the-wild, and fosters a conversation about the ethics of these
experiments. Today, users do not affirmatively consent to the vast
majority of OCEs. Instead, users are unaware, experiments are not
transparent, and experimenters are not accountable to their subjects.
Although our study focuses on Optimizely, we note that they are
just a tool vendor; ultimately it is up to companies using this tool
(and others like it) to grapple with the ethics of experiments and to
obey best practices.
