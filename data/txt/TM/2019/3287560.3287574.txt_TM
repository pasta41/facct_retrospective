
Recent work on interpretability in machine learning and AI has
focused on the building of simplified models that approximate the
true criteria used to make decisions. These models are a useful
pedagogical device for teaching trained professionals how to pre-
dict what decisions will be made by the complex system, and most
importantly how the system might break. However, when consid-
ering any such model it’s important to remember Box’s maxim
that "All models are wrong but some are useful." We focus on the
distinction between these models and explanations in philosophy
and sociology. These models can be understood as a "do it yourself
kit" for explanations, allowing a practitioner to directly answer
"what if questions" or generate contrastive explanations without
external assistance. Although a valuable ability, giving these models
as explanations appears more difficult than necessary, and other
forms of explanation may not have the same trade-offs. We contrast
the different schools of thought on what makes an explanation,
and suggest that machine learning might benefit from viewing the
problem more broadly.
CCS CONCEPTS
• Computing methodologies → Artificial intelligence; Cog-
nitive science;Machine learning; •Human-centered comput-
ing → HCI theory, concepts and models;
KEYWORDS
Interpretability; Explanations; Accountability; Philosophy of Sci-
ence
ACM Reference Format:
Brent Mittelstadt, Chris Russell, and Sandra Wachter. 2019. Explaining
Explanations in AI . In FAT* ’19: Conference on Fairness, Accountability, and
Transparency (FAT* ’19), January 29–31, 2019, Atlanta, GA, USA. ACM, New
York, NY, USA, 10 pages. https://doi.org/10.1145/3287560.3287574
1 INTRODUCTION
As we deploy automated decision-making systems in the real world,
questions of accountability become increasingly important.1 For
system builders questions such as “Is the system working as in-
tended?”, “Do the decisions being made seem sensible?” or “Are we
1This work was supported by The Alan Turing Institute, EPSRC grant EP/N510129/1,
and the British Academy, grant PF170151.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
FAT*’19, January 2019, Atlanta, Georgia USA
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287574
conforming to equality regulation and legislation?” are important,
while a subject of the decision-making algorithm may be more
concerned with topics such as “Am I being treated fairly?” or “What
could I do differently to get a favourable outcome next time?”
These issues are not unique to computerised decision-making
systems, but with the growth of machine learning based systems
they have become even more important (Bodo et al., 2017; Kroll
et al., 2016; Nissenbaum, 1996; Olhede and Wolfe, 2018; Pasquale,
2015; Selbst and Barocas, 2018; Veale and Edwards, 2018). What
distinguishes machine learning is its use of arbitrary black-box
functions to make decisions. These black-box functions may be
extremely complex and have an internal state composed of millions
of interdependent values. As such, the functions used to make
decisions may well be too complex for humans to comprehend; and
it may not be possible to completely understand the full decision-
making criteria or rationale.
Under these constraints, it becomes an open question as to what
forms of explanation are even possible that can answer the earlier
questions. As such, one of the most striking aspects of research into
explainable AI (xAI) is how many different people, be they lawyers,
regulators, machine learning specialists, philosophers, or futurolo-
gists, are all prepared to agree on the importance of explainable AI.
However, very few stop to check what they are agreeing to, and to
find out what explainable AI means to other people involved in the
discussion (Lipton, 2016).
This gap of expectations is largest between machine learning,
which has essentially re-purposed the term “explanation,” and the
fields of law, cognitive science, philosophy and the social sciences
(which we refer to here collectively as the ‘explanation sciences’),
where it has a relatively well-defined technical meaning and a
plethora of research on types of explanations, their purposes, and
their social and cognitive function.