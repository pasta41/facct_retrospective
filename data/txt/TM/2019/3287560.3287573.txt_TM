 
Measures of algorithmic bias can be roughly classified into four 
categories, distinguished by the conditional probabilistic 
dependencies to which they are sensitive. First, measures of 
“procedural bias” diagnose bias when the score returned by an 
algorithm is probabilistically dependent on a sensitive class 
variable (e.g. race or sex). Second, measures of “outcome bias” 
capture probabilistic dependence between class variables and the 
outcome for each subject (e.g. parole granted or loan denied). 
Third, measures of “behavior-relative error bias” capture 
probabilistic dependence between class variables and the 
algorithmic score, conditional on target behaviors (e.g. recidivism 
or loan default). Fourth, measures of “score-relative error bias” 
capture probabilistic dependence between class variables and 
behavior, conditional on score.  Several recent discussions have 
demonstrated a tradeoff between these different measures of 
algorithmic bias, and at least one recent paper has suggested 
conditions under which tradeoffs may be minimized. 
In this paper we use the machinery of causal graphical models to 
show that, under standard assumptions, the underlying causal 
relations among variables forces some tradeoffs. We delineate a 
number of normative considerations that are encoded in different 
measures of bias, with reference to the philosophical literature on 
the wrongfulness of disparate treatment and disparate impact. 
While both kinds of error bias are nominally motivated by 
concern to avoid disparate impact, we argue that consideration of 
causal structures shows that these measures are better understood 
as complicated and unreliable measures of procedural biases (i.e. 
disparate treatment). Moreover, while procedural bias is indicative 
of disparate treatment, we show that the measure of procedural 
bias one ought to adopt is dependent on the account of the 
wrongfulness of disparate treatment one endorses. Finally, given 
that neither score-relative nor behavior-relative measures of error 
bias capture the relevant normative considerations, we suggest 
that error bias proper is best measured by score-based measures of 
accuracy, such as the Brier score. 
CONCEPTS 
• Social and professional topics ~ Computing / technology 
policy   • Theory of computation ~ Design and analysis of 
algorithms 
KEYWORDS 
Algorithmic decision-making, fairness, casual inference, 
discrimination 
ACM Reference format: 
Bruce Glymour and Jonathan Herington. 2019. Measuring the Biases that 
Matter: The Ethical Basis for Measures of Fairness in Algorithms. In 
Proceedings of ACM Conference on Fairness, Accountability and 
Transparency (FAT’19). Atlanta, Georgia, USA 
https://doi.org/10.1145/3287560.3287573 N 
1 INTRODUCTION 
Several recent papers [9,11,15,21,36] have argued that various 
measures of algorithmic bias impose contrary demands on 
algorithms employed to predict behavior.  Here we show how 
those results fall naturally out of simple considerations of 
underlying causal structure.  By attending to the causal structure, 
it is possible to sort measures of bias into four types.  Each type is 
motivated by distinct ethical considerations, and is sensitive to 
different features of the probability distribution over feature 
variables, variables recording target behaviors, and ‘sensitive’ 
variables recording membership in one or another social class.  
Attention to the underlying causal structure makes clear which 
constraints on unbiased distributions can and cannot be jointly 
satisfied, in general and under various relevant special conditions. 
We begin by considering the causal structures that make it 
possible to predict behavior from covariates, turn to a discussion 
of measures and their joint satisfiability, and close with some 
normative considerations. 
 
 
 
Permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned 
by others than ACM must be honored. Abstracting with credit is permitted. To 
copy otherwise, or republish, to post on servers or to redistribute to lists, requires 
prior specific permission and/or a fee. Request permissions from 
Permissions@acm.org. 
FAT* '19, January 29–31, 2019, Atlanta, GA, USA 
© 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6125-
5/19/01…$15.00 
 
 
 
1.1 Causal Structures 
There are a number of discussions of algorithmic bias in terms of 
causation now in the literature [20,22,23]. Here we use the 
machinery of causal modeling to explore the underlying 
structures to which distinct measures of bias are sensitive, classify 
measures with respect to those sensitivities, and note both limits 
to and tradeoffs among measures of various kinds. Algorithms 
predict behavior on the basis of observed values of variables that 
covary with the behavior to be predicted.  That covariation, if not 
due to accident (i.e. sampling error), arises from some underlying 
causal structure.  The machinery of causal graphical modeling 
permits representations of the possible causal structures, and 
allows rigorous inferences from them.  Here we employ the 
modeling conventions elaborated in Spirtes [33] (see also Pearl 
[27]).   
Consider an algorithm that takes as input a vector of model 
variables, and outputs a score representing the (epistemic) 
probability that the subject being evaluated will engage in some 
target behavior. For instance, a credit risk rating algorithm might 
take as input model variables whose values represent a subject’s 
gender, age, employment history, past loans, rental history, etc., 
and generate as output a score that predicts the chance that the 
subject will default on a loan, either quantitatively (e.g. as a 
probability or risk score) or qualitatively (e.g. by assignment to a 
‘high’ or ‘low’ risk group). Such prediction is possible only 
because the model variables are associated with the variable 
encoding the target behavior, and, if the association is not 
accidental, that in turn requires that the model variables be 
causally related to the target behavior.  Since model variables 
take their values before the target behavior occurs, this 
association requires either that: (i) the model variables cause the 
behavioral variable, or (ii) they share a common cause with the 
target behavior.  These two possible structures are represented in 
graphs 1a and1b, respectively, where for ease we employ a single 
model variable M, a single sensitive class variable C, a score 
variable S, a variable recording the (non)occurrence of the target 
behavior, B, and a variable O representing some relevant outcome 
caused by the decision into which the score enters as a 
consideration (e.g. whether a loan is offered).  Arrows in the 
graphs represent causal dependencies between variables, directed 
from the cause to the effect.  The variable U1 represents some 
unmeasured and unknown common cause.  By convention, the 
graphs are understood to be ‘causally complete’, i.e. there are no 
missing common causes of represented variables and all direct 
causal (and definitional) relations between variables are 
represented by arrows in the model. 
Hence, the graphs in Figs. 1a and 1b represent situations in 
which the class variable neither shares a common cause with, nor 
causally influences nor is causally influenced by any other 
variable in the graph.  That assumption is not always warranted, 
but when it holds, the class variable will be statistically associated 
with the score S and the target behavior B only by accident, i.e. as 
a result of non-representative sampling. When it fails, several 
possibilities present themselves. 
 
Figure 1: Causal Structures Permitting Prediction 
Consider, for the moment, the associations between C and S 
that are possible when M causes B directly. One causal structure 
which will generate such an association is that in Fig. 2a. In that 
graph, C causes S directly, as it will when C is itself among the 
feature variables employed by the model.  Alternatively, C might 
cause M, as in Fig. 2b.  A third possibility is that M causes C, as 
in Fig. 2c.  Finally, M might share a common cause with C, as in 
Fig. 2d.  For some sensitive variables, e.g. those encoding race or 
sex, it may be that this third possibility can be ignored, on the 
grounds that the feature in question is necessarily exogenous. But 
some class variables, e.g. disability status, clearly can be 
influenced by model variables.   
 
 
Figure 2: Causal Structures Inducing Associations Between C 
and S 
 
 
 
Turning our attention to an association between C and B, it 
turns out that when the causal structure is as in Fig. 2a, C will not 
be associated with B, while when casual structure is as 
represented in Figs. 2b, 2c or 2d, C will be associated with B.  
But there are two other ways in which C can come to be 
associated with B: it may be that C is a direct cause of B, or that 
they share some common cause (U3), as in Figs. 3a and 3b 
respectively.  
 
 
Figure 3: Causal Structures Inducing an Association between 
Class and Behavior 
For the moment, we will assume that the class variable under 
consideration is exogenous, and hence we will give minimal 
further consideration to the possibilities represented by Fig. 2c, 
2d and 3b. 
Things are somewhat different if M is not a direct cause of B, 
but rather shares a common cause with it.  Of special concern is 
the graph, in Fig. 4a, in which the direct edge between M and B 
in 2d is replaced by a common cause U1.  In that case, C will be 
associated with S unconditionally, but not with B.  We now turn 
to an explanation of the associations we impute on the basis of the 
above graphs. 
 
 
Figure 4: Causal Structures of Special Interest 
Associations in observational data are not accidental only if 
they represent underlying probabilistic dependencies, and those 
probabilistic dependencies themselves hold in virtue of causal 
relations among the variables.  It is possible to read off from a 
causal graph which variables are probabilistically dependent and 
which independent of one another, given a possibly empty set of 
variables on which one is conditioning.  Doing so requires the use 
of the so-called D-separation Theorem, on the assumption that the 
Causal Markov and Faithfulness conditions hold (readers are 
referred to Spirtes [22] for detailed discussion the theorem and 
the axioms from which it follows).  Here we simply recount the 
rules of thumb by which one may read from a graph facts about 
which variables will and will not be probabilistically independent 
from one another [30].  Variables in a path (i.e. a sequence of 
variables connected by arrows) come in four varieties: 
(1) Terminal variables, with only one arrow in or out (C and 
B are terminal variables in the path CMB in Fig. 2b); 
(2) Mediators, which have one arrow in and one arrow out (M 
is a mediator on the path CMB in Fig. 2b); 
(3) Common causes, which have two arrows out (M is a 
common cause on the path CMB in Fig 2c); and  
(4) Colliders, which have two arrows in (S is a collider on the 
path CSMB in Fig 2a). 
Two variables connected by a path are associated provided that 
the path is ‘open’.  A path is open provided each variable in the 
path is ‘on’.  A variable is ‘on’ in a path, relative to a (possibly 
empty) conditioning set {V} of variables in the graph, provided it 
is a terminal variable, a mediator, or a common cause and not in 
the conditioning set, or is a collider and either is in the 
conditioning set or has some effect, direct or distal, which is in 
the conditioning set. If every variable in a path is on, the path is 
open and the two terminal variables are said to be d-connected on 
the conditioning set.  D-connected variables are probabilistically 
dependent, conditional on the variables in the conditioning set, 
and therefore (conditionally) associated in representative data.  
Variables that are not d-connected relative to a conditioning set 
are independent of one another, conditional on the variables in the 
conditioning set. 
Using the d-separation theorem, one can see that in Figs. 1a 
and 1b, C is independent of, and therefore in representative data 
will be statistically unassociated with S, any outcome O 
influenced by S, and the target behavior B.  There is no path, and 
thus no open path, between C and S, O or B.  Similarly, one can 
see that in every graph in Fig. 2, there is an open path between C 
and S: the path is direct, CS, in Fig 2a; in Fig 2b the path 
CMS includes only terminal variables and a mediator, and 
thus open when the conditioning set is empty; in Fig. 2c the path 
CMS includes only terminal variables and a common cause 
and so is also open given an empty conditioning set; and the path 
CUMS includes U as a common cause and M as a 
mediator, and so is, unconditionally, open.  In Figs. 3a and 3b, 
there is a direct path CB is open unconditionally in both 
graphs; consequently C and B will be associated in representative 
data given such causal structures. 
However, in Fig. 4a, there is only one path between C and B, 
CU2MU1B.  M is a collider in that path, and so relative 
to an empty conditioning set, C and B will be unassociated; put 
formally, C and B are probabilistically independent (C_||_B) in 
any probability density faithful to Fig. 4a, and in any 
representative data set they will be statistically unassociated on 
any measure of association. 
 
 
 
