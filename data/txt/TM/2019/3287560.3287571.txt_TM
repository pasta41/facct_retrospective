
Settings such as lending and policing can be modeled by a
centralized agent allocating a scarce resource (e.g. loans or
police officers) amongst several groups, in order to maximize
some objective (e.g. loans given that are repaid, or criminals
that are apprehended). Often in such problems fairness is also
a concern. One natural notion of fairness, based on general
principles of equality of opportunity, asks that conditional on
an individual being a candidate for the resource in question,
the probability of actually receiving it is approximately inde-
pendent of the individual’s group. For example, in lending this
would mean that equally creditworthy individuals in different
racial groups have roughly equal chances of receiving a loan.
In policing it would mean that two individuals committing
the same crime in different districts would have roughly equal
chances of being arrested.
In this paper, we formalize this general notion of fairness
for allocation problems and investigate its algorithmic conse-
quences. Our main technical results include an efficient learn-
ing algorithm that converges to an optimal fair allocation even
when the allocator does not know the frequency of candidates
(i.e. creditworthy individuals or criminals) in each group. This
algorithm operates in a censored feedback model in which
only the number of candidates who received the resource in a
given allocation can be observed, rather than the true number
of candidates in each group. This models the fact that we do
not learn the creditworthiness of individuals we do not give
loans to and do not learn about crimes committed if the police
presence in a district is low.
As an application of our framework and algorithm, we
consider the predictive policing problem, in which the resource
The full technical version of this paper is available at https://arxiv.org/abs/1808.
10549.
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made
or distributed for profit or commercial advantage and that copies bear this
notice and the full citation on the first page. Copyrights for components of this
work owned by others than ACM must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287571
being allocated to each group is the number of police officers
assigned to each district. The learning algorithm is trained on
arrest data gathered from its own deployments on previous
days, resulting in a potential feedback loop that our algorithm
provably overcomes. In this case, the fairness constraint asks
that the probability that an individual who has committed a
crime is arrested should be independent of the district in which
they live. We investigate the performance of our learning
algorithm on the Philadelphia Crime Incidents dataset.
CCS CONCEPTS
• Theory of computation → Online learning algorithms;
Machine learning theory;Online learning theory; •Computing
methodologies → Learning from implicit feedback;
KEYWORDS
algorithmic fairness, resource allocation, censored feedback,
online learning
ACM Reference Format:
Hadi Elzayn, Shahin Jabbari, Christopher Jung, Michael Kearns, Seth
Neel, Aaron Roth, and Zachary Schutzman. 2019. Fair Algorithms for
Learning in Allocation Problems. In FAT* ’19: Conference on Fairness,
Accountability, and Transparency (FAT* ’19), January 29–31, 2019, At-
lanta, GA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/
10.1145/3287560.3287571
1 INTRODUCTION
The bulk of the literature on algorithmic fairness has focused
on classification and regression problems (see e.g. [3, 4, 6–
8, 10, 14, 16, 17, 19, 20, 24–26] for a collection of recent work),
but fairness concerns also arise naturally in many resource
allocation settings. Informally, a resource allocation problem
is one in which there is a limited supply of some resource to
be distributed across multiple groups with differing needs.
Resource allocation problems arise in financial applications
(e.g. allocating loans), disaster response (allocating aid), and
many other domains — but the primary example that we will
focus on in this paper is policing. In the predictive policing
problem, the resource to be distributed is police officers, which
can be dispatched to different districts. Each district has a
different crime distribution, and the goal (absent additional
crimes caught.
recent work (see e.g. [11, 12, 21]) has highlighted the extent to
which algorithmic allocation might exacerbate those concerns.
For example, Lum and Isaac [21] show that if predictive polic-
ing algorithms such as PredPol are trained using past arrest
data to predict future crime, then pernicious feedback loops
can arise, which misestimate the true crime rates in certain
districts, leading to an overallocation of police.
munities that Lum and Isaac [21] showed to be overpoliced
on a relative basis were primarily poor and minority, this is
especially concerning from a fairness perspective. In this work,
we study algorithms that avoid this kind of under-exploration
and incorporate quantitative fairness constraints.
In the predictive policing setting, Ensign et al. [11] implic-
itly consider an allocation to be fair if police are allocated
across districts in direct proportion to the district’s crime rate;
generally extended, this definition asks that units of a resource
are allocated according to the group’s share of the total can-
didates for that resource. In our work, we study a different
notion of allocative fairness that has a similar motivation to
the notion of equality of opportunity proposed by Hardt et al.
[14] in classification settings. Informally speaking, it asks that
the probability that a candidate for a resource be allocated a
resource be independent of his group membership. In the pre-
dictive policing setting, it asks that conditional on committing
a crime, the probability that someone is apprehended should
not depend on the district in which they commit the crime.
To illustrate that our notions of fairness do not depend on
whether individuals would prefer to receive or not receive the
resource, we highlight another setting in which allocative fair-
ness is a natural concern: hiring.
to recruit machine learning programmers by advertising on a
social media platform. Many such platforms offer the ability to
advertise to different demographics of users and charge by the
number of times the advertisement is shown to different users
(i.e., the number of impressions); a fixed advertising budget
can then be viewed as a number of impressions to allocate. De-
pending on how well the platform can identify programmers
within each demographic, the ad may be shown to a higher
or lower number of programmers. In this setting, our notion
of allocative fairness asks that the probability a programmer
is exposed to the hiring ad (and thus, receives the opportu-
nity to apply for a job) does not depend on the programmer’s
demographic, and the allocation problem is to maximize the
number of programmers reached via the choice of impressions
across each demographic, subject to fairness constraints.
criminals, including preventing crimes in the first place, fostering healthy com-
munity relations, and generally promoting public safety. But for concreteness
and simplicity we consider the limited objective of apprehending criminals.
in deployed systems, arrest data (rather than 911 reported crime) is used to train
the models.
and with different research questions in mind.
1.1 Our Results
To define the extent to which an allocation satisfies our fair-
ness constraint, we must model the specific mechanism by
which resources deployed to a particular group reach their
intended targets. We study two such discovery models, and we
view the explicit framing of this modeling step as one of the
contributions of our work; the implications of a fairness con-
straint depend strongly on the details of the discovery model,
and specifying such a model is an important step in making
one’s assumptions transparent.
We study two discovery models, which capture two ex-
tremes of targeting ability. In the random discovery model,
regardless of the number of units allocated to a given group,
all individuals within that group are equally likely to be as-
signed a unit, regardless of whether they are a candidate for
the resource or not. In other words, the probability that a can-
didate receives a resource is equal to the ratio of the number
of units of the resource assigned to his group to the size of his
group (independent of the number of candidates in the group).
At the other extreme, in the precision discovery model, units
of the resource are given only to actual candidates within a
group, as long as there is sufficient supply of the resource. That
is, the probability that a candidate receives a resource is equal
to the ratio of the number of units of the resource assigned to
his group to the number of candidates in his group.
In the policing setting, these models can be viewed as two
extremes of police targeting ability for an intervention like
stop-and-frisk. In the random model, police are viewed as stop-
ping people uniformly at random. In the precision model, po-
lice have the omniscient ability to identify individuals with
contraband, and stop only them. Of course, reality lies some-
where in between.
These discovery models have different implications for fair-
ness. In the random model, fairness constrains resources to be
distributed in amounts proportional to group sizes, regardless
of the distribution of candidates, and so is uninteresting from
a learning perspective. On the other hand, the precision model
yields an interesting fairness-constrained learning problem
when the distribution of the number of candidates in each
group must be learned via observation, and what counts as a
‘fair’ allocation depends greatly on these distributions.
We study learning in a censored feedback setting: each
round, the algorithm can choose a feasible deployment of
resources across groups. Then the number of candidates for
the current round in each group is drawn from a fixed, but
unknown group-dependent distribution (which might be not
be independent from the distributions in other groups). The
algorithm does not observe the number of candidates present
in each group, but only the number of candidates that received
the resource. In the policing setting, this corresponds to the
algorithm being able to observe the number of arrests, but not
the actual number of crimes in each of the districts. Thus, the
extent to which the algorithm can learn about the distribution
in a particular group is limited by the number of resources it
deploys there. The goal of the algorithm is to converge to an
optimal fairness-constrained allocation, where here both the
objective value of the solution and the constraints imposed on
it depend on the unknown distributions.
One trivial solution to the learning problem is to sequen-
tially deploy all of one’s resources to each group in turn for
a sufficient amount of time to accurately learn the candidate
distributions. This would reduce the learning problem to an
offline constrained optimization problem, which we show can
be efficiently solved by a greedy algorithm. But this algorithm
is unreasonable: it has a large exploration phase in which it
uses nonsensical deployments, vastly overallocating to some
groups and underallocating to others. A much more realistic,
natural approach is a greedy-style learning algorithm which at
each round uses its current best-guess estimate for the distribu-
tion in each group and deploys an optimal fairness-constrained
allocation according to these estimates. Unfortunately, as we
show, if one makes no assumptions on the underlying distri-
butions, any algorithm that has a guarantee of converging to
a fair allocation must behave like the trivial one, deploying
vast numbers of resources to each group in turn.
This impossibility result motivates us to consider the learn-
ing problem in which the unknown distributions are from a
known parametric family. The natural greedy algorithm uses
an optimal fair deployment at each round given the maximum
likelihood estimates of candidate distributions given its (cen-
sored) observations so far; for concreteness, we consider the
case of the Poisson distribution, and show that it converges to
an optimal fair allocation, but our analysis generalizes for any
single-parameter Lipschitz-continuous family of distributions.
Finally, we conduct an empirical evaluation of our algorithm
on the Philadelphia Crime Incidents dataset, which records
all crimes reported to the Philadelphia Police Department’s
INCT system between 2006 and 2016. We verify that the crime
distributions in each district are in fact well-approximated
by Poisson distributions, and that our algorithm converges
quickly to an optimal fair allocation (as measured according
to the empirical crime distributions in the dataset). We also
systematically evaluate the Price of Fairness, and plot the Pareto
curves that trade off the number of crimes caught versus the
slack allowed in our fairness constraint, for different sizes of
police force, on this dataset. For the random discovery model,
we prove worst-case bounds on the Price of Fairness.
1.