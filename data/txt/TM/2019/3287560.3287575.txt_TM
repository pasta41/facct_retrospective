
Controversies around race and machine learning have sparked
debate among computer scientists over how to design machine
learning systems that guarantee fairness. These debates rarely en-
gage with how racial identity is embedded in our social experience,
making for sociological and psychological complexity. This com-
plexity challenges the paradigm of considering fairness to be a
formal property of supervised learning with respect to protected
personal attributes. Racial identity is not simply a personal subjec-
tive quality. For people labeled “Black” it is an ascribed political
category that has consequences for social differentiation embedded
in systemic patterns of social inequality achieved through both so-
cial and spatial segregation. In the United States, racial classification
can best be understood as a system of inherently unequal status
categories that places whites as the most privileged category while
signifying the Negro/black category as stigmatized. Social stigma
is reinforced through the unequal distribution of societal rewards
and goods along racial lines that is reinforced by state, corporate,
and civic institutions and practices. This creates a dilemma for so-
ciety and designers: be blind to racial group disparities and thereby
reify racialized social inequality by no longer measuring systemic
inequality, or be conscious of racial categories in a way that itself
reifies race. We propose a third option. By preceding group fairness
interventions with unsupervised learning to dynamically detect
patterns of segregation, machine learning systems can mitigate the
root cause of social disparities, social segregation and stratification,
without further anchoring status categories of disadvantage.
CCS CONCEPTS
• Social and professional topics → Race and ethnicity; Sys-
tems analysis and design; • Applied computing → Sociology;
• Computing methodologies → Dimensionality reduction and
manifold learning;
KEYWORDS
fairness, machine learning, racial classification, segregation
∗Produces the permission block, and copyright information
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287575
ACM Reference Format:
Sebastian Benthall and Bruce D. Haynes. 2019. Racial categories in machine
learning. In FAT* ’19: Conference on Fairness, Accountability, and Trans-
parency (FAT* ’19), January 29–31, 2019, Atlanta, GA, USA. ACM, New York,
NY, USA, 10 pages. https://doi.org/10.1145/3287560.3287575
A growing community of researchers and practitioners now stud-
ies fairness in applications of machine learning in such sensitive
areas as credit reporting, employment, education, criminal justice,
and advertising. This scholarship has been motivated by pragmatic
concerns about machine-learning-produced group biases and com-
pliance with nondiscrimination law, as well as a general concern
about social fairness. While many of the controversies that have
inspired this research have been about discriminatory impact on
particular groups, such as Blacks or women, computer scientists
have tended to treat group fairness abstractly, in terms of generic
protected classes, rather than in terms of specific status groups. This
leads analysts to treat ranked racial and gender status categories
simply as nominal categories of personal identity (a characteristic
of the individual) in computational analysis, rather than under-
standing that male/female or Negro (black)/white are each systems
of hierarchical social statuses.
The typical literature in this field addresses problems in a super-
vised machine learning paradigm, wherein a predictor is trained
on a set of personal data X . One or more features or columns of
the personal data, A, are protected demographic categories. Each
person is labeled with the desired outcome value Y , and a classifier
or predictor Ŷ is trained on the labeled data set. The data is as-
sumed to be accurate. Fairness is then defined as a formal property
of the predictor or prediction algorithm, defined in terms of the
training data. Several different formal definitions of fairness have
been proposed, and their relationships with each other are well
studied. Some proposed definitions of fairness are [31]:
Definition 0.1 (Fairness through unawareness (FTU)). An algo-
rithm is fair so long as protected attributesA are not explicitly used
in the decision-making.
Definition 0.2 (Demographic parity (DP)). A predictor Ŷ satisfies
demographic parity if P (Ŷ |A = 0) = P (Ŷ |A = 1)
Definition 0.3 (Equality of Opportunity (EO)). A predictor Ŷ sat-
isfies equality of opportunity if if P (Ŷ = 1|A = 0,Y = 1) = P (Ŷ =
1|A = 1,Y = 1)
Comparatively little attention is given to how the protected class
labels, A, are assigned, why they are being protected and by whom,
and what that means for the normative presumptions typical of
“fair” machine learning design. This paper addresses these questions
with focus on the particular but also paradigmatic case in which
the protected class is a specific racial category–African-American
(Black). We argue, using this case, that rather than being an ab-
stract, nominal category, race classification is embedded in state
institutions, and reinforced in civil society in ways that are relevant
to the design of machine learning systems. Research demonstrates
that race categories have been socially constructed as unequal cate-
gories in numerous Latin American nations and in the United States.
Race provokes discussions of fairness because racial classification
signifies social, economic, and political inequities anchored in state,
and civic institutional practices.
Racial categories are also unstable social constructions as a brief
history of race since late nineteenth-century America will reveal.
We will show how race categories have been subject to constant po-
litical contestation in meaning and as a consequence racial identity
itself is in fact not stable. Race is ascribed onto individual bodies at
different times and places in society based onmany variables includ-
ing specific ocular-corporeal characteristics, social class, perceived
ancestral origins, and state policy [35, 45].
As a consequence of the social and historical facts about racial
classification, many machine learning applications that perform
statistical profiling, and especially those that use racial statistics,
are both technically and politically problematic. Because race is
not an inherent property of a person but a ‘social fact’ about their
political place and social location in society, racial statistics do not
reflect a stable ‘ground truth’. Moreover, racial statistics by their
very nature mark a status inequality, a way of sorting people’s life
chances, and so are by necessity correlated with social outcomes.
There is nothing fair about racial categories. Scholars of “fairness in
machine learning” using racial categories should be reflexive about
this paradox.
The social facts about race present a dilemma for system design-
ers. Systems that learn from broad population data sets without
considering racial categories will reflect the systemic racial inequal-
ity of society. Through their effects on resource allocation, these
systems will reify these categories by disparately impacting racially
identified groups. On the other hand, systems that explicitly take
racial classification into account must rely on individual identi-
fication and/or social ascription of racial categories that are by
definition unequal. Even when these classifications are used in a
“fair” way, they reify the categories themselves.
We present a third option as a potential solution to this dilemma.
The history of racial formation shows that the social fact of racial
categorization is reinforced through policies and practices of segre-
gation and stratification in housing, education, employment, and
civic life. Racial categories are ascribed onto individual bodies; those
bodies are then sorted socially and in space; the segregated bodies
are then subject to disparate opportunities and outcomes; these
unequal social groups then become the empirical basis for racial
categorization. It is this vicious cycle that is the mechanism of sys-
temic inequality. Rather than considering “fairness” to be a formal
property of a specific machine-learnt system, we propose that sys-
tems can be designed with the objective of combating this cycle
directly and without reference to racial category. Systems designed
with the objective of integration of different kinds of bodies can
discover segregated groups in an unsupervised way before using
fairness modifiers.
Section 1 outlines two controversies about race and machine
learning that have motivated research in this area. We present
these cases so that in subsequent sections we can refer to them to
illustrate our theoretical claims.
Section 2 traces the history of race in the United States from
its roots in institutional slavery and scientific racism through to
changing demographic patterns today. This history reveals how
race has always primarily been a system for stigmatization which
has only recently become the site of ongoing political contest. The
racial categories continue to reproduce inequality.
Section 3 discusses how racial categories get ascribed onto in-
dividual bodies through identification and classification. Seeing
ascription as an event, we identify several different causes for racial
identity, including phenotype, class, and ancestral origin.
Section 4 outlines the implications of the history and sociology
of race for system design. We provide a heuristic for analyzing the
software and data of machine learning systems for racial impact by
categorizing them as either colorblind, as explicit racial projects,
or as facilitators of users engaging in racial projects (which may
be racist, anti-racist, or neither). We argue that designers have a
dilemma: using racial statistics reifies race, perpetuating categories
that are intrinsically unfair. Not using them risks the systematic
failures of ‘color-blind’ analysis, unwittingly reinforcing racial hege-
mony.
Section 5 offers a third alternative: to design systems to be sensi-
tive to segregation in society across dimensions of phenotype, class,
and ancestral origin detected through unsupervised learning, We
sketch techniques for empirically identifying race-like dimensions
of segregation in both spatial distributions and social networks.
These dimensions can then be used to group individuals for fair-
ness interventions in machine learning.
1 RACE AND DATA: CONTROVERSIES AND
CONTEXT
In this section, we summarize two emblematic controversies involv-
ing race and machine learning and some of the ensuing scholarly
debate.
1.1 Recidivism prediction
One areawhere racial bias and automated decision-making has been
widely studied is criminal sentencing. The COMPAS recidivism
prediction algorithm, developed by Northpointe, was determined
to have higher false positive rates for black defendants than for
white defendants [33], and charged with being racially biased, even
though explicitly racial information was not used by the predictive
algorithm [1]. This analysis has been contested on a variety of scien-
tific grounds [20], and themethodological controversy has launched
a more general interest in fairness in statistical classification. Stud-
ies about the statistics of fair classification have discovered that
there is a necessary trade-off between classifier accuracy and group
based false positive and negative rates under realistic distributions
[12, 29]. In light of the difficulties of interpreting and applying anti-
discrimination law to these cases [4], a wide variety of statistical
and algorithmic solutions to the tension between predictive perfor-
mance and fairness have been proposed [11, 23]. Reconsidering the
problem as one of causal inference and the predicted outcomes of
intervention [28, 32], especially in light of the purposes to which
prediction and intervention are intended [3] is a promising path
forward.
The COMPAS algorithm did not explicitly use racial informa-
tion as an input. It used other forms of personal information that
were correlated with race. The fact that the results of an algorithm
that did not take race explicitly into account were correlated with
racial classifications is an illustration of the general fact that group-
based disparate impact cannot be prevented by ignoring the group
memberships statistic; fairness must be accomplished ’through
awareness’ of the sensitive variable [17]. Computer scientists have
responded by identifying methods for detecting the statistical prox-
ies for a sensitive attribute within a machine learnt model, and
removing the effects of those proxies from the results [14].
In this paper, we argue for a different understanding of the role
of racial categorization in the analysis of algorithms. We argue that
because of the socially constructed nature of race, racial categories
are not simple properties of individual persons, but rather are com-
plex results of social processes that are rarely captured within the
paradigm of machine learning. For example, in the analysis of the
algorithm that determined alleged racial bias, the race of defendants
was collected not from the prediction software, but rather from the
Broward County Sheriff’s Office: “To determine race, we used the
race classifications used by the Broward County Sheriff’s Office,
which identifies defendants as black, white, Hispanic, Asian and
Native American. In 343 cases, the race was marked as Other.” [33]
A focus on the potential biases of the recidivism prediction algo-
rithm has largely ignored the question of how the Broward County
Sheriff’s Office developed its racial statistics about defendants. We
argue that rather than taking racial statistics like these at face value,
the process that generates them and the process through which
they are interpreted should be analyzed with the same rigor and
skepticism as the recidivism prediction algorithm. Thematically,
we argue that racial bias is far more likely to come from human
judgments in data generation and interpretation than from an algo-
rithmic model, and that this has broad implications for fairness in
machine learning.
1.