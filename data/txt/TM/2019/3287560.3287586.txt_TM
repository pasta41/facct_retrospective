
Developing classification algorithms that are fair with respect to
sensitive attributes of the data is an important problem due to the
increased deployment of classification algorithms in societal con-
texts. Several recent works have focused on studying classification
with respect to specific fairness metrics, modeled the corresponding
fair classification problem as constrained optimization problems,
and developed tailored algorithms to solve them. Despite this, there
still remain important metrics for which there are no fair classi-
fiers with theoretical guarantees; primarily because the resulting
optimization problem is non-convex. The main contribution of this
paper is a meta-algorithm for classification that can take as input a
general class of fairness constraints with respect to multiple non-
disjoint and multi-valued sensitive attributes, and which comes
with provable guarantees. In particular, our algorithm can handle
non-convex “linear fractional” constraints (which includes fairness
constraints such as predictive parity) for which no prior algorithm
was known. Key to our results is an algorithm for a family of clas-
sification problems with convex constraints along with a reduction
from classification problems with linear fractional constraints to
this family. Empirically, we observe that our algorithm is fast, can
achieve near-perfect fairness with respect to various fairness met-
rics, and the loss in accuracy due to the imposed fairness constraints
is often small.
CCS CONCEPTS
• Computing methodologies→ Supervised learning by clas-
sification;
KEYWORDS
Classification, Algorithmic Fairness
ACM Reference Format:
L. Elisa Celis
†
, LingxiaoHuang
‡
, Vijay Keswani
‡
andNisheeth K. Vishnoi
†
, †
Yale University ‡ EPFL. 2019.Classificationwith Fair-
ness Constraints:, A Meta-Algorithm with Provable Guarantees. In
FAT* ’19: Conference on Fairness, Accountability, and Transparency
(FAT* ’19), January 29–31, 2019, Atlanta, GA, USA. ACM, New York,
NY, USA, 10 pages. https://doi.org/10.1145/3287560.3287586
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287586
1 INTRODUCTION
Classification algorithms are increasingly being used in many so-
cietal contexts such as criminal recidivism [51], predictive polic-
ing [35], and job screening [47]. There are growing concerns that
these algorithms may introduce significant bias with respect to cer-
tain sensitive attributes, e.g., against African-Americans while pre-
dicting future criminals [5, 7, 26], granting loans [17] or NYPD stop-
and-frisk [30], and against women while recommending jobs [16].
The US Executive Office [52] also voiced concerns about discrimina-
tion in automated decision making, including health care delivery
and education. Further, introducing bias may be illegal due to anti-
discrimination laws [2, 6, 45], and can create social imbalance [1, 56].
Thus, developing classification algorithms that are fair with respect
to sensitive attributes has become an important problem.
In classification, one is given a data vector and the goal is to de-
cide whether it satisfies a certain property. The algorithm is allowed
to learn from a set of labeled data vectors that may be assumed to
come from an unknown distribution. The accuracy of a classifier is
measured as the probability that the classifier correctly predicts the
label of a data vector drawn from the same distribution. Each data
vector, however, may also have a small number of multi-valued sen-
sitive attributes such as race, gender, and political opinion, and each
setting of a sensitive attribute gives rise to potentially non-disjoint
groups of data points. Since fairness could mean different things in
different contexts, a number of different metrics have been used to
determine how fair a classifier is with respect to a sensitive group
when compared to another, e.g., statistical parity [21], equalized
odds [34], and predictive parity [20]. In fact, there are currently at
least 21 well-accepted fairness metrics and counting; see [50].
Several recent works use the sensitive attributes and the desired
notion of group fairness to place constraints on the classifier –
formulating it as a constrained optimization problem that max-
imizes accuracy – and develop tailored algorithms to find such
classifiers, e.g., constrained to statistical parity [29, 46, 60] or equal-
ized odds [34, 46, 59]. However, these algorithms do not always
come with provable guarantee, because often the resulting opti-
mization problem turns out to be non-convex; e.g., for statistical
parity [42, 60] and equalized odds [59]. Further, it is open whether
such approaches would work for other important measures of dis-
parate mistreatment such as predictive parity. Predictive parity,
that measures whether the fractions over the class distribution
for the predicted labels are close between different group that are
important in predicting criminal recidivism [20, 26], stopping-and-
frisking pedestrians [30], and predicting heart condition [54]. [59]
left as an open problem to find algorithms to solve the fair classifi-
cation problem with false discovery or false omission parity; two
types of predictive parity.
Our contributions. We present a new classification algorithm
that takes as input any one of a large class of fairness metrics which
can be phrased as “linear-fractional constraints”, and produces an
(approximately) fair solution. Technically, we achieve this by
• identifying a family of classification problems with linear con-
straints (see Section 2),
• developing an algorithm to solve this constrained classification
problem (see Section 4.2), and
• reducing classification with linear-fractional constraints to solv-
ing a small number of linear classification problems above for
carefully chosen parameters (see Section 4.3).
Our approach is very flexible – it allows us to provide classifiers
that are fair with respect to a host of fairness metrics corresponding
to both of linear and non-linear constraints (see Table 1); exam-
ples include several prevalent fairness metrics. In particular, we
obtain classifiers with predictive parity-type constraints for which
there was no previous result with provable guarantees. Additionally,
our algorithmic framework can handle multiple fairness metrics
simultaneously, and the metrics can be defined with respect to
complex sensitive attributes (e.g., multiple attributes, non-disjoint
attributes, and/or multi-valued attributes). Further, we conduct
an empirical evaluation of our algorithm on the Adult, German
credit and COMPAS datasets and compare it against state-of-the
art approaches in fair classification (see Section 5). The results
show that our algorithm can often achieve higher fairness than
prior work, and that the loss in accuracy due to imposing fairness
constraints is often small. Thus, we provide a meta-algorithm for
fair classification, which makes it flexible and easy to use in a vari-
ety of applications, is approximately optimal for whichever fairness
metric is selected, and performs well in practice.
