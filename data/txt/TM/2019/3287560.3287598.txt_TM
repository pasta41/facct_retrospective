
Akey goal of the fair-ML community is to developmachine-learning
based systems that, once introduced into a social context, can
achieve social and legal outcomes such as fairness, justice, and
due process. Bedrock concepts in computer science—such as ab-
straction and modular design—are used to define notions of fairness
and discrimination, to produce fairness-aware learning algorithms,
and to intervene at different stages of a decision-making pipeline
to produce "fair" outcomes. In this paper, however, we contend
that these concepts render technical interventions ineffective, in-
accurate, and sometimes dangerously misguided when they enter
the societal context that surrounds decision-making systems. We
outline this mismatch with five "traps" that fair-ML work can fall
into even as it attempts to be more context-aware in comparison to
traditional data science. We draw on studies of sociotechnical sys-
tems in Science and Technology Studies to explain why such traps
occur and how to avoid them. Finally, we suggest ways in which
technical designers can mitigate the traps through a refocusing of
design in terms of process rather than solutions, and by drawing
abstraction boundaries to include social actors rather than purely
technical ones.
CCS CONCEPTS
• Applied computing→ Law, social and behavioral sciences;
• Computing methodologies → Machine learning;
KEYWORDS
Fairness-aware Machine Learning, Sociotechnical Systems, Inter-
disciplinary
ACM Reference Format:
Andrew D. Selbst, danah boyd, Sorelle A. Friedler, Suresh Venkatasubrama-
nian, and Janet Vertesi. 2019. Fairness and Abstraction in Sociotechnical
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287598
Systems. In FAT* ’19: Conference on Fairness, Accountability, and Trans-
parency (FAT* ’19), January 29–31, 2019, Atlanta, GA, USA. ACM, New York,
NY, USA, 10 pages. https://doi.org/10.1145/3287560.3287598
1 INTRODUCTION
On the typical first day of an introductory computer science course,
the notion of abstraction is explained. Students learn that systems
can be described as black boxes, defined precisely by their inputs,
outputs, and the relationship between them. Desirable properties
of a system can then be described in terms of inputs and outputs
alone: the internals of the system and the provenance of the inputs
and outputs have been abstracted away.
Machine learning systems are designed and built to achieve
specific goals and performance metrics (e.g., AUC, precision, recall).
Thus far, the field of fairness-aware machine learning (fair-ML) has
been focused on trying to engineer fairer and more just machine
learning algorithms andmodels by using fairness itself as a property
of the (black box) system.Many papers have beenwritten proposing
definitions of fairness, and then based on those, generating best
approximations or fairness guarantees based on hard constraints
or fairness metrics [24, 32, 39, 40, 72]. Almost all of these papers
bound the system of interest narrowly. They consider the machine
learning model, the inputs, and the outputs, and abstract away any
context that surrounds this system.
We contend that by abstracting away the social context in which
these systemswill be deployed, fair-ML researchersmiss the broader
context, including information necessary to create fairer outcomes,
or even to understand fairness as a concept. Ultimately, this is be-
cause while performance metrics are properties of systems in total,
technical systems are subsystems. Fairness and justice are prop-
erties of social and legal systems like employment and criminal
justice, not properties of the technical tools within. To treat fairness
and justice as terms that have meaningful application to technology
separate from a social context is therefore to make a category error,
or as we posit here, an abstraction error.
In this paper, we identify five failure modes of this abstraction
error. We call these the Framing Trap, Portability Trap, Formalism
Trap, Ripple Effect Trap, and Solutionism Trap. Each of these traps
arises from failing to consider how social context is interlaced with
technology in different forms, and thus the remedies also require a
deeper understanding of "the social" to resolve problems [1]. After
explaining each of these traps and their consequences, we draw on
the concept of sociotechnical systems to ground our observations in
theory and to provide a path forward.
The field of Science and Technology Studies (STS) describes
systems that consist of a combination of technical and social com-
ponents as "sociotechnical systems." Both humans and machines
are necessary in order to make any technology work as intended.
By understanding fair-ML systems as sociotechnical systems and
drawing on analytical approaches from STS, we can begin to resolve
the pitfalls that we identify. Concretely, we analyze the five traps
using methodology from STS, and present mitigation strategies
that can help avoid the traps, or at the very least help designers un-
derstand the limitations of the technology we produce. Key to this
resolution is shifting away from a solutions-oriented approach to a
process-oriented one—one that draws the boundary of abstraction
to include social actors, institutions, and interactions.
This paper should not be read as a critique of individual con-
tributions within the field of fair-ML, most of which are excellent
on their own terms. Rather, as scholars who have been invested
in doing this work, we take aim at fair-ML’s general methodology,
limited as it is by the bounds of our primarily technical worldview.
We echo themes of STS scholars such as Lucy Suchman [66], Harry
Collins [17], Phil Agre [5], and Diana Forsythe [35]—who in the
1990s critiqued the last wave of artificial intelligence research along
similar lines, highlighting that the culture of and modes of knowl-
edge production in computer science thwarted the social goals that
the field was trying to achieve. An emerging wave of similar cri-
tiques has been directed at the field of fair-ML [9, 29, 36]. We hope
to offer a constructive reform in our nascent field by highlight-
ing how technical work can be reoriented away from solutions to
process and identifying ways in which technical practitioners and
researchers can meaningfully engage social contexts in order to
more effectively achieve the goals of fair-ML work.
