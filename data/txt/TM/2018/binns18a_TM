
What does it mean for a machine learning
model to be ‘fair’, in terms which can be
operationalised? Should fairness consist of
ensuring everyone has an equal probabil-
ity of obtaining some benefit, or should we
aim instead to minimise the harms to the
least advantaged? Can the relevant ideal
be determined by reference to some alter-
native state of affairs in which a particu-
lar social pattern of discrimination does not
exist? Various definitions proposed in re-
cent literature make different assumptions
about what terms like discrimination and
fairness mean and how they can be defined
in mathematical terms. Questions of dis-
crimination, egalitarianism and justice are
of significant interest to moral and polit-
ical philosophers, who have expended sig-
nificant efforts in formalising and defending
these central concepts. It is therefore un-
surprising that attempts to formalise ‘fair-
ness’ in machine learning contain echoes of
these old philosophical debates. This paper
draws on existing work in moral and politi-
cal philosophy in order to elucidate emerg-
ing debates about fair machine learning.
Keywords: fairness, discrimination,
machine learning, algorithmic decision-
making, egalitarianism
1 INTRODUCTION
Machine learning allows us to predict and clas-
sify phenomena, by training models using labeled
data from the real world. When consequential de-
cisions are made about individuals on the basis of
the outputs of such models, concerns about dis-
crimination and fairness inevitably arise. What
if the model’s outputs result in decisions that are
systematically biased against people with certain
protected characteristics like race, gender or reli-
gion? If there are underlying patterns of discrim-
ination in the real world, such biases will likely be
picked up in the learning process. This could re-
sult in certain groups being unfairly denied loans,
insurance, or employment opportunities. Cog-
nisant of this problem, a burgeoning research
paradigm of ‘discrimination-aware data mining’
and ‘fair machine learning’ has emerged, which
attempts to detect and mitigate unfairness Ha-
jian and Domingo-Ferrer (2013); Kamiran et al.
(2013); Barocas and Selbst (2016).
One question which immediately arises in such
an endeavour is the need for formalisation. What
does it mean for a machine learning model to
be ‘fair’ or ‘non-discriminatory’, in terms which
can be operationalised? Various measures have
been proposed. A common theme is compar-
ing differences in treatment between protected
and non-protected groups, but there are multiple
ways to measure such differences. The most basic
would be ‘disparate impact’ or ‘statistical / de-
mographic parity’, which consider the overall per-
centage of positive/ negative classification rates
between groups. However, this is blunt, since it
fails to account for discrimination which is ex-
plainable in terms of legitimate grounds Dwork
et al. (2012). For instance, attempting to enforce
equal impact between men and women in recidi-
vism prediction systems, if men have higher re-
offending rates, could result in women remaining
in prison longer despite being less likely to re-
offend.
A range of more nuanced measures have been
proposed, including; ‘accuracy equity’, which
considers the overall accuracy of a predictive
model for each group Angwin et al. (2016); ‘con-
ditional accuracy equity’, which considers the ac-
curacy of a predictive model for each group, con-
ditional on their predicted class Dieterich et al.
(2016); ‘equality of opportunity’, which considers
c© 2018 R. Binns.
Fairness in Machine Learning: Lessons from Political Philosophy
whether each group is equally likely to be pre-
dicted a desirable outcome given the actual base
rates for that group Hardt et al. (2016); and ‘dis-
parate mistreatment’, a corollary which considers
differences in false positive rates between groups
Zafar et al. (2017). Another definition involves
imagining counterfactual scenarios wherein mem-
bers of protected groups are instead members of
the non-protected group Kusner et al. (2017). To
the extent that outcomes would differ, the system
is unfair; i.e. a woman classified by a fair system
should get the same classification she would have
done in a counterfactual scenario in which she
had been born a man.
Ideally, a system might be expected to meet all
of these intuitively plausible measures of fairness.
But, somewhat problematically, certain measures
turn out to be mathematically impossible to sat-
isfy simultaneously except in rare and contrived
circumstances Kleinberg et al. (2016), and there-
fore hard choices between fairness metrics must
be made before the technical work of detecting
and mitigating unfairness can proceed. A fur-
ther underlying tension is that fairness may also
imply that similar people should be treated sim-
ilarly, but this is often in tension with the ideal
of parity between groups, when base rates for
the target variable are different between those
groups Dwork et al. (2012). Fair machine learn-
ing therefore faces an upfront set of conceptual
ethical challenges; which measures of fairness are
most appropriate in a given context? Which vari-
ables are legitimate grounds for differential treat-
ment, and why? Are all instances of disparity
between groups objectionable? Should fairness
consist of ensuring everyone has an equal prob-
ability of obtaining some benefit, or should we
aim instead to minimise the harms to the least
advantaged? In making such tradeoffs, should
the decision-maker consider only the harms and
benefits imposed within the decision-making con-
text, or also those faced by decision-subjects in
other contexts? What relevance might past, fu-
ture or inter-generational injustices have?
Such questions of discrimination and fairness
have long been of significant interest to moral
and political philosophers, who have expended
significant efforts in formalising, differentiating
and debating many of the central concepts in-
volved. It is therefore unsurprising that attempts
to formalise fairness in machine learning contain
echoes of these old philosophical debates. Indeed,
some seminal work in the FAT-ML community
explicitly refers to political philosophy as inspi-
ration, albeit in a limited and somewhat ad-hoc
way.1 A more comprehensive overview could pro-
vide a wealth of argumentation which may use-
fully apply to the questions arising in the pur-
suit of more ethical algorithmic decision-making.
This article aims to provide an overview of some
of the relevant philosophical literature on dis-
crimination, fairness and egalitarianism in order
to clarify and situate the emerging debate within
the discrimination-aware and fair machine learn-
ing literature. Throughout, I aim to address the
conceptual distinctions drawn between terms fre-
quently used in the fair ML literature–including
‘discrimination’ and ‘fairness’–and the use of re-
lated terms in the philosophical literature. The
purpose of this is not merely to consider similar-
ities and differences between the two discourses,
but to map the terrain of the philosophical de-
bate and locate those points which provide help-
ful clarification for future research on algorith-
mic fairness, or otherwise raise relevant problems
which have yet to be considered in that research.
I begin by discussing philosophical accounts
of what discrimination is and what makes it
wrong, if and when it is wrong. I show how on
certain accounts of what makes discrimination
wrong, the proposed conditions are unlikely to
obtain in algorithmic decision-making contexts.
If correct, these accounts do not necessarily im-
ply that algorithmic decision-making is always
morally benign–only that its potential wrongness
is not to be found in the notion of discrimination
as it is traditionally understood. This leads us
to consider other grounds on which algorithmic
decision-making might be problematic, which are
primarily captured by a variety of considerations
connected to the ideals of egalitarianism–the no-
tion that human beings are in some fundamen-
tal sense equal and that efforts should be made
to avoid and correct certain forms of inequality.
This discussion suggests that ‘fairness’ as used
in the fair machine learning community is best
understood as a placeholder term for a variety
of normative egalitarian considerations. Notably,
1. Notable examples include references to the work of
authors such as H. Peyton Young, John Rawles, and
John Roemer in Dwork et al. (2012); Joseph et al.
(2016).
while egalitarianism is a widely held principle, ex-
actly what it requires is the subject of much de-
bate. I provide an overview of some of this debate
and finish with implications for the incorporation
of ‘fairness’ into algorithmic decision-making sys-
tems.
