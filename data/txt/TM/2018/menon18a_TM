
Binary classifiers are often required to possess
fairness in the sense of not overly discriminating
with respect to a feature deemed sensitive, e.g.
race. We study the inherent tradeoffs in learn-
ing classifiers with a fairness constraint in the
form of two questions: what is the best accu-
racy we can expect for a given level of fairness?,
and what is the nature of these optimal fairness-
aware classifiers? To answer these questions, we
provide threemain contributions. First, we relate
two existing fairness measures to cost-sensitive
risks. Second, we show that for such cost-
sensitive fairness measures, the optimal clas-
sifier is an instance-dependent thresholding of
the class-probability function. Third, we relate
the tradeoff between accuracy and fairness to
the alignment between the target and sensitive
features’ class-probabilities. A practical impli-
cation of our analysis is a simple approach to the
fairness-aware problem which involves suitably
thresholding class-probability estimates.
1 INTRODUCTION
Suppose we wish to learn a classifier to determine if
an applicant will repay a loan. That is, given vari-
ous input features about the applicant – such as their
employment status, income, and credit history – we
wish to predict the target feature, namely likelihood
of repaying the loan. Suppose however that one of
the input features is deemed sensitive, e.g. their race.
Then, we might be required to constrain the classifier
to not be overly discriminativewith respect to this sen-
sitive feature; subject to this constraint, we would of
course like our classifier to be as accurate at predict-
ing the target feature as possible. This fairness-aware
learning problem has received considerable attention
in the machine learning community of late (Pedreshi
et al., 2008; Kamiran and Calders, 2009; Calders and
Verwer, 2010; Dwork et al., 2012; Kamishima et al.,
2012; Fukuchi et al., 2013; Zafar et al., 2016; Hardt
et al., 2016; Zafar et al., 2017).
Existing work on fairness-aware learning has
largely focussed on two central questions: (a) how
does one formally measure the fairness of a classi-
fier?, and (b) given such a measure, how does one
learn a classifier that achieves fairness? For the for-
mer, the challenge is that seemingly sensible defini-
tions of fairness can have subtle, undesirable conse-
quences (Žliobaitė et al., 2011; Dwork et al., 2012);
to address this, a range of progressively refined mea-
sures have been designed (Calders and Verwer, 2010;
Dwork et al., 2012; Hardt et al., 2016; Zafar et al.,
2017). For the latter, the challenge is that merely
ignoring the sensitive feature is inadmissible, owing
to it potentially being predictable by other features
(Pedreshi et al., 2008); to address this, approaches
based on post-hoc correction (Calders and Verwer,
2010; Hardt et al., 2016), regularisation (Kamishima
et al., 2012), and surrogate loss minimisation have
been proposed (Zafar et al., 2016, 2017).
1.1. The limits of the possible in fairness-aware
learning
Despite the impressive advances detailed above, some
basic theoretical aspects of the fairness-aware prob-
lem have received less attention. For example, before
attempting to design an algorithm targetting a partic-
ular measure of fairness, it is natural to ask:
Q1: What is the best we can do? There is typ-
ically an unavoidable tradeoff between how
accurate our classifier is with respect to the
target feature, and how fair it is with respect to
the sensitive feature. One may seek to quan-
tify this tradeoff in terms of properties of the
data source, giving inherent limits of what is
possible for any possible algorithm.
© 2018 A.K. Menon & R.C. Williamson.
The Cost of Fairness in Binary Classification
Q2: How do we achieve the best? Having deter-
mined the inherent accuracy-fairness tradeoff,
one may seek to find what classifiers achieve
this limit. This is not purely of theoretical im-
port, as we may then seek to design methods
to approximate these optimal classifiers.
In machine learning terminology, Q2 concerns the
Bayes-optimal classifiers for the fairness-aware learn-
ing problem. The Bayes-optimal classifier attains the
lowest possible average error for a given problem:
thus, no algorithm, no matter how clever or sophisti-
cated, can attain lower average error than this classi-
fier. Such classifiers are foundational in the study of
standard binary classification (Devroye et al., 1996),
and provide a “limit of the possible” in a manner simi-
lar to what Shannon’s information theory did for prac-
tical problems of telecommunication (Gleick, 2011),
or what the science of thermodynamics did for heat
engines in the 19th century (Bryant, 1973).
1.2. A Mathematics of Morality?
We should strive for a kind of moral geometry
with all the rigor which this name connotes.
(Rawls, 1971, pg. 121)
In this paper, we present three contributions in the
study of Q1 and Q2. First, we show that two pop-
ular fairness measures can be seen as instances of a
more general scheme. We then show that for this gen-
eral scheme of fairness measures, the Bayes-optimal
classifier can be explicitly computed. Intuitively, this
classifier deems an instance to be positive if the prob-
ability of the target feature being active is sufficiently
higher than the probability of the sensitive feature be-
ing active. Finally, we use the explicit form of this
optimal classifier to provide an analytical expression
for the fundamental tradeoff curve of accuracy versus
fairness. This curve is shown to depend on measure
of similarity between the target and sensitive features.
Our approach in answering Q1 and Q2 is math-
ematical in nature, but does not purport to provide
a complete answer to the probems of fairness! It
is motivated by statements such as that due to John
Rawls quoted above. Such an approach provides the
ability to make precise statements at a considerable
level of generality, and is comparable to the formal
analysis of problems (including fairness) in welfare
economics (Harsanyi, 1955; Sen, 2009). In the con-
text of this literature, our work follows the precept
of Sen (2009, Chapter 18) that mere identification of
“fully just social arrangements is neither necessary
nor sufficient.” We embrace Sen’s pragmatism by fo-
cussing on the quantifiable tradeoffs one might make
to approach (certain notions of) fairness. However,
we do not claim to derive the “right” tradeoffs, neither
in the choice of loss functions to be used, nor even
their relative weights in contrast to Rawls (1971, pg
37ff) who not only ackowledges there will be tradeoffs
between overall social utility and fairness but tries to
argue what the “right” tradeoff is. See Appendix H
for further relations to the philosophical and economic
literature on fairness.
Formally, our main contributions C1—C3 are:
C1: we reduce two popular fairnessmeasures (dis-
parate impact and mean difference) to cost-
sensitive risks (Lemmas 1, 2).
C2: we show that for cost-sensitive fairness mea-
sures, the optimal fairness-aware classifier
is an instance-dependent thresholding of the
class-probability function (Propositions 4, 6).
C3: we quantify the intrinsic, method-
independent impact of the fairness re-
quirement on accuracy via a notion of
alignment between the target and sensitive
feature (Proposition 8).
Our results deal with the theoretical limits of what is
possible for any fairness-aware learning method (for
the class of fairness measures we consider), given ac-
cess to the theoretical population distribution. This
leaves some important questions unanswered, such as
how one can construct a classifier that is optimal for a
given dataset (rather than the theoretical population).
While we do not provide a complete answer to this
matter, we do provide a practical means of approxi-
mating the Bayes-optimal classifier. Specifically, the
form of the optimal fairness-aware classifiers (C2) lets
us derive a practically usable algorithm, wherein we
separately estimate class-probabilities for the target
and sensitive features, e.g. by logistic regression, and
combine them suitably (§5.2).
