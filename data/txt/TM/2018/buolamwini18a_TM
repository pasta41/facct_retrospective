
Recent studies demonstrate that machine
learning algorithms can discriminate based
on classes like race and gender. In this
work, we present an approach to evaluate
bias present in automated facial analysis al-
gorithms and datasets with respect to phe-
notypic subgroups. Using the dermatolo-
gist approved Fitzpatrick Skin Type clas-
sification system, we characterize the gen-
der and skin type distribution of two facial
analysis benchmarks, IJB-A and Adience.
We find that these datasets are overwhelm-
ingly composed of lighter-skinned subjects
(79.6% for IJB-A and 86.2% for Adience)
and introduce a new facial analysis dataset
which is balanced by gender and skin type.
We evaluate 3 commercial gender clas-
sification systems using our dataset and
show that darker-skinned females are the
most misclassified group (with error rates
of up to 34.7%). The maximum error rate
for lighter-skinned males is 0.8%. The
substantial disparities in the accuracy of
classifying darker females, lighter females,
darker males, and lighter males in gender
classification systems require urgent atten-
tion if commercial companies are to build
genuinely fair, transparent and accountable
facial analysis algorithms.
Keywords: Computer Vision, Algorith-
mic Audit, Gender Classification
1 INTRODUCTION
Artificial Intelligence (AI) is rapidly infiltrating
every aspect of society. From helping determine
∗ Download our gender and skin type balanced PPB
dataset at gendershades.org
who is hired, fired, granted a loan, or how long
an individual spends in prison, decisions that
have traditionally been performed by humans are
rapidly made by algorithms (O’Neil, 2017; Citron
and Pasquale, 2014). Even AI-based technologies
that are not specifically trained to perform high-
stakes tasks (such as determining how long some-
one spends in prison) can be used in a pipeline
that performs such tasks. For example, while
face recognition software by itself should not be
trained to determine the fate of an individual in
the criminal justice system, it is very likely that
such software is used to identify suspects. Thus,
an error in the output of a face recognition algo-
rithm used as input for other tasks can have se-
rious consequences. For example, someone could
be wrongfully accused of a crime based on erro-
neous but confident misidentification of the per-
petrator from security video footage analysis.
Many AI systems, e.g. face recognition tools,
rely on machine learning algorithms that are
trained with labeled data. It has recently
been shown that algorithms trained with biased
data have resulted in algorithmic discrimination
(Bolukbasi et al., 2016; Caliskan et al., 2017).
Bolukbasi et al. even showed that the popular
word embedding space, Word2Vec, encodes soci-
etal gender biases. The authors used Word2Vec
to train an analogy generator that fills in miss-
ing words in analogies. The analogy man is to
computer programmer as woman is to “X” was
completed with “homemaker”, conforming to the
stereotype that programming is associated with
men and homemaking with women. The biases
in Word2Vec are thus likely to be propagated
throughout any system that uses this embedding.
c© 2018 J. Buolamwini & T. Gebru.
Gender Shades
Although many works have studied how to
create fairer algorithms, and benchmarked dis-
crimination in various contexts (Kilbertus et al.,
2017; Hardt et al., 2016b,a), only a handful of
works have done this analysis for computer vi-
sion. However, computer vision systems with
inferior performance across demographics can
have serious implications. Esteva et al. showed
that simple convolutional neural networks can be
trained to detect melanoma from images, with ac-
curacies as high as experts (Esteva et al., 2017).
However, without a dataset that has labels for
various skin characteristics such as color, thick-
ness, and the amount of hair, one cannot measure
the accuracy of such automated skin cancer de-
tection systems for individuals with different skin
types. Similar to the well documented detrimen-
tal effects of biased clinical trials (Popejoy and
Fullerton, 2016; Melloni et al., 2010), biased sam-
ples in AI for health care can result in treatments
that do not work well for many segments of the
population.
In other contexts, a demographic group that
is underrepresented in benchmark datasets can
nonetheless be subjected to frequent targeting.
The use of automated face recognition by law
enforcement provides such an example. At least
117 million Americans are included in law en-
forcement face recognition networks. A year-
long research investigation across 100 police de-
partments revealed that African-American indi-
viduals are more likely to be stopped by law
enforcement and be subjected to face recogni-
tion searches than individuals of other ethnici-
ties (Garvie et al., 2016). False positives and un-
warranted searches pose a threat to civil liberties.
Some face recognition systems have been shown
to misidentify people of color, women, and young
people at high rates (Klare et al., 2012). Moni-
toring phenotypic and demographic accuracy of
these systems as well as their use is necessary to
protect citizens’ rights and keep vendors and law
enforcement accountable to the public.
We take a step in this direction by making two
contributions. First, our work advances gender
classification benchmarking by introducing a new
face dataset composed of 1270 unique individu-
als that is more phenotypically balanced on the
basis of skin type than existing benchmarks. To
our knowledge this is the first gender classifica-
tion benchmark labeled by the Fitzpatrick (TB,
1988) six-point skin type scale, allowing us to
benchmark the performance of gender classifica-
tion algorithms by skin type. Second, this work
introduces the first intersectional demographic
and phenotypic evaluation of face-based gender
classification accuracy. Instead of evaluating ac-
curacy by gender or skin type alone, accuracy
is also examined on 4 intersectional subgroups:
darker females, darker males, lighter females, and
lighter males. The 3 evaluated commercial gen-
der classifiers have the lowest accuracy on darker
females. Since computer vision technology is be-
ing utilized in high-stakes sectors such as health-
care and law enforcement, more work needs to
be done in benchmarking vision algorithms for
various demographic and phenotypic groups.
