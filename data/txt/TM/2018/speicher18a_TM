
Recently, online targeted advertising plat-
forms like Facebook have been criticized for
allowing advertisers to discriminate against
users belonging to sensitive groups, i.e., to
exclude users belonging to a certain race
or gender from receiving their ads. Such
criticisms have led, for instance, Facebook
to disallow the use of attributes such as
ethnic affinity from being used by adver-
tisers when targeting ads related to hous-
ing or employment or financial services. In
this paper, we show that such measures are
far from sufficient and that the problem
of discrimination in targeted advertising is
much more pernicious. We argue that dis-
crimination measures should be based on
the targeted population and not on the at-
tributes used for targeting. We system-
atically investigate the different targeting
methods offered by Facebook for their abil-
ity to enable discriminatory advertising.
We show that a malicious advertiser can
create highly discriminatory ads without
using sensitive attributes. Our findings call
for exploring fundamentally new methods
for mitigating discrimination in online tar-
geted advertising.
Keywords: Discrimination, advertising,
Facebook
1 INTRODUCTION
Much recent work has focused on detecting in-
stances of discrimination in online services rang-
ing from discriminatory pricing on e-commerce
and travel sites like Staples (Mikians et al., 2012)
and Hotels.com (Hannák et al., 2014) to discrimi-
natory prioritization of service requests and offer-
ings from certain users over others in crowdsourc-
ing and social networking sites like TaskRab-
bit (Hannák et al., 2017). In this paper, we focus
on the potential for discrimination in online ad-
vertising, which underpins much of the Internet’s
economy. Specifically, we focus on targeted ad-
vertising, where ads are shown only to a subset
of users that have attributes (features) selected
c© 2018 T. Speicher et al.
Potential for Discrimination in Online Targeted Advertising
by the advertiser. Targeted ads stand in con-
trast to non-targeted ads, such as banner ads on
websites, that are shown to all users of the sites,
independent of their attributes.
The targeted advertising ecosystem comprises
of (i) advertisers, who decide which users an ad
should (not) be shown to; (ii) ad platforms, such
as Google and Facebook, that aggregate data
about their users and make it available to ad-
vertisers for targeting; and (iii) users of ad plat-
forms that are consumers of the ads. The po-
tential for discrimination in targeted advertising
arises from the ability of an advertiser to use
the extensive personal (demographic, behavioral,
and interests) data that ad platforms gather
about their users to target their ads. An inten-
tionally malicious—or unintentionally ignorant—
advertiser could leverage such data to preferen-
tially target (i.e., include or exclude from tar-
geting) users belonging to certain sensitive social
groups (e.g., minority race, religion, or sexual ori-
entation).
Recently, the Facebook ad platform was the
target of intense media scrutiny (Angwin and
Parris Jr., 2016) and a civil rights lawsuit for
allowing advertisers to target ads with an at-
tribute named “ethnic affinity.” After clarifying
that a user’s “ethnic affinity” does not represent
the user’s ethnicity, but rather represents how
interested the user is in content related to dif-
ferent ethnic communities, Facebook agreed to
not allow ads related to housing, employment,
and financial services be targeted using the at-
tribute (Facebook, 2017) and renamed it to “mul-
ticultural affinity.”1
In this paper, we conduct a systematic study
of the potential for discriminatory advertising on
the Facebook advertisement platform. We focus
on Facebook because it is one of the largest online
advertising platforms in terms of number of users
reached by ads, the number of advertisers, and
the amount of personal data gathered about the
users that is made available to advertisers. Fur-
thermore, Facebook is an innovator in introduc-
ing new methods for targeting users, such as cus-
1. Unfortunately, Facebook was found half a year later
to still accept discriminatory ads, despite the fixes it
claims were put in place (Angwin et al., 2017a).
tom audience2 and look-alike audience3 target-
ing that are then subsequently adopted by other
online social media and social networking plat-
forms like Twitter,4 Pinterest,5 LinkedIn,6 and
YouTube.7 Thus, many of our findings may also
be applicable to these other online ad targeting
platforms as well.
Our study here is driven by the following high-
level question: What are all the different ways
in which a Facebook advertiser, out of malice or
ignorance, can target users in a discriminatory
manner (i.e., include or exclude users based on
their sensitive attributes like race)?
To answer this question, we begin by proposing
an intuitive measure to quantify discrimination
in targeted ads. We then systematically investi-
gate three different targeting methods (attribute-
based targeting, PII-based targeting, and look-
alike audience targeting) offered by Facebook for
their ability to enable discriminatory advertising.
At a high-level, we find that all three methods en-
able advertisers to run highly discriminatory ads.
Worse, we show that the existing solution ap-
proaches of banning the use of certain attributes
like “ethnic affinity” in targeting is not only in-
adequate, but does not even apply in two out of
the three ad targeting methods.
While our findings primarily serve to demon-
strate the perniciousness of the problem of dis-
criminatory advertising in today’s ad platforms,
it also lays the foundations for solving (i.e., de-
tecting and mitigating) ad discrimination.
