
Active learning has long been a topic of
study in machine learning. However, as
increasingly complex and opaque models
have become standard practice, the process
of active learning, too, has become more
opaque. There has been little investigation
into interpreting what specific trends and
patterns an active learning strategy may be
exploring. This work expands on the Local
Interpretable Model-agnostic Explanations
framework (LIME) to provide explanations
for active learning recommendations. We
demonstrate how LIME can be used to
generate locally faithful explanations for
an active learning strategy, and how these
explanations can be used to understand
how different models and datasets explore
a problem space over time.
In order to quantify the per-subgroup
differences in how an active learning strat-
egy queries spatial regions, we introduce a
notion of uncertainty bias (based on dis-
parate impact) to measure the discrepancy
in the confidence for a model’s predictions
between one subgroup and another. Using
the uncertainty bias measure, we show that
our query explanations accurately reflect
the subgroup focus of the active learning
queries, allowing for an interpretable expla-
nation of what is being learned as points
with similar sources of uncertainty have
their uncertainty bias resolved. We demon-
∗ This research was funded in part by the NSF un-
der grants IIS-1633387, DMR-1709351, and DMR-
1307801 and by the Arnold and Mabel Beckman
Foundation.
† https://github.com/rlphilli/
InterpretableActiveLearning
‡ Work done while the author was a student at Haver-
ford College.
strate that this technique can be applied
to track uncertainty bias over user-defined
clusters or automatically generated clusters
based on the source of uncertainty.
Keywords: Interpretability; Active learn-
ing.
1 INTRODUCTION
The importance of interpretability and explain-
ability of machine-learned decisions has recently
been an area of active interest, with the EU
even declaring what has been called a “right to
an explanation” Goodman and Flaxman (2016).
In traditional machine learning contexts, the fo-
cus of interpretability has been two-fold, first on
the receiver of the decision (“why was I rejected
for this job?”) and second on the model creator
(“why is my model giving these answers?”).
Here, we extend this interest in interpretability
to active learning, a domain in which the expla-
nation is additionally of interest to the labeler
(“why am I being asked these questions and why
is it worth it to answer?”). Since active learning
is generally applied in scenarios such as drug dis-
covery where it is expensive (whether in terms of
time or money) to label a query, the labeler in
these contexts is often a domain expert in their
own right (e.g., a chemist). Given this, a query
explanation can serve as a way to both justify an
expensive request and allow the domain expert
to give feedback to the model.
1.1. Results
We demonstrate how active learning choices can
be made more interpretable to non-experts. Us-
ing per-query explanations of uncertainty, we de-
c© 2018 R.L. Phillips, K.H. Chang & S.A. Friedler.
Interpretable Active Learning
velop a system that allows experts to choose
whether to label a query. This allows experts
to incorporate domain knowledge and their own
interests into the labeling process. In addition,
we introduce a quantified notion of uncertainty
bias, the idea that an algorithm may be less cer-
tain about its decisions on some data clusters
than others. In the context of decision-making
about people, this may mean that some protected
groups (e.g., race or gender) may receive less fa-
vorable decisions due to risk aversion Goodman
and Flaxman (2016). In the context of active
learning, this means that these groups are more
likely to be targeted for exploratory queries in or-
der to improve the model. We combine this idea
with the explanations generated per query to de-
scribe the groups most targeted by uncertainty
bias. More broadly, these techniques allow us to
make active learning interpretable to expert la-
belers, so that queries and query batches can be
explained and the uncertainty bias can be tracked
via interpretable clusters.
