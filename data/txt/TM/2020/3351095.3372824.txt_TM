
In various business settings, there is an interest in using more com-
plex machine learning techniques for sales forecasting. It is difficult
to convince analysts, along with their superiors, to adopt these
techniques since the models are considered to be “black boxes,”
even if they perform better than current models in use. We examine
the impact of contrastive explanations about large errors on users’
attitudes towards a “black-box” model. We propose an algorithm,
Monte Carlo Bounds for Reasonable Predictions. Given a large er-
ror, MC-BRP determines (1) feature values that would result in a
reasonable prediction, and (2) general trends between each feature
and the target, both based on Monte Carlo simulations. We evaluate
on a real dataset with real users by conducting a user study with
75 participants to determine if explanations generated by MC-BRP
help users understand why a prediction results in a large error, and
if this promotes trust in an automatically-learned model. Our study
shows that users are able to answer objective questions about the
model’s predictions with overall 81.1% accuracy when provided
with these contrastive explanations. We show that users who saw
MC-BRP explanations understand why the model makes large er-
rors in predictions significantly more than users in the control
group. We also conduct an in-depth analysis of the difference in
attitudes between Practitioners and Researchers, and confirm that
our results hold when conditioning on the users’ background.
CCS CONCEPTS
• Computing methodologies → Artificial intelligence; Machine
learning; Supervised learning by regression; Ensemble methods.
KEYWORDS
Explainability, Interpretability, Erroneous predictions
ACM Reference Format:
Ana Lucic, Hinda Haned, and Maarten de Rijke. 2020. Why Does My Model
Fail? Contrastive Local Explanations for Retail Forecasting. In Conference on
Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020,
Barcelona, Spain. ACM, New York, NY, USA, 9 pages. https://doi.org/
10.1145/3351095.3372824
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372824
1 INTRODUCTION
As more and more decisions about humans are made by machines,
it becomes imperative to understand how these outputs are pro-
duced and what drives a model to a particular prediction [19]. As a
result, algorithmic interpretability has gained significant interest
and traction in the machine learning (ML) community over the past
few years [3]. However, there exists considerable skepticism outside
of the ML community due to a perceived lack of transparency be-
hind algorithmic predictions, especially when errors are produced
[2]. We aim to evaluate the effect of explaining model outputs,
specifically large errors, on users’ attitudes towards trusting and
deploying complex, automatically learned models.
Further motivation for interpretable ML is provided by signif-
icant societal developments. Important examples include the re-
cently enacted EuropeanGeneral Data Protection Regulation (GDPR),
which specifies that individuals will have the right to “the logic
involved in any automatic personal data processing” [4]. In Canada
and the United States, this right to an explanation is an integral
part of financial regulations, which is why banks have not been
able to use high-performing “black-box” models to evaluate the
credit-worthiness of their customers. Instead, they have been con-
fined to easily interpretable algorithms such as decision trees (for
segmenting populations) and logistic regression (for building risk
scorecards) [12]. At NeurIPS 2017, an Explainable ML Challenge
was launched to combat this limitation, indicating the finance in-
dustry’s interest in exploring algorithmic explanations [5].
We use explanations as a mechanism for supporting innova-
tion and technological development while keeping the human “in
the loop” by focusing on predictive modeling as a tool that aids
individuals with a given task. Specifically, our interest lies with
interpretability in a scenario where users with varying degrees of
ML expertise are confronted with large errors in the outcome of
predictive models. We focus on explaining large errors because
people tend to be more curious about unexpected outcomes rather
than ones that confirm their prior beliefs [10].
However, Dietvorst et al. [2] showed that when users are con-
fronted with errors in algorithmic predictions, they are less likely
to use the model. Seeing an algorithm make mistakes significantly
decreases confidence in the model, and users are more likely to
choose a human forecaster instead, even after seeing the algorithm
outperform the human [2]. This indicates that prediction mistakes
have a significant impact on users’ perception of the model. By
focusing on explaining mistakes, we hope to give insight into this
phenomenon of algorithm aversion while also giving users the
types of explanations they are interested in seeing.
Our work was motivated by the needs of analysts at Ahold Del-
haize, a large Dutch retailer, working on sales forecasting. Current
models in production are based on simple autoregressive methods,
but there is an interest in exploring more complex techniques. How-
ever, the added complexity comes at the expense of interpretability,
which is problematic for Ahold Delhaize, especially when a com-
plex model produces a forecast that is very different from the actual
target value. This leads us to focus on explaining errors in regres-
sion predictions in this work. However, it should be noted that our
method can be extended to classification predictions by defining
“distances” between classes or by simply defining all errors as large
errors.
We focus on two aspects of explainability in this scenario: the
generation of explanations of large errors and the corresponding
effectiveness of these explanations. Prior methods for generating
explanations fail at generating explanations for large errors because
they produce similar explanations for predictions resulting in large
errors and those resulting in reasonable predictions (see Table 2
in Section 4 for an example). We propose a method for explaining
large prediction errors, called Monte Carlo Bounds for Reasonable
Predictions (MC-BRP), that shows users:
(1) The required bounds of the most important features in order
to have a prediction resulting in a reasonable prediction.
(2) The relationship between each of these features and the
target.
It should be noted that in our work, we focus on explaining er-
rors in hindsight, that is, we examine large errors once they have
occurred and are not predicting them in advance without having
access to the ground truth. We are also not using these explana-
tions to improve the model, but rather examine the effectiveness
of explaining large errors via MC-BRP on users’ trust in the model
and attitudes towards deploying it, as well as their understanding
of the explanations. We test on a wide range of users, including
both Practitioners and Researchers, and analyze the differences in
attitudes between these users. We also reflect on the process of
conducting a user study by outlining limitations of our study and
make recommendations for future work.
We address the following research questions:
RQ1: Are the contrastive explanations generated by MC-BRP about
large errors in predictions (i) interpretable, or (ii) actionable? More
specifically,
(i) Can contrastive explanations about large errors give users
enough information to simulate the model’s output (forward
simulation)?
(ii) Can such explanations help users understand the model such
that they can manipulate an observation’s input values in
order to change the output (counterfactual simulation)?
RQ2: How does providing contrastive explanations generated by MC-
BRP for large errors impact users’ perception of themodel? Specifically,
we investigate the following:
(i) Does being provided with contrastive explanations gener-
ated by MC-BRP impact users’ understanding of why the
model produces errors?
(ii) Does it impact their willingness to deploy the model?
(iii) Does it impact their level of trust in the model?
(iv) Does it impact their confidence in the model’s performance?
Consequently, we make the following contributions:
• We contribute a method, MC-BRP, for generating contrastive
explanations specifically for large errors in regression tasks.
• We evaluate our explanations through a user study with 75
participants in both objective and subjective terms.
• We conduct an analysis on the differences in attitudes be-
tween Practitioners and Researchers.
In Section 2 we discuss related work and identify how our problem
relates to the current literature. In Section 3 we formally describe
the methodology of explanations based onMC-BRP and in Section 4
we motivate our choice of dataset and describe the user study setup.
In Section 5 we detail the results of the user study; we conduct
further analyses in Section 6. In Section 7 we conclude and make
recommendations for future work.
