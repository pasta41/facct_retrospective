
To design and develop AI-based systems that users and the larger
public can justifiably trust, one needs to understand how machine
learning technologies impact trust. To guide the design and im-
plementation of trusted AI-based systems, this paper provides a
systematic approach to relate considerations about trust from the so-
cial sciences to trustworthiness technologies proposed for AI-based
services and products. We start from the ABI+ (Ability, Benevolence,
Integrity, Predictability) framework augmented with a recently pro-
posed mapping of ABI+ on qualities of technologies that support
trust. We consider four categories of trustworthiness technologies
for machine learning, namely these for Fairness, Explainability, Au-
ditability and Safety (FEAS) and discuss if and how these support
the required qualities. Moreover, trust can be impacted throughout
the life cycle of AI-based systems, and we therefore introduce the
concept of Chain of Trust to discuss trustworthiness technologies
in all stages of the life cycle. In so doing we establish the ways in
which machine learning technologies support trusted AI-based sys-
tems. Finally, FEAS has obvious relations with known frameworks
and therefore we relate FEAS to a variety of international ‘princi-
pled AI’ policy and technology frameworks that have emerged in
recent years.
CCS CONCEPTS
• Applied computing→ Sociology; • Social and professional
topics→ Computing / technology policy; • Security and pri-
vacy→ Human and societal aspects of security and privacy; • Com-
putingmethodologies→Artificial intelligence;Machine learn-
ing.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372834
KEYWORDS
trust, trustworthiness, machine learning, artificial intelligence
ACM Reference Format:
Ehsan Toreini, Mhairi Aitken, Kovila Coopamootoo, Karen Elliott, Carlos
Gonzalez Zelaya, and Aad van Moorsel. 2020. The relationship between
trust in AI and trustworthy machine learning technologies. In Conference on
Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3351095.3372834
1 INTRODUCTION
Growing interest in ethical dimensions of AI and machine learn-
ing has led to the focus on ways of ensuring trustworthiness of
current and future practices (e.g. European Commission 2019, IBM
n.d.). The current emphasis on this area reflects recognition that
maintaining trust in AI may be critical for ensuring acceptance
and successful adoption of AI-driven services and products [67, 81].
This has implications for the many AI-based services and products
that are increasingly entering the market. How trust is established,
maintained or eroded depends on a number of factors including
an individual’s or group’s interaction with others, data, environ-
ments, services, products and factors, which combine to shape an
individual’s perception of trustworthiness or otherwise. Percep-
tions of trustworthiness impact on AI and consequently, influence
a person’s decision and behaviour associated with the service or
product. In this paper, we research the connection between trust
and machine learning technologies in a systematic manner. The
aim is to identify how technologies impact and relate to trust, and,
specifically, identify trust-enabling machine learning technologies.
AI and machine learning approaches are trustworthy if they have
properties that one is justified to place trust in them (see [7] for this
manner of phrasing).
It is important to highlight the difference between studying trust
in AI and studying ethics of AI (and data science). Trustworthy AI
is related to normative statements on the qualities of the technol-
ogy and typically necessitates ethical approaches, while trust is a
response to the technologies developed or the processes through
which they were developed (and may not necessarily - or entirely -
depend on ethical considerations). Ethical considerations behind
the design or deployment of an AI-based product or service can
impact perceptions of trust, for instance if trust depends on having
confidence in the service not discriminating against the trusting
entity (or in general). However, there may be cases where ethics is
not a consideration for the trusting entity when placing trust in a
service, or, more frequently, if ethics is one of the many concerns
the trusting entity has in mind. In what follows, we will also see
that trust-enhancing machine learning technologies can be related
to various ‘Principled AI’ frameworks, such as Asilomar AI Prin-
ciples introduced in 2017, Montréal Declaration for Responsible
Development of Artificial Intelligence in 2018 and IEEE Ethically
Aligned Design Document.
The aim of this paper is to identify the ways in which (classes of)
machine learning technologiesmight enhance or impact trust, based
on trust frameworks drawn from ethics, social sciences and comput-
ing and algorithm design literature on technological trust qualities.
Figure 1 outlines our approach. At the centre of Figure 1 is the
end product of this paper, trust-enhancing technologies in machine
learning and their classification in technologies for Fairness, Ex-
plainability, Auditability and Safety (FEAS). The downwards arrows
indicate that these technologies are derived from trust frameworks
from social science literature (particularly organisational science).
The upward arrow indicates that the FEAS-classification of tech-
nologies was informed by the various Principled AI frameworks
that shape the ethics and policy discussion in many nations (this is
discussed in Section 4.2).
As indicated in Figure 1, we base our discussion on the widely
accepted ABI (Ability, Benevolence, Integrity) principles underlying
trust, as introduced by Mayer et al.[67] and extended to include
Predictability by Dietz and Den Hartog [29] (a.k.a ABI+). We add
to this a temporal dimension, from initial trust to continuous trust,
as discussed by Siau et al.[59]. This gives us a base to understand
trust in general, and we augment this further by integrating Siau‘s
perspective on trust in technology, which identifies that trust is
impacted by Human, Environmental and Technological qualities
(referred to as the technologies’ HET qualities in what follows). We
will discuss these steps to go from the ABI+ model to HET qualities
of trustworthy technologies in Section 2.
To summarise, the contributions of this paper are as follows:
• We draw on social science literature, particularly from or-
ganisational science, to apply established principles of trust
to examine the qualities for technologies to support trust
in AI-based systems (primarily based on the ABI and ABI+
framework and the HET qualities).
• We identify how trust can be enhanced in the various stages
of an AI-based system’s life-cycle, specifically the design,
development and deployment stages. We therefore introduce
the concept of an AI Chain of Trust to discuss the various
stages and their interrelations.
• We introduce a FEAS (Fairness, Explainability, Auditability,
Safety) classification of machine learning technologies that
support and enable trust and establish the relation between
these trust-enhancing technologies and the HET qualities.
• We discuss how our technology classification and trustwor-
thy machine learning techniques relate to various Principled
AI framework considered by policy makers and researchers
in ethics and associated topics.
