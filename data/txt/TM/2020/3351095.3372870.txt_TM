
Explanations in Machine Learning come in many forms, but a con-
sensus regarding their desired properties is yet to emerge. In this
paper we introduce a taxonomy and a set of descriptors that can
be used to characterise and systematically assess explainable sys-
tems along five key dimensions: functional, operational, usability,
safety and validation. In order to design a comprehensive and rep-
resentative taxonomy and associated descriptors we surveyed the
eXplainable Artificial Intelligence literature, extracting the criteria
and desiderata that other authors have proposed or implicitly used
in their research. The survey includes papers introducing new ex-
plainability algorithms to see what criteria are used to guide their
development and how these algorithms are evaluated, as well as
papers proposing such criteria from both computer science and
social science perspectives. This novel framework allows to sys-
tematically compare and contrast explainability approaches, not
just to better understand their capabilities but also to identify dis-
crepancies between their theoretical qualities and properties of
their implementations. We developed an operationalisation of the
framework in the form of Explainability Fact Sheets, which enable
researchers and practitioners alike to quickly grasp capabilities
and limitations of a particular explainable method. When used as a
Work Sheet, our taxonomy can guide the development of new ex-
plainability approaches by aiding in their critical evaluation along
the five proposed dimensions.
CCS CONCEPTS
• General and reference → Evaluation; • Computing method-
ologies → Artificial intelligence; Machine learning.
KEYWORDS
Explainability, Interpretability, Transparency, Fact Sheet, Work
Sheet, Desiderata, Taxonomy, AI, ML
ACM Reference Format:
Kacper Sokol and Peter Flach. 2020. Explainability Fact Sheets: A Framework
for Systematic Assessment of Explainable Approaches. In Conference on
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372870
Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3351095.3372870
1 INTRODUCTION
With the current surge in eXplainable Artificial Intelligence (XAI)
research it has become a challenge to keep track of, analyse and
compare many of these approaches. A lack of clearly defined prop-
erties that explainable systems should be evaluated against hinders
progress of this fast moving research field because of undiscov-
ered (or undisclosed) limitations and properties of explainability
approaches and their implementations (as well as discrepancies
between the two).
In this work we propose that every explainability method de-
signed for predictive systems should be accompanied by a Fact
Sheet that assesses its (1) functional and (2) operational require-
ments. Moreover, the quality of explanations should be evaluated
against a list of (3) usability criteria to better understand their use-
fulness from a user perspective. Their (4) security, privacy and any
vulnerabilities that they may introduced to the predictive system
should also be discussed on this Fact Sheet. Lastly, their (5) vali-
dation, either via user studies or synthetic experiments, should be
disclosed. Therefore, a standardised list of explainability properties
– or, to put it otherwise, desiderata – spanning all of these five
dimensions would facilitate a common ground for easy evaluation
and comparison of explainability approaches, and help their design-
ers consider a range of clearly defined aspects important for this
type of techniques.
Despite theoretical guarantees for selected explainability ap-
proaches, some of these properties can be lost in implementation,
due to a particular application domain or data set used. As it stands,
many implementations do not exploit the full potential of the se-
lected explainability technique, for example, a method based on
counterfactuals might not take advantage [53] of their social and
interactive aspects [34]. A use of guidelines or a systematic eval-
uation of an approach with a standardised Fact Sheet could help
discover these unexpected functionality losses and account for or,
simply, report them for the benefit of the research community and
end users.
To mitigate the current lack of consensus regarding a common
set of properties that every explainable method should be evaluated
against, in this paper we collect, review and organise a compre-
hensive set of characteristics that span both computer science and
social sciences insights on that matter. Our goal is to provide the
community with an Explainability Fact Sheet template that users of
explainable systems (researchers and practitioners) could utilise to
systematically discuss, evaluate and report properties of their tech-
niques. This will not only benefit research, but will be of particular
importance as a guideline for designing, deploying or evaluating
explainability methods, especially if compliance with best practices
or legal regulations is required, e.g., the “right to explanation” intro-
duced by the European Union’s General Data Protection Regulation
(GDPR) [53]. Therefore, creators of explainable methods who need
to understand their specific requirements can use the same tem-
plate as a Work Sheet. We demonstrate the usefulness and practical
aspects of these Explainability Fact Sheets by presenting a particular
instantiation created for the Local Interpretable Model-agnostic Ex-
planations (LIME) algorithm [44] (see the supplementary material).
