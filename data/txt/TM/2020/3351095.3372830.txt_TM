
Counterfactual explanations are gaining prominence within tech-
nical, legal, and business circles as a way to explain the decisions
of a machine learning model. These explanations share a trait with
the long-established “principal reason” explanations required by
U.S. credit laws: they both explain a decision by highlighting a set
of features deemed most relevant—and withholding others.
These “feature-highlighting explanations” have several desirable
properties: They place no constraints on model complexity, do
not require model disclosure, detail what needed to be different to
achieve a different decision, and seem to automate compliance with
the law. But they are far more complex and subjective than they
appear.
In this paper, we demonstrate that the utility of feature-highlighting
explanations relies on a number of easily overlooked assumptions:
that the recommended change in feature values clearly maps to real-
world actions, that features can be made commensurate by looking
only at the distribution of the training data, that features are only
relevant to the decision at hand, and that the underlying model is
stable over time, monotonic, and limited to binary outcomes.
We then explore several consequences of acknowledging and
attempting to address these assumptions, including a paradox in the
way that feature-highlighting explanations aim to respect auton-
omy, the unchecked power that feature-highlighting explanations
grant decision makers, and a tension between making these expla-
nations useful and the need to keep the model hidden.
While new research suggests several ways that feature-highlighting
explanations can work around some of the problems that we iden-
tify, the disconnect between features in the model and actions in the
real world—and the subjective choices necessary to compensate for
this—must be understood before these techniques can be usefully
implemented.
ACM Reference Format:
Solon Barocas, Andrew D. Selbst, and Manish Raghavan. 2020. The Hidden
Assumptions Behind Counterfactual Explanations and Principal Reasons. In
Conference on Fairness, Accountability, and Transparency (FAT* ’20), January
27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3351095.3372830
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/01. . . $15.00
https://doi.org/10.1145/3351095.3372830
1 INTRODUCTION
Explanations are increasingly seen as a way to restore the agency
that algorithmic decision making takes from its subjects. Advocates
believe that explanations can enhance the autonomy of people
subject to automated decisions, allow people to navigate the rules
that govern their lives, help people recognize when they should
contest decisions or object to the decision making process, and
facilitate direct oversight and regulation of algorithms [37, 47].
In this paper, we examine two related approaches to explanation:
the counterfactual explanations that have been explored in recent
computer science research and which are gaining traction in in-
dustry, and the “principal reason” approach drawn from U.S. credit
laws. We will call these two approaches “feature-highlighting ex-
planations.” At a high level, these approaches provide the subject of
a decision with a set of factors that “explain” the decision. Though
they are distinct in operation and motivation, both methods high-
light a certain subset of features that are deemed most deserving of
the decision subject’s attention.
There are at least five reasons for the growing popularity of
feature-highlighting explanations. First, this approach appears to al-
low practitioners to abandon any constraints onmodel complexity—
a constraint often seen as a barrier to improved model performance.
Second, it allows businesses to avoid disclosing models in their
entirety, thereby protecting trade secrets and businesses’ other pro-
prietary interests, while limiting decision subjects’ ability to game
the model. Third, the approach promises a concrete justification for
a decision or precise instructions for achieving a different outcome.
Fourth, it allows firms to automate the difficult task of generating
explanations for a model’s decisions. And finally, it appears to gen-
erate explanations that comply with legal requirements both in the
United States and Europe.
Generating feature-highlighting explanations is far from straight-
forward, however, and requires decision makers to make many
consequential and subjective choices along the way. In this paper,
we demonstrate that the promised utility of feature-highlighting
explanations rests on four key assumptions, easily overlooked, and
rarely justified: (1) that a change in feature value clearly maps to
an action in the real world; (2) that features can be made commen-
surate by looking only at the distribution of feature values in the
training data; (3) that explanations can be offered without regard
to decision making in other areas of people’s lives; and (4) that the
underlying model is stable over time, monotonic, and limited to
binary outcomes.
The paper then explores three tensions at the heart of feature-
highlighting explanations. First, while feature-highlighting explana-
tions are designed to respect or enhance the autonomy of decision
subjects, the decision maker is put in the position of having to make
determinations about what is best for the decision subject; this is
paternalism in the name of autonomy. Furthermore, the only way
for a decision maker to be sensitive to decision subjects’ needs and
preferences is to further intrude into their lives, gathering enough
information to respect their autonomy, while compromising the
autonomy afforded by privacy in the process. Second, partial disclo-
sure puts the decision subject at the mercy of the decision maker.
The choice of what to disclose grants a great deal of power to the
decision maker. By granting such power to businesses, we invite
them to use that power for interests other than those of the decision
subject. At best, this leads to beneficent paternalism, at worst, self-
dealing. Finally, attempts to overcome some of these challenges by
providing decision subjects a larger number and more diverse set
of explanations or affording them the opportunity to explore the
consequences of specific changes will eventually risk revealing the
model altogether. If these techniques fail to protect their intellectual
property, firms are unlikely to adopt them.
