
Research within the social sciences and humanities has long char-
acterized the work of data science as a sociotechnical process, com-
prised of a set of logics and techniques that are inseparable from
specific social norms, expectations and contexts of development
and use. Yet all too often the assumptions and premises underlying
data analysis remain unexamined, even in contemporary debates
about the fairness of algorithmic systems. This blindspot exists
in part because the methodological toolkit used to evaluate the
fairness of algorithmic systems remains limited to a narrow set of
computational and legal modes of analysis. In this paper, we expand
on Elish and Boyd’s [17] call for data scientists to develop more ro-
bust frameworks for understanding their work as situated practice
by examining a specific methodological debate within the field of
anthropology, frequently referred to as the practice of "studying
up". We reflect on the contributions that the call to "study up" has
made in the field of anthropology before making the case that the
field of algorithmic fairness would similarly benefit from a reori-
entation "upward". A case study from our own work illustrates
what it looks like to reorient one’s research questions "up" in a
high-profile debate regarding the fairness of an algorithmic system
– namely, pretrial risk assessment in American criminal law. We
discuss the limitations of contemporary fairness discourse with
regard to pretrial risk assessment before highlighting the insights
gained when we reframe our research questions to focus on those
who inhabit positions of power and authority within the U.S. court
system. Finally, we reflect on the challenges we have encountered
in implementing data science projects that "study up". In the pro-
cess, we surface new insights and questions about what it means to
ethically engage in data science work that directly confronts issues
of power and authority.
ACM Reference Format:
Chelsea Barabas, Colin Doyle, JB Rubinovitz, and Karthik Dinakar. 2020.
Studying Up: Reorienting the study of algorithmic fairness around issues
Permission to make digital or hard copies of all or part of this work for personal or classroom use
is granted without fee provided that copies are not made or distributed for profit or commercial
advantage and that copies bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
FAT* '20, January 27–30, 2020, Barcelona, Spain
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6936-7/20/02…$15.00
https://doi.org/10.1145/3351095.3372859
of power. In ACM Conference on Fairness, Accountability, and Transparency, 
January 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. 
https://doi.org/10.1145/3351095.3372859
1 INTRODUCTION
A key aim of the algorithmic fairness community is to develop
frameworks and standards to evaluate algorithmic systems against
social and legal principles such as equity, fairness, and justice. Ini-
tial efforts to grapple with the ethical and social implications of
algorithmic systems have come in the form of technical "fairness
criteria" – computational formalisms used to define and evaluate
complex legal concepts such as "disparate impact," "equal opportu-
nity," and "affirmative action" [14, 19, 27]. But recent scholarship
in the field of algorithmic fairness has begun to point out the lim-
itations of this approach. The fairness of a data science project
extends far beyond the technical properties of a given model and
includes normative and epistemological issues that arise during pro-
cesses of problem formulation [43], data collection and preparation
[50], claims-making [3, 40] and everyday use in applied contexts
[48]. Scholars have argued that narrow technical conceptualizations
of algorithmic fairness elide more fundamental issues and, in the
process, run the risk of legitimizing harmful practices based on fun-
damentally unsound truth claims about the world [3, 6, 17, 40, 55].
In light of these concerns, some scholars have suggested that de-
signers of algorithmic systems embrace a process-driven approach
to algorithmic justice, one that explicitly recognizes the active and
crucial role that the data scientist plays in constructing meaning
from data [10, 17, 24, 48]. Researchers have problematized common-
place characterizations of data science as an objective and neutral
process [32, 56], arguing that data and their subsequent analyses
are always the by-product of socially contingent processes of mean-
ing making and knowledge production [6, 31, 43, 56]. Rather than
strive to develop narrow technical solutions to the issue of "fair-
ness," scholars have recently encouraged the algorithmic fairness
community to draw broader boundaries around what they con-
ceive of as an "algorithmic system," to include social actors and key
contextual considerations [48].
Research within the fields of sociology, anthropology, and sci-
ence, technology and society (STS) has long characterized the work
of data science as a sociotechnical process, comprised of a set of log-
ics and techniques that are inseparable from specific social norms,
expectations and contexts of development and use [6, 17, 18]. But
all too often the assumptions and premises underlying data analysis
remain unexamined, even in contemporary debates regarding the
fairness of algorithmic systems. This blindspot has appeared in part
because the conceptual and methodological toolkit used to evaluate
the fairness of algorithmic systems remains limited to a narrow set
of computational and legal modes of analysis.
To grapple with the full social and ethical implications of data-
intensive algorithmic systems, wemust developmore robustmethod-
ological frameworks that enable data scientists to reflect on how
their research practices and design choices influence and distort the
insights generated from their work. To this end, Elish and Boyd [17]
have called for researchers to reconceptualize data science as a form
of "computational ethnography," whereby data scientists actively
participate in the production of partial and situated knowledge
claims. Like ethnographers, data scientists "surround themselves
with data (‘a field site’), choose what to see and what to ignore, and
develop a coherent mental model that can encapsulate the observed
insights" [17]. Elish and Boyd argue that reconceptualizing data
science as a novel form of qualitative inquiry opens up a new set
of methodological frameworks that can guide data-driven practices
of modeling the world and illuminate the ways that power shapes
and operates through the work of data science.
The algorithmic fairness community can benefit greatly from
ongoing methodological debates and insights gleaned from fields
such as anthropology and sociology, where scholars have been
working for decades to develop more robust frameworks for under-
standing their work as situated practice. In this paper, we expand
on Elish and Boyd’s [17] call for the development of more reflex-
ive data science practices, by examining a specific methodological
debate within the field of anthropology, frequently referred to as
the practice of "studying up." In what is now considered a classic
anthropological text, Laura Nader [38] called for her fellow anthro-
pologists to expand their field of inquiry to include the study of
elite individuals and institutions, who remained significantly under-
examined in the anthropological cannon. Rather than study exotic
cultures in far-flung lands, Nader appealed for a critical repatriated
anthropology that would shed light on processes of exploitation and
domination by refocusing the anthropological lens on the cultures
of the powerful. As Nader argued, "If one’s pivot point is around
those who have responsibility by virtue of being delegated power,
then the questions change" [38].
This paper is organized into two parts. In part one, we reflect on
the contributions that the call to "study up" has made to the field of
anthropology. Nader’s provocation came at a time when anthropol-
ogists were grappling with the epistemological and methodological
limits of their tradition. The call to "study up" was a call for schol-
ars to move beyond their default orientations – their tendency to
study the "underdog" in isolation from larger structural forces – in
order to deal directly with issues of power and domination in their
work. We draw parallels between this debate in anthropology and
similar issues that data scientists are grappling with today in their
pursuit of "fair, accountable, and transparent" algorithmic systems.
We then make the case for why the field of algorithmic fairness
would benefit from such a reorientation "upward." The political and
social impacts of algorithmic systems cannot be fully understood
unless they are conceptualized within larger institutional contexts
and systems of oppression and control. Data science projects that
"study up" could lay the foundation for more robust forms of ac-
countability and a deeper understanding of the structural factors
that produce undesirable social outcomes via algorithmic systems.
In part two, we present a case study from our own work as a
group of interdisciplinary researchers from the fields of computer
science, sociology and law. This case study illustrates how research
questions can be reoriented "up" in contemporary discourse regard-
ing the fairness of algorithmic systems. For the past two years,
we have been deeply engaged in the public and academic debate
regarding the use of pretrial risk assessment as a means of bail
reform in the United States. Pretrial risk assessment has become
one of the prototypical examples of the ethical stakes of contempo-
rary algorithmic decision making regimes. But the ethical debate
regarding these tools has by-and-large uncritically accepted the
premise that the best way to address mass pretrial incarceration is
by modeling and forecasting the risk of some of the most marginal-
ized and disempowered factions of American society – individuals
arrested and charged with a crime.
We discuss the limitations of contemporary fairness discourse
regarding pretrial risk assessment before illustrating the insights
gained when we reframe our research questions to focus on those
who inhabit positions of power and authority within the U.S. court
system. Finally, we reflect on the challenges we have encountered
in implementing data science projects that aim to shift the gaze
"upward." In the process, we develop new insights and questions
about what it means to ethically engage in data science work that
directly confronts issues of power and authority in the field. To do
this, we draw on a feminist tradition of rigorously examining the
"micropolitics of research" to unpack the ways that our positional-
ity as researchers who are interacting with powerful institutions
impacts the production of specific knowledge claims in our work
[8]. Our hope is that in doing so, we will expand the conversa-
tion about ethical engagement in data science and open up new
lines of inquiry that, until now, have been left unexamined by the
algorithmic fairness community.
