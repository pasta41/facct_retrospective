 
Machine learning technologies are increasingly developed for use 
in healthcare. While research communities have focused on 
creating state-of-the-art models, there has been less focus on real 
world implementation and the associated challenges to fairness, 
transparency, and accountability that come from actual, situated 
use. Serious questions remain underexamined regarding how to 
ethically build models, interpret and explain model output, 
recognize and account for biases, and minimize disruptions to 
professional expertise and work cultures. We address this gap in 
the literature and provide a detailed case study covering the 
development, implementation, and evaluation of Sepsis Watch, a 
machine learning-driven tool that assists hospital clinicians in the 
early diagnosis and treatment of sepsis. Sepsis is a severe 
infection that can lead to organ failure or death if not treated in 
time and is the leading cause of inpatient deaths in US hospitals. 
We, the team that developed and evaluated the tool, discuss our 
conceptualization of the tool not as a model deployed in the world 
but instead as a socio-technical system requiring integration into 
existing social and professional contexts. Rather than focusing 
solely on model interpretability to ensure fair and accountable 
machine learning, we point toward four key values and practices 
that should be considered when developing machine learning to 
support clinical decision-making: rigorously define the problem 
in context, build relationships with stakeholders, respect 
professional discretion, and create ongoing feedback loops with 
stakeholders. Our work has significant implications for future 
research regarding mechanisms of institutional accountability 
and considerations for responsibly designing machine learning 
systems. Our work underscores the limits of model 
interpretability as a solution to ensure transparency, accuracy, 
and accountability in practice. Instead, our work demonstrates 
other means and goals to achieve FATML values in design and in 
practice.   
CCS CONCEPTS 
• Computing methodologies ® Machine learning; • 
Human-centered computing ® Field study; • Social and 
professional topics ® Government technology policy 
KEYWORDS 
Deep learning; Interpretability; Medicine; Trust; Expertise 
ACM Reference format: 
Mark Sendak, Madeleine Elish, Michael Gao, Joseph Futoma, William 
Ratliff, Marshall Nichcols, Armando Bedoya, Suresh Balu, Cara O’Brien. 
2020. “The Human Body is a Black Box”: Supporting Clinical Decision-
Making with Deep Learning. In Proceedings of ACM Conference on 
Fairness, Accountability, and Transparency (FAT* 2020), January 
27-30. 2020. ACM, Barcelona, Spain, 10 pages. 
https://doi.org/10.1145/3351095.3372827 
1 INTRODUCTION 
Machine learning technologies are increasingly developed for use 
in healthcare. From consumer facing apps to hospital readmission 
predictors, the healthcare industry includes a rapidly expanding 
†Joseph Futoma also retains a research position in the department of Statistics at 
Duke University 
Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for third-party components of this 
work must be honored. For all other uses, contact the Owner/Author. 
FAT* '20, January 27–30, 2020, Barcelona, Spain © 2020 Copyright is held by the 
owner/author(s). ACM ISBN 978-1-4503-6936-7/20/02. 
https://doi.org/10.1145/3351095.3372827  
 
 
 
set of use cases for machine learning applications [59]. The 
machine learning community has focused much research on 
creating state-of-the-art models, but there has been less focus on 
real world implementation and the associated challenges to 
fairness, transparency, and accountability that come from actual, 
situated use. Serious questions remain underexamined regarding 
how to ethically build models, interpret and explain model output, 
recognize and account for biases, and minimize disruptions to 
professional expertise and work cultures. 
This paper contributes a case study through which to examine 
how issues of transparency, trust, and accountability are grappled 
with in practice. We present an empirical case study covering the 
development, implementation, and evaluation of Sepsis Watch, a 
machine learning-driven tool that assists hospital clinicians in the 
early diagnosis and treatment of sepsis. Sepsis is an inflammatory 
response to infection that can lead to organ failure and is the 
leading cause of inpatient deaths in US hospital [47]. We, the 
authors, were part of the team developing and evaluating this tool 
and our case study is based on our practitioner and research 
experiences. 
Some of the unique challenges facing the development of 
Sepsis Watch were that sepsis is not only hard to predict but also 
lacks a universally accepted definition. A model to predict sepsis 
needed to articulate a ground truth where in fact there was none. 
Moreover, when and why sepsis develops is incompletely 
understood. These aspects contributed to a de-prioritization of 
model interpretability in favor of other ways to establish trust in 
the accuracy of the model with clinicians, including rigorous 
documentation and institution-specific validation and evaluation. 
At the same time, our interdisciplinary team approached the 
development of Sepsis Watch as the development of a socio-
technical system, not an isolated model. By using the term “socio-
technical system,” we mean to foreground the interconnected 
social and technical dimensions of a technology, in which, for 
instance, the use of a machine learning tool cannot be considered 
apart from the people and institutions who interact with the tool 
and the beliefs, contexts, and power hierarchies that shape its 
development and use. From this perspective, Sepsis Watch is a 
great deal more than a deep learning model that generates risk 
scores; it is a complex socio-technical system that combines 
technical and institutional infrastructures with professionals who 
are making critical and highly contextual decisions. 
In this paper, we present our approach and describe the 
processes of designing, developing, and implementing Sepsis 
Watch. We begin by situating our analysis in related work on 
machine learning healthcare applications and the literature on 
technology adoption in healthcare institutions. This is followed 
by an overview of the Sepsis Watch tool and a discussion of our 
work building trust and mechanisms of accountability at multiple 
levels of the project, over time and with different stakeholders. 
We employ the term trust to describe a belief held by individuals 
that the system in place is appropriate and accurate, and 
accountability to refer to the ways in which technologists and 
designers can be held responsible for the performance of the 
system. We also discuss the complexities of integrating the output 
of the model into clinical decision-making. Drawing on these 
experiences and observations, we conclude by presenting four key 
values and practices that should be considered when developing 
machine learning to support clinical decision-making: rigorously 
define the problem in context, build relationships with 
stakeholders, respect professional discretion, and create ongoing 
feedback loops with stakeholders. 
Our contribution to the growing literature on fair, 
accountable, and transparent machine learning (FATML) is two-
fold. First, we provide a detailed case study of a tool’s 
development and implementation, presenting empirical and 
socially-situated (as opposed to experimental) evidence of a tool 
in use and its intersections with existing FATML concerns. To our 
knowledge, Sepsis Watch is comparable to no other clinical 
decision support system, and is one of the first deep learning 
models to be fully integrated into routine clinical care. Second, we 
demonstrate the implications for trust, accountability, and 
transparency when a machine learning implementation is 
conceptualized not as a model deployed in the world but rather as 
a socio-technical system that must be integrated into existing 
social and professional contexts. From this perspective, model 
interpretability itself does not ensure a fair or accountable 
technology. Rather, our experience and discussion points toward 
significant alternative mechanisms to achieve FATML values in 
design and in practice. 
