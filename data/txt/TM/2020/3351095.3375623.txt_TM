
Detecting biases in artificial intelligence has become difficult be-
cause of the impenetrable nature of deep learning. The central
difficulty is in relating unobservable phenomena deep inside mod-
els with observable, outside quantities that we can measure from
inputs and outputs. For example, can we detect gendered percep-
tions of occupations (e.g., female librarian, male electrician) using
questions to and answers from a word embedding-based system?
Current techniques for detecting biases are often customized for
a task, dataset, or method, affecting their generalization. In this
work, we draw from Psychophysics in Experimental Psychology—
meant to relate quantities from the real world (i.e., “Physics”) into
subjective measures in the mind (i.e., “Psyche”)—to propose an in-
tellectually coherent and generalizable framework to detect biases
in AI. Specifically, we adapt the two-alternative forced choice task
(2AFC) to estimate potential biases and the strength of those biases
in black-box models. We successfully reproduce previously-known
biased perceptions in word embeddings and sentiment analysis
predictions. We discuss how concepts in experimental psychology
can be naturally applied to understanding artificial mental phe-
nomena, and how psychophysics can form a useful methodological
foundation to study fairness in AI.
KEYWORDS
Biases in word embeddings, Biases in Sentiment Analysis, Artificial
Psychophysics, Two-alternative forced choice task
ACM Reference Format:
Lizhen Liang and Daniel E. Acuna. 2020. Artificial mental phenomena:
Psychophysics as a framework to detect perception biases in AI models. In
Conference on Fairness, Accountability, and Transparency (FAT* ’20), January
27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3351095.3375623
∗Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6936-7/20/01. . . $15.00
https://doi.org/10.1145/3351095.3375623
1 INTRODUCTION
Recent artificial intelligence models have shown remarkable perfor-
mance in a variety of tasks that were once thought to be solvable
only by humans [47]. With such promising results, companies and
governments begun deploying such systems for increasingly crit-
ical tasks, including job candidate screening [13], justice system
decisions [2], and credit scoring [26]. Due to training with data that
might contain biases, however, deep learning models inadvertently
fit those biases and create decisions that discriminate against gen-
der and other protected statuses. If we were to find those biases in
humans, we could interrogate them and determine whether such
biases have occurred. Several researchers have attempted to de-
velop methods for detecting biases in AI models, but these methods
are specific to the task (e.g., [46]), data (see [11]), or type of model
[8]—hindering their potential adoption. Here we entertain the idea
of using Experimental Psychology to develop novel and coherent
methods for probing AI systems. Experimental Psychology has a
very rich tradition of treating human consciousness as a black box,
developing and extracting potential biases from subjective judge-
ments in behavioral tasks [17]. We hypothesize that we can adapt
these methods to uncover biases in AI models in a similar way.
In particular, Psychophysics and signal detection theory offer a
concrete set of tools for querying black boxes and extracting useful
measures on the direction and strength of bias. In this work, we
describe how we adapt the standard two-alternative forced choice
(2AFC) task, the workhorse of Psychophysics, to extract biases in
word embeddings and sentiment analysis predictions.
The dramatic increase in the usage of AI systems has called into
question potential biases made against vulnerable groups. Part of
the issue is that current systems have exploded in their complexity,
going from hundreds of parameters linearly related to outputs, to
billions of parameters non-trivially related to outputs (as discussed
broadly in [23], [48], and [39]). If biases are present in modern AI
systems, they are therefore significantly harder to detect just by
inspecting fitted parameters. This has resulted in dramatic cases of
discrimination in recidivism prediction [2] and credit scoring [26],
which are only discovered once systems are deployed. One proposed
solution to the problem of veiled discrimination, recently imple-
mented in Europe’s General Data Protection Regulation (GDPR),
is to force AI models to be “explainable” [15]. While forcing ex-
plainability appears as a natural countermeasure, the well-known
interpretability–accuracy trade-off would predict that these sys-
tems have decreased accuracy [43]. This is not always desirable
[14].
One solution for preventing biases present in AI models is to
develop techniques for detecting them, as a natural first step to
fixing them. There have been several research programs aimed
at detecting biases in AI models. Many of them, however, require
detailed knowledge of the inner workings of the algorithm or the
datasets. For example, in the work of [35], the authors propose a
form of “classifier interrogation” which requires labeled data to
explore the space of parameters that might cause biases. Also, tech-
niques for detecting biases are somewhat task specific and difficult
to generalize. In [7], for example, the authors adapt the Implicit
Association Task (IAT) for detecting biases in word embeddings.
While this is a natural application of the original intention of IAT,
it is unclear how to move beyond bias detection in unsupervised
settings. Recently, researchers in DeepMind proposed Psychlab, a
highly-complex synthetic environment to test AI models as if they
were humans living in a virtual world [33]. Detecting biases of AI
models is an important step but it would be beneficial to develop
more general-purpose and simpler techniques.
Interestingly, Experimental Psychology has had to develop meth-
ods for understanding latent, mental phenomena based on questions
and answers that are exerted verbally or physically. In particular,
the field of Psychophysics uses methods to measure the perception
biases and sense’s accuracies of animals [21]. Importantly, a key
requirement of Psychophysics is to avoid relying on verbal or other
highly-cognitive responses that are prone to noise and rather use
simple behavioral responses that are hard to fake (e.g., movements,
yes/no answers). In the whole of Psychophysics, perhaps one of the
oldest and most well-developed techniques for performing estima-
tions based on these cues is the two-alternative forced choice task
(2AFC) [1]. This method is used, for example, to measure how the
size of objects biases our perception of weight [10], and to measure
the precision of the human retina when detecting light [22]. We hy-
pothesize that we can adapt these techniques for measuring biases
and the strength of those biases in AI models. Thus, Experimental
Psychology is a rich area of research with potential applications to
examine artificial intelligence decisions.
In this work, we develop a framework to study biases in AI
models. Our primary goal is to develop a framework that is coherent
across datasets, tasks, and algorithms, allowing a researcher to
describe biased perception using a common language. We draw
inspiration from Psychophysics, a field of Experimental Psychology,
and adapt the two-alternative forced choice (2AFC) task. As an
example, we examine potential biases in word embeddings and
sentiment analysis predictions and we validate our results with
real-world data. Our findings show that we are able to detect biases
and the strength of them in decisions that involve gender and
occupations. In sum, our work contributes to the field of fairness,
accountability, and transparency in the following manner:
• A discussion of the current bias detection techniques for AI
models
• Anewmethod for detecting biases andmeasuring the strength
of those biases based on a coherent set of concepts and lan-
guage drawn from Experimental Psychology
• A demonstration of the application of the technique to word
embeddings and sentiment analysis prediction
