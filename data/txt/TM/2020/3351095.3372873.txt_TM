
Rising concern for the societal implications of artificial intelligence
systems has inspired a wave of academic and journalistic literature
in which deployed systems are audited for harm by investigators
from outside the organizations deploying the algorithms. However,
it remains challenging for practitioners to identify the harmful
repercussions of their own systems prior to deployment, and, once
deployed, emergent issues can become difficult or impossible to
trace back to their source.
In this paper, we introduce a framework for algorithmic auditing
that supports artificial intelligence system development end-to-end,
to be applied throughout the internal organization development life-
cycle. Each stage of the audit yields a set of documents that together
form an overall audit report, drawing on an organization’s values
or principles to assess the fit of decisions made throughout the pro-
cess. The proposed auditing framework is intended to contribute to
closing the accountability gap in the development and deployment
of large-scale artificial intelligence systems by embedding a robust
process to ensure audit integrity.
CCS CONCEPTS
• Social andprofessional topics→ Systemmanagement;Tech-
nology audits; • Software and its engineering→ Software de-
velopment process management.
KEYWORDS
Algorithmic audits, machine learning, accountability, responsible
innovation
∗Both authors contributed equally to this paper. This work was done by Inioluwa
Deborah Raji as a fellow at Partnership on AI (PAI), of which Google, Inc. is a partner.
This should not be interpreted as reflecting the official position of PAI as a whole, or
any of its partner organizations.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6936-7/20/02.
https://doi.org/10.1145/3351095.3372873
ACM Reference Format:
Inioluwa Deborah Raji, Andrew Smart, Rebecca N.White, Margaret Mitchell,
Timnit Gebru, BenHutchinson, Jamila Smith-Loud, Daniel Theron, and Parker
Barnes. 2020. Closing the AI Accountability Gap: Defining an End-to-End
Framework for Internal Algorithmic Auditing. In Conference on Fairness,
Accountability, and Transparency (FAT* ’20), January 27–30, 2020, Barcelona,
Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3351095.
With the increased access to artificial intelligence (AI) development
tools and Internet-sourced datasets, corporations, nonprofits and
governments are deploying AI systems at an unprecedented pace,
often in massive-scale production systems impacting millions if not
billions of users [1]. In the midst of this widespread deployment,
however, come valid concerns about the effectiveness of these auto-
mated systems for the full scope of users, and especially a critique
of systems that have the propensity to replicate, reinforce or am-
plify harmful existing social biases [8, 37, 62]. External audits are
designed to identify these risks from outside the system and serve
as accountability measures for these deployed models. However,
such audits tend to be conducted after model deployment, when
the system has already negatively impacted users [26, 51].
In this paper, we present internal algorithmic audits as a mecha-
nism to check that the engineering processes involved in AI system
creation and deployment meet declared ethical expectations and
standards, such as organizational AI principles. The audit process is
necessarily boring, slow, meticulous and methodical—antithetical
to the typical rapid development pace for AI technology. However,
it is critical to slow down as algorithms continue to be deployed
in increasingly high-stakes domains. By considering historical ex-
amples across industries, we make the case that such audits can be
leveraged to anticipate potential negative consequences before they
occur, in addition to providing decision support to design mitiga-
tions, more clearly defining and monitoring potentially adverse out-
comes, and anticipating harmful feedback loops and system-level
risks [20]. Executed by a dedicated team of organization employees,
internal audits operate within the product development context and
can inform the ultimate decision to abandon the development of
AI technology when the risks outweigh the benefits (see Figure 1).
Inspired from the practices and artifacts of several disciplines, we
go further to develop SMACTR, a defined internal audit framework
meant to guide practical implementations. Our framework strives
to establish interdisciplinarity as a default in audit and engineering
processes while providing the much needed structure to support
the conscious development of AI systems.
