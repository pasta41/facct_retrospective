
Algorithmic fairness aims to address the economic, moral, social,
and political impact that digital systems have on populations through
solutions that can be applied by service providers. Fairness frame-
works do so, in part, by mapping these problems to a narrow defi-
nition and assuming the service providers can be trusted to deploy
countermeasures. Not surprisingly, these decisions limit fairness
frameworks’ ability to capture a variety of harms caused by systems.
We characterize fairness limitations using concepts from require-
ments engineering and from social sciences. We show that the focus
on algorithms’ inputs and outputs misses harms that arise from
systems interacting with the world; that the focus on bias and
discrimination omits broader harms on populations and their envi-
ronments; and that relying on service providers excludes scenarios
where they are not cooperative or intentionally adversarial.
We propose Protective Optimization Technologies (POTs). POTs,
provide means for affected parties to address the negative impacts
of systems in the environment, expanding avenues for political con-
testation. POTs intervene from outside the system, do not require
service providers to cooperate, and can serve to correct, shift, or
expose harms that systems impose on populations and their envi-
ronments. We illustrate the potential and limitations of POTs in two
case studies: countering road congestion caused by traffic-beating
applications, and recalibrating credit scoring for loan applicants.
CCS CONCEPTS
• Social and professional topics → Socio-technical systems;
KEYWORDS
Fairness and Accountability; Protective Optimization Technologies
ACM Reference Format:
Bogdan Kulynych, Rebekah Overdorf, Carmela Troncoso, and Seda Gürses.
2020. POTs: Protective Optimization Technologies. In Conference on Fairness,
Accountability, and Transparency (FAT* ’20), January 27–30, 2020, Barcelona,
Spain. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3351095.
Advances in computational power, software engineering, and ma-
chine learning algorithms have been instrumental in the rise of
Bogdan Kulynych and Rebekah Overdorf contributed equally to this work.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Association for Computing Machinery.
This is the author’s version of the work. It is posted here for your personal use. Not
for redistribution. The definitive Version of Record was published in Conference on
Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020, Barcelona,
Spain, https://doi.org/10.1145/3351095.3372853.
digital systems. Their ubiquity in our everyday activities raises con-
cerns regarding the centralization of decisional power [1]. These
concerns are amplified by the opaque and complex nature of these
systems which results in hard-to-explain outputs [2] and unjust
outcomes for historically marginalized populations [3–6].
Computer scientists counter these inequities through frame-
works studied under the rubric of fairness, using a variety of formal
fairness notions [7]. Their results have been instrumental in our un-
derstanding of the discriminatory effects of algorithmic decisions.
The frameworks, however, rely on narrowing the inequity problem
to primarily consider the discriminatory impact of algorithms, and
assuming trustworthy providers. The narrow view has enabled
valuable breakthroughs centered around the service providers’ abil-
ity to address some inequities, but fails to capture broader harms
or explore other ways to contest service-provider power [8].
In this work we investigate digital systems from a new perspec-
tive in order to understand how to address the broader class of
harms that they cause. To achieve this, we characterize the type of
systems in which algorithms are integrated and deployed. These
systems typically build on distributed service architectures and
incorporate real-time feedback from both users and third-party
service providers [9, 10]. This feedback is leveraged for a variety of
novel forms of optimization that are geared towards extraction of
value through the system. Typically, optimization is used for tech-
nical performance and minimizing costs, e.g., by optimizing cloud
usage orchestration or allocation of hardware resources. It has also
become part and parcel of “continuous development” strategies
based on experimentation that allow developers to define dynamic
objective functions and build adaptive systems. Businesses can now
design for “ideal” interactions and environments by optimizing
feature selection, behavioral outcomes, and planning that is in line
with a business growth strategy. We argue that optimization-based
systems are developed to capture and manipulate behavior and
environments for the extraction of value. As such, they introduce
broader risks and harms for users and environments beyond the
outcome of a single algorithm within that system. These impacts
go beyond the bias and discrimination stemming from algorithmic
outputs fairness frameworks consider.
Borrowing concepts and techniques from software and require-
ments engineering, and from economics and social sciences, we
characterize the limitations of current fairness approaches in cap-
turing and mitigating harms and risks arising from optimization.
Among others, we show that focusing on the algorithms and their
outputs overlooks the many ‘externalities’ caused by optimizing
every aspect of a system; that discrimination is only one of the in-
justices that can arise when systems are designed to maximize gain;
and that ignoring service providers’ incentives and capabilities to
enforce proposed solutions limits our understanding of their opera-
tion and further consolidates the power providers have regarding
decisions and behaviors that have profound effects in society.
FAT* ’20, January 27–30, 2020, Barcelona, Spain Bogdan Kulynych, Rebekah Overdorf, Carmela Troncoso, and Seda Gürses
Finally, we propose Protective Optimization Technologies (POTs),
which aim at addressing risks and harms that cannot be captured
from the fairness perspective and cannot be addressed without a
cooperative service provider. The ultimate goal of POTs is to elim-
inate the harms, or at least mitigate them. When these are not
possible, POTs can shift harms away from affected users or expose
abusive or non-social practices by service providers. We illustrate
the potential of POTs to address externalities of optimization-based
systems in two case studies: traffic-beating routing applications
and credit scoring. We also identify numerous techniques devel-
oped by researchers in the fields of security and privacy, by artists,
and by others, that, though not necessarily designed to counter
externalities, can be framed as POTs.
