
There has been rapidly growing interest in the use of algorithms
in hiring, especially as a means to address or mitigate bias. Yet, to
date, little is known about how these methods are used in practice.
How are algorithmic assessments built, validated, and examined for
bias? In this work, we document and analyze the claims and prac-
tices of companies offering algorithms for employment assessment.
In particular, we identify vendors of algorithmic pre-employment
assessments (i.e., algorithms to screen candidates), document what
they have disclosed about their development and validation proce-
dures, and evaluate their practices, focusing particularly on efforts
to detect and mitigate bias. Our analysis considers both technical
and legal perspectives. Technically, we consider the various choices
vendors make regarding data collection and prediction targets, and
explore the risks and trade-offs that these choices pose. We also
discuss how algorithmic de-biasing techniques interface with, and
create challenges for, antidiscrimination law.
CCS CONCEPTS
• Social andprofessional topics→Employment issues; •Com-
puting methodologies→Machine learning; • Applied comput-
ing → Law.
KEYWORDS
algorithmic hiring, discrimination law, algorithmic bias
ACM Reference Format:
Manish Raghavan, Solon Barocas, Jon Kleinberg, and Karen Levy. 2020.
Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices. In
Conference on Fairness, Accountability, and Transparency (FAT* ’20), January
27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 13 pages. https:
//doi.org/10.1145/3351095.3372828
1 INTRODUCTION
The study of algorithmic bias and fairness in machine learning has
quickly matured into a field of study in its own right, delivering a
wide range of formal definitions and quantitative metrics. As indus-
try takes up these tools and accompanying terminology, promises
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372828
of eliminating algorithmic bias using computational methods have
begun to proliferate. In some cases, however, rather than forcing
precision and specificity, the existence of formal definitions and
metrics has had the paradoxical result of giving undue credence to
vague claims about “de-biasing” and “fairness.”
In this work, we use algorithmic pre-employment assessment as
a case study to show how formal definitions of fairness allow us to
ask focused questions about the meaning of “fair” and “unbiased”
models. The hiring domain makes for an effective case study be-
cause of both its prevalence and its long history of bias. We know
from decades of audit studies that employers tend to discriminate
against women and ethnic minorities [9, 10, 12, 52], and a recent
meta-analysis suggests that little has improved over the past 25
years [75]. Citing evidence that algorithms may help reduce human
biases [48, 58], advocates argue for the adoption of algorithmic
techniques in hiring [20, 30], with a variety of computational met-
rics proposed to identify and prevent unfair behavior [35]. But to
date, little is known about how these methods are used in practice.
One of the biggest obstacles to empirically characterizing indus-
try practices is the lack of publicly available information. Much
technical work has focused on using computational notions of eq-
uity and fairness to evaluate specific models or datasets [2, 16].
Indeed, when these models are available, we can and should investi-
gate them to identify potential problems. But what do we do when
we have little or no access to models or the data they produce? Cer-
tain models may be completely inaccessible to the public, whether
for practical or legal reasons, and attempts to audit these models
by examining their training data or outputs might jeopardize users’
privacy. With algorithmic pre-employment assessments, we find
that this is very much the case: models, much less the sensitive em-
ployee data used to construct them, are in general kept private. As
such, the only information we can consistently glean about industry
practices is limited to what companies publicly disclose. Despite
this, one of the key findings of our work is that even without ac-
cess to models or data, we can still learn a considerable amount by
investigating what corporations disclose about their practices for
developing, validating, and removing bias from these tools.
Documenting claims and evaluating practices. Following a review
of firms offering recruitment technologies, we identify 18 vendors
of pre-employment assessments. We document what each company
has disclosed about its practices and consider the implications of
these claims. In so doing, we develop an understanding of industry
attempts to mitigate bias and what critical issues are unaddressed.
Prior work has sought to taxonomize the points at which bias
can enter machine learning systems, noting that the choice of target
variable or outcome to predict, the training data used, and labelling
of examples are all potential sources of disparities [6, 59]. Following
these frameworks, we seek to understand how practitioners handle
these key decisions in the machine learning pipeline. In particular,
we surface choices and trade-offs vendors face with regard to the
collection of data, the ability to validate on representative popula-
tions, and the effects of discrimination law on efforts to prevent
bias. The heterogeneity we observe in vendors’ practices indicates
evolving industry norms that are sensitive to concerns of bias but
lack clear guidance on how to respond to these worries.
Of course, analyzing publicly available information has its lim-
itations. We are unable, for example, to identify issues that any
particular model might raise in practice. Nor can we be sure that
vendors aren’t doing more behind the scenes to ensure that their
models are non-discriminatory. And while other publicly accessible
information (e.g., news articles and videos from conferences) might
offer further details about vendors’ practices, for the sake of con-
sistent comparison, we limit ourselves to statements on vendors’
websites. As such, our analysis should not be viewed as exhaus-
tive; however, as we will see, it is still possible to draw meaningful
conclusions and characterize industry trends through our methods.
One notable limitation we encounter is the lack of information
about the validity of these assessments. It is of paramount impor-
tance to know the extent to which these tools actually work, but
we cannot do so without additional transparency from vendors.
We stress that our analysis is not intended as an exposé of indus-
try practices. Many of the vendors we study exist precisely because
they seek to provide a fairer alternative to traditional hiring prac-
tices. Our hope is that this work will paint a realistic picture of the
landscape of algorithmic techniques in pre-employment assessment
and offer recommendations for their effective and appropriate use.
Organization of the rest of the paper. Section 2 contains an overview
of pre-employment assessments, their history, and relevant legal
precedents. In Section 3, we systematically review vendors of algo-
rithmic screening tools and empirically characterize their practices
based on the claims that they make. We analyze these practices in
detail in Sections 4 and 5 from technical and legal perspectives, ex-
amining ambiguities and particular causes for concern. We provide
concluding thoughts and recommendations in Section 6.
