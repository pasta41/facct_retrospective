
Organizations cannot address demographic disparities that they
cannot see. Recent research on machine learning and fairness has
emphasized that awareness of sensitive attributes, such as race and
sex, is critical to the development of interventions. However, on
the ground, the existence of these data cannot be taken for granted.
This paper uses the domains of employment, credit, and health-
care in the United States to surface conditions that have shaped
the availability of sensitive attribute data. For each domain, we
describe how and when private companies collect or infer sensi-
tive attribute data for antidiscrimination purposes. An inconsistent
story emerges: Some companies are required by law to collect sen-
sitive attribute data, while others are prohibited from doing so.
Still others, in the absence of legal mandates, have determined that
collection and imputation of these data are appropriate to address
disparities.
This story has important implications for fairness research and
its future applications. If companies that mediate access to life
opportunities are unable or hesitant to collect or infer sensitive
attribute data, then proposed techniques to detect and mitigate bias
in machine learning models might never be implemented outside
the lab. We conclude that today’s legal requirements and corporate
practices, while highly inconsistent across domains, offer lessons
for how to approach the collection and inference of sensitive data
in appropriate circumstances. We urge stakeholders, including ma-
chine learning practitioners, to actively help chart a path forward
that takes both policy goals and technical needs into account.
ACM Reference Format:
Miranda Bogen, Aaron Rieke, and Shazeda Ahmed. 2020. Awareness in
Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimina-
tion. In Conference on Fairness, Accountability, and Transparency (FAT* ’20),
January 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 9 pages.
https://doi.org/10.1145/3351095.3372877
∗Author was affiliated with Upturn at time of writing.
†Author was a Fellow at Upturn at time of writing.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372877
1 INTRODUCTION
Statistical models, including those created with machine learning,
can reproduce biases in the historical data used to train them. As
powerful institutions increase their reliance upon these models to
automate decisions that affect people’s rights and life opportunities,
researchers have begun developing new techniques to help detect
and address these biases. The real-world implementation of these
techniques could be an essential part of ensuring the continued
viability of civil and human rights protections.
Many machine learning fairness practitioners rely on awareness
of sensitive attributes—that is, access to labeled data about people’s
race, ethnicity, sex, or similar demographic characteristics—to test
the efficacy of debiasing techniques or directly implement fairness
interventions. A significant body of research presumes the modeler
has ready access to data on these characteristics as they build and
test their models [13, 19, 21]. The need for this data is plain to see.
As a 2003 analysis of racial disparities in healthcare powerfully
concluded: "The presence of data on race and ethnicity does not,
in and of itself, guarantee any subsequent actions ... to identify
disparities or any actions to reduce or eliminate disparities that are
found. The absence of data, however, essentially guarantees that
none of those actions will occur." [23]
Increasingly, companies that utilize machine learning are being
asked to detect and address bias in their products. But they are not
the first to grapple with these issues. This paper explores the legal
and institutional norms surrounding the collection, inference, and
use of sensitive attribute data in three key corporate domains. This
analysis has significant implications for machine learning fairness
research: If private institutions that mediate access to life opportuni-
ties are unable or hesitant to collect or infer sensitive attribute data,
then emerging awareness-based techniques to detect and mitigate
bias in machine learning models might never be implementable in
real-world settings.
Notably, this paper does not discuss complex and important
questions about how "fairness" should be measured or addressed,
recognizing that definitions are manifold [53]. Rather, we make a
simpler point: If sensitive attribute data are not available, interven-
tions that rely on them will be severely impaired.
We conduct this exploration through the lens of U.S. civil rights
law in the domains of credit, employment, and healthcare. For each
domain, we describe when and how private companies collect or in-
fer sensitive attribute data to pursue antidiscrimination goals. These
are not the only contexts where collection of sensitive attributes is
likely to be justified or important, but they are quintessential areas
where such data are already being used to measure and mitigate
discrimination. They also highlight major divergences in policy,
motivation, and practice.
Comparing these sectors, a complex and inconsistent story emerges.
In credit, the law requires some lenders to collect sensitive attribute
data, while largely prohibiting others from doing so. In employ-
ment, the collection of sensitive attribute data is a familiar part of
large employers’ day-to-day practice. And in health care, compa-
nies’ motivation for collecting sensitive attribute data is not just
basic antidiscrimination compliance, but rather a moral imperative
to address staggering disparities in health outcomes.
We observe that these norms and practices, divergent as they are,
typically extend only to traditionally regulated actors. Technology
companies that mediate access to opportunities as platforms (e.g.,
social networks, job boards, and rental sites) or act as vendors
to other companies rarely receive clear guidance about when to
collect or infer sensitive attribute data. As a result, today, many
major technology companies do not collect or infer certain kinds of
sensitive attribute data and may therefore struggle to define, detect,
and address harms to those protected groups.
We conclude that there are few clear, generally accepted prin-
ciples about when and why companies should collect sensitive
attribute data for antidiscrimination purposes. We emphasize the
importance of the machine learning research community engaging
on the future development of policy in this area, and urge conversa-
tions among stakeholders about whether and how to adapt existing
practices or establish new ones.
1.1 Defining "sensitive attribute data"
Throughout this paper, we use the term "sensitive attribute data" to
refer to details about people’s membership in "protected classes" as
defined throughout U.S. civil rights laws. This approach to classifi-
cation is not without its problems: Rigid categories such as these
do not currently accommodate nonbinary identities or membership
across multiple groups [3, 34, 37, 51]. We acknowledge the reductive
and potentially harmful nature of these classification regimes, while
simultaneously emphasizing the importance of understanding how
they have motivated data collection practices for bias mitigation,
and how the history of these practices can inform contemporary
contexts.
1.