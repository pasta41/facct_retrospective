
Non-profits, as well as the media, have hypothesized the existence
of a radicalization pipeline on YouTube, claiming that users system-
atically progress towards more extreme content on the platform.
Yet, there is to date no substantial quantitative evidence of this
alleged pipeline. To close this gap, we conduct a large-scale audit of
user radicalization on YouTube. We analyze 330,925 videos posted
on 349 channels, which we broadly classified into four types: Media,
the Alt-lite, the Intellectual DarkWeb (I.D.W.), and the Alt-right. Ac-
cording to the aforementioned radicalization hypothesis, channels
in the I.D.W. and the Alt-lite serve as gateways to fringe far-right
ideology, here represented by Alt-right channels. Processing 72M+
comments, we show that the three channel types indeed increas-
ingly share the same user base; that users consistently migrate
from milder to more extreme content; and that a large percentage
of users who consume Alt-right content now consumed Alt-lite
and I.D.W. content in the past. We also probe YouTube’s recom-
mendation algorithm, looking at more than 2M video and channel
recommendations between May/July 2019. We find that Alt-lite con-
tent is easily reachable from I.D.W. channels, while Alt-right videos
are reachable only through channel recommendations. Overall, we
paint a comprehensive picture of user radicalization on YouTube.
CCS CONCEPTS
•Human-centered computing→Empirical studies in collab-
orative and social computing.
KEYWORDS
Radicalization, hate speech, extremism, algorithmic auditing
ACM Reference Format:
Manoel Horta Ribeiro, Raphael Ottoni, Robert West, Virgílio A. F. Almeida,
and Wagner Meira Jr.. 2020. Auditing Radicalization Pathways on YouTube.
In Conference on Fairness, Accountability, and Transparency (FAT* ’20), Jan-
uary 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3351095.3372879
∗Work done mostly while at UFMG.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372879
1 INTRODUCTION
Video channels that discuss social, political and cultural subjects
have flourished on YouTube. Frequently, the videos posted in such
channels focus on highly controversial topics such as race, gender,
and religion. The users who create and post such videos span a wide
spectrum of political orientation, from prolific podcast hosts like
Joe Rogan to outspoken advocates of white supremacy like Richard
Spencer. These individuals not only share the same platform but
often publicly engage in debates and conversations with each other
on the website [24]. This way, even distant personalities can be
linked in chains of pairwise co-appearances. For instance, Joe Rogan
interviewed YouTuber Carl Benjamin [35], who debated with white
supremacist Richard Spencer [6].
According to Lewis [24], this proximity may create “radicaliza-
tion pathways” for audience members and content creators. Exam-
ples of these journeys are plenty, including content creator Roosh
V’s trajectory from pick-up artist to Alt-right supporter [23, 37] and
Caleb Cain’s testimony of his YouTube-driven radicalization [36].
The claim that there is a “radicalization pipeline” on YouTube
should be considered in the context of decreasing trust in main-
stream media and increasing influence of social networks. Across
the globe, individuals are skeptical of traditional media vehicles
and growingly consume news and opinion content on social me-
dia [21, 31]. In this setting, recent research has shown that fringe
websites (e.g., 4chan) and subreddits (e.g., /r/TheDonald) have great
influence over which memes [43] and news [44] are shared in large
social networks, such as Twitter. YouTube is extremely popular,
especially among children and teenagers [5], and if the streaming
website is actually radicalizing individuals this could push fringe
ideologies like white supremacy further into the mainstream [41].
A key issue in dealing with topics like radicalization and hate
speech is the lack of agreement over what is “hateful” or “ex-
treme” [38]. A workaround is to perform analyses based on com-
munities, large sets of loosely associated content creators (here
represented by their YouTube channels). For the purpose of this
work, we consider three “communities” that have been associated
with user radicalization [24, 36, 42] and that differ in the extremity
of their content: the “Intellectual Dark Web” (I.D.W.), the “Alt-lite”
and the “Alt-right”. While users in the I.D.W. discuss controversial
subjects like race and I.Q. [42] without necessarily endorsing ex-
treme views, members of the Alt-right sponsor fringe ideas like
that of a white ethnostate [18]. Somewhere in the middle, individ-
uals of the Alt-lite deny to embrace white supremacist ideology,
although they frequently flirt with concepts associated with it (e.g.,
the “Great Replacement”, globalist conspiracies).
Present work. In this paper, we audit whether users are indeed
becoming radicalized on YouTube and whether the recommenda-
tion algorithms contribute towards this radicalization. We do so
by examining three prominent communities: the Intellectual Dark
Web, the Alt-lite and the Alt-right. More specifically, considering
Alt-right content as a proxy for extreme content, we ask:
RQ1 How have these channels grown on YouTube in the last
decade?
RQ