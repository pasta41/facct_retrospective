
Explainable machine learning offers the potential to provide stake-
holders with insights into model behavior by using various meth-
ods such as feature importance scores, counterfactual explanations,
or influential training data. Yet there is little understanding of how
organizations use these methods in practice. This study explores
how organizations view and use explainability for stakeholder con-
sumption. We find that, currently, the majority of deployments are
not for end users affected by the model but rather for machine
learning engineers, who use explainability to debug the model it-
self. There is thus a gap between explainability in practice and the
goal of transparency, since explanations primarily serve internal
stakeholders rather than external ones. Our study synthesizes the
limitations of current explainability techniques that hamper their
use for end users. To facilitate end user interaction, we develop a
framework for establishing clear goals for explainability. We end
by discussing concerns raised regarding explainability.
CCS CONCEPTS
• Human-centered computing; • Computing methodologies
→ Philosophical/theoretical foundations of artificial intel-
ligence;Machine learning;
KEYWORDS
machine learning, explainability, transparency, deployed systems,
qualitative study
ACM Reference Format:
Umang Bhatt1,2,3,4, Alice Xiang2, Shubham Sharma5, Adrian Weller 3,4,6,
Ankur Taly7, Yunhan Jia8, JoydeepGhosh5,9, Ruchir Puri10, JoséM. F.Moura1,
Peter Eckersley2. 2020. Explainable Machine Learning in Deployment. In
Conference on Fairness, Accountability, and Transparency (FAT* ’20), Jan-
uary 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3351095.3375624
1 INTRODUCTION
Machine learning (ML) models are being increasingly embedded
intomany aspects of daily life, such as healthcare [16], finance [26],
and social media [5]. To build ML models worthy of human trust,
researchers have proposed a variety of techniques for explaining
ML models to stakeholders. Deemed “explainability,” this body of
previous work attempts to illuminate the reasoning used by ML
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of thisworkmust be honored.
For all other uses, contact the owner/author(s).
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6936-7/20/02.
https://doi.org/10.1145/3351095.3375624
models. “Explainability” loosely refers to any technique that helps
the user or developer of ML models understand why models be-
have the way they do. Explanations can come in many forms: from
telling patients which symptoms were indicative of a particular di-
agnosis [35] to helping factory workers analyze inefficiencies in a
production pipeline [17].
Explainability has been touted as away to enhance transparency
of ML models [33]. Transparency includes a wide variety of efforts
to provide stakeholders, particularly end users, with relevant infor-
mation about how a model works [67]. One form of this would be
to publish an algorithm’s code, though this type of transparency
would not provide an intelligible explanation to most users. An-
other form would be to disclose properties of the training pro-
cedure and datasets used [39]. Users, however, are generally not
equipped to be able to understand how raw data and code trans-
late into benefits or harms that might affect them individually. By
providing an explanation for how the model made a decision, ex-
plainability techniques seek to provide transparency directly tar-
geted to human users, often aiming to increase trustworthiness
[44]. The importance of explainability as a concept has been re-
flected in legal and ethical guidelines for data andML [53]. In cases
of automated decision-making, Articles 13-15 of the EuropeanGen-
eral Data Protection Regulation (GDPR) require that data subjects
have access to “meaningful information about the logic involved,
as well as the significance and the envisaged consequences of such
processing for the data subject” [45]. In addition, technology com-
panies have released artificial intelligence (AI) principles that in-
clude transparency as a core value, including notions of explain-
ability, interpretability, or intelligibility [1, 2].
With growing interest in “peering under the hood” of ML mod-
els and in providing explanations to human users, explainability
has become an important subfield of ML. Despite a burgeoning lit-
erature, there has been little work characterizing how explanations
have been deployed by organizations in the real world.
In this paper, we explore how organizations have deployed local
explainability techniques so that we can observe which techniques
work best in practice, report on the shortcomings of existing tech-
niques, and recommend paths for future research. We focus specif-
ically on local explainability techniques since these techniques ex-
plain individual predictions, making them typically the most rele-
vant form of model transparency for end users.
Our study synthesizes interviewswith roughly fifty people from
approximately thirty organizations to understand which explain-
ability techniques are used and how. We report trends from two
sets of interviews and provide recommendations to organizations
deploying explainability. To the best of our knowledge, we are the
first to conduct a study of how explainability techniques are used
by organizations that deploy ML models in their workflows. Our
main contributions are threefold:
• We interview around twenty data scientists, who are not
currently using explainability tools, to understand their or-
ganization’s needs for explainability.
• We interview around thirty different individuals on how
their organizations have deployed explainability techniques,
reporting case studies and takeaways for each technique.
• We suggest a framework for organizations to clarify their
goals for deploying explainability.
The rest of this paper is organized as follows:
(1) We discuss the methodology of our survey in Section 2.
(2) We summarize our overall findings in Section 3.
(3) We detail how local explainability techniques are used at
various organizations and discuss technique-specific take-
aways in Section 4.
(4) We develop a framework for establishing clear goals when
deploying local explainability in Section 5.1 and discuss con-
cerns of explainability in Section 5.2.
