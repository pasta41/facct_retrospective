
The use of word embeddingmodels in sentiment analysis has gained
a lot of traction in the Natural Language Processing (NLP) commu-
nity. However, many inherently neutral word vectors describing
demographic identity have unintended implicit correlations with
negative or positive sentiment, resulting in unfair downstream
machine learning algorithms. We leverage adversarial learning to
decorrelate demographic identity term word vectors with posi-
tive or negative sentiment, and re-embed them into the word em-
beddings. We show that our method effectively minimizes unfair
positive/negative sentiment polarity while retaining the semantic
accuracy of the word embeddings. Furthermore, we show that our
method effectively reduces unfairness in downstream sentiment
regression and can be extended to reduce unfairness in toxicity
classification tasks.
CCS CONCEPTS
• Computing methodologies → Natural language processing.
KEYWORDS
embeddings, fairness, NLP
ACM Reference Format:
Chris Sweeney and Maryam Najafian. 2020. Reducing Sentiment Polarity
for Demographic Attributes in Word Embeddings using Adversarial Learn-
ing. In Conference on Fairness, Accountability, and Transparency (FAT* ’20),
January 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3351095.3372837
1 INTRODUCTION
Sentiment analysis is used in plenty of applications to mine text
data. Ratingmovies, ranking restaurants by reviews, censoring toxic
online comments, etc. all benefit from tools that can infer sentiment
from written text. The creation of word embeddings through algo-
rithms like word2vec [16] has led to increased performance in NLP
tasks like sentiment analysis, but has also served as a new vector
for unintended bias. Recently, works like [5, 6] have shown that
word embeddings can contain many types of unfair biases related
to gender and race. In this work, we focus on word embeddings
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372837
that contain unintended bias due to demographic attributes having
unfair polarization towards positive or negative sentiment. A demo-
graphic attribute is a loose concept where the exact definition can
vary between applications. We focus on demographic attributes in
the form of demographic identity terms. We define a demographic
identity term as a one word descriptor that can be used to assign a
person to a particular demographic. This can range anywhere from
national origin terms like American, Mexican, religious identifiers
like Catholic, Jewish, gendered words such as Female, Male, or even
names that tend to belong to African American demographics like
Darnell, Lakisha. The unequal treatment of demographics via such
textual demographic attributes is concerning given how entangled
sentiment analysis is with domains that directly impact society. As
studied in [19], biased word embeddings can have adverse conse-
quences when deployed in applications such as movie sentiment
analyzers or messaging apps.
In a perfectly fair scenario, we argue that demographic identity
word vectors should be neutral with respect to sentiment. We refer
to the unfair distribution of sentiment among demographic identity
terms as sentiment bias or sentiment polarity. Unfortunately, unlike
gender bias, which has been thoroughly explored in the research
community, sentiment bias is a fairly loose concept. Consequently,
it is a much more difficult problem to decorrelate word vectors
with sentiment, while retaining the usefulness of the embeddings.
However, adversarial learning techniques have recently proven to
be a powerful tool for reducing unintended bias in machine learning
[2, 8, 23, 28]. We reduce sentiment bias at the word embedding level,
using adversarial learning to decorrelate demographic identity term
word vectors with sentiment, thereby, removing sentiment bias at
its source.
Unintended bias, especially in NLP, can enter a machine learning
application through many different sources (i.e word embeddings,
dataset, choice of machine learning algorithm, etc.). However, due
to the ubiquity of pre-trained word embedding models used in NLP,
mitigating bias at this level has the potential for a very large impact.
Focusing in on sentiment analysis applications, practitioners need
to have control over the possibly unfair sentiment associated with
demographic identity terms, without losing the semantic meaning
of the vector space. A tool that decorrelates word vectors with
sentiment is very general and would enable the practitioner to
re-embed chosen words without putting impedance on the prac-
titioner’s downstream algorithms. This is beneficial over further
downstream debiasing algorithms that require the practitioner to
change the learning algorithm or classification thresholds.
This paper starts by reviewing related works in Section 2. In Sec-
tion 3, we formalize the notion of sentiment polarity and present
our adversarial learning algorithm for debiasing word vectors with
respect to sentiment. Section 4 evaluates the effectiveness of our
method. We show that our method increases fairness for sentiment
applications and even toxicity prediction applications via bench-
marks created in [14] and [7]. In Section 5, we discuss possible
future directions for our work. Finally, we conclude this work in
Section 6.
