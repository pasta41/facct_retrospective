
Recent work has documented instances of unfairness in deployed
machine learning models, and signi￿cant researcher e￿ort has been
dedicated to creating algorithms that intrinsically consider fairness.
In this work, we highlight another source of unfairness: market
forces that drive di￿erential investment in the data pipeline for dif-
fering groups. We develop a high-level model to study this question.
First, we show that our model predicts unfairness in a monopoly
setting. Then, we show that under all but the most extreme mod-
els, competition does not eliminate this tendency, and may even
exacerbate it. Finally, we consider two avenues for regulating a
machine-learning driven monopolist - relative error inequality and
absolute error-bounds - and quantify the price of fairness (and who
pays it). These models imply that mitigating fairness concerns may
require policy-driven solutions, not only technological ones.
CCS CONCEPTS
• Theory of computation→Market equilibria;Machine learn-
ing theory; Sample complexity and generalization bounds; Quality of
equilibria; • Applied computing→ Economics.
KEYWORDS
learning theory, algorithmic fairness, data markets, game theory,
industrial organization, economics
ACM Reference Format:
Hadi Elzayn and Benjamin Fish. 2020. The E￿ects of Competition and
Regulation on Error Inequality in Data-Driven Markets. In Conference on
Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020,
Barcelona, Spain. ACM, New York, NY, USA, 18 pages. https://doi.org/10.
1145/3351095.3372842
1 INTRODUCTION
As machine learning has become more integrated into products,
markets, and decision-making throughout society, researchers, prac-
titioners, and activists have identi￿ed many instances of unfairness
in predictions or decisions made by machine-learned models (or by
humans in￿uenced by said models). A large and developing body
*This work was completed while the author was an intern at Microsoft Research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro￿t or commercial advantage and that copies bear this notice and the full citation
on the ￿rst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speci￿c permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/01. . . $15.00
https://doi.org/10.1145/3351095.3372842
of work, which we brie￿y survey in Section 2, has empirically docu-
mented unfairness in practical machine learning settings, identi￿ed
many theoretical sources and mechanisms of unfairness, and con-
structed innovative fairness-aware algorithms. Researchers have
developed many innovative technical solutions to these problems,
yet the issue in practice remains far from solved. This paper high-
lights a simple and important point: while technical solutions to
unfairness are certainly important, mitigating unfairness in practice
may require tackling economic incentives promoting unfairness.
Most of the existing literature assumes that a ￿xed dataset, pos-
sibly biased, arrives in the hands of a data scientist, and solutions
often revolve around clever ways to mitigate this bias. In practice,
however, economic incentives may create disparities well before
the data scientist enters the picture. Consider, for example, the task
of speech recognition: producing accurate models may require a
large amount of data, and data from speakers with accented or
rarer dialects may be more costly to collect. If the market size of a
minority group is small relative to the costs a ￿rm would expend
in developing accurate speech recognition software, it is likely that
the group will be served with lower quality products.
In this paper, we model the unfairness that arises when data-
driven, pro￿t-maximizing ￿rms choose to di￿erentially invest in
data collection across groups, creating unequal error rates. In order
to focus on this speci￿c source of unfairness, we use a simple
framework that elides the many other sources of bias that can seep
into the machine learning pipeline. For instance, we assume that
￿rms have unlimited budgets to purchase data at a cost from group-
speci￿c data sources of potentially in￿nite quantity. We also assume
that both ￿rms and users bene￿t from more accurate models, so
that incentives are aligned. Furthermore, we assume that ￿rms must
build separate models for each group, to avoid unfairness that may
come from ￿tting to the majority.
In order to construct our models, we borrow from the tools of
learning theory and microeconomics to build simple, stylized mod-
els with crisp predictions of quanti￿able unfairness. We assume
each pro￿t-maximizing ￿rm faces a known demand curve as a func-
tion of the worst-case error rates for each group. Standard results
from learning theory allow us to model worst-case error rates as
a function of the amount of data the ￿rm buys. We investigate
three models of demand: linear demand, demand proportional to
error rates, and (approximately) rational demand. For the precise
description of our models and these assumptions, see Section 3.
We show in Section 4 that a pro￿t-maximizing monopolist will
choose to serve minorities (as de￿ned by their market power) with
lower quality models. Assuming linear demand, an oft-used bench-
mark in the economics literature, we quantify the di￿erence in
relative model quality between groups as a function of their market
size, elasticity, and cost of data.
We then consider two classical remedies to the ills of monop-
olies: competition and regulation. Under two natural models of
competition – multilinear demand (Section 5.1) and proportional
demand (Section 5.2) – introducing competition does not mitigate
inequality, and proportional demand even exacerbates it. Only a
model in which all consumers choose the ￿rm with (even in￿nitesi-
mally) smaller error until ￿rms reach su￿cient accuracy suggests
that competition will mitigate inequality (Section 5.3); to do so,
however, this model assumes a stringent notion of rationality that
may not be re￿ective of consumer behavior in the real world.
Given that our most plausible models suggest that competition
does improve the situation, we askwhether regulation could be used
to mitigate error inequality by design. In particular, in Section 6
we examine two simple kinds of constraints: a ‘relative equality’
constraint where error rates across groups must be multiplicatively
close to each other, and an ‘absolute equality’ constraint where
error rates across all groups must be su￿ciently small, but may
be far apart from each other. We then formally quantify the costs
to pro￿ts (and when relevant, to the majority group’s error rate)
as a function of the threshold chosen. Finally, we conclude with
takeaways, limitations, and directions for future work in Section 7.
