
Today, AI is being increasingly used to help human experts make
decisions in high-stakes scenarios. In these scenarios, full automa-
tion is often undesirable, not only due to the signiicance of the
outcome, but also because human experts can draw on their domain
knowledge complementary to the model’s to ensure task success.
We refer to these scenarios as AI-assisted decision making, where
the individual strengths of the human and the AI come together
to optimize the joint decision outcome. A key to their success is
to appropriately calibrate human trust in the AI on a case-by-case
basis; knowing when to trust or distrust the AI allows the human
expert to appropriately apply their knowledge, improving decision
outcomes in cases where the model is likely to perform poorly. This
research conducts a case study of AI-assisted decision making in
which humans and AI have comparable performance alone, and ex-
plores whether features that reveal case-speciic model information
can calibrate trust and improve the joint performance of the hu-
man and AI. Speciically, we study the efect of showing conidence
score and local explanation for a particular prediction. Through
two human experiments, we show that conidence score can help
calibrate people’s trust in an AI model, but trust calibration alone is
not suicient to improve AI-assisted decision making, which may
also depend on whether the human can bring in enough unique
knowledge to complement the AI’s errors. We also highlight the
problems in using local explanation for AI-assisted decision mak-
ing scenarios and invite the research community to explore new
approaches to explainability for calibrating human trust in AI.
KEYWORDS
decision support, trust, conidence, explainable AI
ACM Reference Format:
Yunfeng Zhang, Q. Vera Liao, and Rachel K. E. Bellamy. 2020. Efect of Con-
idence and Explanation on Accuracy and Trust Calibration in AI-Assisted
DecisionMaking. InConference on Fairness, Accountability, and Transparency
(FAT* ’20), January 27ś30, 2020, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3351095.3372852
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proit or commercial advantage and that copies bear this notice and the full citation
on the irst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciic permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27ś30, 2020, Barcelona, Spain
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372852
1 INTRODUCTION
Artiicial Intelligence (AI) technologies, especially Machine Learn-
ing (ML), have become ubiquitous and are increasingly used in a
wide range of tasks. While algorithms can perform impressively, in
many situations full delegation to ML models is not desired because
their probabilistic nature means that there is never a guarantee
of correctness for a particular decision. Furthermore, ML models
are only as accurate as the historical data used to train them, and
this data could sufer from input error, unknown laws, and biases.
ML models can assist human decision-makers to produce a joint
decision outcome that is hopefully better than what could be pro-
duced by either the model or human alone. Ultimately however,
humans would be responsible for the decisions made. Therefore
ML decision-support applications should be developed not only
with the goal of high performance, safety and fairness, but also
allowing the decision-maker to understand the predictions made by
the model. This is especially important for decision-making in high-
stakes situations afecting human lives such as medical diagnosis,
law enforcement, and inancial investment.
A key to success in AI-assisted decision making is to form a cor-
rect mental model of the model’s error boundaries [2]. That is, the
decision-makers need to know when to trust or distrust the model’s
recommendations. If they mistakenly follow the model’s recom-
mendations at times when it is likely to err, the decision outcome
would sufer, and catastrophic failures could happen in high-stakes
decisions. Many have called out the challenges for humans to form
a clear mental model of an AI, since opaque, "black-box" models are
increasingly used. Furthermore, by exclusively focusing on opti-
mizing model performance, developers of AI systems often neglect
the system users’ needs for developing a good mental model of
the AI’s error boundaries. For example, frequently updating the AI
algorithm may cause confusion to the human decision-maker, who
may accept or reject the AI’s recommendations at a wrong time,
even if the algorithm’s overall performance improved [2].
To help people develop a mental model of an ML model’s error
boundaries means to correctly calibrate trust on a case-by-case basis.
We emphasize that this goal is distinct from enhancing trust in AI.
For example, while research repeatedly demonstrates that providing
high-performance indicators of an AI system, such as showing high
accuracy scores, could enhance people’s trust and acceptance of the
system [17, 30, 32], they may not help people distinguish cases they
can trust from those they should not. Meanwhile, ML is probabilistic
and the probability of each single prediction can be indicated by a
conidence score. In other words, the conidence scores relect the
chances that the AI is correct. Therefore, to optimize for the joint
decisions, in theory people should rely on the AI in cases where it
has high conidence, and use their own judgment in cases where
it has low conidence. However, in practice, we know little about
how conidence scores are perceived by people, or how they impact
human trust and actions in AI-assisted decisions.
To improve people’s distrust in ML models, many considered the
importance of transparency by providing explanations for the ML
model [4, 9, 28]. In particular, local explanations that explain the
rationale for a single prediction (in contrast to global explanations
describing the overall logic of the model) are recommended to help
people judge whether to trust a model on a case-by-case basis [28].
For example, many local explanation techniques explain a predic-
tion by how each attribute of the case contributes to the model’s
prediction [19, 28]. It is possible that in low-certainty cases none of
the features stands out to make strong contributions. So the expla-
nation may appear ambivalent, thus alarming people to distrust the
prediction. While such a motivation to help people calibrate trust
underlies the development of local explanation techniques, to the
best of our knowledge, this assumption has not been empirically
tested in the context of AI-assisted decision making.
In this paper, we conduct a case study of AI-assisted decision-
making and examine the impact of information designs that re-
veal case-speciic model information, including conidence score
and local explanation, on people’s trust in the AI and the decision
outcome. We explored two types of AI-assisted decision-making
scenarios. One where the AI gave direct recommendation, and the
other where the decision-maker had to choose whether to delegate
the decision without seeing the AI’s prediction, the latter of which
represents a stricter test of trust. We designed the study in a way
to have the human decision-makers performing comparably to the
AI, and also explored a situation where the humans know they
had more domain knowledge than the AI. In contrast, prior works
studying AI-assisted decision-making often used setups where hu-
mans’ decision performance was signiicantly inferior than the
model’s [17, 30], which would by default reward people to rely
on the AI. While such a setup is appropriate for studying how to
enhance trust in AI, our focus is to study the calibration of trust
for cases where the AI has high or low certainty. This paper makes
the following contribution:
• We highlight the problem of trust calibration in AI at a
prediction speciic level, which is especially important to the
success of AI-assisted decision-making.
• Wedemonstrate that showing prediction speciic conidence
information could support trust calibration, even in situa-
tions where the human has to blindly delegate the decision
to the AI. However, whether trust calibration could translate
into improved joint decision outcome may depend on other
factors, such as whether the human can bring in a unique
set of knowledge that complements the AI’s errors. We con-
sider the concept of error boundary alignment between the
human and the AI, and its implication for studying diferent
AI-assisted decision making scenarios.
• We show that local, prediction speciic explanations may
not be able to create a perceivable efect for trust calibration,
even though theywere theoretically motivated for such tasks.
We discuss the limitations of the explanation design we used,
and future directions for developing explanations that can
better support trust calibration.
