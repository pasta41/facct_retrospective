
Word embeddings are a widely used set of natural language process-
ing techniques that map words to vectors of real numbers. These
vectors are used to improve the quality of generative and predictive
models. Recent studies demonstrate that word embeddings con-
tain and amplify biases present in data, such as stereotypes and
prejudice. In this study, we provide a complete overview of bias in
word embeddings. We develop a new technique for bias detection
for gendered languages and use it to compare bias in embeddings
trained on Wikipedia and on political social media data. We investi-
gate bias diffusion and prove that existing biases are transferred to
further machine learning models. We test two techniques for bias
mitigation and show that the generally proposed methodology for
debiasing models at the embeddings level is insufficient. Finally,
we employ biased word embeddings and illustrate that they can
be used for the detection of similar biases in new data. Given that
word embeddings are widely used by commercial companies, we
discuss the challenges and required actions towards fair algorithmic
implementations and applications.
CCS CONCEPTS
•Human-centered computing→HCI design and evaluation
methods; • Computing methodologies→Machine learning;
• Information systems → Data mining.
KEYWORDS
word embeddings, bias, detection, diffusion, mitigation, fairness,
sexism, racism, homophobia
ACM Reference Format:
Orestis Papakyriakopoulos, Simon Hegelich, Juan Carlos Medina Serrano,
and Fabienne Marco. 2020. Bias in Word Embeddings. In Conference on
Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020,
Barcelona, Spain. ACM, Barcelona, Spain, 12 pages. https://doi.org/10.1145/
3351095.3372843
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6936-7/20/02.
https://doi.org/10.1145/3351095.3372843
1 INTRODUCTION
The growing ubiquity of algorithms in society poses questions
about their social, political, and ethical consequences [77]. One of
the issues research focuses on is algorithmic bias, which denotes the
deviation of the algorithmic results from specific social expectations,
based on epistemic or normative reasons [75].
Prior research has shown that algorithmic bias might result in
unfair or discriminative decisions and statements, initiating a multi-
level debate on the ethical use of algorithms [62, 102]. Under that
framework, researchers, decision makers and institutions try to
answer the following questions:
• What definitions of fairness and discrimination are appro-
priate and under what conditions? [15, 62]
• At which part of an algorithm does bias emerge and in what
form? [42, 85, 93]
• What are the actual consequences of biased algorithms and
who is accountable for them? [6, 68, 76]
• How can researchers and decision makers mitigate the de-
tected bias? [8, 13]
Problem Statement
This study investigates bias in word embeddings, a set of natural
language processing techniques for the mapping of words into nu-
merical vectors. These vectors can then be used for the improvement
of the predictions and inferences of other machine learning models
[91]. Previous work has proven that word embeddings contain bias
[13], and researchers have already developed methodologies for
tracing, quantifying, andmitigating it [12, 16]. Recently, researchers
have also started to develop methods for comparing biases existing
in different datasets [40, 64].
Despite recent scientific findings, computer scientists in the in-
dustry widely use word embeddings for the development of highly
accurate models that perform text generation, translation, classifi-
cation and regression, without taking into consideration the impact
of their inherent biases. Similarly, researchers have not yet inves-
tigated the diffusion and impact of biased word embeddings on
further machine learning algorithms. Therefore, we want to pro-
vide a complete overview of bias in word embeddings: its detection
in the embeddings, its diffusion in algorithms using the embed-
dings, and its mitigation at the embeddings level and at the level
of the algorithm that uses them. We also investigate whether the
employment of biased word embeddings contributes to the location
of the bias in new data. The study raises additional awareness about
a technique, whose implementation can lead to unfair algorithmic
decisions and inferences. We achieve this, by seeking the answer
to the following research questions:
RQ1: How canwe evaluate andmitigate theword embed-
dings’ bias diffusion in furthermachine learning algo-
rithms?
RQ2: Can we employ bias in word embeddings for trac-
ing bias in new data?
Original Contribution
• We train state-of-the-art word embeddings based on the
German version of Wikipedia and on unique social media
data in the German language. For that, we gather over 22
million tweets and Facebook comments related to German
politics. We develop a new method for locating biases in
gendered languages, trace niches of sexist, xenophobic and
homophobic prejudice and stereotypes on the two sets of
vectors, and quantify the overall bias for each dataset.
• We transform and compare the vector spaces without distort-
ing the immanent bias by borrowing techniques from em-
beddings translation [87]. We then compare the spaces and
prove that the social media data containd a higher level of in-
tergroup prejudice, while Wikipedia data contain a stronger
bias in terms of stereotypes.
• We create a sentiment classifier based on the two embedding
datasets and show how the model replicates bias immanent
in the embeddings.
• We compare methodologies to mitigate bias without distort-
ing the accuracy of the classifier. We compare debiasing at
the embeddings level and at the level of the classifier. We
illustrate that the standard technique for mitigating bias at
the embeddings level [13] is insufficient for removing biases
completely.
• We develop a new sexism dataset by labeling 100.000 Face-
book comments as sexist or neutral and illustrate that embed-
dings with bias similar to the one in the target data perform
better on the classification task.
• Finally, we discuss the issues, possibilities and challenges
that accompany the use of biased word embeddings.
Paper Organization
The paper is organized as follows. Section 2 presents the theoretic
background and related work. Section 3 describes the data and
methodology we followed. Section 4 presents the results. Section
5 discusses the results, demonstrates the implications of the study
and concludes the analysis.
