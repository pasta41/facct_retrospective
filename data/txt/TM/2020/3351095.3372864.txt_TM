
A distinction has been drawn in fair machine learning research
between ‘group’ and ‘individual’ fairness measures. Many technical
research papers assume that both are important, but conflicting,
and propose ways to minimise the trade-offs between these mea-
sures. This paper argues that this apparent conflict is based on a
misconception. It draws on discussions from within the fair ma-
chine learning research, and from political and legal philosophy,
to argue that individual and group fairness are not fundamentally
in conflict. First, it outlines accounts of egalitarian fairness which
encompass plausible motivations for both group and individual fair-
ness, thereby suggesting that there need be no conflict in principle.
Second, it considers the concept of individual justice, from legal
philosophy and jurisprudence, which seems similar but actually
contradicts the notion of individual fairness as proposed in the fair
machine learning literature. The conclusion is that the apparent
conflict between individual and group fairness is more of an artefact
of the blunt application of fairness measures, rather than a matter
of conflicting principles. In practice, this conflict may be resolved
by a nuanced consideration of the sources of ‘unfairness’ in a par-
ticular deployment context, and the carefully justified application
of measures to mitigate it.
CCS CONCEPTS
• Social and professional topics → Computing / technology
policy; Codes of ethics; •Computingmethodologies→Machine
learning; • Applied computing → Law; Sociology.
KEYWORDS
fairness, individual fairness, justice, machine learning, discrimina-
tion, statistical parity
ACM Reference Format:
Reuben Binns. 2020. On the Apparent Conflict Between Individual and
Group Fairness. In Conference on Fairness, Accountability, and Transparency
(FAT* ’20), January 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3351095.3372864
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372864
1 INTRODUCTION
Research on ‘fair’ machine learning (hereafter: Fair-ML) has pro-
posed a variety of ways to assess and ensure the fairness of super-
vised models for prediction and classification. These are largely
focused on the application ofML in decision-making contexts which
involve allocating more or less positive and negative outcomes to
people, on the basis of predictions based on a person’s features.
Imagine the following decision-making scenario. An employer
must select candidates for interview from a large set of applica-
tions. They decide to use a machine learning model which uses
information contained in an application and returns a prediction
about whether a candidate would make a good employee. Those
whose applications have sufficiently positive predictions are invited
to interview.
The employer finds that the model is more likely to give positive
predictions for one gender (e.g. men) than others (e.g. women and
non-binary people). So they decide to apply some Fair-ML tech-
nique to the model to prevent this. However, as a result of this
modification, a man applicant is not invited to an interview. The
applicant complains, pointing to examples of women who were
invited to interview despite having qualifications very similar to his.
Should the employer continue to interview the women candidates,
or adjust its model again to ensure that any ‘more qualified’ men
get interviews instead?
Dilemmas like this one have been raised within discussions of
Fair-ML to motivate a supposedly important distinction between
two competing kinds of fairness. On the one hand, group fairness
ensures some form of statistical parity (e.g. between positive out-
comes, or errors) for members of different protected groups (e.g.
gender or race) [18]. On the other hand, individual fairness ensures
that people who are ‘similar’ with respect to the classification task
receive similar outcomes. These measures appear to conflict in
cases where, as a result of trying to satisfy group fairness, pairs
of individuals who are otherwise similar but differ in a protected
characteristic are assigned different outcomes.
The purpose of this paper is to critically assess the apparent
conflict between these two kinds of fairness measure. While the
debate that follows is primarily theoretical, its implications are
significant both for researchers and practitioners:
• For Fair-ML researchers, the apparent conflict between in-
dividual and group measures continues to motivate new
research papers many years after it was initially formulated
(in [18]). While many such papers propose technical means
to reconcile the two measures, they generally lack sustained
theoretical discussion regarding the underlying principles
supposedly in conflict.
• For practitioners, as ML models continue to be applied in
a wide variety of applications in the public and private sec-
tor, decision-makers will turn towards governance measures
proposed by the Fair-ML community. In particular, the intu-
itions which motivated the proposal of individual fairness in
the academic debate - namely, that ‘less qualified’ individuals
may unfairly be given opportunities in order that the model
can acheive some statistical measure of group fairness - may
lead some to object to particular implementations of group
fairness in practice. It is therefore essential that organisations
deploying such measures can provide sound justifications
for the selection of particular fairness measures.
Such justifications will require careful consideration of the prin-
ciples behind, and relationship between, group and individual fair-
ness. The contention of this paper is that the supposed conflict
between the measures does not arise at the level of principle; these
are not two fundamentally different and conflicting types of fairness.
Rather, the appearance of conflict between the two is an artefact
of the failure to fully articulate assumptions behind them, and the
reasons for applying them in a particular context.
1.1 Overview of argument and contributions
Section 2 introduces the notions of individual and group fairness as
they have been developed in the Fair-ML literature, and elucidates
their perceived merits and shortcomings, and explains why it has
generally been assumed that they are in tension with each other. It
also provides an overview of various recent proposals to minimise
the trade-offs between the two measures.
Section 3 aims to map the relationship between these fairness
measures and a range of concepts and theories from moral, politi-
cal, and legal philosophy that they might be plausibly thought to
correspond to. I argue that while there are differences in the way
individual and group fairness are applied in specific contexts, they
don’t necessarily correspond to distinct and conflicting principles.
I argue that, at this abstract level, individual and group fairness
are not only not in conflict, but are in fact just different ways of
reflecting the same set of moral and political concerns.
In section 4, I pursue a complementary line of argument, which
examines a possible underlying motivation for individual fairness,
namely: individualized or particularized justice. This is the idea that
people deserve to be treated as unique individuals, and assessed
on a case-by-case basis. Like individual fairness, this concept is
grounded in the treatment of individuals. However, on this view,
even supposedly individually-fair models can be seen as individu-
ally unjust because they still generalise between people who share
the same features. In this sense, what passes for individual fairness
actually amounts to a special case of group fairness. So again the
apparent distinction disappears when considered at the level of
principle.
Section 5 aims to persuade the reader that in so far as fairness
measures can be applied in particular contexts, there are very few
such contexts in which what have been termed ‘individual’ and
‘group’ fairness measures would be simultaneously applicable and
conflicting. In so far as they may appear to conflict in a given cir-
cumstance, this will be down to unstated conflicting moral and em-
pirical assumptions regarding the purpose of the decision-making
procedure and the nature of the fairness concerns inherent in that
particular context. This section also connects this argument to a
similar one made in [22].
In the interests of promoting interdisciplinary dialogue, to satisfy
readers from both computer science and social science / humanities,
I avoid formal notation, in favour of prose descriptions of technical
definitions.
2 ‘INDIVIDUAL’ AND ‘GROUP’ FAIRNESS IN
FAIR-ML
This section introduces the notions of individual and group fairness
as they have been developed in the Fair-ML literature, and elucidates
their perceived merits and shortcomings, and why it has generally
been assumed that they are in tension with each other. It also
provides an overview of a variety of recent proposals to minimise
the trade-offs between the two measures.
2.1 Initial formulations of group and
individual fairness
Research on bias, fairness and discrimination in socio-technical
systems has a long history which significantly predates specific
work on fairness in machine learning (see e.g. [23, 24]). However,
such work did not propose formalised quantitative measures of
fairness.1 The first examples of Fair-ML fairness definitions arose
in the field initially known as ‘discrimination-aware data mining’
in the mid 2000’s.
2.1.1 Group fairness measures. These early papers used fairness
measures based on statistical parity between protected groups (e.g.
gender, race) in each outcome class, and hence are classed as group
fairness measures [10, 39].
According to statistical parity, a classifier is fair if there are equal
proportions of each protected group in each outcome class. A wider
range of different group fairness measures (also known as ‘sta-
tistical’ measures) have since been proposed. They are all based
on some measure of statistical parity between people who have
different values for a set of protected attributes. Some look at the
distribution of errors between groups, e.g. false negative and posi-
tive rates [11]. Other group fairness measures focus on calibration:
e.g., a model is fair if it is equally calibrated between members of
different protected groups (where calibration measures how closely
the model’s estimation of the likelihood of something happening
corresponds to the actual frequency of the event happening) [35].
One problem that is often raised for group fairness measures
is that they are only suited to a limited number of coarse-grained,
prescribed protected groups. They may miss unfairness against
people as a result of their being at the intersection of multiple
kinds of discrimination [15], or groups which are not (yet) defined
in anti-discrimination law but may need protecting [49]. If group
measures are only applied to coarse-grained groups separately
(e.g. gender, race), they might permit unfairness for structured
combinations of those groups (e.g. black women), also known as
fairness gerrymandering [34]. Some proposals aim to solve this
1Formalised definitions of fairness which foreshadow those in Fair-ML did appear
in areas other than ML, such as testing in education and hiring. See Hutchinson &
Mitchell 2019 [30].
problem by defining fairness across many different combinations
of protected characteristics. A challenge here is that testing on
every possible subgroup doesn’t scale well to the large number of
possible combinations, and may lead to overfitting [25, 34]. While
these works are in part motivated by notions of individual fairness,
since they still deal with groups (albeit potentially very fine-grained
intersectional groups) they can still be seen as a form of group
fairness measure.
2.1.