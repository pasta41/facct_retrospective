
Discriminatory practices in recruitment and hiring are an ongoing
issue that is a concern not just for workplace relations, but also
for wider understandings of economic justice and inequality. The
ability to get and keep a job is a key aspect of participating in
society and sustaining livelihoods. Yet the way decisions are made
on who is eligible for jobs, and why, are rapidly changing with the
advent and growth in uptake of automated hiring systems (AHSs)
powered by data-driven tools. Evidence of the extent of this uptake
around the globe is scarce, but a recent report estimated that 98%
of Fortune 500 companies use Applicant Tracking Systems of some
kind in their hiring process, a trend driven by perceived efficiency
measures and cost-savings. Key concerns about such AHSs include
the lack of transparency and potential limitation of access to jobs
for specific profiles. In relation to the latter, however, several of
these AHSs claim to detect and mitigate discriminatory practices
against protected groups and promote diversity and inclusion at
work. Yet whilst these tools have a growing user-base around the
world, such claims of ‘bias mitigation’ are rarely scrutinised and
evaluated, and when done so, have almost exclusively been from a
US socio-legal perspective.
In this paper, we introduce a perspective outside the US by criti-
cally examining how three prominent automated hiring systems
(AHSs) in regular use in the UK, HireVue, Pymetrics and Applied,
understand and attempt to mitigate bias and discrimination. These
systems have been chosen as they explicitly claim to address issues
of discrimination in hiring and, unlike many of their competitors,
provide some information about how their systems work that can
inform an analysis. Using publicly available documents, we describe
how their tools are designed, validated and audited for bias, high-
lighting assumptions and limitations, before situating these in the
socio-legal context of the UK. The UK has a very different legal back-
ground to the US in terms not only of hiring and equality law, but
also in terms of data protection (DP) law. We argue that this might
be important for addressing concerns about transparency and could
mean a challenge to building bias mitigation into AHSs definitively
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6936-7/20/01.
https://doi.org/10.1145/3351095.3372849
capable of meeting EU legal standards. This is significant as these
AHSs, especially those developed in the US, may obscure rather
than improve systemic discrimination in the workplace.
CCS CONCEPTS
• Social and professional topics → Socio-technical systems;
Systems analysis and design; • Applied computing→ Law; Soci-
ology.
KEYWORDS
Socio-technical systems, automated hiring, algorithmic decision-
making, fairness, discrimination, GDPR, social justice
ACM Reference Format:
Javier Sánchez-Monedero, Lina Dencik, and Lilian Edwards. 2020. What
does it mean to ‘solve’ the problem of discrimination in hiring? Social, tech-
nical and legal perspectives from the UK on automated hiring systems. In
Conference on Fairness, Accountability, and Transparency (FAT* ’20), Jan-
uary 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3351095.3372849
1 INTRODUCTION
The use of data systems and automated decision-making as a way
to monitor, allocate, assess and manage labour is a growing feature
of the contemporary workplace. Of increasing significance is the
way Human Resources, and hiring practices in particular, are be-
ing transformed through various forms of automation and the use
of data-driven technologies which we will collectively term Auto-
mated Hiring Systems (AHSs) [15, 41, 48]. Whilst there is a lack of
data on the global uptake of such technologies, a recent report esti-
mated that 98% of Fortune 500 companies use Applicant Tracking
Systems of some kind in their hiring process [45]. The so-called ‘hir-
ing funnel’ [15] consists of sourcing, screening, interviewing, and
selection/rejection as a set of progressive filtering stages to identify
and recruit the most suitable candidates. Each of these stages are
undergoing forms of automation, as part of not only perceived
efficiency measures and cost-savings that data-driven technologies
afford, but also as a means to detect and mitigate discriminatory
practices against protected groups and promoting diversity and
inclusion at work [15]. Hiring platforms such as PeopleStrong or
TribePad implement basic measures to mitigate human unconscious
biases, such as anonymisation of candidates, while other platforms
such as HireVue, Pymetrics and Applied1 claim to specifically tackle
1Note that Applied does not automate the evaluation of candidates but it assists in the
process of bias discovery and mitigation.
the problem of discrimination in hiring. Yet the basis upon which
such claims are made is rarely scrutinised and evaluated. While
algorithmic bias generally and in employment law specifically [14]
has had extensive investigation in the FAT* community, literature
on bias mitigation in AHSs is at an early stage and so far primarily
focused on the US context of employment both socially and legally
[7, 15, 40]. We know much less about how this phenomena is devel-
oping in Europe [8]. This is the first scholarly attempt to consider
the question of how satisfactorily bias and discrimination might be
mitigated in AHSs within the UK context.2
We start by outlining how data-driven technologies are trans-
forming hiring practices, before turning to focus on three AHSs
widely in use in the UK which make claims to deal with bias and
whose claims could be evaluated using publicly available materi-
als such as company white papers and reports, patents, marketing
resources, seminars and, in one case, source code. Access to fur-
ther information relating to code, data sets, features design, trained
models, or even the application user interface was not possible,
and will often vary depending on client. Based on this publicly
available material that outline their technological and procedural
frameworks, we examine how these products implement bias dis-
covery and mitigation. In doing so, and in building on recent work
in this area [5, 15, 40, 48] we explore the assumptions made about
the meaning of bias and discrimination in hiring practices embed-
ded within these tools. The three prominent systems we examined
were Pymetrics, HireVue and Applied. For each of these, we outline
the design of the tool, focusing on how bias in hiring is addressed.
The first two of these come from the US, and the third, Applied,
from the UK, for comparative purposes. Other UK systems were
considered but either made no claims as to debiasing, or did not
make available sufficient public material to analyse. Although not
rigorously addressed, this gap, either in implementation of debi-
asing, or its disclosure, is in itself, we feel, a significant finding
deserving further research.
Next we briefly discuss the background to UK and EU equal-
ity law in the context of employment. Importantly, we raise the
point that in the EU, all natural persons whose personal data is
processed are given data rights, including rights to transparency
and protection against power imbalance, which may be as useful
in combating bias in AHSs as employment equality rights; these
data protection (DP) rights, ensconced in the EU General Data Pro-
tection Regulation (GDPR), are not found in omnibus fashion in
the US and thus have largely not been analysed in the US hiring
algorithm literature.
In analysing these systems, we started from the premise that
there is no singular or unified way of interpreting the meaning
of discrimination, or how it might feature in hiring practices, nor
is there consensus on any computational criteria for how “bias”
should be defined, made explicit, or mitigated [22, 37, 40, 47, 54].
Ongoing analysis point out issues such as the unsuitability of maths
2In this paper we are not looking at the general scientific validation of the system
design as we do not have access to any independent studies of that kind. Instead,
we are focusing on how ‘bias’ and discrimination is understood in the design. Our
assessment does therefore not directly engage with the effectiveness of using AHSs to
evaluate candidates, even though issues of discrimination would be very pertinent in
such an assessment.
to capture the full meaning of social concepts such as fairness, es-
pecially in a general sense, or the risk of technological solutionism
[43]. Even when considering one statistical definition for bias such
as the error rate balance amongst groups, the understanding and
implication of that approach radically varies with the context and
the consequential decisions that are driven by the algorithmic out-
put [26]. Also, all sociotechnical systems, even when designed to
mitigate biases, are designed with use cases in mind that may not
hold in all scenarios [43]. Moreover, fairness can be procedural, as
in the equal treatment of individuals, or substantive, as in the equal
impact on groups [14], what is also referred to as opportunity-based
vs. outcome-based notions of bias. These do not necessarily align
and may actually be contradictory [33].
We conclude by arguing that, while common practices may be
emerging to mitigate bias in AHSs built in the US, these are in-
evitably likely to reflect US legal and societal conceptions of bias
and discrimination, and yet are exported wholesale as products
to UK and other markets beyond the US. If this mismatch is not
explicitly disclosed and analysed, there is a patent danger that inap-
propriate US-centric values and laws relating to bias in hiring (and
more generally in society) may be exported to UK workplaces. Data
protection rights, such as the alleged “right to an explanation” may
by contrast not be implemented, potentially rendering useless the
rights of candidates. What is more, while biased workplace hiring
practices in traditional modes of hiring may at least to some extent
be evident, and combatted in traditional ways by unions, strategic
litigation and regulators, there are severe dangers that such biases
buried within “black box” AHSs may not be manifestly obvious
and so remain unchallenged. As AHSs become ever more popular,
especially in sectors which are precarious, such as retail and the
gig economy where prospective employees (or contractors) may
have little economic power or knowledge of rights [7, 15], this is,
we argue, a serious problem.
