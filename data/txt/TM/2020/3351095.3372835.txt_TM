
Search engines, by ranking a few links ahead of million others
based on opaque rules, open themselves up to criticism of bias.
Previous research has focused on measuring political bias of search
engine algorithms to detect possible search engine manipulation
effects on voters or unbalanced ideological representation in search
results. Insofar that these concerns are related to the principle of
fairness, this notion of fairness can be seen as explicitly oriented
toward election candidates or political processes and only implic-
itly oriented toward the public at large. Thus, we ask the following
research question: how should an auditing framework that is explic-
itly centered on the principle of ensuring and maximizing fairness
for the public (i.e., voters) operate? To answer this question, we
qualitatively explore four datasets about elections and politics in
the United States: 1) a survey of eligible U.S. voters about their
information needs ahead of the 2018 U.S. elections, 2) a dataset of
biased political phrases used in a large-scale Google audit ahead of
the 2018 U.S. election, 3) Google’s “related searches” phrases for two
groups of political candidates in the 2018 U.S. election (one group
is composed entirely of women), and 4) autocomplete suggestions
and result pages for a set of searches on the day of a statewide
election in the U.S. state of Virginia in 2019. We find that voters
have much broader information needs than the search engine audit
literature has accounted for in the past, and that relying on political
science theories of voter modeling provides a good starting point
for informing the design of voter-centered audits.
CCS CONCEPTS
• Information systems →Web search engines.
KEYWORDS
algorithm audits, search engines, Google, voters, elections, bias
ACM Reference Format:
Eni Mustafaraj, Emma Lurie, and Claire Devine. 2020. The Case for Voter-
Centered Audits of Search Engines During Political Elections. In Conference
on Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020,
Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3351095.3372835
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372835
1 INTRODUCTION
Aweek after the 2016 U.S. presidential election, a news story1 about
a problematic Google search result page made the media rounds.
The journalist had searched for the query “final vote count 2016”,
and Google’s top search result had claimed that “Trump won both
[the] popular [vote] and electoral college.” (President Trump lost
the popular vote by 2.87 million votes.2)
This top search result was published by a conspiracy blog, one
of the dubious sources that contributed to the creation and spread
of a conspiracy theory (repeated by President Trump) that Trump
would have won the popular vote if not for voting irregularities
(unauthorized immigrant voters, voter fraud, etc.).3
How did a false news story from a conspiracy blog end up as
the top news story on the Google search results page? Google’s
response, through a spokesperson,4 was non-revealing:
The goal of Search is to provide the most relevant and
useful results for our users. In this case we clearly
didn’t get it right, but we are continually working to
improve our algorithms.
Other questions that can be asked in this context are: how many
users searched Google for this topic (“final vote count 2016”)? How
many users saw the conspiracy blog story as the top result? What
fraction of users clicked to read it? These questions, aimed at trans-
parency, are important when trying to understand how disinfor-
mation spreads and affects users, in order to mitigate its potential
harm. For example, Twitter notified 1.4 million users in the United
States, that they were exposed to content generated by Russia’s
Internet Research Agency during the 2016 U.S. Election.5 Although
journalists have documented several examples of disinformation6
or other harmful content7 being displayed at the top of Google
Search results, Google’s response has been to change the algorithms
to fix the particular problem, without providing transparency to
stakeholders.
This lack of transparency about the how of such scenarios is
often defended as a protective measure: malicious third-party actors
could benefit from efforts of transparency to increase the success
1https://www.mediaite.com/uncategorized/now-even-google-search-aiding-in-
scourge-of-fake-inaccurate-news-about-election-2016/
2https://en.wikipedia.org/wiki/2016_United_States_presidential_election
3New York Times: Trump Repeats Lie about Popular Vote in Meeting with Lawmakers
(January 23, 2017).
4https://www.theverge.com/2016/11/14/13622566/google-search-fake-news-election-
results-algorithm
5https://www.reuters.com/article/us-twitter-russia/twitter-notifies-more-users-
exposed-to-russian-propaganda-idU.S.KBN1FK388
6https://theoutline.com/post/1192/google-s-featured-snippets-are-worse-than-fake-
news
7https://www.theguardian.com/technology/2016/dec/04/google-democracy-truth-
internet-search-facebook
rate of their attacks.8 However, Google’s secrecy about the extent
of exposure to such highly ranked disinformation is potentially
harmful to the public. In at least one highly-publicized case, one
individual’s path to a devastating hate crime—that of Dylan Roof,
who killed nine black people in their own church—started with the
search for “black on white crime” on Google.9
There is a difference between the two search phrases we have
discussed so far: “final vote count 2016” and “black on white crime”.
The first one is a seemingly neutral query that anyone can perform
innocently, out of curiosity. The second query is problematic from
the start as it was pushed on the Internet discourse by groups with a
white supremacist agenda.10 The researchers Michael Golebiewski
and danah boyd coined the concept “data void” [17] to refer to
phrases like this one, which, when searched, lead to either low
quality or problematic content. As they explain, once “black on
white” was publicized by Roof’s crime, high quality websites created
content that filled the “data void”.
One could argue that these are isolated cases, that the majority
of queries are not problematic and will not lead to problematic
content. However, Google’s general lack of transparency has opened
the door for continuous political attacks. Most recently, President
Trump accused Google of manipulating the search results to favor
his 2016 political opponent, Hillary Clinton,11 basing his claim on a
white paper of a small-scale Google audit, which had hypothesized
the possibility of Google swaying the elections.
If Google is unable to provide transparency, researchers can try
to use audits as a tool for increasing literacy around the complexity
of generating search results. The public should be able to search
for anything, problematic content as well, but it should be Google’s
responsibility to not serve harmful content, at the very least, at
the top of the search result page. Google was able to fix the issues
of consumer-harming web spam that plagued online shopping re-
sults in the early 2000’s [33]. We should demand the same kind of
commitment to high-quality results for political content, too.
This is particularly important, because research has found that
the public believes that Google shows trustworthy information at
the top of its search results [34]. However, if the 2016 U.S. presiden-
tial election was any indication, efforts to propagate disinformation
in novel ways will only increase in frequency and sophistication
[28]. Continuous and large-scale audits can serve to raise aware-
ness about vulnerabilities in the information ecosystem, for which
search engines are a central gateway.
1.1 Auditing for Bias
Social scientists have been worried about the power of search en-
gines and their potential bias since they came into prominence in
the 2000’s [13, 15, 20, 48]. Thus, auditing search engines in gen-
eral, and especially their role in political elections, is not a new
research problem. However, the underlying assumptions that have
motivated such research over the past ten years have been different
8https://www.theguardian.com/commentisfree/2016/nov/13/good-luck-in-making-
google-reveal-its-algorithm
9https://www.npr.org/sections/thetwo-way/2017/01/10/508363607/what-happened-
when-dylann-roof-asked-google-for-information-about-race
10https://www.gq.com/story/dylann-roof-making-of-an-american-terrorist
11https://www.nytimes.com/2019/08/19/us/politics/google-votes-election-
trump.html
and led to specific approaches. Concretely, we can distinguish three
different kinds of efforts to detect political bias in search platforms:
(1) Third-party manipulation. In the early 2000’s, Google
was susceptible to forms of political activism that came to
be known as “Google bombing”.1