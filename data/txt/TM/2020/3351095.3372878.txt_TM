
As machine learning becomes increasingly incorporated within
high impact decision ecosystems, there is a growing need to un-
derstand the long-term behaviors of deployed ML-based decision
systems and their potential consequences. Most approaches to un-
derstanding or improving the fairness of these systems have focused
on static settings without considering long-term dynamics. This
is understandable; long term dynamics are hard to assess, partic-
ularly because they do not align with the traditional supervised
ML research framework that uses ￿xed data sets. To address this
structural di￿culty in the ￿eld, we advocate for the use of sim-
ulation as a key tool in studying the fairness of algorithms. We
explore three toy examples of dynamical systems that have been
previously studied in the context of fair decision making for bank
loans, college admissions, and allocation of attention. By analyz-
ing how learning agents interact with these systems in simula-
tion, we are able to extend previous work, showing that static or
single-step analyses do not give a complete picture of the long-
term consequences of an ML-based decision system. We provide
an extensible open-source software framework for implementing
fairness-focused simulation studies and further reproducible re-
search, available at https://github.com/google/ml-fairness-gym.
CCS CONCEPTS
• Computing methodologies → Simulation environments;
Supervised learning by classi￿cation.
ACM Reference Format:
Alexander D’Amour, Hansa Srinivasan, James Atwood, Pallavi Baljekar, D.
Sculley, and Yoni Halpern. 2020. Fairness Is Not Static: Deeper Understand-
ing of Long Term Fairness via Simulation Studies. In Conference on Fairness,
Accountability, and Transparency (FAT* ’20), January 27–30, 2020, Barcelona,
Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3351095.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro￿t or commercial advantage and that copies bear this notice and the full citation
on the ￿rst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6936-7/20/02.
https://doi.org/10.1145/3351095.3372878
1 INTRO: BEYOND STATIC FAIRNESS
Much of the literature on fairness in machine learning is motivated
by the concern that high impact decisions made bymachine-learned
systems may have negative consequences for vulnerable popula-
tions [23, e.g., reviewed in]. However, much of this prior work has
focused on the fairness implications of decisions made in a static
or one-shot context in which long-term e￿ects and system level
dynamics are not considered. Despite recent work that has shown
that long-term implications can be at odds with fairness objectives
optimized in the static setting [16, 21, 22], long-term implications
remain relatively under-studied.
In this work, we propose that simulation studies can serve as a
framework for systematically exploring the long-term implications
of deploying a machine learning based decision system (hence-
forth, a learning agent). Simulation studies address a gap between
currently-favored evaluation of fairness policies on static, real-
world datasets, and more recent forays into simple, analytically
tractable models of dynamics [16, 20–22]. Simulations allow access
to counterfactual information about how the data would have varied
if a di￿erent data collection or decision-making policy had been in
place, a dimension that is missing from static datasets. For example,
the COMPAS [25] and German Credit [5] datasets do not provide
counterfactual information about how the data would have looked
if a di￿erent data collection or decision-making policy had been
in place. In addition, simulations allow for concrete exploration of
system level dynamics, feedback loops, and other long-term e￿ects
that may be intractable to analyze in closed form or to demonstrate
empirically in static settings.
To demonstrate the utility of simulations, we perform expanded
analyses of three canonical scenarios that have been treated in
previous fairness papers. In the ￿rst demonstration, we consider
the dynamic lending scenario studied in Liu et al. [21], where the
credit score of loan applicants can change in response to the agent’s
decision to lend or reject. Here, we perform a long-term analysis
of a dynamic scenario where only short-term analyses had been
performed before.
In the second demonstration, we consider the attention allocation
problem presented in Ensign et al. [7] and Elzayn et al. [6], where
an agent is tasked with allocating ￿nite units of attention across
di￿erent sites with the goal of discovering harmful incidents. Here,
we add dynamics to this scenario, which was previously treated
under an assumption of stationary rates.
Finally, we consider a college admissions scenario studied in Hu
et al. [16] and Milli et al. [22], where applicants are able to manipu-
late their scores (at a cost) to obtain a desirable decision from the
agent. Here, we examine equilibria that are realized from repeated
play of a two-player game where previous work only considered
the one-shot setting. In each of these scenarios, we ￿nd that the
concrete, long-term view o￿ered by simulation supports qualita-
tively di￿erent (though not incompatible) fairness conclusions from
those obtained before.
1.1 Contributions
This work makes several contributions, with the goal of raising the
pro￿le of simulation studies in the ML fairness community.
Our primary contribution is to demonstrate in several settings
how feedback dynamics complicate the analysis of long term fair-
ness consequences. We show that, in each of these settings, the
long-run dynamics depart substantially from those that are mea-
sured in one-shot contexts. Although these simulations may be
too stylized to draw substantive conclusions about each of the de-
cision problems that they model, they nonetheless demonstrate
key complications in framing and measuring fairness in dynamic
environments.
Along theway, we show that fairness-oriented simulations can ￿t
into the standard framework of Markov Decision Processes (MDPs),
which are commonly used in a number of sub￿elds, including ro-
botics and reinforcement learning. This framing is ￿exible, and puts
dynamical analyses of fairness into a language that is more familiar
to many ML researchers than the economic concepts highlighted
in previous work.
Finally, along with code to reproduce the results in this paper,
we provide a general library ml-fairness-gym for specifying new
simulation environments and agents with a uni￿ed interface for
easy extensibility and development at https://github.com/google/
ml-fairness-gym.
1.