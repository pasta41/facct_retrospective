
Computer vision technology is being used by many but remains
representative of only a few. People have reported misbehavior of
computer vision models, including ofensive prediction results and
lower performance for underrepresented groups. Current computer
vision models are typically developed using datasets consisting of
manually annotated images or videos; the data and label distribu-
tions in these datasets are critical to the models’ behavior. In this
paper, we examine ImageNet, a large-scale ontology of images that
has spurred the development of many modern computer vision
methods. We consider three key factors within the person subtree
of ImageNet that may lead to problematic behavior in downstream
computer vision technology: (1) the stagnant concept vocabulary of
WordNet, (2) the attempt at exhaustive illustration of all categories
with images, and (3) the inequality of representation in the images
within concepts. We seek to illuminate the root causes of these
concerns and take the irst steps to mitigate them constructively.
CCS CONCEPTS
· Social and professional topics → Gender; Race and ethnicity;
Age; · Computing methodologies → Computer vision.
KEYWORDS
computer vision, fairness, dataset construction, representative datasets
ACM Reference Format:
Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky.
2020. Towards Fairer Datasets: Filtering and Balancing the Distribution of
the People Subtree in the ImageNet Hierarchy. In Conference on Fairness,
Accountability, and Transparency (FAT* ’20), January 27ś30, 2020, Barcelona,
Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3351095.
classroom use is granted without fee provided that copies are not made or distributed
for proit or commercial advantage and that copies bear this notice and the full citation
on the irst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciic permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27ś30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3375709
1 INTRODUCTION
As computer vision technology becomes widespread in people’s
Internet experience and daily lives, it is increasingly important for
computer vision models to produce results that are appropriate
and fair. However, there are notorious and persistent issues. For
example, face recognition systems have been demonstrated to have
disproportionate error rates across race groups, in part attributed
to the underrepresentation of some skin tones in face recognition
datasets [10]. Models for recognizing human activities perpetuate
gender biases after seeing the strong correlations between gender
and activity in the data [36, 82]. The downstream efects range from
perpetuating harmful stereotypes [55] to increasing the likelihood
of being unfairly suspected of a crime (e.g., when face recognition
models are used in surveillance cameras).
Many of these concerns can be traced back to the datasets used
to train the computer vision models. Thus, questions of fairness
and representation in datasets have come to the forefront. In this
work, we focus on one dataset, ImageNet [18], which has arguably
been the most inluential dataset of the modern era of deep learn-
ing in computer vision. ImageNet is a large-scale image ontology
collected to enable the development of robust visual recognition
models. The dataset spearheaded multiple breakthroughs in ob-
ject recognition [35, 45, 69]. In addition, the feature representation
learned on ImageNet images has been used as a backbone for a
variety of computer vision tasks such as object detection [33, 61],
human activity understanding [68], image captioning [74], and re-
covering depth from a single RGB image [49], to name a few. Works
such as Huh et al. [37] have analyzed the factors that contributed
to ImageNet’s wide adoption. Despite remaining a free education
dataset released for non-commercial use only,1 the dataset has had
profound impact on both academic and industrial research.
With ImageNet’s large scale and diverse use cases, we examine
the potential social concerns or biases that may be relected or
ampliied in its data. It is important to note here that references to
łImageNetž typically imply a subset of 1,000 categories selected for
the image classiication task in the ImageNet Large Scale Visual
Recognition Challenge (ILSVRC) of 2012-2017 [64], and much of
the research has focused on this subset of the data. So far, Dulhanty
and Wong [22] studied the demographics of people in ILSVRC
data by using computer vision models to predict the gender and
1Please refer to ImageNet terms and conditions at image-net.org/download-faq
age of depicted people, and demonstrated that, e.g., males aged
15 to 29 make up the largest subgroup.