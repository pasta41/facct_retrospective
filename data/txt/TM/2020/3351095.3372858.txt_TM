
Implicit bias is the unconscious attribution of particular qualities
(or lack thereof) to a member from a particular social group (e.g.,
defined by gender or race). Studies on implicit bias have shown
that these unconscious stereotypes can have adverse outcomes in
various social contexts, such as job screening, teaching, or policing.
Recently, [33] considered a mathematical model for implicit bias
and showed the effectiveness of the Rooney Rule as a constraint
to improve the utility of the outcome for certain cases of the sub-
set selection problem. Here we study the problem of designing
interventions for the generalization of subset selection – ranking
– that requires to output an ordered set and is a central primitive
in various social and computational contexts. We present a family
of simple and interpretable constraints and show that they can
optimally mitigate implicit bias for a generalization of the model
studied by Kleinberg and Raghavan. Subsequently, we prove that
under natural distributional assumptions on the utilities of items,
simple, Rooney Rule-like, constraints can also surprisingly recover
almost all the utility lost due to implicit biases. Finally, we augment
our theoretical results with empirical findings on real-world distri-
butions from the IIT-JEE (2009) dataset and the Semantic Scholar
Research corpus.
CCS CONCEPTS
• Information systems → Content ranking; • Mathematics of
computing → Probability and statistics;
KEYWORDS
Implicit bias, ranking, algorithmic fairness, interventions
ACM Reference Format:
L. Elisa Celis, Anay Mehrotra, and Nisheeth K. Vishnoi. 2020. Interventions
for Ranking in the Presence of Implicit Bias. In Conference on Fairness,
Accountability, and Transparency (FAT* ’20), January 27–30, 2020, Barcelona,
Spain. ACM, New York, NY, USA, 28 pages. https://doi.org/10.1145/3351095.
Implicit bias is the unconscious attribution of particular qualities
(or lack thereof) to a member from a particular social group defined
by characteristics such as gender, origin, or race [26]. It is well un-
derstood that implicit bias is a factor in adverse effects against sub-
populations in many societal contexts [1, 6, 41] as also highlighted
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372858
by recent events in the popular press [21, 37, 60]. For instance, in
employment decisions, men are perceived as more competent and
given a higher starting salary even when qualifications are the
same [51], and in managerial jobs, it was observed that women had
to show roughly twice as much evidence of competence as men to
be seen as equally competent [36, 58]. In education, implicit biases
have been shown to exist in ways that exacerbate the achievement
gap for racial and ethnic minorities [52] and female students [40],
and add to the large racial disparities in school discipline which
particularly affect black students’ school performance and future
prospects [44]. Beyond negatively impacting social opportunities,
implicit biases have been shown to put lives at stake as they are
a factor in police decisions to shoot, negatively impacting people
who are black [19] and of other racial or ethnic minorities [47].
Furthermore, decision making that relies on biased measures of
quantities such as utility can not only adversely impact those per-
ceived more negatively, but can also lead to sub-optimal outcomes
for those harboring these unconscious biases.
To combat this, a significant effort has been placed in developing
anti-bias training with the goal of eliminating or reducing implicit
biases [23, 38, 63]. However, such programs have been shown to
have limited efficacy [43]. Furthermore, as algorithms increasingly
take over prediction and ranking tasks, e.g., for ranking and short-
listing candidates for interview [8], algorithms can learn from and
encode existing biases present in past hiring data, e.g., against gen-
der [20] or race [53], resulting in algorithmic biases of their own.
Hence, it is important to develop interventions that can mitigate
implicit biases and hence result in better outcomes.
As a running example we will consider hiring, although the
interventions we describe would apply to any domain in which
people are selected or ranked. Hiring usually works in multiple
stages, where the candidates are first identified and ranked in order
of their perceived relevance, a shortlist of these candidates are then
interviewed with more rigor, and finally a one or few of them are
hired [8]. The Rooney Rule is an intervention to combat biases dur-
ing the shortlisting phase, it requires the “shortlister” (an entity that
shortlists) to select at least one candidate from an underprivileged
group for interview. It was originally introduced for coach positions
in the National Football League [18], and subsequently adopted by
other industries [11, 27, 45, 49]. The idea is that including the under-
privileged candidates would give opportunity to these candidates,
with a higher (hidden or latent) potential. Whereas without the
Rooney Rule these candidates may not have been selected for in-
terview. While the Rooney Rule appears to have been effective
is just one of many possible interventions one could design. How
can we theoretically compare two proposed interventions?
[33] study the Rooney Rule under a theoretical model of implicit
bias, with two disjoint groupsGa ,Gb ⊆ [m] ofm candidates, where
since Rooney Rule’s introduction in 2002 to 2006 [18].
FAT* ’20, January 27–30, 2020, Barcelona, Spain L. Elisa Celis, Anay Mehrotra, and Nisheeth K. Vishnoi
Gb is the underprivileged group. Each candidate i ∈ [m] has a true,
latent utility wi ∈ R, which is the utility they would generate if
hired, and an observed utility ŵi ≤ wi , which is the shortlister’s
(possibly biased) estimate of wi . The shortlister selects n ≤ m
candidates with the highest observed utility. For example, in the
context of peer-review, the latent utility of a candidate could be
their total publications, and the observed utility could be the total
weight the reviewer assigns to the publications (“impact points” in
[56]). They model implicit bias as a multiplicative factor β ∈ [0, 1],
where the observed utility is ŵi B β ·wi if i ∈ Gb , and ŵi B wi if
i ∈ Ga . [33] characterize conditions on n,m, β and the distribution
ofwi , where Rooney rule increases the total utility of the selection.
However, before the shortlisting phase, applicants or potential
candidates must be identified and ranked. For example, LinkedIn
Recruiter predicts a candidate’s “likelihood of being hired” from
their activity, Koru Hire analyzes a candidates (derived) personality
traits to generate a “fit score”, and HireVue “grades” candidates
to produce an “insight score” [8]. In a ranking, the candidates are
ordered (in lieu of an intervention, by their observed utilities), and
the utility of a ranking is defined by a weighted sum of the latent
utilities of the ranked candidates where the weight decreases the
farther down the ranking a candidate is placed. Such weighting
when evaluating rankings is common practice, and due to the fact
that candidates placed lower in the list receive a lower attention
as compared to those placed higher [30]; this translates into being
less likely to be shortlisted, and contributing less to the total utility
of the hiring process. Therefore, it becomes important to consider
interventions to mitigate bias in the ranking phase, and understand
their effectiveness in improving the ranking’s latent utility.
Can we construct simple and interpretable interventions that in-
crease the latent utility of rankings in the presence of implicit bias?
1.1 Our contributions
We consider the setting where items to be ranked may belong
to multiple intersectional groups and present a straightforward
generalization of the implicit bias model in [33] for this setting. We
consider a set of interventions (phrased in terms of constraints)
for the ranking problem which require that a fixed number of
candidates from the under privileged group(s) be represented in
the top k positions in the ranking for all k . We show that for any
input to the ranking problem – i.e., any set of utilities or bias –
there is an intervention as above that leads to optimal latent utility
(Theorem 3.1). We then prove a structural result about the optimal
ranking when all the utilities are drawn from the same distribution,
making no assumption on the distribution itself (Theorem 3.2).
This theorem gives a simple Rooney Rule-like intervention for
ranking in the case when there are two groups: For some α , require
that in the top k positions, there are at least α · k items from the
underprivileged group. We then show that when the utilities are
drawn from the uniform distribution, the latent utility of a ranking
that maximizes the biased utility but is subject to our constraints
(for an appropriate α ) is close to that of the optimal latent utility
(Theorem 3.3). We evaluate the performance of ranking under our
constraints empirically on two real-world datasets, the IIT-JEE
2009 dataset (see Section 4) and the Semantics Scholar dataset
(see Section E.2 in the Supplementary Material). In both cases we
observe that our simple constraints significantly improve the latent
utility of the ranking, and in fact attain near-optimal latent utility.
Finally, while we phrase these results in the context of implicit
bias, such interventions would be effective whenever the observed
utilities are systematically biased against a particular group.
1.