
Many machine learning projects for new application areas involve
teams of humans who label data for a particular purpose, from
hiring crowdworkers to the paper’s authors labeling the data them-
selves. Such a task is quite similar to (or a form of) structured
content analysis, which is a longstanding methodology in the so-
cial sciences and humanities, with many established best practices.
In this paper, we investigate to what extent a sample of machine
learning application papers in social computing — specifically pa-
pers from ArXiv and traditional publications performing an ML
classification task on Twitter data — give specific details about
whether such best practices were followed. Our team conducted
multiple rounds of structured content analysis of each paper, mak-
ing determinations such as: Does the paper report who the labelers
were, what their qualifications were, whether they independently
labeled the same items, whether inter-rater reliability metrics were
disclosed, what level of training and/or instructions were given to
labelers, whether compensation for crowdworkers is disclosed, and
if the training data is publicly available. We find a wide divergence
in whether such practices were followed and documented. Much of
machine learning research and education focuses on what is done
once a “gold standard” of training data is available, but we discuss
issues around the equally-important aspect of whether such data is
reliable in the first place.
CCS CONCEPTS
• Information systems → Content analysis and feature se-
lection; • Computing methodologies→ Supervised learning
by classification; • Social and professional topics → Project
and people management.
KEYWORDS
machine learning, data labeling, human annotation, content analy-
sis, training data, research integrity, meta-research
ACM Reference Format:
R. Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai, Jie Qiu, Rebekah Tang,
and Jenny Huang. 2020. Garbage In, Garbage Out? Do Machine Learning
Application Papers in Social Computing Report Where Human-Labeled
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/01. . . $15.00
https://doi.org/10.1145/3351095.3372862
Training Data Comes From?. In Conference on Fairness, Accountability, and
Transparency (FAT* ’20), January 27–30, 2020, Barcelona, Spain. ACM, New
York, NY, USA, 18 pages. https://doi.org/10.1145/3351095.3372862
1 INTRODUCTION
1.1 Garbage In, Garbage Out?
Machine learning (ML) has become widely used in many academic
fields, as well as across the private and public sector. Supervised
machine learning is particularly prevalent, in which training data is
collected for a set of entities with known properties (a “ground truth”
or “gold standard”), then used to create a classifier that will make
predictions about new entities of the same type. One of the earliest
applications of supervised ML is in spam detection [e.g. 11, 48].
Humans would label e-mails as spam or non-spam, then classifiers
trained on these data would be used to predict if future e-mails were
spam or non-spam. As Brunton details in his history of spam [8],
spam is a messy concept, especially when trying to define it globally
at scale. Like most problems of classification, we typically rely on an
“I know it when I see it” standard, which often breaks down at edge
cases [7]. One challenge in spam detection is in operationalization:
not just defining spam, but also creating a systematic procedure
to measure it. Then, assuming a fuzzy social concept like spam is
sufficiently defined and operationalized, humans are typically still
needed to label cases according to this definition and procedure.
“Garbage In, Garbage Out” is a longstanding aphorism in com-
puting about how flawed input data or instructions will produce
flawed outputs [1, 39]. SupervisedML requires high-quality training
data to produce high-quality classifiers. However, contemporary
ML research and education tends to focus less on obtaining and
validating such a training dataset, with such considerations often
passed over in major textbooks [e.g. 15, 20, 28]. The predominant
focus is typically on what is done with the training data to produce
a classifier, with heavy emphasis on mathematical foundations and
routine use of clean and tidy “toy” datasets. The process of creating
a “gold standard” or “ground truth” dataset is routinely black-boxed.
Many papers in ML venues are expected to use a standard, public
training dataset, with authors comparing various performance met-
rics on the same dataset. While such a focus on what is done to a
training dataset may be appropriate for theoretically-oriented basic
research in ML, this is not the case for supervised ML applications.
1.