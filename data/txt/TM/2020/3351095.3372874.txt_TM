
Research to date aimed at the fairness, accountability, and trans-
parency of algorithmic systems has largely focused on topics such
as identifying failures of current systems and on technical inter-
ventions intended to reduce bias in computational processes. Re-
searchers have given less attention to methods that account for the
social and political contexts of specific, situated technical systems at
their points of use. Co-developing algorithmic accountability inter-
ventions in communities supports outcomes that are more likely to
address problems in their situated context and re-center power with
those most disparately affected by the harms of algorithmic systems.
In this paper we report on our experiences using participatory and
co-design methods for algorithmic accountability in a project called
the Algorithmic Equity Toolkit. The main insights we gleaned from
our experiences were: (i) many meaningful interventions toward
equitable algorithmic systems are non-technical; (ii) community
organizations derive the most value from localized materials as op-
posed to what is “scalable” beyond a particular policy context; (iii)
framing harms around algorithmic bias suggests that more accurate
data is the solution, at the risk of missing deeper questions about
whether some technologies should be used at all. More broadly,
we found that community-based methods are important inroads to
addressing algorithmic harms in their situated contexts.
∗Authors contributed equally.
†Work conducted at the University of Washington.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6936-7/20/02.
https://doi.org/10.1145/3351095.3372874
ACM Reference Format:
Michael Katell, Meg Young, Dharma Dailey, Bernease Herman, Vivian
Guetler, Aaron Tam, Corinne Binz, Daniella Raz, and P. M. Krafft. 2020.
Toward Situated Interventions for Algorithmic Equity: Lessons from the
Field. In Conference on Fairness, Accountability, and Transparency (FAT* ’20),
January 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3351095.3372874
1 INTRODUCTION
Existing scholarship on algorithmic fairness often begins with a
recognition of real-world harms [7, 11, 49], yet many contributions
to date offer technical solutions that precede application or evalua-
tion of systems in use. A situated investigation is one that forefronts
the details of the context of specific people and places at particular
points in time, rather than trying to study a system or question
with an abstract approach removed from social context. We invoke
this use of the term situated from feminist philosopher of science
Donna Haraway [27], who describes situated knowledge as the
product of embodied vantage points, located historically (see also
[28, 29]). Situated investigations offer opportunities to reveal not
only the technical features and limitations of a system, but also the
different ways a system affects different people in its context of
use. Many recent studies have pointed to the need to incorporate
sociotechnical context in design of algorithmic systems. Selbst et
al. call for scholars in this area to mind the traps associated with
abstract analysis [58]. Green & Chen [25] demonstrate that human
decision-making interacts with algorithmic processing in the ul-
timate disparate impacts of algorithmic systems. Other scholars
have pursued empirical case studies [19, 50, 52], field evaluations
[10, 65], context-aware uses of datasets [46, 63], and questions of
institutional access to real-world data [67]. In each case, attending
to end-use provides a richer design space and new opportunities
for improvements and safeguards. In our work we move towards
a deeper engagement with systems’ social and political context.
We report on a process of using a participatory design and action
FAT* ’20, January 27–30, 2020, Barcelona, Spain Katell, Young, Dailey et al.
research approach to craft interventions supporting the agendas
of people most affected by the disparate impacts of algorithmic
systems.
We look to the field of human-computer interaction (HCI) to
deepen our commitment to engaging communities. As Paul Dour-
ish [16] notes, HCI joins distinct scholarly traditions that are often
viewed as at oddswith each other: a positivist engineering approach,
which seeks to achieve precisely defined objectives through system
design; and a phenomenological, interpretivist approach [1, 61],
which understands the meaning of both social action and technical
systems as constructed, contigent, and constantly in flux—in defi-
ance of the rigid operationalization of an instrumentalist view. At
the intersection of these two traditions, HCI has fostered a wide ar-
ray of participatory design methods. Here, we report on our process
drawing on participatory methods and a distinct tradition (sharing
similar goals) known as participatory action research in support of
algorithmic accountability and community decision-making.
Participatory methodologies are valuable complements to the
practice of data science more broadly. The work of data science
has a normative dimension, but there is little agreement about
which norms or values should be prioritized [5]. Research methods
that center the people disparately affected by the practice of data
science can guide design choices and clarify the implications of
these choices by situating the work where the social costs and
benefits can be more directly assessed [6, 13, 18]. Researchers in
critical data studies have explored similar themes in the context of
mixed methods data science [44].
Our work joins others in understanding algorithmic discrimina-
tion as a product of societal inequity rather than as solely a result of
inaccurate performance by models or under-representative training
data. When systems are deployed in contexts historically shaped by
discrimination in policy and practice, such as policing [33, 47] or
medical care [50], data employed as inputs for decision systems con-
tributes to feedback loops and amplified discrimination [33, 47, 51].
As a result, it is not enough to attend to bias in systems from the
perspective of accuracy; interventions must acknowledge the his-
torical, political, and institutional contexts in which systems are
used, and by whom.
Our Contributions: The imperative to account for situated con-
text in algorithmic accountability requires a move beyond the labo-
ratory and academy. In this work we ask,What are the opportunities
and challenges in using participatory design and action methods for
research on fairness in algorithmic systems? We approach this ques-
tion by drawing on insights from our use of these methods in a
project aimed at empowering community organizers and commu-
nity members in advocacy and public comment efforts in state
and local technology policy. In work described in more detail in
a companion paper [36], we engaged in a co-design project that
included local community groups, advocacy campaigns, and policy
stakeholders, resulting in the Algorithmic Equity Toolkit [36], a set
of heuristic tools and an interactive demo that assists users in rec-
ognizing algorithmic systems, understanding potential algorithmic
harms, and holding policymakers accountable to public input.
Our central contributions here are lessons learned from the de-
velopment of the Algorithmic Equity Toolkit for the practice of sit-
uated investigations into fairness, accountability, and transparency
in algorithmic systems. Our interactions with community partners
re-framed our objectives and highlighted the opportunities and
challenges of clearly communicating concepts from artificial intel-
ligence and scholarship on algorithmic bias. We also note existing
frictions to further incorporation of community input in settings
where data science and machine learning research conventionally
happens. The algorithmic fairness community has a unique oppor-
tunity to foster the intersection of data science, machine learning,
and community-based work. This shift will require us to learn
from other fields as to how to build reciprocal partnerships with
community organizations, elicit their input, and attend to the prac-
tical dimension of managing trust and power dynamics in these
relationships. A more inclusive data science practice will result in
novel conventions of work which attend to fairness, accountability,
transparency, and equity as an explicit part of research method and
practice– rather than as topic alone.
