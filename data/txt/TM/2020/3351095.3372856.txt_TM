
Regulatory regimes designed to ensure transparency often struggle
to ensure that transparency ismeaningful in practice. This challenge
is particularly great when coupled with the widespread usage of
dark patterns — design techniques used to manipulate users. The
following article analyses the implementation of the transparency
provisions of the German Network Enforcement Act (NetzDG)
by Facebook and Twitter, as well as the consequences of these
implementations for the effective regulation of online platforms.
This question of effective regulation is particularly salient, due to an
enforcement action in 2019 by Germany’s Federal Office of Justice
(BfJ) against Facebook for what the BfJ claim were insufficient
compliance with transparency requirements, under NetzDG.
This article provides an overview of the transparency require-
ments of NetzDG and contrasts these with the transparency re-
quirements of other relevant regulations. It will then discuss how
transparency concerns not only providing data, but also how the
visibility of the data that is made transparent is managed, by decid-
ing how the data is provided and is framed. We will then provide
an empirical analysis of the design choices made by Facebook and
Twitter, to assess the ways in which their implementations differ.
The consequences of these two divergent implementations on in-
terface design and user behaviour are then discussed, through a
comparison of the transparency reports and reporting mechanisms
used by Facebook and Twitter. As a next step, we will discuss the
BfJ’s consideration of the design of Facebook’s content reporting
mechanisms, and what this reveals about their respective interpre-
tations of NetzDG’s scope. Finally, in recognising that this situation
is one in which a regulator is considering design as part of their
action – we develop a wider argument on the potential for regula-
tory enforcement around dark patterns, and design practices more
generally, for which this case is an early, indicative example.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372856
1 INTRODUCTION
There are numerous challengeswith ensuring effective transparency
in a socio-technical context. In practice, transparency undertakings
are often lacking, with many actors preferring to create the illusion
of transparency rather than actually engaging in transparent prac-
tices [20]. Notably, even though legal requirements for transparency
are common [17, 54], it is relatively uncommon for regulators to en-
force these transparency requirements systematically [9, 43]. This
makes the fine issued by the BfJ against Facebook "for violating the
provisions of the Network Enforcement Act" (NetzDG) [2] partic-
ularly interesting. The core job of regulators is to oversee private
sector actors and in some cases like the GDPR to oversee public
sector actors as well. The issuance of the fine is an evidence that
the BfJ was taking on this oversight role and pushing for greater
transparency. At the same time, online platforms have a commer-
cial interest in handling complaints using their own Community
Standards mechanisms rather than by the provisions of NetzDG, as
handling complaints using their Community Standards is consider-
ably easier and cheaper for them [31, 36, 57]. This leads to our main
research question: How do Facebook and Twitter implement the
transparency provisions of NetzDG and what are the consequences
of their respective implementations for the regulation of online
platforms?
Platform Community Standards have been heavily criticised as
lacking transparency, accountability, and procedural safeguards for
the human beings affected by them [1, 13, 14, 19, 30, 64, 65, 68].
Regulation of platforms through instruments like NetzDG can thus
be seen as a response to this criticism. The BfJ’s decision also needs
to be seen in the context of a wider push for platform regulation
in Europe; the UK Government’s ‘Online Harms White Paper’[21]
and the French proposal on making social media platforms more ac-
countable [26] are two further examples. The way in which NetzDG
is interpreted provides a key indicator of what the future European
platform regulation could look like.
1.1 Definitions, Scope and Case Selection
In order to answer the research question, we will first define a few
key terms to ensure their meaning is clear within this article, as
well as clarify the scope of our analysis. The term complaints will be
used extensively, as this is the official legal terminology used by the
German NetzDG and the BfJ. When speaking about the technical
mechanisms used to receive these complaints, we will use the more
common terminology of reporting mechanism for users to be able to
report problematic content. In the context of this article, the terms
complaint and report and flag can be understood interchangeably
as they all refer to a way in which users can inform platforms about
problematic content.
We use dark patterns to refer to the“instances where designers
use their knowledge of human behavior (e.g., psychology) and the
desires of end users to implement deceptive functionality that is not
in the user’s best interest” [27]. §5 explores the concept in detail.
As mentioned in the introduction, this article discusses the
transparency provisions of NetzDG. We do not discuss the nu-
merous criticisms of NetzDG in regards to freedom of expressions,
which we note have already been extensively discussed elsewhere
[16, 33, 35, 37, 38, 69]. Instead, we will focus on the transparency-
related aspects of NetzDG, which have not yet received the same
level of scrutiny and analysis, and importantly, forms the basis
for an enforcement action regarding the means by which online
transparency mechanisms are implemented.
In regards to the concept of transparency, we draw on the work
of Flyverbom to “conceptualize transparency projects as a form
of visibility management with extensive and often paradoxical
implications for the organizations and actors involved” [20]. From
this perspective, it is important to explore “(a) the technological
and mediated foundations of transparency and (b) the dynamics
of visibility practices involved in efforts to make people, objects,
and processes knowable and governable” [20]. As such, this article
considers both the technical foundations, the legal and regulatory
context, and the attempts by different private sector actors to create
visibility through specific forms of compliance with transparency
obligations.
As this article includes an empirical analysis, we believe it is
important to justify why specific empirical cases were chosen. We
decided to analyse how Facebook and Twitter attempt to comply
with NetzDG in greater detail. The choice of Facebook was clear,
not only as it was the subject of a BfJ enforcement action, but also
because it is one of the only platforms we are aware of that exhibits
two separate reporting mechanisms in its implementation of Net-
zDG and platform Community Standards – one of the main reasons
it was targeted by the BfJ. Twitter was selected, as it provides a
good example of an implementation of NetzDG that combines the
reporting mechanisms for Community Standards and NetzDG into
a single reporting mechanism. As such, the choice of Facebook and
Twitter as cases represents a ‘most-different case’ [24] selection,
allowing for a systematic comparison of the two approaches to
implementing NetzDG.
Finally, we contrast the transparency provisions around the con-
tent reporting mechanisms provided under NetzDG, with exist-
ing mechanisms for transparency and content reporting already
provided by online platforms such as Facebook or Twitter. These
mechanisms are referred to here as Community Standards and con-
sist of all relevant documents and processes which influence how
platforms decide which content to remove. For Facebook, this in-
cludes the Facebook Community Standards and Terms of Service,1
as well as internal manuals on how to implement these rules on an
1See: https://www.facebook.com/communitystandards/ and
https://www.facebook.com/terms.php
everyday basis [4, 67]. For Twitter, we consider the Twitter Rules
and policies, general guidelines and Terms of Service.2
