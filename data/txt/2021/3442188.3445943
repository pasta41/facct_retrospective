Epistemic values in feature importance methods: Lessons from
feminist epistemology
Leif Hancox-Li
Capital One
New York, New York, USA
leif.hancox-li@capitalone.com
I. Elizabeth Kumar
University of Utah
Salt Lake City, UT, USA
kumari@cs.utah.edu
ABSTRACT
As the public seeks greater accountability and transparency from
machine learning algorithms, the research literature on methods to
explain algorithms and their outputs has rapidly expanded. Feature
importance methods form a popular class of explanation methods.
In this paper, we apply the lens of feminist epistemology to recent
feature importance research. We investigate what epistemic values
are implicitly embedded in feature importance methods and how
or whether they are in conflict with feminist epistemology. We
offer some suggestions on how to conduct research on explanations
that respects feminist epistemic values, taking into account the
importance of social context, the epistemic privileges of subjugated
knowers, and adopting more interactional ways of knowing.
KEYWORDS
explanation, philosophy, epistemology, machine learning, feature
importance, feminism, methodology
ACM Reference Format:
Leif Hancox-Li and I. Elizabeth Kumar. 2021. Epistemic values in feature
importance methods: Lessons from feminist epistemology. In Conference
on Fairness, Accountability, and Transparency (FAccT â€™21), March 3â€“10, 2021,
Virtual Event, Canada. ACM, New York, NY, USA, 10 pages. https://doi.org/
10.1145/3442188.3445943
1 INTRODUCTION
In recent years, the number of new methods for measuring feature
importance for machine learning (ML) models has exploded, leav-
ing ML practitioners spoilt for choice. As black-box algorithms with
inscrutable inner mechanisms are increasingly used for crucial de-
cisions, demands for greater transparency and accountability have
increased, leading to legal requirements for explanation like that in
the European Unionâ€™s General Data Protection Regulation (GDPR).
Media coverage and public awareness of potentially problematic
algorithmic decisions have also increased, forcing ML practitioners
to come up with improved and novel ways to explain their models.
In this paper, we focus on a subclass of explanation methods, known
as feature importance or feature attribution methods. Broadly con-
strued, these assign a quantitative measure of importance to the
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
Â© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8309-7/21/03. . . $15.00
https://doi.org/10.1145/3442188.3445943
inputs to a function. They are a popular way of approaching de-
mands for explainability, as they fit the human tendency to attribute
outcomes to specific factors.
As feature importancemethods proliferate, practitioners struggle
to decide which methods to use, and creators of methods strive to
demonstrate that their methods are superior to others. However,
much of the evaluation of various methodsâ€™ pros and cons proceeds
in an implicitly value-neutral context: rarely do participants in
this discourse pause to consider if certain supposed theoretical
or practical virtues would be virtues in all social or applicational
contexts.1 Papers introducing new feature importance methods
state that certain properties of the proposed methods are â€œdesirableâ€
without indicating the contexts or uses for which those properties
are desirable [51, 76]. Stating a property as â€œdesirableâ€ without
further elaboration elides questions such as: Desirable to whom?
For what purposes? When applied to what types of data?
In this paper, we argue that a number of popular feature impor-
tance methods implicitly contain values that are counter to the kind
of pluralistic, contextual, and interactional view of epistemology
advocated by many feminist epistemologists. The existence of these
values is not problematic in itself unless one adheres to the ideal
of a â€œview from nowhereâ€ [56]. However, the partiality of feature
importance methods is often not acknowledged by the creators or
users of thosemethods. Instead, debates focus on various apparently
value-neutral properties of the methods, to the point where some
aspire to create universal benchmarks to evaluate all explanation
methods [21, 62].
In the next section, we provide a brief overview of feminist epis-
temology, in particular, the frameworks of situated knowledges
and standpoint epistemology. In Section 3, we describe the various
ways in which values counter to feminist epistemology may be em-
bedded in feature importance methods. Finally, in Section 4, we end
by offering some suggestions on how ML research on explanations
can include more feminist values.
2 FEMINIST EPISTEMIC VALUES
Against the unitary view from nowhere that is assumed in much of
science and mainstream analytic epistemology, feminist epistemol-
ogists have argued for understanding science through the idea of
situated knowledges. Instead of aiming for one correct, objective
view of things, they argue that we should instead accept that know-
ers are social beings, who bring perspectives to each issue that are
conditioned by their social experiences. In this view, there is no
â€œgod trickâ€ that lets us know from the point of view of the unmarked
[31]. It is not, however, a completely relativistic viewâ€”we can still
1The exceptions to this tend to occur in interdisciplinary venues like the FAccT con-
ference [7], or in venues for human-computer interaction professionals [36, 41].
817
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Leif Hancox-Li and I. Elizabeth Kumar
speak of views being â€œless falseâ€, even if we cannot speak of truth
or truth-likeness [32].
Feminist epistemologists came to this view as a reaction against
what they saw as problematic claims to unitary knowledge in pro-
fessions, like the natural sciences, where largely white and male
knowers have constructed one way of seeing as the â€œobjectiveâ€ one,
while alternative ways of knowing, many propounded by marginal-
ized groups in science, are ignored [43]. Feminist epistemologists
have described numerous examples of how the natural sciences
could have benefited from accommodating more ways of know-
ing [42, 53]. Indeed, some argue that subjugated knowledges, that
is, the perspectives of those with the least power, have an advan-
tage in providing more empirically accurate and comprehensive
accounts of the world [32, 78]. Given the non-representative nature
of machine learning practitionersâ€™ demographics [11], we should be
similarly wary of claims by ML practitioners about certain methods
being universally or objectively better than others.
Indeed, compared to many of the natural sciences examined by
feminist epistemologists, there may be even greater reason to be
wary of claims of value-neutrality in ML, because much of the value
of ML explanations comes from their utility in practical contexts.
The same type of explanation may be useful in one type of practical
application but not another.2 This is unlike many hypotheses in
the natural sciences, which are often evaluated not solely by their
practical utility, but also by how they fit with existing scientific
theory as part of a coherent, realistic description of the world.
In contrast to the view from nowhere, the situated knowledge
framework focuses on the pragmatics of acquiring knowledge in
particular contexts. This kind of contextual thinking includes con-
sidering how traditional epistemic goals intersect with social goals.
As Sandra Harding points out, â€œThe kinds of explanations favored
by modern science have not always been the most effective ones
for all projectsâ€”for example, for achieving environmental balance
or preventing chronic bodily malfunctionsâ€ [32]. In the case of ML
explanations, the intertwining of epistemic and social goals is par-
ticularly striking, since the demand for an explanation is often not
just a demand for knowledge of how the algorithm works, but also
part of an ethical demand for greater accountability or transparency
[22, 60].
Another aspect of feminist epistemology that is particularly
relevant to ML explanations is its implications for formal frame-
works. As Donna Haraway argues, â€œrational knowledge does not
pretend to disengagement... to by [sic] fully self-contained or fully
formalizable. Rational knowledge is a process of ongoing critical
interpretation among â€˜fieldsâ€™ of interpreters and decodersâ€ [31]. In
reducing a complex system to one formal framework, we often
reduce the possible meanings it can have and the possible relations
it can have to other systems. Focusing on formalisms also distracts
from the meanings we attach to formal symbols or formulae, even
though the same formalism may take on different meanings in
different social or applicational contexts.
2For example, a feature-highlighting explanation may provide an actionable guide to
a decision subject on how to change an algorithmic decision in some contexts, but not
in others, depending on what factors the explanation recommends that the subject
change [7]. Some factors are impossible to change due to physical impossibility or
environmental factors. However, the same feature-highlighting explanation that is
unhelpful to a decision subject may be useful to a data scientist doing feature selection
or feature engineering.
Haraway also advocates for a critical stance on boundaries and
objects. By paying attention to how objects of study are partly
constructed by social processes, we can also see how the boundaries
that define those objects â€œmaterialize in social interactionâ€ [31].
Similarly, in taking a feminist approach to ML explanations, we
should critically question how we draw the boundaries around
what the realm of â€œexplanationâ€ is. Are we defining the realm of
legitimate study of explanations in an unnecessarily narrow way?
Are there possible agents who can help construct explanations or
be part of explanations, that we have left out of our idealized view
of explanations?
Finally, feminist epistemology also encourages a more interac-
tive way of how knowledge is created, considering the object of
knowledge not just as a passive object with static properties, but
also as an agent that can enter in conversation with the knower. In
this view, â€œ[t]he world neither speaks itself nor disappears in favor
of a master decoderâ€ [31]. For example, Evelyn Fox Keller attributes
Barbara McClintockâ€™s successful insight into maize genetics to Mc-
Clintockâ€™s willingness to erode the boundaries between subject (the
scientist) and object (the maize plants) [43]. Keller also attributes
McClintockâ€™s openness to questioning the central dogma of molec-
ular biology to her lack of investment in the idea of the passivity
of nature. A related idea, from black feminist epistemology, is that
of using dialogue to assess knowledge claimsâ€”interacting with the
object of your knowledge rather than observing it from a detached
distance [16].
In short, feminist epistemology provides a more pluralistic, con-
textual, interactional, and critical corrective to various traditionally
â€œmasculineâ€ ways of doing science, a corrective that can also be
applied to how we create, use, and evaluate feature importance
methods in ML. As weâ€™ll see in the next section, the apparently
objective endeavor of figuring out how to attribute feature impor-
tances contains various implicit values, many of which are counter
to the principles of feminist epistemology.
3 VALUES IN FEATURE IMPORTANCE
METHODS
We define feature importance to be any quantitative assignment of
importance or influence to each input feature of somemodel learned
from data. How the notion of â€œimportance" is formalized varies
between methods, and can be defined in reference to either the
entire model and its training procedure, or to the modelâ€™s prediction
on one particular input.
To provide broader context for our more general claim that fea-
ture importance methods can be inflected with particular epistemic
values, a look at the history of feature importance methods is in-
structive. This history shows how contingent choices influenced by
practitionersâ€™ social environments led to the current state of affairs.
In particular, the widespread modern emphasis on predictive accu-
racy as the preeminent virtue of feature importance comes from
instrumentalist epistemic values championed by the likes of Leo
Breiman.
Informal descriptions of the utility of feature importance often
cite the high dimensionality of useful models, implying that they
could be made smaller [15, 30, 45]. In the popular textbook â€œEle-
ments of Statistical Learning,â€ for instance, Hastie, Tibshirani and
818
Epistemic values in feature importance methods FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
Friedman write, â€œIn data mining applications the input predictor
variables are seldom equally relevant. Often only a few of them
have substantial influence on the response; the vast majority are
irrelevant and could just as well have not been included. It is often
useful to learn the relative importance or contribution of each input
variable in predicting the responseâ€ [33]. Indeed, in the traditional,
statistical data-modeling setting, feature importance is often de-
scribed in terms of feature selection (such as in the case of F-tests),
with the ultimate goal being a simplified model that is more likely
to describe a real phenomenon. Data modeling in this sense has an
emphasis on description, that is, the objective of scientific truth.
In describing a change of direction away from data modeling,
Matthew Jones argues that modern research in data science is born
â€œmore from an engineering culture of predictive utility than from a
scientific culture of truth" [39], and the same can be said of feature
importance. This can be traced to Leo Breimanâ€™s novel proposal of
using feature importance for purposes other than feature selection.
Breiman argued against â€œassuming that the data are generated by a
given stochastic data modelâ€ in favor of a new algorithmic culture
[10].
Breiman proposed a new, influential view of data analysis which
de-emphasized the importance of developing probabilistic models of
underlying data generating processes. He argued that a prediction-
centered view was the right way to investigate the relationship
between the response and predictor variables, and that black-box
algorithmic models can provide more reliable and interesting in-
formation than weakly predictive data models can: â€œThe goal is
not interpretability, but accurate information. Interpretability is
a secondary goal that can be finessed.â€ Regardless, he proposed a
general way to measure the importance of predictors in a model:
â€œMy definition of variable importance is based on prediction. A vari-
able might be considered important if deleting it seriously affects
prediction accuracyâ€ [10].
This logic makes sense on its face, but Breiman never worked
towards providing a â€œsatisfactory theoretical definitionâ€ [10] of im-
portance. The mechanism of variable deletion, a step in the process
of feature selection, was the basis of the analysis; but for Breiman,
the advantage of using trees and ensembles of trees was exactly
that it allowed one to avoid doing feature selection: â€œExisting meth-
ods, he and his collaborator Meisel argued in a report to the US
Air Force, required the analyst to choose, without reference to the
data, a means for reducing its dimensionalityâ€ [38]. Breimanâ€™s ver-
sion of feature importance was not intended to help create more
veridical models of reality by deleting extraneous information, and
this is precisely the reason that permutation feature importance,
his approximation of the definition he intuited, has been shown
to be problematic [37]. Breiman wanted to calculate the change
in prediction accuracy under the removal of information, but the
removal is simulated instead of actually tested because there was
no intention of actually removing the feature.
Yet permute-and-predict notions of importance persisted through-
out recent algorithmic history. Hooker and Mentch observed that
this was because of their pragmatic properties: â€œthey are each com-
putationally cheap, requiring ğ‘‚ (ğ‘ ) operations, apply to the ğ‘“ (ğ‘¥)
derived from any learning method, have no tuning parameters,
and in relying only on averages, they are statistically very stable.
Moreover, the approach is readily understood and easily explained
to users in applied areasâ€ [37]. Permutation feature importance
influenced many ideas throughout the history of the problem of
calculating feature importance for black-box models [2, 4, 17, 28].
As argued in [23], accuracy is not necessarily a value-neutral eval-
uation criterion; the popularity of black-box algorithmic models
reflect a value-laden disciplinary shift in the field of machine learn-
ing, and this is paralleled in the history of feature importance.
While instrumentalism of the kind advocated by Breiman may
not be particularly feminist or anti-feminist, itâ€™s certainly an epis-
temic value that became dominant due to historically contingent
factors. Replies by Brad Efron and Bruce Hoadley in [10] argued
against Breimanâ€™s conception of feature importance and indicate a
possible alternate path for feature importance that failed to become
the dominant conception. Absent the social factors that influenced
Breimanâ€™s thinking and that facilitated his stature in the discipline,
perhaps we would have discussions of feature importance that have
a more pluralistic tone, rather than discussions that over-emphasize
the same â€œvirtuesâ€. The anti-pluralism inherent in Breimanâ€™s cham-
pioning of predictive accuracy can be seen as counter to feminist
epistemology. In addition, some feminists have regarded instrumen-
talist values as counter to feminism.3
This historical preamble provides crucial context for what fol-
lows, by demonstrating that the epistemic values favored today are
the product of historical processes influenced by social factors. In
the following sections, we describe more epistemic values that in-
flect modern feature importance methods, arguing that these values
conflict with feminist epistemology.
3.1 Universality as an epistemic value
One aspect of how values enter into feature importance measures is
the ways in which these are derived and evaluated. Many methods
are arrived at axiomatically, and many proposed evaluation stan-
dards pin their hopes on a universal â€œbenchmarkâ€ or â€œgold standardâ€
[21, 62]. We argue that the axiomatic approach towards derivation
and the â€œgold standardâ€ narrative for evaluation embed universal,
anti-pluralist values, which go against feminist epistemic values
[1, 20]. At the same time, human-centered studies on how expla-
nation methods are used in real-world contexts demonstrate that
these methods are used for very different purposes and audiences,
cautioning against applying context-free desiderata or evaluation
criteria [35, 36].
Many proposals of new explanation techniques take an axiomatic
approach, where they specify desirable properties for explanations,
then construct or derive a method that has those properties [9, 13,
51, 68, 71, 76]. They also often cite the fact that a method â€œuniquelyâ€
satisfies some set of axioms as a reason to accept it, such as in local
explanations based on Shapley values [51, 75]. The Shapley value
is the solution to a fundamental problem in game theory, which
is uniquely defined with respect to certain axiomatic conditions
put forth by many authors as fundamentally desirable. However,
even in the original context of game theory, some of these axioms
were thought by mathematicians to be â€œmathematically convenient"
[57] and â€œnot so innocent" [47]. By the former, they meant that the
solution is constrained to be unique only if it is required to satisfy
3See [34] for an example of this view, and, as a counterpoint, [1] for why anti-positivism
or anti-instrumentalism may be unhelpful for feminist goals.
819
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Leif Hancox-Li and I. Elizabeth Kumar
all of the axioms, which makes it tempting to say that the axioms
are desirable. Despite this, the creators of SHAP, a popular feature
importance method based on Shapley values, explicitly position
their method as embodying what they hope will be universally
desirable principles for all explanation methods: â€œThe thread of
unity that SHAP weaves through the literature is an encouraging
sign that common principles about model interpretation can inform
the development of future methodsâ€ [51]. Similar remarks on unity
can be found in further elaborations on SHAP [17].
There are some understandable reasons for taking an axiomatic
approach. As [76] argues, when evaluating an explanation method,
it can be unclear if a provided explanation looks wrong because the
model itself is wrong, or if it looks wrong because the explanation
method is at fault. The axiomatic approach sidesteps these empirical
ambiguities by providing a guarantee that the method has certain
ostensibly desirable mathematical properties, regardless of its em-
pirical performance. However, proponents of axiomatic approaches
position their methods as providing â€œintuitivelyâ€ desirable proper-
ties [17, 76]. Implicit in this way of justifying axiomatic properties
is that the intuition referred to is universally held or applicable to
all situations.4 This ignores the fact that those properties may not
be the most appropriate ones in every context of application.
A more pluralistic view of explanatory virtues would cast a criti-
cal eye on the idea of universally desirable properties and the ability
of a researcher to know what those are a priori. User studies back
up the idea that explanation methods are used in very different
contexts, for different purposes [35, 36]. In the absence of user stud-
ies comparing the usefulness of explanation methods for different
purposes, itâ€™s not obvious that a method that is more effective for
communicating with non-technical stakeholders is necessarily also
more effective for helping data scientists debug a model. As a con-
crete example, one user study found that SHAP is not very helpful
for helping data scientists spot obvious problems with the data [41],
but another user study found that SHAP is good at producing ex-
planations that fit human intuition [50]. As we point out in Section
4.2, many papers on ML feature importance methods evaluate the
methods on â€œtasksâ€ that are untied to any specific pragmatic goal,
even as the feminist epistemology literature has long pointed out
the problems with â€œsupposedly universal principles drawn from
bounded artificial examples" [1].
In the broader context of ML research in general, others have
warned of the pitfalls of having benchmarks that determine what
â€œsuccessâ€ or â€œprogressâ€ is [19, 23]. Even the organizers of ML compe-
titions admit that competing on fixed metrics can lead to a kind of
meaningless optimization after a while [27]. These broader lessons
also apply to ML explanations. Favoring a fixed number of bench-
marks for ML explanations risks channelizing new methods into
a few directions, privileging some goals or audiences over others.
Without broader participation from different stakeholders in the
design of ML explanations, this runs the risk of not just restricting
directions of research, but also creating explanation methods that
privilege the interests of certain groups over others.
3.1.1 Additively partitioning influence. Weâ€™ve argued that the idea
of a universally applicable explanationmethod, based on universally
4See below for examples of specific axioms that are unlikely to be intuitive or valid in
many real-world situations.
desirable axioms, is contrary to pluralism. In addition, some axioms
that are touted as obviously desirable contain their own embedded
values. One type of axiom that is common in axiomatic approaches
towards explanations is an axiom that encodes the additivity of
feature importances.
The idea behind this notion is that the individual contributions
of each feature, as measured by the feature importance method,
should add up to the interventional effect of all the features taken
together, thus partitioning the total effect of the feature set among
its components. This axiom either goes unnamed [5] or appears un-
der different names in different papers, such as â€œcompletenessâ€ [76],
â€œsummation-to-deltaâ€ [67], or â€œlocal accuracyâ€ [51]. In particular,
given a machine learning model ğ‘“ which computes a prediction for
a specific instance ğ‘¥ , the axioms can be described as the following.
Axiom 1 (Completeness). Given feature attributions ğœ™1, ğœ™2, ...ğœ™ğ‘›
corresponding to features 1 throughğ‘› for a certain input ğ‘¥ to a function
ğ‘“ , ğ‘“ (ğ‘¥) = ğ‘“ (ğ‘¥ â€²) + âˆ‘ğ‘›
ğ‘–=1 ğœ™ğ‘– for some baseline input ğ‘¥ â€².
Axiom 2 (LocalAccuracy). Given feature attributionsğœ™1, ğœ™2, ...ğœ™ğ‘›
corresponding to features 1 through ğ‘› for a certain input ğ‘¥ to a func-
tion ğ‘“ and its simplified features â„ğ‘– (ğ‘¥) representing the â€œpresence" of
feature ğ‘– , ğ‘“ (ğ‘¥) = ğœ™0 +
âˆ‘ğ‘›
ğ‘–=1 ğœ™ğ‘–â„ğ‘– (ğ‘¥) for some baseline ğœ™0.
These two axioms are equivalent if we take ğœ™0 = ğ‘“ (ğ‘¥ â€²) and
define â„ğ‘– (ğ‘¥) to be 1 for all ğ‘– .
A stronger desideratum, which requires the sums of individual
contributions to be proportional to the joint effect for each subset
of features, is called faithfulness [9]. Faithfulness is proposed as
a metric, rather than an axiom, which measures closeness to an
ideal: a perfect faithfulness score implies completeness and local
accuracy.
Definition 1 (Faithfulness). Given feature attributionsğœ™1, ğœ™2, ...ğœ™ğ‘›
corresponding to features 1 through ğ‘› for a certain input ğ‘¥ to a func-
tion ğ‘“ , for any subset ğ‘† , let ğ‘¥ğ‘† be equal to ğ‘¥ with the features not in ğ‘†
replaced with those from some baseline vector ğ‘¥ â€². Then faithfulness is
defined as
corrğ‘†âŠ†{ğ‘›}
(âˆ‘
ğ‘–âˆˆğ‘†
ğœ™ğ‘– , ğ‘“ (ğ‘¥) âˆ’ ğ‘“ (ğ‘¥ğ‘† )
)
Axioms like completeness, local accuracy, and faithfulness align
with the natural properties of coefficient size in linear models of
independently distributed features. 5 Given a linear model ğ‘“ (ğ‘¥) =
ğ›½0 +
âˆ‘ğ‘›
ğ‘–=1 ğ›½ğ‘–ğ‘¥ğ‘– , it is easy to see what happens when we change the
features in some set ğ‘† to be taken from some baseline vector ğ‘¥ â€²:
the difference between ğ‘¥ and ğ‘¥ğ‘† is exactly
âˆ‘
ğ‘–âˆˆğ‘† ğ›½ğ‘– (ğ‘¥ğ‘– âˆ’ ğ‘¥ â€²
ğ‘–
). Thus,
the feature importance term ğœ™ğ‘– = ğ›½ğ‘– (ğ‘¥ğ‘– âˆ’ ğ‘¥ â€²
ğ‘–
), where ğ‘¥ â€² is taken
to be the empirical mean of the data, satisfies Completeness and
Local Accuracy, as well as minimizing Faithfulness, and is in fact
the value assigned by SHAP for this model. In this linear setting,
ğœ™ğ‘– represents the univariate effect of perturbing feature ğ‘– from its
expectation to the value represented in the input.
5The related axiom of linearity in [76], that linear combinations of models should
result in linear combinations of feature attributions, is justified â€œintuitively." Linearity is
implied by how we would want coefficients to combine if we were talking about linear
regression. This â€œintuition" thus again presumes that coefficient size is a reasonable
way to explain linear models.
820
Epistemic values in feature importance methods FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
If a user interprets feature importance to represent univariate
effects in all modeling settings, however, this property is funda-
mentally misleading in many applications of complex ML models.
Often, input features are highly correlated, and ML models learn to
infer complex interactions between them. Additivity axioms seek
to impose an ideal which does not hold in the real world. Impos-
ing additivity in real-world conditions thus attempts to optimally
simplify real interactions in the data in order to fulfill some ideal
of analytic and cognitive tractability. In doing so, researchers privi-
lege achieving some kind of mathematical ideal over modeling the
genuine complexity in the data.
This is not to say that it is always bad to impose any of these
axioms. Rather, they serve as examples where a value-laden choice
is made, namely, to stipulate a mathematical ideal to obtain certain
analytic goals as opposed to creating new tools to handle complexity
at face value. This starting from what is formally tractable means
that â€œwhat is deemed interpretable comes to affect what is presented
as explainable, and thus what can be considered meaningful in an
algorithmic systemâ€ [8]. Other potential sources of meaning are
foreclosed.
There might be certain purposes for which favoring a mathe-
matical ideal is appropriate. But for other purposes, such as un-
derstanding complex interactions between features without over-
simplifying, it is less appropriate. The value-laden nature of this
design choice in creating explanation methods is hidden behind
the apparently objective and â€œrigorous" approach of starting from
axioms and proving uniqueness theorems based on them.
It is important to acknowledge that the idealized nature of the
properties discussed above arenâ€™t just a mathematical ideal, but
also a cognitive ideal: it is thought that itâ€™s easier for people to
understand and use explanations if the feature attributions behave
as described in the relevant axioms. But again, if the actual features
do not behave as stipulated in the axioms, then the cognitive ideal
merely gives the appearance of understandability. This is poten-
tially dangerous when it leads to data scientists having unjustified
confidence in their model and thus to premature intents to deploy,
as found in [41]. In addition, to the extent that there are cognitive
limitations on the kinds of explanations that can be understood,
this doesnâ€™t mean that the correct way to impose those limitations
is by the deductive approach of starting from axioms that translate
those limitations into mathematics. While the deductive approach
may be more in line with conventions of scholarship in machine
learning, perhaps more inductive approaches that start with em-
pirical work on what users need in specific contexts, rather than
using user experiments to validate a deductive approach after the
fact, would lead to different types of explanation methods.
3.2 Epistemic values in the data-versus-model
debate
Another way in which epistemic values may enter into particular
definitions of a feature importance method is the extent to which
the explanation emphasizes structure in the data, as opposed to
structure in the model. This contrast is pertinent in the ongoing
debate about the appropriate type of probability distribution to
use in computing various feature importance measures, including
(but not limited to) SHAP [44]. The choice is between using â€œinter-
ventionalâ€ distributions, which ignore correlations between input
variables, and using â€œobservationalâ€ distributions, which take those
correlations into account.
Specifically, given an input ğ‘¥ to a function ğ‘“ and a specific feature
set ğ‘† , the question is how to quantify the effect of removing infor-
mation about ğ‘† by looking at the value of the quantity ğ‘“ (ğ‘¥) âˆ’ ğ‘“ (ğ‘¥ğ‘† ),
where ğ‘¥ğ‘† is ğ‘¥ but with the features in ğ‘† replaced by samples from
some distribution. One option is to intervene on those features,
using (for instance) their marginal distributions or a fixed baseline
value, as in [18, 55]; the other is to condition on the features not in
ğ‘† and sample from what the values in ğ‘† might be from the other
features in ğ‘† , based on correlations in the observational data.
The authors of SHAP recommend using interventional distri-
butions if we want the explanation to be â€œtrue to the modelâ€, and
observational distributions if we want the explanation to be â€œtrue to
the dataâ€ [14]. In short, we should use interventional distributions
if we want to know how the model works independently of the
structure of the particular data itâ€™s used on. On the other hand, they
argue, if our interest is in a â€œnatural mechanism in the world,â€ then
the explanation derived from observational distributions captures
the important causal features more accurately.
Going back to the importance of contextual thinking in feminist
values, we can see that the â€œtrue to the modelâ€ option abstracts
the model away from its application context, which is particularly
unhelpful in assessing its impact on real populations. The authorsâ€™
justification for interventional distributions in this option is illu-
minating, as they use an example that is particularly ill-suited for
interventional distributions once we carefully consider the context
of application.
To compare interventional and observational distributions, the
authors of [14] consider a model that predicts whether an applicant
will default on a loan. They then test how useful the interventional
and observational SHAP values would be to potential applicants,
by considering how effectively an applicant can modify their risk
by setting a feature to its mean. However, this way of testing is
decontextualized from the reality of the deployment context, be-
cause in the real world, itâ€™s often impractical for most applicants to
be able to change only one feature without also changing others
[7]. Based on this method of testing, the authors conclude that the
interventional distribution is better if we want to be â€œtrue to the
model,â€ and in addition that â€œ[b]eing true to the model is the best
choice for most applications of explainable AI, where the goal is to
explain the model itself.â€ This statement reveals the model-centered
orientation that is an implicit epistemic value in their research. A
countervailing perspective might consider that many real-world ap-
plications work on data thatâ€™s highly correlated, in contexts where
individuals are often not able to change the value of one feature
at a time.6 The authors implicitly center understanding the model
6In particular, the credit data that the authors use in their experiment is probably
highly correlated and therefore hard for decision subjects to intervene on â€œcleanlyâ€ in
the way the authors do in the experiment. The original dataset used in [14] has been
removed from public view, but given that itâ€™s in the context of credit, the criticisms of
[7] on how feature-highlighting explanations of credit models are hard for decision
subjects to act on are probably applicable to it.
821
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Leif Hancox-Li and I. Elizabeth Kumar
independently of application context. In contrast, feminist episte-
mologists often criticize the use of simple examples and advocate
considering real-life examples in all their complexity [1].
Other researchers have taken sides in this debate in the context
of their own work. The authors of [76], for instance, argue that
implementation invariance is a desirable property of feature impor-
tance: if two different models practically describe the same function,
their feature attributions should be the same. Asserting that the
mechanics of how the inputs relate to the outputs are not important
reflects the rejection of data modeling as understood in [10]. The
authors of [76] additionally assert that we should not be able to
attribute influence to features not in the model, and instead only
explain the model itself. Merrick and Taly share this view, calling
features not in the model â€œirrelevant" [55]. This mathematically
implies the choice of interventional distributions in SHAP, as any
influence notion that can be captured through correlation could
assign influence to unmodelled variables. Although individual coun-
terarguments to these ideas have been pointed out [37, 70], they
remain a persistent trend in the explainable ML literature.
3.3 Computational modularity as an epistemic
value
Another way in which post-hoc feature importance methods con-
tain embedded values is in design decisions that abstract from
real-world complexities and hide implementation details behind
an easy-to-use interface, all in a plug-and-play package. Previous
research on values in computer science has identified this as a key
part of computational culture [29, 52], and indeed, this kind of mod-
ular design, where details are hidden except on a need-to-know
basis, is part of what constitutes good design in programming [26].
Other researchers have cautioned against the same tendency when
it comes to â€œfairness" solutions in machine learning, calling it the
â€œPortability Trapâ€ [64]. Lucy Suchman has characterized this ten-
dency as â€œconstruing technical systems as commodities that can
be stabilized and cut loose from the sites of their production long
enough to be exported en masse to the sites of their useâ€ [74]. As-
suming that a solution is portable in this way is possible only if you
think contextual factors (e.g. the circumstances that the model and
explanation are used in) are less important than the solution itself.
In the context of feature importance methods, hiding the details
is not a value-neutral decision, especially when the very virtues of
the methods themselves are under debate. User research has shown
that many data scientists interpret SHAP outputs in a relatively
uncritical manner. Many are unable to describe the visualizations
produced by these methods, even though they trust them [41]. The
highly â€œusableâ€ nature of the application programming interface
(API), which allows data scientists to quickly produce visualizations
without having to understand how things work under the hood,
encourages this combination of high trust in results and poor under-
standing of methods. This relates to our account in Section 3 of how
black-box predictive modeling culture arose out of Breimanâ€™s gov-
ernment contracting milieu, where obtaining results was prioritized
over understanding. Ironically, this push for obtaining results has
been extended to the case of explanation methods, which ostensibly
exist in order to improve understanding.
4 TOWARDS PLURALISTIC, CONTEXTUAL,
AND INTERACTIVE APPROACHES TO
EXPLANATION
Based on examples of overly universal approaches to ML explana-
tions, and on the values espoused by feminist epistemologists, we
suggest some preliminary steps towards creating explanations that
are more consonant with feminist epistemic values. Much of the
following advice is not necessarily specific to feature importance
methods: many parts are also applicable to other aspects of ML.
In part, this reflects the fact that ML as a field has not paid much
attention to feminist epistemology, so that there are ample oppor-
tunities to apply feminist epistemological methods to almost any
aspect of ML.
4.1 Incorporating subjugated points of view
In line with the idea that the viewpoints of the marginalized may
have a crucial critical edge over the perspectives of the powerful,
we should place more emphasis on ways for subjugated populations
to participate in designing ML explanations. Participation was iden-
tified as a crucial quality for feminist design in HCI by Shaowen
Bardzell, who argued that since knowers are not substitutable for
one another, â€œongoing participation and dialogue among designers
and users can lead to valuable insights that could not be achieved
scientificallyâ€ [6]. Participatory machine learning practices [54, 73]
could be extended from participating in algorithm design to partic-
ipating in the design of explanations, albeit with an awareness that
participatory design is not a cure-all [72].
Incorporating the perspectives of marginalized populations is
particularly important when they are stakeholders in the decisions
being explained, for example, if they are the subjects of automatic
allocations of resources like health or housing. Given the demo-
graphics of ML practitioners [11], the designers of algorithms mak-
ing such decisions are unlikely to have a range of social experiences
similar to the decision subjectsâ€™. Leaving design decisions about end-
user explanations up to data scientists would thus likely exclude
subjugated points of view.
4.2 Evaluating explanations contextually
One important recommendation in feminist epistemology is to ac-
knowledge the situatedness of knowledgeâ€”that different knowers
have different perspectives. A situated investigation, then, is one
that â€œforefronts the details of the context of specific people and
places at particular points in time, rather than trying to study a
system or question with an abstract approach removed from social
contextâ€ [40]. Elish and boyd [25] argue that the act of develop-
ing machine learning models itself should be framed as a situated
practice, since humans are engaged in personally interpreting data
during the development process. So, too, should any attempt to
incorporate explanations into a machine learning system.
In evaluating explanations in a feminist spirit, we should con-
sider various contextual factors that affect how well an explanation
can perform its social function, such as how explanations are in-
terpreted by stakeholders, unintended consequences, how explana-
tions perform in realistic applications as opposed to toy datasets
or problems, and what contexts exist that could cause an expla-
nation method to fail. By bringing in contextual considerations,
822
Epistemic values in feature importance methods FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
feminist epistemology enlarges the problem definition of traditional
analytic epistemology to include social and ethical considerations.
Similarly, one feminist direction that ML explanations could ex-
pand in is enlarging traditional problem definitions for work on
explanations. Currently, a large proportion of work follows this
template [5, 51, 67, 76]:
(1) Propose a new method of generating explanations for black-
box models.
(2) Show that the new method satisfies certain desirable proper-
ties. Often, these properties are defined by axioms and the
method is proven to satisfy them. Sometimes, the method
is shown to satisfy them by carrying out computational
experiments. Rarely are the allegedly desirable properties
validated by considering what uptake theyâ€™ll get in different
social contexts and different audiences.
(3) Apply the method to one or more datasets. Use it to explain
predictions trained and made on the dataset(s). Validate that
these explanations â€œmake sense.â€
(3) Is typically the last step of evaluation, but occasionally, re-
searchers include an additional step of conducting user studies
on the explanation method. Test subjects are often recruited via
general surveying sites like Amazonâ€™s Mechanical Turk, and asked
to evaluate explanations with little to no context about why the
predictions are being made and what purposes the explanations
would serve in a real-world applications [49, 61, 66, 69]. Test sub-
jects are typically asked to evaluate the explanation on relatively
context-free properties such as using the provided explanations
to improve the algorithm or simulating the modelâ€™s predictions,
without explaining what the predictions are being used for [59, 61].
However, it is hard to evaluate the virtues of explanations with-
out considering the context in which they are meant to operateâ€”the
audience, the goals of the explanation, the type of model or data
they are applied to, etc. An explanation that best helps data scien-
tists improve a model is not necessarily the best type of explanation
for a customer whose request has been denied by an algorithmâ€”if,
for example, the most important features according to the expla-
nation are ones that the customer cannot change [7]. For similar
reasons, explanations that are tailored to satisfy legal requirements
like GDPR are also not necessarily the most helpful explanations
for decision subjects or model builders [24]. Model builders may
benefit more from more sophisticated or technical explanations
that are harder for general audiences to understand.
To apply lessons from feminist epistemology to ML explanations,
a first step would be to evaluate explanations in a more context-
sensitive manner. Axioms should be considered in relation to how
desirable they are in specific contexts. User studies on different
types of consumers of explanations, rather than on generic con-
sumers, would help. In line with the epistemic advantages that may
accrue to marginalized persons, user studies focusing on groups
marginalized by algorithms may be particularly helpful in surfac-
ing contextual harms that might be overlooked by more privileged
groups. Benchmarks or gold standards tend to impose universal,
context-insensitive norms, so they should be used with extreme
caution or avoided, as we argued in section 3.1. While using bench-
marks may foster an impression that the results are more objective,
testing explanation methods on users trying to solve real prob-
lems, real (and novel) datasets, all embedded in real social contexts,
would provide more comprehensive and nuanced information on
the practicalities of implementing the methods. De-contextualizing
explanation methods allows for easier comparison across contexts,
but also abstracts away from social factors that potentially affect
the methodsâ€™ usefulness.
4.3 Encouraging seamfulness and pluralism
Designers have proposed the idea of â€œseamfulâ€ design as a way to
encourage technology users to avoid over-trusting one interpreta-
tion and keep multiple interpretations in mind [12]. These ideas can
also be applied when we design visualizations for ML explanations.
As an antidote against data scientists over-trusting explanations
[41], creators of explanatory outputs could design for the following
effects [65]:
(1) Purposefully block the most obvious interpretations of any
visualizations, especially if they are misleading, in order
to stimulate new ways of understanding the visualizations.
One example where this would be helpful is in designing
visualizations of log-odds probabilities7 to thwart the more
â€œobviousâ€ way of reading those quantities in a more intuitive
probability space. Another possibility is to design feature
attribution charts in a way that reminds viewers that the
separate â€œbars" displayed for each feature arenâ€™t actually
separate in reality, due to strong interactions and correlations
between features.
(2) Downplay the explanationâ€™s authority. We should work on
ways to indicate how charts may be inaccurate or mislead-
ing. For example, perhaps feature attributions for strongly
correlated features can be presented in a more â€œuncertainâ€
manner relative to feature attributions for less correlated fea-
tures. Here, the uncertainty quantification literature could be
particularly helpful [58]. Alternatively, instead of presenting
feature importances in the familiar forms of bar charts or a
list of numbers, they could be presented in less familiar ways,
providing room for reinterpretation of what they mean and
thwarting tendencies to understand them in the same way
one understands other bar charts. In an example described
in [65], the frequencies of various email categories were rep-
resented by changes in a plantâ€™s shape. By presenting the
data in an unfamiliar form, the artists invite active inter-
pretation and reflection, subverting naively straightforward
interpretations.
(3) Design explanatory visualizations in a way that is ambigu-
ous and thwarts any one authoritative interpretation. One
concrete suggestion for doing so is to integrate the spatial
distortions that dimensionality reduction algorithms pro-
duce directly into cluster visualizations [8]. While this might
seem to be at odds with the highly quantitative nature of
most explanationmethods, the presence of numbers attached
to particular features doesnâ€™t mean that those numbers are
accurate or adequate representations of the featuresâ€™ actual
7SHAP, a popular post-hoc explanation method, displays feature importance values
for classification algorithms in a log-odds space [48].
823
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Leif Hancox-Li and I. Elizabeth Kumar
importances, especially when the explanation method is tai-
lored to satisfy axioms that donâ€™t hold in the real world (see
Sections 3.1 and 3.2). Given the uncertain relationships be-
tween those numbers and the actual features in the data,
visualizing them as though they are certain and have unam-
biguous importance values is misleading. For example, one
can imagine an interface that includes multiple explanatory
accounts of a model and helps users see the differences be-
tween them. In contrast, we currently have multiple, discrete
explanation methods that each present their own seemingly
authoritative accounts, hiding the uncertainty that is inher-
ent in each of them.
4.4 Acknowledging the role of problem
definition
Section 4.2 outlined a â€œrecipe" for writing a paper about a new
ML explanation method. That this â€œrecipe" exists suggests some
kind of common understanding about what the â€œproblem" of ML
explanation is, andwhat counts as a solution to it. One of the lessons
of feminist epistemology is to question how boundaries are drawn
around objects. The recipe described suggests a certain limited view
of what an explanation is, which largely leaves out social context
except at the possible but infrequent step of conducting a user study.
If we instead question the boundaries of the problem, new research
projects open up.
The field of human-computer interaction (HCI) has helped in
expanding the boundaries of ML explanation research by doing
better user studies [41] and exploring the social functions of ML
explanations [35, 36]. In other parts of ML, HCI has also added
new perspectives by studying how improving user interfaces can
improve ML systems even without intervening on the algorithm
[63]. Others have studied how co-creating models with end users
leads to more more explainable rules andmore generalizable models
[79].
The increased contributions to this field from HCI is a positive
trend that will hopefully accelerate the incorporation of feminist
epistemic values into ML explanations. One can only hope for
more work that integrates traditional technical work with human-
centered approaches, as advocated in [77].
4.5 Using formalisms critically
As explained in Section 2, feminist epistemology has long taken
a critical stance towards fully formalized systems, instead empha-
sizing the interactive nature of knowledge creation and the impor-
tance of exploring multiple possible meanings. Other researchers
have remarked on the tendency of ML to over-value formal proofs
[46], and the doubts we raise above about axiomatic approaches to
explanation can be seen as following this line of thought.
However, jettisoning formalisms altogether is not a solution
either, since explaining a formally defined ML algorithm requires
engaging with formalisms at some level. Rather, a feminist approach
would use formalisms when appropriate, with a careful attention
to what possibilities a particular formal framework excludes or
renders unsayable, and what it inadvertently favors as prescriptive
[3]. When one does use a formal framework, its limitations and
idealizations should be clearly acknowledged.
4.6 Using interactive approaches to
explanation
As explained in Section 2, feminist epistemology emphasizes in-
teractional ways of knowing, where the object of study and the
knower are engaged in a back-and-forth conversation, rather than
a one-directional process where the knower decodes a passive ob-
ject. Most mainstream ML explanation methods require minimal
interaction between the user and the algorithm before creating
the explanation, conforming more to the latter mold of an active
knower and a passive object. The user typically enters the features
they are interested in and the type of explanation they want (e.g.
local or global), and gets a static visualization or some numbers in
return.
HCI research suggests there might be more interactive ways of
creating explanations. One example is Gamut, a visual analytics
system that provides an interactive interface to support the inter-
pretation of generalized additive models [35]. Users found Gamut
to be helpful and wanted to use it to understand their own data.
One possible way in which interactivity could be extended beyond
what Gamut did is by having users interactively design explana-
tion methods, rather than just having an interactive interface for
generating pre-determined types of explanations.
5 CONCLUSION
Weâ€™ve attempted to apply feminist epistemological values to ML
feature importance methods, diagnosing how the ways we con-
struct and evaluate these methods may insufficiently incorporate
these values. Many explanation methods over-emphasize theoret-
ical properties that may seem desirable in idealized contexts but
have questionable usefulness in many applicational contexts. Popu-
lar feature importance methods also embed strong instrumentalist
values that arose from a government contracting and defense mi-
lieu. Practitioners value computational modularity in the form of
plug-and-play software packages for generating explanations. All
of these tendenciesâ€”de-emphasizing real-world data, instrumen-
talism, and computational modularityâ€”are value-laden and should
not be considered to be objectively desirable in all contexts.
In particular, we have argued that popular methods of con-
structing and evaluating feature importance methods do not suffi-
ciently incorporate feminist epistemological values such as context-
sensitivity, critically interrogating boundaries, taking the expertise
of subjugated knowers seriously, pluralism, and interactional ways
of knowing. This is not to say that these popular methods are use-
less, but to point to the uniformity of values that they embody,
and suggest further possibilities that fall outside the existing para-
digm. To this end, weâ€™ve suggested some directions in which future
research on explanations can proceed, guided by lessons from fem-
inist epistemology. In our view, we can incorporate feminist values
by better incorporating subjugated points of view into the process
of designing explanations, evaluating explanations more contextu-
ally, resisting the temptation to create benchmarks, thinking criti-
cally about how problem definitions favor some values over others,
using formalisms with a critical eye, and working on interactive
approaches to constructing explanations.
Our proposals mirror recent calls to move towards more human-
centered approaches to machine learning [77]. To this end, itâ€™s
824
Epistemic values in feature importance methods FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
heartening that user studies on ML explanations have started ap-
pearing in major HCI conferences [36, 41]. However, at mainstream,
â€œtechnicalâ€ ML conferences, this kind of work is still largely rele-
gated to special workshops rather than being in the main program.
As ML expands its reach into human life, we hope that ML re-
searchers will also correspondingly acknowledge that this broader
reach necessitates allowing a broader set of methodologies into
its boundaries and including more diverse approaches towards
evaluating explanations.
ACKNOWLEDGMENTS
We are grateful to the FAccT reviewers and Suresh Venkatasubrama-
nian for their feedback and suggestions, as well as to Miriah Meyer
and the members of her Research Paradigms for Human-Centered
Computing seminar for the thoughtful discussions which inspired
this work. This research was partially supported by the NSF under
grant EAGER-2041960.
REFERENCES
[1] Alison Adam and Helen Richardson. 2001. Feminist philosophy and information
systems. Information Systems Frontiers 3, 2 (2001), 143â€“154.
[2] Philip Adler, Casey Falk, Sorelle A Friedler, Tionney Nix, Gabriel Rybeck, Carlos
Scheidegger, Brandon Smith, and Suresh Venkatasubramanian. 2018. Auditing
black-box models for indirect influence. Knowledge and Information Systems 54,
1 (2018), 95â€“122.
[3] Philip Agre. 1997. Computation and human experience. Cambridge University
Press, USA.
[4] AndrÃ© Altmann, Laura ToloÅŸi, Oliver Sander, and Thomas Lengauer. 2010. Per-
mutation importance: a corrected feature importance measure. Bioinformatics
26, 10 (2010), 1340â€“1347.
[5] Sebastian Bach, Alexander Binder, GrÃ©goire Montavon, Frederick Klauschen,
Klaus-Robert MÃ¼ller, and Wojciech Samek. 2015. On pixel-wise explanations for
non-linear classifier decisions by layer-wise relevance propagation. PloS one 10,
7 (2015), e0130140.
[6] Shaowen Bardzell. 2010. Feminist HCI: Taking Stock and Outlining an Agenda for
Design. Association for Computing Machinery, New York, NY, USA, 1301â€“1310.
https://doi.org/10.1145/1753326.1753521
[7] Solon Barocas, Andrew D. Selbst, and Manish Raghavan. 2020. The Hidden
Assumptions behind Counterfactual Explanations and Principal Reasons. In
Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency
(Barcelona, Spain) (FAccT â€™20). Association for Computing Machinery, New York,
NY, USA, 80â€“89. https://doi.org/10.1145/3351095.3372830
[8] Jesse Josua Benjamin and Claudia MÃ¼ller-Birn. 2019. Materializing Interpretabil-
ity: Exploring Meaning in Algorithmic Systems. In Companion Publication of the
2019 on Designing Interactive Systems Conference 2019 Companion (San Diego, CA,
USA) (DIS â€™19 Companion). Association for Computing Machinery, New York,
NY, USA, 123â€“127. https://doi.org/10.1145/3301019.3323900
[9] Umang Bhatt, Adrian Weller, and JosÃ© M. F. Moura. 2020. Evaluating and Aggre-
gating Feature-based Model Explanations. In Proceedings of the Twenty-Ninth In-
ternational Joint Conference on Artificial Intelligence, IJCAI-20, Christian Bessiere
(Ed.). International Joint Conferences on Artificial Intelligence Organization,
3016â€“3022. https://doi.org/10.24963/ijcai.2020/417
[10] Leo Breiman et al. 2001. Statistical modeling: The two cultures (with comments
and a rejoinder by the author). Statistical science 16, 3 (2001), 199â€“231.
[11] Alex Campolo, Madelyn Sanfilippo, Meredith Whittaker, and Kate Crawford.
2017. AI Now 2017 Report. https://ainowinstitute.org/AI_Now_2017_Report.pdf.
Accessed: 2020-09-23.
[12] M. Chalmers, I. MacColl, and M. Bell. 2003. Seamful design: showing the seams
in wearable computing. In IEE Eurowearable â€™03. Institution of Engineering and
Technology, 11â€“16(5). https://digital-library.theiet.org/content/conferences/10.
1049/ic_20030140
[13] Aditya Chattopadhyay, Piyushi Manupriya, Anirban Sarkar, and Vineeth N Bala-
subramanian. 2019. Neural Network Attributions: A Causal Perspective. In Pro-
ceedings of the 36th International Conference on Machine Learning (Proceedings of
Machine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdi-
nov (Eds.). PMLR, 981â€“990. http://proceedings.mlr.press/v97/chattopadhyay19a.
html
[14] Hugh Chen, Joseph D. Janizek, Scott Lundberg, and Su-In Lee. 2020. True to the
Model or True to the Data? arXiv:2006.16234 [cs.LG]
[15] Q. Chen, M. Zhang, and B. Xue. 2017. Feature Selection to Improve Generalization
of Genetic Programming for High-Dimensional Symbolic Regression. IEEE
Transactions on Evolutionary Computation 21, 5 (2017), 792â€“806.
[16] Patricia Hill Collins. 1990. Black feminist thought: Knowledge, consciousness, and
the politics of empowerment. New York: Routledge.
[17] Ian Covert, Scott Lundberg, and Su-In Lee. 2020. Understanding Global Feature
Contributions Through Additive Importance Measures. arXiv:2004.00668 [cs.LG]
[18] A. Datta, S. Sen, and Y. Zick. 2016. Algorithmic Transparency via Quantitative
Input Influence: Theory and Experiments with Learning Systems. In 2016 IEEE
Symposium on Security and Privacy (SP). 598â€“617. https://doi.org/10.1109/SP.
2016.42
[19] Emily Denton, Alex Hanna, Razvan Amironesei, Andrew Smart, Hilary Nicole,
and Morgan Klaus Scheuerman. 2020. Bringing the People Back In: Contesting
Benchmark Machine Learning Datasets. https://arxiv.org/abs/2007.07399
[20] Catherine Dâ€™Ignazio and Lauren F Klein. 2020. Data feminism. MIT Press.
[21] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Inter-
pretable Machine Learning. arXiv:1702.08608 [stat.ML]
[22] Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman,
David Oâ€™Brien, Stuart Schieber, James Waldo, David Weinberger, and Alexandra
Wood. 2017. Accountability of AI under the law: The role of explanation. arXiv
preprint arXiv:1711.01134 (2017).
[23] Ravit Dotan and Smitha Milli. 2020. Value-Laden Disciplinary Shifts in Machine
Learning. Extended abstract. In Proceedings of the 2020 Conference on Fairness,
Accountability, and Transparency (Barcelona, Spain) (FAT* â€™20). Association for
ComputingMachinery, New York, NY, USA, 294. https://doi.org/10.1145/3351095.
3373157
[24] Lilian Edwards and Michael Veale. 2017. Slave to the algorithm: Why a right to
an explanation is probably not the remedy you are looking for. Duke L. & Tech.
Rev. 16 (2017), 18.
[25] Madeleine Clare Elish and Danah Boyd. 2018. Situating methods in the magic of
Big Data and AI. Communication monographs 85, 1 (2018), 57â€“80.
[26] Eric Evans. 2004. Domain-driven design: tackling complexity in the heart of software.
Addison-Wesley Professional.
[27] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John
Winn, and Andrew Zisserman. 2015. The PASCAL visual object classes challenge:
A retrospective. International journal of computer vision 111, 1 (2015), 98â€“136.
[28] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2019. Model Class Reliance:
Variable Importance Measures for any Machine Learning Model Class, from the
â€œRashomonâ€ Perspective. arXiv:1801.01489 [stat.ME]
[29] David Golumbia. 2009. The cultural logic of computation. Harvard University
Press.
[30] Isabelle Guyon and AndrÃ© Elisseeff. 2003. An introduction to variable and feature
selection. Journal of machine learning research 3, Mar (2003), 1157â€“1182.
[31] Donna Haraway. 1988. Situated knowledges: The science question in feminism
and the privilege of partial perspective. Feminist studies 14, 3 (1988), 575â€“599.
[32] Sandra Harding. 1995. â€œStrong objectivityâ€: A response to the new objectivity
question. Synthese 104, 3 (1995), 331â€“349.
[33] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2009. The elements of
statistical learning: data mining, inference, and prediction. Springer Science &
Business Media.
[34] Rudy Hirschheim, Heinz K Klein, and Kalle Lyytinen. 1996. Exploring the intel-
lectual structures of information systems development: a social action theoretic
analysis. Accounting, Management and Information Technologies 6, 1-2 (1996),
1â€“64.
[35] Fred Hohman, Andrew Head, Rich Caruana, Robert DeLine, and Steven M.
Drucker. 2019. Gamut: A Design Probe to Understand How Data Scientists
Understand Machine Learning Models. In Proceedings of the 2019 CHI Con-
ference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI
â€™19). Association for Computing Machinery, New York, NY, USA, 1â€“13. https:
//doi.org/10.1145/3290605.3300809
[36] Sungsoo Ray Hong, Jessica Hullman, and Enrico Bertini. 2020. Human Factors in
Model Interpretability: Industry Practices, Challenges, and Needs. Proceedings of
the ACM on Human-Computer Interaction 4, CSCW1 (2020), 1â€“26.
[37] Giles Hooker and Lucas Mentch. 2019. Please stop permuting features: An
explanation and alternatives. arXiv preprint arXiv:1905.03151 (2019).
[38] Matthew Jones. Forthcoming. Decision trees, random forests, and the genealogy
of the black box. In Algorithmic Modernity: Mechanizing thought and action,
1500-2000, Massimo Mazzotti and Morgan Ames (Eds.). Oxford University Press.
[39] Matthew L Jones. 2018. How We Became Instrumentalists (Again): Data Posi-
tivism since World War II. Historical Studies in the Natural Sciences 48, 5 (2018),
673â€“684.
[40] Michael Katell, Meg Young, Dharma Dailey, Bernease Herman, Vivian Guetler,
Aaron Tam, Corinne Bintz, Daniella Raz, and P. M. Krafft. 2020. Toward Situated
Interventions for Algorithmic Equity: Lessons from the Field. In Proceedings of
the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona,
Spain) (FAT* â€™20). Association for Computing Machinery, New York, NY, USA,
45â€“55. https://doi.org/10.1145/3351095.3372874
825
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Leif Hancox-Li and I. Elizabeth Kumar
[41] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach,
and Jennifer Wortman Vaughan. 2020. Interpreting Interpretability: Under-
standing Data Scientistsâ€™ Use of Interpretability Tools for Machine Learning. In
Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
(Honolulu, HI, USA) (CHI â€™20). Association for Computing Machinery, New York,
NY, USA, 1â€“14. https://doi.org/10.1145/3313831.3376219
[42] Evelyn Fox Keller. 1984. A feeling for the organism, 10th aniversary edition: the
life and work of Barbara McClintock. Macmillan.
[43] Evelyn Fox Keller. 1995. Reflections on Gender and Science. Yale University Press.
[44] I. Elizabeth Kumar, Suresh Venkatasubramanian, Carlos Scheidegger, and
Sorelle A. Friedler. 2020. Problems with Shapley-value-based explanations as
feature importance measures. In International Conference on Machine Learning.
[45] Savio L. Y. Lam and Dik Lun Lee. 1999. Feature Reduction for Neural Network
Based Text Categorization. In Proceedings of the Sixth International Conference
on Database Systems for Advanced Applications (DASFAA â€™99). IEEE Computer
Society, USA, 195â€“202.
[46] Zachary C Lipton and Jacob Steinhardt. 2018. Troubling trends in machine
learning scholarship. arXiv preprint arXiv:1807.03341 (2018).
[47] R Duncan Luce and Howard Raiffa. 1957. Games & Decisions. John Willey &
Sons Inc. (1957).
[48] Scott Lundberg. 2018. Reply to â€œOutput value in binary classification task is out-
side [0, 1] rangeâ€. https://github.com/slundberg/shap/issues/29#issuecomment-
362330453. Accessed: 2020-09-28.
[49] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M. Prutkin,
Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. 2019.
Explainable AI for Trees: From Local Explanations to Global Understanding.
arXiv:1905.04610 [cs.LG]
[50] Scott M Lundberg, Gabriel G Erion, and Su-In Lee. 2018. Consistent individualized
feature attribution for tree ensembles. arXiv preprint arXiv:1802.03888 (2018).
[51] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting
Model Predictions. In Proceedings of the 31st International Conference on Neural
Information Processing Systems (Long Beach, California, USA) (NIPSâ€™17). Curran
Associates Inc., Red Hook, NY, USA, 4768â€“4777.
[52] James W Malazita and Korryn Resetar. 2019. Infrastructures of abstraction: how
computer science education produces anti-political subjects. Digital Creativity
30, 4 (2019), 300â€“312.
[53] Emily Martin. 1991. The egg and the sperm: How science has constructed a
romance based on stereotypical male-female roles. Signs: Journal of Women in
Culture and Society 16, 3 (1991), 485â€“501.
[54] DonaldMartin Jr, Vinod Prabhakaran, Jill Kuhlberg, Andrew Smart, andWilliam S
Isaac. 2020. Participatory Problem Formulation for Fairer Machine Learning
Through Community Based System Dynamics. arXiv preprint arXiv:2005.07572
(2020).
[55] Luke Merrick and Ankur Taly. 2020. The Explanation Game: Explaining Machine
Learning Models Using Shapley Values. In Machine Learning and Knowledge
Extraction, Andreas Holzinger, Peter Kieseberg, A Min Tjoa, and Edgar Weippl
(Eds.). Springer International Publishing, Cham, 17â€“38.
[56] Thomas Nagel. 1989. The view from nowhere. Oxford University Press.
[57] Martin J Osborne and Ariel Rubinstein. 1994. A course in game theory. MIT press.
[58] Kristin Potter, Paul Rosen, and Chris R. Johnson. 2012. From Quantification to
Visualization: A Taxonomy of Uncertainty Visualization Approaches. In Uncer-
tainty Quantification in Scientific Computing, Andrew M. Dienstfrey and Ronald F.
Boisvert (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 226â€“249.
[59] Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wort-
man Vaughan, and Hanna Wallach. 2018. Manipulating and measuring model
interpretability. arXiv preprint arXiv:1802.07810 (2018).
[60] Emilee Rader, Kelley Cotter, and Janghee Cho. 2018. Explanations as Mechanisms
for Supporting Algorithmic Transparency. Association for Computing Machinery,
New York, NY, USA, 1â€“13. https://doi.org/10.1145/3173574.3173677
[61] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. â€œWhy should I
trust you?â€ Explaining the predictions of any classifier. In Proceedings of the 22nd
ACM SIGKDD international conference on knowledge discovery and data mining.
1135â€“1144.
[62] Philipp Schmidt and Felix Biessmann. 2019. Quantifying interpretability and
trust in machine learning systems. arXiv preprint arXiv:1901.08558 (2019).
[63] Tobias Schnabel, Paul N. Bennett, and Thorsten Joachims. 2018. Improving
Recommender Systems Beyond the Algorithm. arXiv:1802.07578 [cs.HC]
[64] Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian,
and Janet Vertesi. 2019. Fairness and Abstraction in Sociotechnical Systems.
In Proceedings of the Conference on Fairness, Accountability, and Transparency
(Atlanta, GA, USA) (FAT* â€™19). Association for Computing Machinery, New York,
NY, USA, 59â€“68. https://doi.org/10.1145/3287560.3287598
[65] Phoebe Sengers and Bill Gaver. 2006. Staying open to interpretation: engaging
multiple meanings in design and evaluation. In Proceedings of the 6th Conference
on Designing Interactive Systems. 99â€“108.
[66] Hua Shen and Ting-Hao â€œKennethâ€ Huang. 2020. How Useful Are the Machine-
Generated Interpretations to General Users? A Human Evaluation on Guessing
the Incorrectly Predicted Labels. arXiv preprint arXiv:2008.11721 (2020).
[67] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning
Important Features Through Propagating Activation Differences. In Proceed-
ings of the 34th International Conference on Machine Learning (Proceedings of
Machine Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.).
PMLR, International Convention Centre, Sydney, Australia, 3145â€“3153. http:
//proceedings.mlr.press/v70/shrikumar17a.html
[68] Raghav Singal, Omar Besbes, Antoine Desir, Vineet Goyal, and Garud Iyengar.
2019. ShapleyMeets Uniform: An Axiomatic Framework for Attribution in Online
Advertising. In The World Wide Web Conference (San Francisco, CA, USA) (WWW
â€™19). Association for Computing Machinery, New York, NY, USA, 1713â€“1723.
https://doi.org/10.1145/3308558.3313731
[69] Dylan Slack, Sorelle A Friedler, Carlos Scheidegger, and Chitradeep Dutta Roy.
2019. Assessing the Local Interpretability of Machine Learning Models. arXiv
preprint arXiv:1902.03501 (2019).
[70] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju.
2020. Fooling LIME and SHAP: Adversarial Attacks on Post Hoc Explanation
Methods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society
(New York, NY, USA) (AIES â€™20). Association for Computing Machinery, New
York, NY, USA, 180â€“186. https://doi.org/10.1145/3375627.3375830
[71] Jakub Sliwinski, Martin Strobel, and Yair Zick. 2019. Axiomatic characterization
of data-driven influence measures for classification. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 33. 718â€“725.
[72] Mona Sloane, Emanuel Moss, Olaitan Awomolo, and Laura Forlano. 2020. Partic-
ipation is not a Design Fix for Machine Learning. arXiv preprint arXiv:2007.02423
(2020).
[73] C. Estelle Smith, Bowen Yu, Anjali Srivastava, Aaron Halfaker, Loren Terveen,
and Haiyi Zhu. 2020. Keeping Community in the Loop: Understanding Wikipedia
Stakeholder Values for Machine Learning-Based Systems. In Proceedings of the
2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI,
USA) (CHI â€™20). Association for Computing Machinery, New York, NY, USA, 1â€“14.
https://doi.org/10.1145/3313831.3376783
[74] Lucy Suchman. 2002. Located accountabilities in technology production. Scandi-
navian journal of information systems 14, 2 (2002), 7.
[75] Mukund Sundararajan, Kedar Dhamdhere, and Ashish Agarwal. 2020. The Shap-
ley Taylor Interaction Index. In Proceedings of the 37th International Conference on
Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal DaumÃ©
III and Aarti Singh (Eds.). PMLR, 9259â€“9268. http://proceedings.mlr.press/v119/
sundararajan20a.html
[76] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution
for deep networks. arXiv preprint arXiv:1703.01365 (2017).
[77] Jennifer Wortman Vaughan and Hanna Wallach. Forthcoming. A Human-
Centered Agenda for Intelligible Machine Learning. InMachines We Trust: Getting
Along with Artificial Intelligence, Marcello Pelillo and Teresa Scantamburlo (Eds.).
[78] Alison Wylie and Sergio Sismondo. 2015. Standpoint Theory, in Science. In
International Encyclopedia of the Social and Behavioral Sciences (Second Edition),
James D. Wright (Ed.). Elsevier, 324â€“330.
[79] Yiwei Yang, Eser Kandogan, Yunyao Li, Prithviraj Sen, and Walter S Lasecki.
2019. A Study on Interaction in Human-in-the-Loop Machine Learning for Text
Analytics.. In IUI Workshops.
826
