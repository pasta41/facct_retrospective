A Semiotics-based epistemic tool to reason about ethical issues
in digital technology design and development
Simone Diniz Junqueira Barbosa∗
Gabriel Diniz Junqueira Barbosa∗
Clarisse Sieckenius de Souza∗
simone@inf.puc-rio.br
gbarbosa@inf.puc-rio.br
clarisse@inf.puc-rio.br
Department of Informatics, PUC-Rio
Rio de Janeiro, RJ
Carla Faria Leitão∗
cfaria@inf.puc-rio.br
Department of Psychology, PUC-Rio
Rio de Janeiro, RJ
ABSTRACT
One of the important challenges regarding the development of
morally responsible and ethically qualified digital technologies is
how to support designers and developers in producing those tech-
nologies, especially when conceptualizing their vision of what the
technology will be, how it will benefit users, and avoid doing harm.
However, traditional software design and development life cycles
do not explicitly support the reflection upon either ethical or moral
issues. In this paper we look at how a number of ethical issues may
be dealt with during digital technology design and development, to
prevent damage and improve technological fairness, accountability,
and transparency. Starting from mature work on semiotic theory
and methods in human-computer interaction, we propose to extend
the core artifact used in semiotic engineering of human-centered
technology design, so as to directly address moral responsibility
and ethical issues. The resulting extension is an epistemic tool, that
is, an instrument to create and elaborate on this specific kind of
knowledge. The paper describes the tool, illustrates how it is to be
used, and discusses its promises and limitations against the back-
ground of related work. It also includes proposed empirical studies,
accompanied by briefly described methodological challenges and
considerations that deserve our attention.
CCS CONCEPTS
• Human-centered computing → HCI theory, concepts and
models; • Computing methodologies→ Artificial intelligence.
KEYWORDS
semiotic engineering,ethics,epistemic tool
∗All authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAccT ’21, March 3–10, 2021, Virtual Event, Canada
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8309-7/21/03. . . $15.00
https://doi.org/10.1145/3442188.3445900
ACM Reference Format:
Simone Diniz Junqueira Barbosa, Gabriel Diniz Junqueira Barbosa, Clarisse
Sieckenius de Souza, and Carla Faria Leitão. 2021. A Semiotics-based epis-
temic tool to reason about ethical issues in digital technology design and
development. In ACM Conference on Fairness, Accountability, and Trans-
parency (FAccT ’21), March 3–10, 2021, Virtual Event, Canada. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3442188.3445900
1 INTRODUCTION
Supporting responsible technology design and development, from
beginning to end of the technology life cycle, is a challenge, espe-
cially in the early stages, when technologists are conceptualizing
their vision of what the technology will be, how it will benefit users
and other stakeholders, and avoid doing harm. However, traditional
software development life cycles [51] do not explicitly support the
reflection upon either ethical or moral issues. They involve users
and other stakeholders mostly as information sources, when elic-
iting requirements or evaluating the software. By contrast, agile
approaches involve “the customer” throughout the process, but
their priority “to satisfy the customer” and their “preference to
the shorter timescale”1 encourage such a quick pace and narrow
view of customer needs (versus broader societal impact) that they
may discourage more thoughtful design. In human-computer in-
teraction (HCI) design [58], the reflection upon ethical issues is
usually considered very important, but it is not instrumented nor
systematically supported.
We need an approach that supports the reflection upon ethical
and moral issues throughout the design and development process,
by customers, target users, designers, and other stakeholders and
experts who may have a say in the potential impact of the software
in question. Our goal is not to propose a specific process, but an
epistemic tool to support development teams in reflecting upon
these issues as they follow their preferred development process.
In his paper on artificial intelligence (AI) and moral responsibil-
ity, Coeckelbergh [11] proposes that a change in perspective, from
an agent-centered to a relational agent-patient one, has the potential
to advance new kinds of solutions to AI justification and explain-
ability. His line of argument is that people who use technologies
(or are somehow affected by such use), the patients in the relation,
are entitled to ask questions about technologies and get answers
from their creators and owners. Coeckelbergh [11] refers to this as
1https://agilemanifesto.org/principles.html
363
FAccT ’21, March 3–10, 2021, Virtual Event, Canada Barbosa, Barbosa, de Souza, Leitão
answerability, underlining the importance of thinking about moral
responsibility questions and answers throughout the development
process, and not only as an a posteriori kind of analysis.
We respond to Coeckelbergh’s call to dialogue by presenting the
initial results of our work on using Semiotic Engineering [12] to
support epistemic reasoning on ethical and moral issues related to
the design of digital technologies in general, and AI in particular.
Semiotic Engineering is originally a semiotic theory of HCI [12, 14],
which has been extended to investigate how human meanings are
inscribed in software in design and development stages [13]. As
most other semiotic theories and approaches devoted to study-
ing computers and computation [2, 43, 48], Semiotic Engineering
frames computation as a form of computer-mediated, extensively
(but not exclusively) automated social communication through (nat-
ural and artificial) language(s), which not only expresses human
technical rationality and design intent with respect to a wide variety
of purposes and domains, but also carries the conscious or uncon-
scious values, beliefs and expectations of those who are expressing
themselves, and those to whom communication is directed.
As a quick example of what this perspective means, let us take
the case of "likes" in social networks. Presumably, when they first
appeared on screen, the designers and developers of the technology
meant to provide users with a quick and easy way to demonstrate
their appreciation to other people’s online content, to strengthen
or create social connections and emotional ties. This may also have
been the immediate perception of users, back then, who engaged
in the proposed conversation with and through social network in-
terfaces, and thus began to manifest their likes accordingly. From
a semiotic point of view, it is noteworthy that the designers of
technology proposed a binary sign as the only element for this
particular interface segment. You could either like the content (typ-
ically by clicking on thumb up or heart icons), or not. If you did not
like it, what could you be saying by that? That you really disliked
the content, that you would rather refrain from manifesting your
opinion, that you would like to postpone your manifestation till
you had the time to think about it? Or could the absence of a like
simply mean that you were not aware of the content?
Studies on how the binary semiotic system proposed by social
network platform designers was appropriated and evolved address
the emotional, commercial, political, and maybe even the civiliza-
tory impacts of overloadingmeanings and placing a narrow-band ex-
pressive system at the disposal of people who would probably have
so much more to say than liked it (see for example, [37, 55, 57, 60]).
The easiest computation on binary signs is, of course, to count
them. Therefore, the quantity of likes have begun to be interpreted,
used, and manipulated to mean a wide range of different things,
whose distinction from one another and corresponding validation
is humanly impossible to verify.
Semiotic Engineering theory and methods have been created to
investigate the processes of computer-mediated social communica-
tion between technology producers and consumers, which naturally
aligns to Coeckelbergh’s agent-patient relational perspective [11].
It has been used and evolved over more than two decades and now
turns to urgent societal needs framed as a call for more fairness,
accountability and transparency. Our contribution is to help tech-
nology producers in reasoning about ethics andmoral responsibility
in relation with the users (i.e., the technology consumers).
At the current stage, we have conceptualized and analyzed the
consistency and cohesion of an epistemic tool, which we will test
empirically only after having exposed it to wider consideration,
critical analysis, and debate. Given the complexity, the multidisci-
plinarity, and the benefit of discussions that can take place prior
to delving into empirical studies using the tool, we focus on the
theoretical foundations and rationale of our tool’s design. This
choice does not prevent us, however, from concluding the paper
with suggested empirical studies, accompanied by briefly described
methodological challenges and considerations that we already know
will deserve our attention.
This paper is structured as follows. The next section (section 2)
presents the foundations, background, an rationale for our work.
Section 3 presents the guiding questions and process we propose
to follow in order to build an extended metacommunication tem-
plate, taking into account ethical considerations from a conceptual
framework of the designers’ choosing. Section 4 illustrates how the
extended metacomunication template can be built using the guid-
ing questions we designed, and section 5 discusses the epistemic
nature of the tool, describing the main reflections the designer has
engaged in when building the example metacommunication tem-
plate. Prior to conclusion, section 6 positions our contribution in
view of existing research. Finally, section 7 closes our paper with
some final considerations and directions for future work.
2 FOUNDATIONS, BACKGROUND AND
RATIONALE
In this section we briefly introduce the key points of the founda-
tional theory for our research, Semiotic Engineering [12] (subsec-
tion 2.1), and comment on background work that, like Coeckel-
bergh’s [11], has provided us with insights on either the problem
we want to address, or the solution we propose (subsection 2.2). We
wrap up the section with a description of the rationale behind the
new epistemic tool (subsection 2.3).
2.1 Foundations in Semiotic Engineering
This subsection presents only a few core concepts in Semiotic Engi-
neering to help readers understand and evaluate our proposal. We
have selected them from prior publications with unabridged pre-
sentations of the theory [12–14]. As already mentioned, Semiotic
Engineering views HCI as a particular case of computer-mediated
social communication. Its original contribution to the field is to
define a specific object of investigation, to create new methods to
investigate it, and finally to propose a set of foundational concepts
to orient long-term research in HCI and related areas. This is in
clear contrast with the most popular perspective in HCI — user cen-
tered design (UCD) –, whose origins can be largely traced back to
Norman’s cognitive engineering model of HCI [46, 47]. In it, inter-
action is characterized by a set of seven iterative actions, all carried
out by the users (hence the user-centeredness): establishing a goal,
forming an immediate intention, specifying the action sequence,
executing the action, perceiving the system state, interpreting the
system state, and evaluating the system state with respect to the
goals and intentions. Note that no other agent but the user is active
in Norman’s model.
364
A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development FAccT ’21, March 3–10, 2021, Virtual Event, Canada
Following the alternative computer-mediated social communi-
cation perspective, Semiotic Engineering characterizes HCI as a
twofold metacommunication process, involving software develop-
ers and software users. At the top level, developers send to users a
one-shot, complete message telling them — through the system’s
interface and supported interactions — how, where, when, what
for, and why the users can communicate back with the system.
The latter kind of communication is achieved at the lower level
through user-system interactions. The entire process is analogous
to playwrights’ (top-level) communication of their global message,
through (lower-level) communications among the characters in
their play. The analogy is useful to underline the interplay of mu-
tual beliefs, expectations, intentions, and other subjective aspects
of developers–users relations not typically explored in HCI the-
ories. Moreover, because of the interactive medium provided by
computer artifacts, software developers have a proxy that virtually
speaks for them at interaction time, namely their system’s user
interface. As users interact with the developers’ proxy, they truly
achieve computer-mediated social communication. Metacommuni-
cation is thus communication about and through (other levels of)
communication. The content of the metacommunication top-level
message sent by systems developers to systems users is represented
abstractly in the metacommunication template, which says:
“Here is my understanding of who you are, what I’ve
learned you want or need to do, in which preferred
ways, andwhy. This is the system that I have therefore
designed for you, and this is theway you can or should
use it in order to fulfill a range of purposes that fall
within this vision.” [12, p.25]
To illustrate how this is achieved, take the user interface of a
fictitious search engine, Haystack (Figure 1a). Just like in natural
interpersonal communication, speakers and listeners assume the
presence ofmutual communication goals. Therefore, as speakers, de-
velopers use a blinking cursor to communicate to users that they are
waiting for users to tell (them/their proxy) the terms for the search.
As listeners, users may take the blinking cursor and the empty text
box to mean that it is their (the users’) turn to communicate their
intent textually. The example also points at the “automated” dimen-
sion of social communication, the underlying computer processes
that interpret and generate conversational turns, and their corre-
sponding effects on both front-end and back-end system states.
Haystack
Find
(a)
Haystack
Find
image.png
(b)
Figure 1:Haystack’s (a) search interface and (b) response for
an attempt to search for an image through drag-and-drop.
Mutual (and frequently unconscious) beliefs between computer-
mediated communication participants can be convergent, as in the
example just mentioned, or divergent. Divergent mutual beliefs
typically give rise to breakdowns or discovery. For example, a user
may wonder whether Haystack can search for images by taking an
image as search term. An easy way to communicate this question
interactively is to drag an image and drop it onto the search area
and watch what happens. In Figure 1b, we show the developers’
proxy’s response to the user’s communication expressed as a drag-
and-drop interaction. The conversation between the developers’
proxy and the user, which unfolds over a short period of time, can
be loosely verbalized as the following dialog:
User: Can you search for images like this one?
[Drags the file icon onto the search term area.]
Developer’s Proxy: This cannot be done.
[Shows the “forbidden” sign next to the cursor.]
Closer to the topic of this paper, notice that easy-to-understand
communicative exchanges in both examples are different from very
complex metacommunication that may actually never be achieved,
not even partially, for most users, about why certain items have
a higher ranking than others when retrieved by Haystack. Would
it make a difference if Haystack developers told users, through
efficient metacommunication, even if superficially, why certain
items show first? Or why other items may never show? What may
‘saying nothing’ about ranking imply, or be taken to mean? Could
this be a problem? When and why?
According to Semiotic Engineering, the developers’ choice about
whether and how to communicate ranking principles information
to users at interaction time is the result of their “understanding of
who [their users] are, what [they] have learned [the users] want or
need to do, in which preferred ways, and why.” Moreover, to com-
municate efficiently and effectively their message, developers must
have technical knowledge and abilities that allow them to increase
the chances of successful communication. Therefore, maximal com-
municability is achieved with metacommunication that efficiently
and effectively expresses a system’s underlying design intent and
construction (implementation) principles. In other words, it clearly
tells users “the way [they] can or should use [the system] in order
to fulfill a range of purposes that fall within [the developers’] vision
[of themselves, their product, their product’s users, etc.].”
By explicitly engaging designers and developers in introspec-
tive analyses, Semiotic Engineering tools have a great practical
potential to promote ethical and socially responsible reasoning dur-
ing software development. Suppose that Haystack has a shopping
section, and that designers and developers view their users as shop-
pers navigating in the aisles of a shopping mall. This might favor
a map-like representation of the shopping space as a mall, which
users can navigate at leisure. However, if Haystack offers users
a list of shopping options instead, the mere switch of metaphors
greatly influences what shoppers are most likely to buy or not buy.
Team members’ introspective reflection upon their own images and
notions regarding shoppers might enable a series of questions about
whom they are communicating with, in which capacity, on whose
behalf, and to what end, possibly raising ethical issues.
365
FAccT ’21, March 3–10, 2021, Virtual Event, Canada Barbosa, Barbosa, de Souza, Leitão
2.2 Background Work
The 21st century has seen the increase and intensification of societal
and scholarly debate about moral responsibility and ethics with re-
spect to AI. Voices from inside and outside the Information and Com-
munication Technology (ICT) community have manifested their
concerns. For example, among ICT experts, Nissembaum [15, 44, 45]
is one of the pioneering figures in discussions about human val-
ues and computing, in general, and Johnson [29, 30], a pioneer in
Computer Ethics (for pre-AI ethics and morality related discussions
in ICT see [23, 62]). Recent insiders’ research broadly concerned
with ethics in software development include, for example, Karim
et al. [31], discussing a code of ethics for software development,
Al-Fedaghi [1], interested in how software engineers should think
about ethics, Shilton and Anderson [59], looking at ethics in tech-
nology design activities, and Scott and Barlowe [56], calling the
attention to computer education.
As AI enters the stage, the debate has been engaging ICT insiders
and outsiders, alike. Philosophers, in particular, have been espe-
cially active, as is also the case with many jurists concerned with
personal data protection. To illustrate the kinds of philosophical
debate focusing or touching on AI, we can mention research on
the ethics of decision making based on algorithms and data car-
ried out by authors associated with the Oxford Internet Institute
(e.g., [19, 40, 41]). Further work includes the already mentioned
work by Coeckelbergh [11] on a relational justification of AI ex-
plainability. The author has also discussed ethics with health care
systems and assistive technologies [9] and robots [10]. Researchers
who have previously published in the ACM Conference series on
Fairness, Accountability and Transparency, with an emphasis on
philosophical perspectives, include Binns [5], who proposes a po-
litical philosophy perspective to approach machine fairness, and
Bietti [4], who discusses the trivialization of ethics and moral phi-
losophy now understood as a kind of ethics washing carried out by
big tech industries and analyzes the origin and consequences of
such misunderstanding.
Work about legal aspects, in turn, have been gaining greater
attention in view of personal data protection regulations in many
countries. For example,Wachter et al. [66] andDoshi-Velez et al. [16]
examine the right to explanation in view of the EU General Data
Protection Regulation. Other topics of attention include how the
law should approach self-driving cars (e.g., [65]) and the use of AI
and robotics in health systems (e.g., [8]), a broad interdisciplinary
study about ethical, legal and social implications of using AI in
breast cancer care, and in-depth discussions about the notion of
causation in AI and in Law [34].
AI has of course stirred up many insiders concerned with inter-
pretability and explainability (e.g., [22, 32, 38]), and more generally
with fairness, accountability, and transparency (e.g., [64]). As a back-
ground for our research, we should specifically mention existing
work proposing technical artifacts to address ethical concerns and
precautions for the developers of digital technologies. Among these
are those artifacts meant to register technologists’ concerns about
how data sets or machine learning models are used by people who
haven’t created them originally. A noteworthy example is the fam-
ily of cards like model cards [39], data sheets [21], and metaphorical
‘nutrition labels’ to inform users about the ‘ingredients’ of machine
learning data sets [27, 61].
A special example in the context of this paper is the call to action
regarding how knowledge gained from philosophical discussions
on moral responsibility and ethics in current technologies translate
into software engineering practices. As will be seen in subsequent
sections, we have used Morley et al.’s attempt to “close the gap
between principles and practices” [42]. The authors respond to
Floridi and Cowls’s proposal of five ethical principles, which in
their view synthesize and unify all previous proposals of ethical
guidance in the age of AI [18]. They conclude that the four principles
of bioethics plus that of explainability offer sufficient top-level
orientation for ethical decisions [18]. From top-level orientation to
practical software design and decisions there is, however, a long
distance to travel. Hence Morley et al.’s call to action.
2.3 Rationale for the New Epistemic Tool in
Semiotic Engineering
In the previous subsections, we presented the gist of our work in
Semiotic Engineering (subsection 2.1) and the background work
that has guided and inspired the kind of contribution that we want
to make with our research (subsection 2.2). In this final subsection
we present the rationale behind our proposal.
Ethics and moral responsibility are relational matters. Therefore,
judgment about both requires that one be able to view himself or
herself in relation to others. The core concept of metacommunica-
tion in Semiotic Engineering establishes a strong 1st person–2nd
person relation between technology producer and technology con-
sumer. Therefore, working with the Semiotic Engineering metacom-
munication template (page 3), technology designers and developers
are stimulated to psychologically connect with people who are go-
ing to use their product and can reason about how the interactions
supported by what users can see and do convey their message to
users, express their intentions and design goals, manifest their be-
liefs and expectations regarding the users (including the users’ own
beliefs and expectations), promote productive and safe user-system
exchanges, and so on, and so forth.
Another important aspect of Semiotic Engineering regarding
ethical and moral reasoning during design and development stages
of digital technology is that its tools for metacommunication de-
sign, evaluation, and investigation are all eminently epistemic. In
their book about epistemic fluency and its critical role in contem-
porary professional activities, Markauskaite and Goodyear remark
that “knowledge is produced in a multitude of places and that
it flows rapidly across organizational, disciplinary and national
boundaries” [36, p. 30]. Professionals must be equipped to perform
constant epistemic activity, as they move knowledge from place to
place, and build new knowledge structures to take along as they
travel across rapidly changing contexts. In their words:
[T]he dynamics of professional work situations are
such that professionals have not only to work with
knowledge and use knowledge to justify their action;
they also need to be adept at practices of creating and
testing new, applicable knowledge. In this sense, pro-
fessional cultures are taking on more of the qualities
and practices of epistemic cultures [...]. (p. 30)
366
A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development FAccT ’21, March 3–10, 2021, Virtual Event, Canada
Here is my understanding of who you are, what I’ve learned you want or
need to do, in which preferred ways, and why
Analysis
1
This is the system that I have therefore designed for you Design
2
and this is the way you can or should use it Prototype
3
in order to fulfill a range of purposes that fall within this vision Evaluation
4
Figure 2: Alignment of metacommunication template and design lifecycle stages.
In agreement with Markauskaite and Goodyear [36], Semiotic
Engineering tools are conceived to support situated knowledge
construction and organization, which by the same token supports
the expansion of professionals’ (or researchers’) expert knowledge
about computer-mediated metacommunication involving digital
technology producers and users. This epistemic perspective reveals
a Socratic inclination in our belief that Semiotic Engineering can
support ethical reasoning [50, 52]. Regardless of whether they start
from a set of explicit ethical principles and good practices, a key
point in our elaboration is that by seeking to know more about
themselves, their users, the context and situation before and after
technology is introduced, the impact of change, the risks of disrup-
tion, and so on, designers and developers will continually approach
morally acceptable and socially responsible behavior.
In order to operationalize an epistemic tool, we extended our
metacommunication template with ethical and moral responsibility
content. By being prompted to fill out the extended version of
the template, we believe that software designers and developers
will be confronted with concrete questions they should answer
when organizing the meaning and the form of their computer-
mediated message to the end users. If such extension per se passes
the test of analytical examination (which we initiate with this paper)
and empirical validation (which we will carry out only once the
analytical examination is satisfactorily concluded), our future steps
will be to use Semiotic Engineering methods specifically designed
to investigate the emission and reception of meanings inscribed in
software [13, 14] to verify the solidity and in situ applicability of
our proposal. The promise, in this case, is to benefit from mature
theoretical foundations and methodology that could represent a
significant step forward in bringing ethics and moral responsibility
reasoning into the culture of software development.
3 EXTENDED METACOMMUNICATION
TEMPLATE
Most HCI design and development lifecycles present similar ver-
sions of the following stages [25, 26, 58]: analysis (understanding
user needs and defining requirements), (conceptual) design, proto-
typing (and implementation), and evaluation. All models recognize
the iterative nature of the design and development process, so that
every stage can be revisited as necessary.
The Semiotic Engineering metacommunication template can be
segmented according to those stages. In Figure 2, we show the cor-
respondence between the metacommunication template segments
and a general lifecycle model. The numbers above the stages match
the numbers used in the next sections. We have extended the Semi-
otic Engineering’s metacommunication template by adding a list
of explicit guiding questions designers should ask at each step of
the design and development lifecycle. The next subsection presents
the guiding questions and how we propose to use them.
3.1 Guiding Questions
The guiding questions can be classified into base questions and
ethical questions. The base questions are variations of questions con-
sidered in usual design lifecycles, with the distinction that, within
semiotic engineering, the designer adopts a 1st-person (I, my) per-
spective when asking and answering them, addressing the user as
the 2nd person (you, your) in the conversation. By contrast, the
ethical questions focus on explicitly addressing ethical issues, and
can be supported by diverse ethical frameworks and principles
(for instance, the biomedical ethics principles of beneficence, non-
maleficence, autonomy, and justice [3]). To visually distinguish
the ethical questions, they are preceded with an asterisk. All ques-
tions are numbered so they can be referenced in the usage example
presented in the next section.
The design process is iterative, but we tend to answer the base
questions somewhat sequentially, revisiting them when addressing
the ethical questions in the same stage, as illustrated in Figure 3.
(any stage)
base questions
ethical questions
chosen ethical principles
beneficence
non-
maleficence
autonomy justice
Figure 3: Schema of base and ethical questions in each stage,
aigned with the chosen ethical principles.
1. Analysis (understandingneeds anddefining requirements
1.1. What do I know or don’t know about (all of) you and how?2
1.2. What do I know or don’t know about affected others and
how?
1.3. What do I know or don’t know about the intended (and
other anticipated) contexts of use?
2For everything I realize I do not know, I should ask whether I should know it and, if
so, how I can learn it.
367
FAccT ’21, March 3–10, 2021, Virtual Event, Canada Barbosa, Barbosa, de Souza, Leitão
1.4. *What ethical questions can be raised bywhat I have learned?
Why?
2. Design
2.1. What have I designed for you?
2.2. Which of your goals have I designed the system to support?
2.3. In what situations/contexts do I intend/accept you will use
the system to achieve each goal? Why?
2.4. How should you use the system to achieve each goal, ac-
cording to my design?
2.5. For what purposes do I not want you to use the system?
2.6. *What ethical principles influenced my design decisions?
2.7. *How is the system I designed for you aligned with those
ethical considerations?
3. Prototyping, implementation, and formative evaluation
3.1. How have I built the system to support my design vision?
3.2. What have I built into the system to prevent undesirable
uses and consequences?
3.3. What have I built into the system to help identify and rem-
edy unanticipated negative effects?
3.4. *What ethical scenarios have I used to evaluate the system?
4. Continuous, post-deployment evaluation andmonitoring
4.1. How much of my vision is reflected in the system’s actual
use?
4.2. What unanticipateduses have beenmade? Bywhom?Why?
4.3. What anticipated and unanticipated effects have resulted
from its use? Whom do they affect? Why?
4.4. *What ethical issues need to be handled through system
redesign, redevelopment, policy, or even decommissioning?
All questions keep with the 1st-person perspective proposed by
the original Semiotic Engineering template. They also create an
explicit and direct relation between 1st (me, us) and 2nd persons
(you) through the product being created by the 1st person (singular,
in the expression above, but typically plural in team development
practices). The design stage is where most questions occur, perhaps
reflecting the inheritance of Semiotic Engineering’s original focus
on design.
3.2 Getting from we to I
Software development involves a team of designers, developers,
professionals from multiple disciplines, and other stakeholders,
who may have a say in the design or who may be influenced by
the deployment and use of the system. They may bring diverging
forces to the design process in such a way that the outcome may be
an incohesive product. This issue is related to the problem of “many
hands”, an expression coined by Thompson [63] – in the context of
public officials and goverment policies – to define situations when
many people contribute in many ways to (policy) decisions, so it is
difficult to identify who is morally responsible for the outcomes.
To reduce the negative effects of this “distributed” view of what
the product should be, we propose to have them negotiate meanings
to achieve a “collective” vision of the product and how it should
be used, later converging into a cohesive “1st-person” vision. To
assist in this negotiation, we propose that each one of those inter-
ested parties should write their version of the metacommunication
message, guided by the questions presented in the previous subsec-
tion. This would give us the different perspectives, expectations,
and visions for the product each member of the group holds. The
plain, natural language in which the metacommunication message
is written makes it easier for people to engage in sense making,
meaning negotiation, and consensus building, as the wide adoption
of scenarios in design processes attest [7, 53]. Our goal is to move
from a distributed we, to a collective we, and, finally to a 1st person I
in the design vision for the product, as illustrated in Figure 4. By
converging into a 1st-person vision, we aim to avoid the dilution or
distortion of responsibility found in the hierarchical and collective
models investigated by Thompson [63].
distributed we collective we I
metacomm
1
metacomm
2
metacomm
3
metacomm
1’
metacomm
2’
metacomm
3’
final
metacomm
Figure 4: From distributed we, to collective we, to I in design.
4 TOWARD THE METACOMMUNICATION
MESSAGE: ANSWERING THE GUIDING
QUESTIONS
To illustrate the creation of the metacommunication message using
the proposed guiding questions, we have selected as an example a
mobile app for the gig economy. The direct users of the mobile app
are service contractors and service providers. We next describe the
final metacommunication message considering the service contrac-
tor as the interlocutor (you), indexed by the guiding questions. Each
fragment that answers a base question may be followed by one or
more ethical questions or issues, in italics. When appropriate, we
identify the ethical principles that led us to think about each issue
(but not necessarily the only principle that is applicable to the issue
at hand). In the example, we used the ethical principles of biomed-
ical ethics [3], which we represent using the following notation:
B for beneficence, nM for non-maleficence, A for autonomy, and
J for justice.
1.1. What do I know or don’t know about (all of) you and
how?
About you, I know that: (i) You are an actual or potential
service contractor who wants good-quality service. (ii) You accept
having small problems or inconveniences a few times, but not if it
becomes a trend. —What if you misrepresent small inconveniences
as poor service? A nM (iii) Where you are when you ask for a service.
—What if accepting to render you a service puts the provider at risk
(e.g., dangerous locations)? nM . But I don’t know: (i) Your profile
(age, gender, race, religion. . . ). (ii) How experienced you are with
technology. — What if you hire the wrong service by accident? A nM
(iii) Whether you may be prejudiced against a group of people
(who may end up serving you). —What if I inadvertently put you in
harm’s way (e.g., by exposing a black person to a racist person)? nM
(iv) How much feedback you would be willing to provide about
your service provider. — What if you cannot express yourself as
needed (e.g., to denounce gross misbehavior)? nM A — What if you
368
A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development FAccT ’21, March 3–10, 2021, Virtual Event, Canada
misrepresent your opinion because of (mistaken) assumptions about
what the evaluation may mean to the service providers? nM J
1.2. What do I know or don’t know about affected others and
how?
About the service providers’ families, I know: (i) They may
depend on the service provider’s salary, which in turn depends on
the provider’s working hours and on the demand for their services.
But I don’t know: (i) Their role in helping (or hampering) the
service provider to do their job.
About the company owner, I know: (i) They want to maxi-
mize their profit. — What if they exploit the service contractors or
providers? nM J (ii) They have an influence on my design. —What
if they ask me to design something that harms others to make them
more money?A nM But I don’t know: (i) How (emotionally and
financially) invested they are in the business. About the company
analyst, I know: (i) They want data to analyze. — What if they
rely on insufficient or unreliable data? B nM —What if this reduces
service contractors’ or service providers’ privacy? J But I don’t
know: (i) How independent they are in their functions. — What if
their actions are influenced by someone who does not have your best
interests at heart? nM (ii) What they have been ordered to do with
the data. — What if they are ordered to do something harmful to you
with the data collected by our system? nM
About the citizens:, I know: (i) They do not want their lives
to be disrupted by the service being provided (e.g., excessive accu-
mulation of delivery bikes near where they live; increasing traffic).
—What if negative disruptions are unavoidable? nM — Do the benefits
outweigh the harms? J But I don’t know: (i) How else the service
may affect them. —What if something I have overlooked affects them
negatively? nM
1.3. What do I know or don’t know about the intended (and
other anticipated) contexts of use?
I know: (i) (intended) You will only use our system when
you want a service provided to you. —What if you cannot control
when and how you receive notifications)? A — What if the company
wants to know where you are and what you are doing all the time?
B J (ii) (anticipated) When a service provider misbehaves (in your
view), you may try to avoid them in the future. — What if you
block someone or post a complaint in our system for no justifiable
reason? nM But I don’t know: (i) What motivates you to hire a
service. — What if you want to do harm to service providers through
our system? nM (ii) How regularly you will hire services. —What
if I nudge you to hire a service I suspect you do not really need? A
(iii) When you would like to provide feedback on a service you
hired. —What if I force you to provide feedback at an inconvenient
time? A
2.1. What have I designed for you?
A mobile app – available in various app stores – through which
you can hire services from various providers.
2.2. Which of your goals have I designed the system to sup-
port?
I have designed the system so you will be able to:
(i) Find and hire services — You may easily find the services you
seek. B — What if providers outside of our platform experience de-
creasing demand and lose their businesses? nM (ii) Check your service
provider’s information — You may verify that the provider is who
they say they are through their photo. B (iii) Track the execution of
these services — You may verify whether the process is being correctly
executed, provide feedback or even cancel it if not. A B (iv) Track the
route the service provider will take to the service location. — You
may check where the provider is and plan accordingly. B — You
may not influence the route that our algorithm recommends to the
provider. A— What if the route that our algorithm recommends to
the provider is suboptimal or even dangerous? nM (v) Pay for these
services through the platform using a pre-registered credit card.
— What if you want to pay for the service outside of the platform? A
(vi) Provide feedback about the service — You may help our system
to ensure the service quality. B— What if your feedback negatively
affects the provider’s livelihood? nM — You will be able to skip giving
feedback. A— What if the company considers this as an indication of
poor-quality service? nM
2.3. In what situations/contexts do I intend/accept you will
use the system to achieve each goal? Why?
I intend you to use the system: (i) Whenever you need
to hire a service. — Your request will be processed and, if possible,
directed to a provider. B—What if the request is directed to a provider
who is unavailable so you will have to wait? nM — The provider will
be able to fulfill their role. B — You will only see the options provided
by our recommender system, without knowing how it works. A — Our
surge pricing algorithm may give you different prices for the services
at different times. J (ii) In the place where the provider will render
the service. — You will be there to talk to the provider while they
render the service, to make adjustments and avoid misunderstandings.
A B nM
I accept that you may use the system: (i) In a different place
from where the service will be rendered. —What if the provider has
questions about the service and which you are not there to answer?
A nM
2.4. How should you use the system to achieve each goal, ac-
cording to my design?
You should: (i) Use the search bar to look for the services you
may want to hire. — Our recommender system may prioritize some
options over others, based on what it knows about you. B A— What if
our recommender system fails to choose good service options for you?
nM J — You will not fully understand how our recommender system
works. A (ii) Register your credit card before hiring a service so
that we can charge it when the service is provided. — It facilitates
the payment. B — It ensures that the provider will be paid. B — It
gives you some control as to how to pay for the service. A (iii) Rate
the service after it is rendered to ensure its quality. — It allows us to
ensure the overall service quality. B — The provider cannot rate you in
the same way. J — What if our system harms the providers’ income
by sending fewer requests to those who receive lower ratings? nM
—What if our system considers your lack of rating as an indication
that the service was poorly rendered? A nM
2.5. For what purposes do I not want you to use the system?
You may not use my system to:
(i) Keep requesting and canceling until you find a service provider
who is more convenient to you. — It places an undue burden on our
company. nM A — It wastes the provider’s time and may harm them,
e.g., causing them to lose other services. nM J — It would influence our
surge pricing algorithm and inflate prices for all other contractors.
nM J
369
FAccT ’21, March 3–10, 2021, Virtual Event, Canada Barbosa, Barbosa, de Souza, Leitão
3.1. How have I built the system to support my design vision?
I have built the system: (i) As a mobile app so that you
will able to hire services wherever you are. (ii) With a search tool,
so that you can find the services you want to hire. — Our recom-
mender algorithm will tailor the search results for you. B nM— What
if our recommender system limits your options for hiring services
on behalf of others (e.g., by prioritizing services near your location)?
A (iii) With a map, so you can track the provider’s location as
they move towards the place where the service will be provided.
— What if you use this information to locate and intercept the service
provider for nefarious reasons? nM— In these cases, the provider will
have to report you through the complaint tool afterwards. A J — You
cannot influence the route the provider takes, even if you know it is
suboptimal or dangerous. A (iv) With a third-party payment service
so that you don’t need to worry how you will pay for the service
when you hire it. — What if the third-party service misuses your
information (violating your privacy)? nM— You have accepted the
terms and conditions of our system, thus accepting this kind of risk.
nM A (v) With a rating system that you can use to evaluate the
service provider, to help us decide whether or not they get any
future service requests. — What if you use this tool to discriminate,
bully, and harm them (assuming that these ratings have an effect on
the provider’s livelihood)? nM
3.2. What have I built into the system to prevent undesirable
uses and consequences?
I have built into the system:
(i) A requirement that you have a valid email and phone number
to create an account, so that I can be sure that I can reach you. — If
you lose access to your account, you can use your email or phone
number to regain it. B (ii) A requirement that you have a valid
credit card registered, so that you cannot avoid paying for services
rendered. — To prevent fraud, when you register the card, we make a
small charge to verify it is valid. B
I have not built into the system: (i) A way to check for sys-
tematic bias in ratings. —What if the rating system is abused? nM
(ii) A way to check whether or not you have received emails from
our system. — What if you miss information that is relevant to you?
nM
3.3. What have I built into the system to help identify and
remedy unanticipated negative effects?
I have built into the system: (i) A section where you can
complain about the services rendered and possibly even get a refund.
— What if the complaints may have an impact on the provider’s
livelihood? nM — What if the service providers falsely claim they
have provided the service? nM— You will need to provide evidence that
they have not, and the company will refund you and punish them. J
4.1. Howmuch of my vision is reflected in the system’s actual
use?
My design vision has been reflected in: (i) How little effort
it takes for you to create an account to start using our system.
(ii) How you have been able to easily find and hire the services you
need. — Our recommender system has reduced the time you need to
spend searching for the services you desire. B— Is it possible that it is
not fair to all providers? J (iii) How the service can reliably charge
you for the services that have been provided.
My design vision has not been reflected in: (i) How you
have not always rated the quality of the service provided. — What
if the company considers this lack of rating as an indication that
the provider has done a poor job? nM (ii) How much difficulty the
company has in handling complaints. —What if the company cannot
process the complaints in a timely fashion to prevent future harmful
outcomes? nM
4.2.Whatunanticipateduses have beenmade? Bywhom?Why?
I have not anticipated: (i) That you would use the ratings and
complaints to discriminate against the service providers. —What if
you harm the provider’s livelihood by doing this? nM (ii) That you
would repeatedly check the prices of services to try and find when
you would be able get the smallest price. —What if our surge pricing
algorithm causes you to waste your time checking prices? nM
4.3.What anticipated andunanticipated effects have resulted
from its use? Whom do they affect? Why?
I have anticipated that: (i) You would end up relying more
on hiring services through our system than through other means.
—What if this disproportionately harms services that are not in our
system? nM J— Does this coerce them into joining the platform? A
I have not anticipated that: (i) You would end up paying
inflated prices because the providers might engage in price fixing.
— I did not foresee that providers would collude to fix prices, harming
you. nM— I may be able to redesign our system to detect these cases
and stop them by punishing those involved. J (ii) You would not be
able to find certain services due to the recommender system. — I do
not fully understand how the algorithm I used chooses which services
to recommend to you, so I cannot assure you that it is fair. J
5 DISCUSSING THE DESIGNER’S EPISTEMIC
JOURNEY
This section discusses some aspects regarding the epistemic nature
of the tool, narrating the main reflections we, as designers, have
engaged in when building the example metacommunication tem-
plate, including the challenges of finding the right expressions to
convey our meanings (subsection 5.1), our commitment to adopting
a 1st-person perspective in design (subsection 5.2), and the non-
linear nature of our reflection (subsection 5.3). Note that, due to
the epistemic nature of the tool, different designers could have
generated a different narrative and engaged in different reflections.
5.1 Expressing Design Intent in Natural
Language
When answering the guiding questions and creating the metacom-
munication message, we endeavored to find the right words and
structure to express ourselves so that the reader would be able to
gain an understanding of our design intent, vision, and rationale.
This process led us to various interesting insights about our own
way of thinking about our design. Choosing the right expression
required us to face our own subjective perspective on what was
being built. Instead of aiming at solving a problem objectively, we
had to acknowledge the subjective meanings that we inherently
ascribe to our own creation. Critically comparing what the arti-
fact meant to us to how those meanings were actually expressed
revealed ways in which our design did not match our intentions.
370
A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development FAccT ’21, March 3–10, 2021, Virtual Event, Canada
Writing down our design decisions in natural language, instructed
by the guiding questions, helped us uncover presuppositions behind
them. The questions prompted us to reflect upon diverse design
alternatives and the arguments in favor or against each one so we
could justify our design decisions. By engaging in this reflection,
we could identify ways in which our reasoning was flawed or in-
sufficient. By directly challenging our assumptions, the questions
also provoked us to reflect on what we believed we knew.
Working critically, we gained insights not only about our under-
standing of the problem, but also about ourselves and our culture.
If we, as designers, made choices that we could not reasonably jus-
tify, we would be assuming the risks that these gaps in knowledge
create. If we proceeded anyway, we would do so knowingly, and
this would be telling about how we dealt with this design problem.
Of course, our proposal requires that we honestly represent our
intentions throughout the design process.
Framing was key here. We often felt tempted to highlight our
positive intentions when creating the artifact, but the act of making
our rationale explicit facilitated our own recognition of the negative
aspects of our actions, assumptions, and decisions. Painting an ex-
cessively rosy picture of our design intent would become apparent
in the text, forcing us, as writers, to face equivocations that we
might be trying to avoid.
Another interesting consequence of writing down our design
intent in natural language is that what we do not write says as
much about our design, and ourselves, as what we do. Leaving out
crucial information could be perceived as carelessness or even a de-
liberate attempt to hide something. By requiring that we expressed
our decisions and rationale through writing, it became easier to
find conceptual gaps and inconsistencies, either they accidental
or deliberate. Having identified these cases, we could revisit our
thinking behind these decisions, gaining insights on the problem,
on our design process, and on ourselves.
5.2 Commitment to a First-person Perspective
Once we understand how explicitly writing down our intent makes
us face our own decisions, wemay seek to hide behind other entities.
Attributing actions to “the system,” for example, without acknowl-
edging our major role as its creators, might allow us to deflect some
of the blame for any negative consequences that arise from its use.
In that way, “the system” becomes the agent, and we fade into the
background. Detecting these cases was important, since they gave
us insights about our mindset and our own uncertainties when
creating the artifact.
This was an important advantage of having the guiding ques-
tions frame us as 1st-person creators of the artifact. Having had to
acknowledge our role as designers and explicitly state our presup-
positions and decisions, we recognized ourselves as the true agents
behind the system’s actions. Being a reactive “thing”, the compu-
tational artifact can only react to the user’s interaction according
to the rules we encode in it. Framing this as a relation between us
as designers (I ) and the end user (you), mediated by the artifact
we create, we became central figures in moral situations, having
to face our decisions and their consequences. Not only our agency,
but also our agent-patient relation with our users (as discussed by
Coeckelbergh [11]) became explicit.
It was not always easy to maintain this framing, however. As
we were experiencing the process of building the metacommu-
nication message by answering the guiding questions, we often
caught ourselves relying on the image of “the system” to take our
place. This revealed itself to be an interesting phenomenon, as we
then reflected on when and why we instinctively tried to distance
ourselves from whatever we were working on. Often we would
find that we were unsure about what we were writing down, be it
the decisions or the underlying assumptions. This was especially
the case when identifying potentially negative consequences and
ethical issues that our artifact could create.
By maintaining the 1st-person perspective and noticing when
we instinctively tried to avoid it, we came face-to-face with the un-
derlying ethical questions of our relationship with the end user and
other stakeholders. By recognizing our role as agents in this rela-
tionship, we acknowledged our moral responsibility. Consequently,
we were encouraged to reflect deeper on our intentions behind our
design, ensuring that we were acting morally and thoughtfully of
the consequences of the software use.
5.3 Non-linear Reflection
At each stage of writing the metacommunication message we were
asked to identify the ethical issues that our design choices might
raise. By revisiting each of our answers to the guiding questions,
we were prompted to analyze the meanings of the words we used
to describe our intent. This process led us to identify ethical issues
not only with our choices in relation to each question, but also with
previous statements made in other parts of the metacommunication
message, as well. For example, when analyzing an ethical issue
within the design part, we were able to see that the issue was not
directly caused by design decisions, but actually by some of the
presuppositions behind them, generated in the analysis. In this way,
insights gathered in one part of the metacommunication message
led us to revisit and revise concepts expressed in other parts. Since
our ethical reflection concerns the artifact as a whole, and each part
of our design intent is related to every other, this is to be expected.
Looking at our statements for each of the guiding questions, we
could identify issues in both the ethical and design domains, since
our analysis connects the two. When we looked at a certain ethical
issue we could recognize when it resulted from a flaw in our design
process, from missing a certain key piece of information, from
misidentifying user goals and needs. Correcting these faults can, in
turn, generate new ethical issues, and so it goes on, iteratively.
6 RELATEDWORK
Part of the work related to our research formed the background
against which we characterized the focus of our proposal (see sub-
section 2.2). In this section we complement the background against
which we position our contribution.
We start with previous work that has explicitly used the 1st
person of discourse to support ethical reasoning or questions. A
recent example is the Consequence Scanning guide proposed by
Brown [6]. It poses two 1st-person questions for developers to
answer (“What are the positive consequences we want to focus on?”
and “What are the consequences we want to mitigate?”). The main
difference compared to our work is the absence of an explicit 2nd
371
FAccT ’21, March 3–10, 2021, Virtual Event, Canada Barbosa, Barbosa, de Souza, Leitão
person (you, the user(s)), which creates a conspicuous and engaging
me–you relationship. Older approaches to supporting computer
professionals’ decisions about ethics have also posed 1st-person
ethical questions, but have not engaged professionals in an explicit
me–you relation. Such is the case of O’Boyle’s decision-making
process questions [49], which refer to users and stakeholders as
“another person” (e.g., “Does my action or inaction diminish another
person by deceiving that person or rendering him/her less trusting
or less secure?”).
Heuristics-oriented approaches early reviewed by Maner [35]
and their successors, which now tend to concentrate on more or less
unified ethical guidelines for the AI or the ICT industry in general
(e.g., [18, 28, 33, 54]), vary in emphasis. Some concentrate more
on the principles, others on the process of integrating them into
existing practices with software industry. An important initiative to
operationalize ethical assessment among computer professionals is
Harris et al.’s work with DIODE [24], a meta-methodology for the
ethical assessment of new technologies. DIODE is the product of
collaborative efforts by researchers, government agents, and com-
mercial professionals, presenting a comprehensive set of tools, like
flowcharts and templates, for practical use. The meta-methodology
is carried out in five stages: definitions, issues, options, decisions,
and explanations. The main difference between DIODE and our
work is that DIODE is clearly process-oriented, whereas our work
privileges epistemic activity. Rather than making sure that all steps
of a methodology are followed, we are hoping to make sure that
anyone using our approach, even if partially, learns continually
about themselves, others, and the world we live in.
We should also mention that work on value-sensitive design [20]
and software engineering [17, 67] is related to ours, even though
values and ethics are different things. The important link is that we
share with this perspective an integrative view of newAI challenges
against the backdrop of persisting HCI and Software Engineering,
human-centered issues for any computer technology, not just AI.
Ethical challenges are added to the existing list, and should not be
treated as an aside.
We finally acknowledge initiatives to use statements and de-
scriptions of data and model features to express ethical concerns or
precautions that ML experts identify with respect to the technology
they contribute to build [21, 27, 39]. The common contrast between
these and our work is the focus on the internal technical aspects
of ML (datasets, training algorithms, and models), rather than on
the external contextualized use situations illustrated in section 4.
The explicit answers to the guiding questions and the extended
metacommunication template we propose may complement and
serve as resources for building their models.
7 CONCLUDING REMARKS
We conclude this paper with a brief discussion of its promises and
limitations. We also include a list of empirical studies, accompanied
by briefly described methodological challenges and considerations
that deserve our attention. Some of the items in this list are part of
our own immediate future work.
In our view, the promise of this work lies in its epistemic nature.
We mentioned that Markauskaite and Goodyear [36] underline how
critical the ability to generate actionable knowledge constantly and
fluently is for 21st-century workers. By integrating ethical consider-
ations into a larger framework of computer-mediated interpersonal,
extensively, but not exclusively, automated communication, where
computer interfaces speak for whoever has built them, our epis-
temic tool has the potential to avoid the risks of turning ethical
evaluation into a bureaucratic process. Filling out the metacommu-
nication template, as we have learned over the years, is anything
but a bureaucratic task. And even when it is not done in its entirety,
the parts achieved already have a beneficial effect. Moreover, since
the kind of knowledge gained explicitly refers to the knowledge
agent personally, because of its radical 1st-person perspective, the
memorability of learned lessons is likely to increase.
The main limitation of our proposal, at this initial stage, is the
lack of empirical indications about the plausibility of its promises.
Additionally, standing fully as a conceptual object, it lacks the
practical adjustments and corrections typically imposed by findings
of empirical studies in situ. Yet, as we have repeatedly mentioned
above, the proposed tool is a complex one, whose conception is
deeply anchored in existing knowledge from Semiotics and HCI.
Therefore, as a complex object per se, our epistemic tool is open to
productive discussion and critique, even in such conceptual state.
As an example of the advantages of maturing our understand-
ing of the tool prior to engaging into empirical studies, we can
mention that the epistemic nature of the tool strongly suggests
that predictive research designs are not likely to work. Knowledge
constructed with an epistemic tool can grow at any time, in vir-
tually any direction. Thus, the kind of research approach we use
must be carefully examined. Likewise, a separation of ethical from
non-ethical knowledge built with the metacommunication template
may be difficult to achieve. Since ethical dimensions are integrated
in the overall computer-mediated social communication in which
systems designers and developers are engaged, we are not sure that
such separation is possible (or needed).
Interesting future work with our tool includes case studies with
professional developers trying to understand the epistemic impact
of using the extended template. Empirical studies can also be car-
ried out in educational settings, using the extended template as a
learning resource par excellence. In a different direction, an impor-
tant kind of research is to investigate the separability of ethical
principles and paradigms. For example, if we want to use virtue
ethics, or a utilitarian ethics, instead of bioethics principles, would
we be able to use the extended template questions as currently
defined, instantiating them to those paradigms? Or would we need
to adapt the ethical questions themselves? Similarly, given the ac-
knowledged Socratic inclination of our proposal, the illustration
of our approach with the use of a priori principles borrowed from
bioethics (following Floridi and Cowls [18]) suggests an avenue
of investigation about converging or diverging evolution of the
results of ethical introspections triggered by this tool, vis à vis orga-
nizational codes of ethics that may be in place. This will, however,
require time and carefully designed longitudinal studies.
ACKNOWLEDGMENTS
Simone Barbosa and Clarisse de Souza are supported by CNPq
- Brasil, with grants #311316/2018-2 and #3304224/2017-0, respec-
tively. Gabriel Barbosa is supported by CAPES – Brasil #001.
372
A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development FAccT ’21, March 3–10, 2021, Virtual Event, Canada
REFERENCES
[1] Sabah Al-Fedaghi. 2018. Thinging Ethics for Software Engineers. IJCSIS Interna-
tional Journal of Computer Science and Information Security 16, 9 (2018), 11 pages.
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3257050
[2] Peter Bøgh Andersen. 1997. A Theory of Computer Semiotics: Semiotic Approaches
to Construction and Assessment of Computer Systems. Cambridge University Press,
Cambridge, MA.
[3] Tom L. Beauchamp and James F. Childress. 2019. Principles of Biomedical Ethics
(8th edition ed.). Oxford University Press, New York.
[4] Elettra Bietti. 2020. FromEthicsWashing to Ethics Bashing: AView on Tech Ethics
from within Moral Philosophy. In Proceedings of the 2020 Conference on Fairness,
Accountability, and Transparency (FAT* ’20). Association for Computing Machin-
ery, New York, NY, USA, 210–219. https://doi.org/10.1145/3351095.3372860
[5] Reuben Binns. 2018. Fairness in Machine Learning: Lessons from Political Phi-
losophy. In Proceedings of the 1st Conference on Fairness, Accountability and
Transparency (Proceedings of Machine Learning Research, Vol. 81), Sorelle A.
Friedler and Christo Wilson (Eds.). PMLR, New York, NY, USA, 149–159. http:
//proceedings.mlr.press/v81/binns18a.html
[6] S. Brown. 2019. Consequence Scanning | doteveryone (version 1 ed.). Doteveryone,
London. https://doteveryone.org.uk/project/consequence-scanning/
[7] J. Carroll and M. Rosson. 1990. Human-Computer Interaction Scenarios as a
Design Representation. In Twenty-Third Annual Hawaii International Conference
on System Sciences, Vol. 2. IEEE Computer Society, Los Alamitos, CA, USA, 555–
561 vol.2. https://doi.org/10.1109/HICSS.1990.205231
[8] Stacy M. Carter, Wendy Rogers, Khin Than Win, Helen Frazer, Bernadette
Richards, and Nehmat Houssami. 2020. The ethical, legal and social implications
of using artificial intelligence systems in breast cancer care. The Breast 49 (2020),
25 – 32. https://doi.org/10.1016/j.breast.2019.10.001
[9] Mark Coeckelbergh. 2010. Health Care, Capabilities, and AI Assistive Tech-
nologies. Ethical Theory and Moral Practice 13, 2 (2010), 181–190. https:
//doi.org/10.1007/s10677-009-9186-2
[10] Mark Coeckelbergh. 2011. Humans, Animals, and Robots: A Phenomenological
Approach to Human-Robot Relations. International Journal of Social Robotics 3, 2
(2011), 197–204. https://doi.org/10.1007/s12369-010-0075-6
[11] Mark Coeckelbergh. 2020. Artificial Intelligence, Responsibility Attribution, and
a Relational Justification of Explainability. Science and Engineering Ethics 26, 4
(Aug. 2020), 2051–2068. https://doi.org/10.1007/s11948-019-00146-8
[12] Clarisse Sieckenius de Souza. 2005. The Semiotic Engineering of Human-computer
Interaction. MIT Press, Cambridge, MA.
[13] Clarisse Sieckenius de Souza, Renato Fontoura de Gusmão Cerqueira, Luiz Mar-
ques Afonso, Rafael Rossi de Mello Brandão, and Juliana Soares Jansen Ferreira.
2016. Software Developers as Users: Semiotic Investigations in Human-Centered
Software Development. Springer, New York City, NY.
[14] Clarisse Sieckenius de Souza and Carla Faria Leitão. 2009. Semiotic En-
gineering Methods for Scientific Research in HCI. Synthesis Lectures on
Human-Centered Informatics 2, 1 (Jan. 2009), 1–122. https://doi.org/10.2200/
S00173ED1V01Y200901HCI002
[15] Christian Detweiler, Alina Pommeranz, Jeroen v. d. Hoven, and Helen Nis-
senbaum. 2011. Values in Design - Building Bridges between RE, HCI and
Ethics. In Human-Computer Interaction – INTERACT 2011 (Lisbon), Pedro Cam-
pos, Nicholas Graham, Joaquim Jorge, Nuno Nunes, Philippe Palanque, andMarco
Winckler (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 746–747.
[16] Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman,
David O’Brien, Kate Scott, Stuart Schieber, James Waldo, David Weinberger,
Adrian Weller, and Alexandra Wood. 2017. Accountability of AI Under the Law:
The Role of Explanation. arXiv:1711.01134 [cs.AI]
[17] Maria Angela Ferrario, Will Simm, Stephen Forshaw, Adrian Gradinar, Mar-
cia Tavares Smith, and Ian Smith. 2016. Values-first SE: Research Princi-
ples in Practice. In Proceedings of the 38th International Conference on Soft-
ware Engineering Companion (ICSE ’16). ACM, New York, NY, USA, 553–562.
https://doi.org/10.1145/2889160.2889219
[18] Luciano Floridi and Josh Cowls. 2019. A Unified Framework of Five Principles
for AI in Society. Harvard Data Science Review (June 2019). https://doi.org/10.
1162/99608f92.8cd550d1
[19] Luciano Floridi and Mariarosaria Taddeo. 2016. What is data ethics? Philosoph-
ical Transactions of the Royal Society of London A: Mathematical, Physical and
Engineering Sciences 374, 2083 (2016), 1–5. https://doi.org/10.1098/rsta.2016.0360
[20] Batya Friedman, Peter H. Kahn, Alan Borning, and Alina Huldtgren. 2013. Value
Sensitive Design and Information Systems. In Early engagement and new tech-
nologies: Opening up the laboratory, Neelke Doorn, Daan Schuurbiers, Ibo van de
Poel, and Michael E. Gorman (Eds.). Springer Netherlands, Dordrecht, 55–95.
https://doi.org/10.1007/978-94-007-7844-3_4
[21] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan,
HannaWallach, Hal Daumeé III, and Kate Crawford. 2018. Datasheets for Datasets.
http://arxiv.org/abs/1803.09010
[22] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Kagal. 2018. Ex-
plaining Explanations: An Overview of Interpretability of Machine Learning. In
2018 IEEE 5th International Conference on Data Science and Advanced Analytics
(DSAA). IEEE, Turin, Italy, Italy, 80–89.
[23] Donald Gotterbarn. 2001. Informatics and professional responsibility. Science
and Engineering Ethics 7, 2 (2001), 221–230. https://doi.org/10.1007/s11948-001-
0043-5
[24] Ian Harris, Richard C. Jennings, David Pullinger, Simon Rogerson, and Penny
Duquenoy. 2011. Ethical assessment of new technologies: a meta-methodology.
Journal of Information, Communication and Ethics in Society 9, 1 (Jan. 2011), 49–64.
https://doi.org/10.1108/14779961111123223
[25] H. Rex Hartson and Deborah Hix. 1989. Human-computer Interface Development:
Concepts and Systems for Its Management. ACM Comput. Surv. 21, 1 (March
1989), 5–92. https://doi.org/10.1145/62029.62031
[26] Rex Hartson and Pardha Pyla. 2012. The UX Book: Process and Guidelines for
Ensuring a Quality User Experience (1 edition ed.). Morgan Kaufmann, Amsterdam
; Boston.
[27] SarahHolland, AhmedHosny, SarahNewman, Joshua Joseph, and Kasia Chmielin-
ski. 2018. The Dataset Nutrition Label: A Framework To Drive Higher Data
Quality Standards. http://arxiv.org/abs/1805.03677
[28] Anna Jobin, Marcello Ienca, and Effy Vayena. 2019. The global landscape of
AI ethics guidelines. Nature Machine Intelligence 1, 9 (2019), 389–399. https:
//doi.org/10.1038/s42256-019-0088-2
[29] Deborah G. Johnson. 2007. Computer Ethics. John Wiley & Sons, Ltd, Hoboken,
NJ, USA, Chapter 45, 608–619. https://doi.org/10.1002/9780470996621.ch45
[30] Deborah G. Johnson. 2008. Computer Ethics. John Wiley & Sons, Ltd, Hoboken,
NJ, USA, Chapter 5, 63–75. https://doi.org/10.1002/9780470757017.ch5
[31] N. S. A. Karim, F. A. Ammar, and R. Aziz. 2017. Ethical Software: Integrating
Code of Ethics into Software Development Life Cycle. In 2017 International
Conference on Computer and Applications (ICCA) (Doha, United Arab Emirates).
IEEE, Piscataway, New Jersey, 290–298. https://doi.org/10.1109/COMAPP.2017.
8079763
[32] Maya Krishnan. 2020. Against Interpretability: a Critical Examination of the
Interpretability Problem in Machine Learning. Philosophy & Technology 33, 3
(2020), 487–502. https://doi.org/10.1007/s13347-019-00372-9
[33] Angelica Lahti, Sanna Naraha, Jesper Svensson, and Pontus Wärnestål. 2012.
Ethical Heuristics – A Tool for Applying Ethics in User-Involved IS Projects.
[34] Jos Lehmann, Joost Breuker, and Bob Brouwer. 2004. Causation in AI and Law.
Artificial Intelligence and Law 12, 4 (2004), 279–315. https://doi.org/10.1007/
s10506-005-4157-y
[35] Walter Maner. 2002. Heuristic Methods for Computer Ethics. Metaphilosophy 33,
3 (2002), 339–365. https://doi.org/10.1111/1467-9973.00231
[36] Lina Markauskaite and Peter Goodyear. 2017. Epistemic fluency and professional
education. Springer, Dordrecht. https://doi.org/10.1007
[37] Alexandre Mayol and Thierry Pénard. 2017. Facebook use and individual well-
being: Like me to make me happier! Revue d’économie industrielle 158 (2017),
101–127. https://doi.org/10.4000/rei.6570
[38] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social
sciences. Artificial Intelligence 267 (2019), 1–38. https://doi.org/10.1016/j.artint.
2018.07.007
[39] Margaret Mitchell, SimoneWu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019.
Model Cards for Model Reporting. In Proceedings of the Conference on Fairness,
Accountability, and Transparency (FAT* ’19). ACM, New York, NY, USA, 220–229.
https://doi.org/10.1145/3287560.3287596
[40] Brent Mittelstadt. 2019. Principles Alone Cannot Guarantee Ethical AI. Nature
Machine Intelligence , (Nov. 2019), 1–19. https://doi.org/10.2139/ssrn.3391293
[41] Brent Daniel Mittelstadt, Patrick Allo, Mariarosaria Taddeo, Sandra Wachter, and
Luciano Floridi. 2016. The ethics of algorithms: Mapping the debate. Big Data &
Society 3, 2 (2016), 1–21. https://doi.org/10.1177/2053951716679679
[42] Jessica Morley, Luciano Floridi, Libby Kinsey, and Anat Elhalal. 2019. FromWhat
to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and
Research to Translate Principles into Practices. http://arxiv.org/abs/1905.06876
[43] Mihai Nadin. 1988. Interface design: A semiotic paradigm. Semiotica 69, 3-4
(1988), 269–302. https://doi.org/10.1515/semi.1988.69.3-4.269
[44] Helen Nissenbaum. 1996. Accountability in a computerized society. Science and
Engineering Ethics 2, 1 (1996), 25–42. https://doi.org/10.1007/BF02639315
[45] H. Nissenbaum. 2001. How computer systems embody values. Computer 34, 3
(March 2001), 120–119. https://doi.org/10.1109/2.910905
[46] Donald A. Norman. 1986. Cognitive Engineering. Springer, Cham, Chapter 3,
31–62. https://doi.org/10.1007/978-3-319-17885-1_100154
[47] Donald A. Norman. 1988. The psychology of everyday things. Basic Books, New
York, NY, US.
[48] Horst Oberquelle, Ingbert Kupka, and Susanne Maass. 1983. A view of hu-
man—machine communication and co-operation. International Journal of Man-
Machine Studies 19, 4 (Oct. 1983), 309–333. https://doi.org/10.1016/S0020-
7373(83)80057-1
[49] Edward J. O’Boyle. 2002. An ethical decision-making process for computing
professionals. Ethics and Information Technology 4, 4 (Dec. 2002), 267–277. https:
//doi.org/10.1023/A:1021320617495
373
FAccT ’21, March 3–10, 2021, Virtual Event, Canada Barbosa, Barbosa, de Souza, Leitão
[50] Richard Parry. 2014. Ancient Ethical Theory. In The Stanford Encyclopedia
of Philosophy (fall 2014 ed.), Edward N. Zalta (Ed.). Metaphysics Research Lab,
Stanford University, online.
[51] Roger S. Pressman and Bruce Maxim. 2014. Software Engineering: A Practitioner’s
Approach (8th edition ed.). McGraw-Hill Education, New York, NY.
[52] Naomi Reshotko. 2006. Socratic Virtue: Making the Best of the Neither-Good-
Nor-Bad. Cambridge University Press, Cambridge. https://doi.org/10.1017/
CBO9780511482601
[53] Mary Beth Rosson and John M Carroll. 2007. Scenario-based design. In The
human-computer interaction handbook. CRC Press, Boca Raton, FL, 1067–1086.
[54] Lea Rothenberger, Benjamin Fabian, and Elmar Arunov. 2019. Relevance of Ethical
Guidelines for Artificial Intelligence - A Survey and Evaluation. In Proceedings of
the 27th European Conference on Information Systems (ECIS), Stockholm & Uppsala,
Sweden. Association for Information Systems, Stockholm & Uppsala, Sweden,
1–11. https://aisel.aisnet.org/ecis2019_rip/26
[55] Fabio Sabatini and Francesco Sarracino. 2017. Online Networks and Subjective
Well-Being. Kyklos 70, 3 (2017), 456–480. https://doi.org/10.1111/kykl.12145
[56] A. Scott and S. Barlowe. 2016. How software works: Computational thinking and
ethics before CS1. In 2016 IEEE Frontiers in Education Conference (FIE), Vol. 2016-
November. IEEE, , 1–9. https://doi.org/10.1109/FIE.2016.7757436
[57] Indira Sen, Anupama Aggarwal, Shiven Mian, Siddharth Singh, Ponnurangam
Kumaraguru, and AnwitamanDatta. 2018. Worth ItsWeight in Likes: Towards De-
tecting Fake Likes on Instagram. In Proceedings of the 10th ACMConference onWeb
Science (Amsterdam, Netherlands) (WebSci ’18). Association for Computing Ma-
chinery, New York, NY, USA, 205–209. https://doi.org/10.1145/3201064.3201105
[58] Helen Sharp, Jennifer Preece, and Yvonne Rogers. 2019. Interaction Design: Beyond
Human-Computer Interaction. John Wiley & Sons, Hoboken, NJ, USA.
[59] K. Shilton and S. Anderson. 2017. Blended, not bossy: Ethics roles, responsibilities
and expertise in design. Interacting with Computers 29, 1 (2017), 71–79. https:
//doi.org/10.1093/iwc/iww002
[60] Tasos Spiliotopoulos and Ian Oakley. 2019. Altruistic and selfish communication
on social media: the moderating effects of tie strength and interpersonal trust.
Behaviour & Information Technology 0, 0 (2019), 1–17. https://doi.org/10.1080/
0144929X.2019.1688392
[61] Chenkai Sun, Abolfazl Asudeh, H. V. Jagadish, Bill Howe, and Julia Stoyanovich.
2019. MithraLabel: Flexible Dataset Nutritional Labels for Responsible Data Sci-
ence. In Proceedings of the 28th ACM International Conference on Information and
Knowledge Management (Beijing, China) (CIKM ’19). Association for Computing
Machinery, New York, NY, USA, 2893–2896. https://doi.org/10.1145/3357384.
3357853
[62] Herman T. Tavani. 1999. Computer Ethics Textbooks: A Thirty-Year Retrospective.
SIGCAS Comput. Soc. 29, 3 (Sept. 1999), 26–31. https://doi.org/10.1145/572183.
572190
[63] Dennis F. Thompson. 1980. Moral Responsibility of Public Officials: The Problem
of Many Hands. American Political Science Review 74, 4 (Dec. 1980), 905–916.
https://doi.org/10.2307/1954312
[64] Michael Veale, Max Van Kleek, and Reuben Binns. 2018. Fairness and Accountabil-
ity Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-
Making. In Proceedings of the 2018 CHI Conference on Human Factors in Computing
Systems (Montreal QC, Canada) (CHI ’18). Association for Computing Machinery,
New York, NY, USA, 1–14. https://doi.org/10.1145/3173574.3174014
[65] Nynke E. Vellinga. 2017. From the testing to the deployment of self-driving cars:
Legal challenges to policymakers on the road ahead. Computer Law & Security
Review 33, 6 (2017), 847 – 863. https://doi.org/10.1016/j.clsr.2017.05.006
[66] Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Counterfactual
explanations without opening the black box: Automated decisions and the GDPR.
Harv. JL & Tech. 31 (2017), 841.
[67] Emily Winter, Stephen Forshaw, Lucy Hunt, and Maria Angela Ferrario. 2019.
Advancing the Study of Human Values in Software Engineering. In Proceedings
of the 12th International Workshop on Cooperative and Human Aspects of Software
Engineering (CHASE ’19). IEEE Press, Piscataway, NJ, USA, 19–26. https://doi.
org/10.1109/CHASE.2019.00012 event-place: Montreal, Quebec, Canada.
374
