Corporate Social Responsibility via Multi-Armed Bandits
Tom Ron
âˆ—
ront@campus.technion.ac.il
Technion - Israel Institute of
Technology
Omer Ben-Porat
âˆ—
omerbenporat@mail.tau.ac.il
Tel-Aviv University
Uri Shalit
urishalit@technion.ac.il
Technion - Israel Institute of
Technology
ABSTRACT
We propose a multi-armed bandit setting where each arm corre-
sponds to a subpopulation, and pulling an arm is equivalent to
granting an opportunity to this subpopulation. In this setting the
decision-makerâ€™s fairness policy governs the number of opportuni-
ties each subpopulation should receive, which typically depends
on the (unknown) reward from granting an opportunity to this
subpopulation. The decision-maker can decide whether to provide
these opportunities, or pay a pre-defined monetary value for every
withheld opportunity. The decision-makerâ€™s objective is to maxi-
mize her utility, which is the sum of rewards minus the cost paid for
withheld opportunities. We provide a no-regret algorithm that max-
imizes the decision-makerâ€™s utility and complement our analysis
with an almost-tight lower bound. Finally, we discuss the fairness
policy and demonstrate its downstream implications on the utility
and opportunities via simulations.
ACM Reference Format:
Tom Ron, Omer Ben-Porat, and Uri Shalit. 2021. Corporate Social Responsi-
bility via Multi-Armed Bandits. In ACM Conference on Fairness, Accountabil-
ity, and Transparency (FAccT â€™21), March 3â€“10, 2021, Virtual Event, Canada.
ACM,NewYork, NY, USA, 15 pages. https://doi.org/10.1145/3442188.3445868
1 INTRODUCTION
Algorithmic decision making plays a fundamental role in many
facets of our lives; criminal justice [10, 11, 29], banking [3, 18, 32, 40],
online-advertisement [28, 30], hiring [1, 2, 4, 7] , and college ad-
mission [5, 26, 36] are just a few examples. With the abundance
of applications in which algorithms operate, concerns about their
ethics, fairness, and privacy have emerged. For instance, classifi-
cation algorithms that were deemed to be unfair and discriminate
based on factors like gender, race, and more [16, 20, 38, 41]. Algo-
rithmic fairness is a framework that, among other means, is aimed
at ensuring the long-term welfare of such subpopulations when
subject to algorithmic decision making.
Consider the following online advertisement use-case. A com-
pany wants to publish a job ad online and optimizes its campaign
based on the cost-per-click. As witnessed by Lambrecht and Tucker
[24], women are less likely to see job ads for STEM positions since
âˆ—
Both authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
Â© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8309-7/21/03. . . $15.00
https://doi.org/10.1145/3442188.3445868
they have higher cost-per-click than men. If women are not exposed
to information about STEM career opportunities, they may never
apply to such jobs [15]. In order to act fairly and display ads to
all the subpopulations the company will need to sacrifice part of
its short-term utility and pay a higher cost-per-click. This is an
example of a cost of fairness. Our goal in this paper is to better
understand the trade-offs facing companies who wish to ensure
their algorithms are more equitable.
We focus on exploring the cost of fairness versus the cost of
alternatives such as Corporate Social Responsibility [13] (CSR here-
inafter). CSR is a self-regulation act of philanthropic responsibility
in response to the rising concerns on ethical issues in businesses.
For example, in 2019, Microsoft spent more than three billion dollars
with minority, disabled, veteran, LGBTQ, and woman-owned busi-
nesses
1
. Starbucks focuses on creating meaningful opportunities
for their employees. As of June 2020, more than 4,500 employees
have earned first-time bachelorâ€™s degrees since Starbucksâ€™ College
Achievement Plan was announced in 2014
2
.
In this paper, we suggest an algorithmic approach to CSR in
the setting of sequential decision making. Sequential decision mak-
ing is often modeled as Multi-armed bandit problems (hereinafter
MAB; see Auer et al. [8] for a brief introduction). MABs enjoy mas-
sive commercial success and have myriad real-world applications
[14, 17, 37, 39]. Here, we treat arms as subpopulations, and require
that subpopulations would not starve from lack of opportunities.
Opportunities can be granted to a subpopulations either explicitly,
i.e., by pulling the subpopulationâ€™s arm, or implicitly via CSR chan-
nels. Given the example above, companies have the choice whether
to display ads to subpopulations with higher cost-per-click, or al-
ternatively to invest money in organizations that promote the long
term well-being of those subpopulations.
We highlight the tension between the decision-maker that wants
to maximize her reward and the cost of CSR. We consider the bandit
reward to be the direct benefit derived from granting the opportu-
nity to the subpopulation represented by the arm. For simplicity we
use the term expected reward from here on. The amount of opportu-
nities depends on how fairness is perceived by the decision-maker
and the expected rewards. Unfortunately, information about the
expected rewards is not known in advance and has to be explored
by the decision-maker. We take a utilitarian approach: The utility
of the decision-maker is composed of the rewards (e.g. clicks on
displayed ads), and a transfer cost. The transfer cost is the amount
the decision-maker invests in CSR for every deferred opportunity.
Knowing the transfer cost in advance, the decision-maker can make
an informed decision on how to allocate its resources. Our model
casts light on the trade-off between the cost of directly granting
1
https://aka.ms/2019CSRReport
2
https://stories.starbucks.com/uploads/2020/06/2019-Starbucks-Global-Social-
Impact-Report.pdf
26
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Tom Ron, Omer Ben-Porat, and Uri Shalit
an opportunity and the cost of granting the opportunity via an
external route such as CSR.
1.1 Our Contribution
Our contribution is two-fold: technical and conceptual. On the con-
ceptual side, our framework reflects the trade-off betweenmonetary
rewards and subpopulation opportunities, which can be viewed as
a means of providing long-term welfare. This perspective follows,
e.g., self-regulation in revenue-driven commercial companies (as
decision-makers) contributing to societal goals, or a policy maker
that ensures that the decision-maker is fairness aware. In the for-
mer, sufficient opportunities are a CSR [19] that is integrated in the
companyâ€™s objective by design. In the latter, the decision-maker
provides opportunities explicitly by arm pulls, or implicitly by
payments that are invested in that subpopulation by the policy
maker (for, e.g., better computer labs in public schools). Crucially,
the number of required opportunities depends on the expected
rewards, which are only known in hindsight.
Technically, we consider the typicalMAB settingwithğ¾ Bernoulli
arms with horizon ğ‘‡ and expectation vector ğ, which is unknown.
In addition, we introduce a fairness function ğ‘“ , ğ‘“ : [0, 1]ğ¾ â†’ [0, 1]ğ¾ ,
which determines the minimal number of pulls for each arm given
the expected reward vector ğ. The term ğ‘‡ Â· ğ‘“ (ğ)ğ‘– quantifies the
amount of opportunities subpopulation ğ‘– deserves, which, impor-
tantly, is a function of its own expected reward and the expected
rewards of the other subpopulations. The decision-maker gains
rewards, but pays a transfer cost of _ for every round of unmet
opportunity; namely, _
âˆ‘ğ¾
ğ‘–=1max{0,ğ‘‡ Â· ğ‘“ (ğ)ğ‘– âˆ’ğ‘ğ‘– } where ğ‘ğ‘– is the
number of pulls of arm ğ‘– . We assume that both ğ‘“ and _ are known
in advance. We characterize the optimal algorithm that achieves a
sub-linear regret of ?Ìƒ? (ğ‘‡ 2/3), and show a matching lower bound. We
augment our theoretical analysis with experimental one, examining
the implications of different fairness functions ğ‘“ and values of _.
1.2 Related Work
Multi-armed bandits have been the subject of many fairness-related
research [21, 22, 25, 31, 33]. Joseph et al. [21, 22] study fairness in
MABs from the eyes of the decision-maker. In their work, a learning
process is considered unfair if an arm with a lower expected reward
is favored over an arm with a higher expected reward. We study
fairness from the perspective of the arms and view arm pulling as
granting an opportunity. This view was also adopted by Liu et al.
[25]. The authors define a calibrated fair policy to be a policy that
selects action ğ‘– with probability equal to the probability that the
reward realization of arm ğ‘– is the highest. By measuring only the
number of rounds the algorithm is miscalibrated, Liu et al. [25]
neglect the decrease in reward incurred by the fairness require-
ment. In contrast to these works, we explicitly suggest a utilitarian
approach that penalizes for lack of pulls (in our terminology, op-
portunities). A penalty of similar flavor was suggested recently in
other work on fair ML [9, 23], but in different settings. Schumann
et al. [33] studied group fairness in MAB where several arms can
belong to the same protected group and the reward can be biased.
The work most related to ours is Patil et al. [31]. The authors de-
fine fairness as pulling each arm at least a minimal number of times
according to a predefined vector (where each entry corresponds
to a subpopulation). The predefined vector is given by the policy
maker and is independent of the subpopulation properties. How-
ever, our work differs from Patil et al. [31] in two crucial aspects.
First, while Patil et al. [31] model fairness as a hard constraint, we
better address real-world applications and treat it as a soft one. Our
utilitarian approach, which is well-studied in economic contexts
[27, 35], accounts for trading rewards with opportunities. If pro-
viding opportunities explicitly by pulling the arms is financially
unbearable, the decision-maker can do that implicitly by monetary
transfers. Second, Patil et al. [31] construct the fairness constraint
by a predefined vector, while in our work the opportunity require-
ments depend on each armâ€™s expected rewards, a quantity which
is uncertain during the decision making process, and is only fully
known in hindsight. This uncertainty makes the problem we ad-
dress markedly harder. These differences and others lead to a lower
bound of ?Ìƒ? (ğ‘‡ 2/3) compared to a ?Ìƒ? (
âˆš
ğ‘‡ ) in theirs.
2 MODEL
We consider a stochastic bandit problem; a decision-maker is given
ğ¾ arms, and pulls one at each time step ğ‘¡ = 1, 2, . . . ,ğ‘‡ . We denote
by ğ‘–ğ‘¡ the arm pulled at time ğ‘¡ . When arm ğ‘– is pulled at time ğ‘¡ , the
decision-maker receives a random reward, ğ‘Ÿğ‘¡ âˆ¼ Dğ‘– . We assume
that for every ğ‘– âˆˆ [ğ¾], the reward distribution Dğ‘– is a Bernoulli
distribution with expected value `ğ‘– . This is without loss of general-
ity, since we can reduce any instance with general [0, 1]-supported
distribution to an instance with Bernoulli arms using the technique
of Agrawal and Goyal [6]. We use ğ to denote the vector of expected
rewards, i.e., ğ = (`1, . . . , `ğ¾ ). We denote by ğ‘ğ‘–,ğ‘¡ the number of
times arm ğ‘– is pulled by the end of round ğ‘¡ , and let Î”ğ‘– = `
âˆ— âˆ’ `ğ‘– be
the gap between the expected reward of the optimal arm and the
expected reward of arm ğ‘– .
We now present the Reward-Opportunity MAB (R-OMAB) model.
An instance of R-O MAB is represented by a tuple âŸ¨ğ¾,ğ‘‡ , ğ, ğ‘“ , _âŸ©.
The tuple âŸ¨ğ¾,ğ‘‡ , ğâŸ© is an instance of standard stochastic bandit as
described above. The combination of ğ‘“ and _ creates what we call
the â€œfairness policyâ€.
The fairness requirements are expressed by a function ğ‘“ , ğ‘“ :
[0, 1]ğ¾ â†’ [0, 1]ğ¾ . ğ‘“ receives as input a vector of expected re-
wards and outputs a vector of minimal fraction of times each arm
has to be pulled in order not to be penalized. We let ğ‘“ (ğ)ğ‘– de-
note the ğ‘–â€™th entry of ğ‘“ (ğ). We assume that
âˆ‘ğ¾
ğ‘–=1 ğ‘“ (ğ)ğ‘– â‰¤ 1 and
that ğ‘“ is Lipschitz continuous with a Lipschitz constant ğ¿ with
respect to the ğ‘™1 norm. That is, for all ğ, ğ â€² âˆˆ [0, 1]ğ‘˜ it holds that
âˆ¥ ğ‘“ (ğ) âˆ’ ğ‘“ (ğ â€²)âˆ¥
1
â‰¤ ğ¿ âˆ¥ğ âˆ’ ğ â€²âˆ¥
1
. Intuitively, satisfying the Lips-
chitz condition implies that two similar expected reward vectors
get similar fairness requirements. For simplicity, from here on we
call ğ‘“ the fairness function.
The difference between the fairness requirement and the number
of times an arm was pulled,ğ‘‡ ğ‘“ (ğ)ğ‘– âˆ’ğ‘ğ‘–,ğ‘‡ , represents the deviation
from the fairness constraint. If the deviation is positive, it means
the arm was not pulled enough times, i.e. the subpopulation did not
receive enough opportunities according to the fairness function.
In such a case, the decision-maker pays a cost. The paid cost for a
single arm pullâ€™s deviation from the fairness requirement is given
by _ğ‘– , the transfer cost for arm ğ‘– . If arm ğ‘– was pulled less than
ğ‘‡ ğ‘“ (ğ)ğ‘– times, the reward will be deducted by _ğ‘– (ğ‘‡ ğ‘“ (ğ)ğ‘–âˆ’ğ‘ğ‘–,ğ‘‡ ). The
27
Corporate Social Responsibility via Multi-Armed Bandits FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
transfer cost is known to the decision-maker in advance. To account
for all cases, the possible cost which stems from the deviation
is _ğ‘– max{ğ‘‡ ğ‘“ (ğ)ğ‘– âˆ’ ğ‘ğ‘–,ğ‘‡ , 0}. For simplicity, we use _ğ‘– = _ for all
ğ‘– âˆˆ [ğ¾], but stress that our results hold with minor modifications
in the general case as well.
The utility of the decision-maker is denoted by U_,ğ‘“ . It is an
additive utility of the reward minus the total deviation from the
fairness requirement. Notice that ğ‘–ğ‘¡ and consequently ğ‘ğ‘–,ğ‘‡ depend
on the algorithm playing the arms. Formally, given an algorithm
ğ´ğ¿ğº ,
U_,ğ‘“ (ğ´ğ¿ğº ;ğ‘‡ )
def
=
ğ‘‡âˆ‘
ğ‘¡=1
ğ‘Ÿğ‘–ğ‘¡ âˆ’ _
ğ‘˜âˆ‘
ğ‘–=1
max{ğ‘‡ ğ‘“ (ğ)ğ‘– âˆ’ ğ‘ğ‘–,ğ‘‡ , 0}. (1)
As is customary in the MAB literature, we focus on the regret of
the decision-maker, which we denote R_,ğ‘“ (ğ´ğ¿ğº ;ğ‘‡ ). Letğ‘‚ğ‘ƒğ‘‡ be an
algorithm maximizing the utilityU_,ğ‘“ (ğ‘‚ğ‘ƒğ‘‡ ) (we discuss ğ‘‚ğ‘ƒğ‘‡ in
Subsection 2.1). The regret is the gap between the expected utility
of ğ‘‚ğ‘ƒğ‘‡ and ğ´ğ¿ğº :
R_,ğ‘“ (ğ´ğ¿ğº ;ğ‘‡ )
def
= E(U_,ğ‘“ (ğ‘‚ğ‘ƒğ‘‡ ;ğ‘‡ )) âˆ’ E(U_,ğ‘“ (ğ´ğ¿ğº ;ğ‘‡ )) . (2)
When _ and ğ‘“ are arbitrary or clear from the context, we omit
the subscript and simply denote U and R. Full proofs appear in
Section A.
2.1 Optimal Algorithm
The structure of the optimal algorithm in classic MABs is straight-
forward: In every round, pick the arm with the highest expectation.
However, in our case, the transfer cost makes the optimal algorithm
a bit more complex, as we now elucidate. Let ğ‘– denote an arbitrary
index of a sub-optimal arm, i.e., an arm such that `ğ‘– < maxğ‘–â€²âˆˆ[ğ¾ ] `ğ‘–â€² .
The decision-maker has to decide whether to support the subpopula-
tion associated with that arm explicitly (by pulling itğ‘‡ ğ‘“ (ğ)ğ‘– times)
or implicitly (by paying _ğ‘‡ ğ‘“ (ğ)ğ‘– ). Note that ğ‘‡ ğ‘“ (ğ)ğ‘– can be non-
integer, in this case, we assumeğ‘‡ ğ‘“ (ğ)ğ‘– is rounded to the preceding
integer. In each one of those ğ‘‡ ğ‘“ (ğ)ğ‘– rounds, the decision-maker
losses Î”ğ‘– if she pulls arm ğ‘– (as she could pick the optimal arm) but
saves _ (as she does not need the pay the transfer cost). Therefore,
if the reward gap of arm ğ‘– is greater than the transfer cost, Î”ğ‘– > _,
the decision-maker does not pull arm ğ‘– at all and pays the transfer
cost. Otherwise, if Î”ğ‘– < _, the decision-maker would have greater
utility by pulling arm ğ‘– exactly ğ‘‡ ğ‘“ (ğ)ğ‘– times and not incurring the
transfer cost. If Î”ğ‘– = _, the decision-maker is indifferent between
the two options. More formally,
Lemma 1. Fix an arbitrary instance âŸ¨ğ¾,ğ‘‡ , ğ, ğ‘“ , _âŸ© and let ğ‘‚ğ‘ƒğ‘‡ be
an optimal algorithm for that instance. For every sub-optimal arm ğ‘– ,
if Î”ğ‘– < _ thenğ‘‚ğ‘ƒğ‘‡ pulls ğ‘– exactlyğ‘‡ ğ‘“ (ğ)ğ‘– times; if Î”ğ‘– > _,ğ‘‚ğ‘ƒğ‘‡ does
not pull ğ‘– at all. If Î”ğ‘– = _, ğ‘‚ğ‘ƒğ‘‡ pulls arm ğ‘– between zero and ğ‘‡ ğ‘“ (ğ)ğ‘–
times.
Proof. Notice that the utility (Equation 1) is order insensitive;
hence,ğ‘‚ğ‘ƒğ‘‡ is not unique. Therefore, we only care about the vector
ğ‘µ = (ğ‘1, . . . , ğ‘ğ¾ ). We prove that any vector ğ‘µ that violates the
structure described above is sub-optimal.
Denote by ğ‘µ âˆ— = (ğ‘ âˆ—
1
, . . . , ğ‘ âˆ—
ğ¾
) a vector of counts where for
every sub-optimal arm ğ‘– , if Î”ğ‘– < _, ğ‘ âˆ—
ğ‘–
= ğ‘‡ (ğ)ğ‘– and if Î”ğ‘– > _,
ğ‘ âˆ—
ğ‘–
= 0. If Î”ğ‘– = _, 0 â‰¤ ğ‘ âˆ—ğ‘– â‰¤ ğ‘‡ ğ‘“ (ğ)ğ‘– . Note that ğ‘µ
âˆ—
is not unique.
Let ğ‘µ = (ğ‘1, . . . , ğ‘ğ¾ ) be a vector that violates the structure of ğ‘µ âˆ—
.
Since ğ‘µ violates the structure of ğ‘µ âˆ—
, there exists ğ‘–, ğ‘— âˆˆ [ğ¾], ğ‘– â‰  ğ‘—
such that ğ‘ğ‘– < ğ‘ âˆ—
ğ‘–
and ğ‘ ğ‘— > ğ‘ âˆ—
ğ‘—
. We prove that by moving pulls
from ğ‘— to ğ‘– in a way specified below the expected utility increases.
Let ğ‘µ â€² = (ğ‘ â€²
1
, . . . , ğ‘ â€²
ğ¾
) be the modified vector after moving pulls
from ğ‘— to ğ‘– . That is, for all ğ‘™ âˆˆ [ğ¾], ğ‘™ â‰  ğ‘–, ğ‘— , ğ‘ğ‘™ = ğ‘ â€²ğ‘™ . Additionally,
we show that either ğ‘ â€²
ğ‘–
= ğ‘ âˆ—
ğ‘–
or ğ‘ â€²
ğ‘—
= ğ‘ âˆ—
ğ‘—
.
If 0 â‰¤ ğ‘ğ‘– < ğ‘ âˆ—
ğ‘–
it implies that Î”ğ‘– â‰¤ _. Otherwise ğ‘ âˆ—ğ‘– = 0. If Î” ğ‘— <
_ it implies that ğ‘ ğ‘— > ğ‘‡ ğ‘“ (ğ)ğ‘— . In this case movingğ‘š = min{ğ‘ ğ‘— âˆ’
ğ‘‡ ğ‘“ (ğ)ğ‘— ,ğ‘‡ ğ‘“ (ğ)ğ‘– âˆ’ ğ‘ğ‘– } pulls from ğ‘— to ğ‘– changes the expected utility
byğ‘š(_ + `ğ‘– âˆ’ ` ğ‘— ) =ğ‘š(_ âˆ’ Î”ğ‘– + Î” ğ‘— ). Since Î”ğ‘– < _ and Î” ğ‘— â‰¥ 0 the
expected utility increases. By the definition ofğ‘š either ğ‘ â€²
ğ‘–
= ğ‘ âˆ—
ğ‘–
or
ğ‘ â€²
ğ‘—
= ğ‘ âˆ—
ğ‘—
. Otherwise, if Î” ğ‘— > _, it implies that `ğ‘– > ` ğ‘— . By moving
ğ‘š = min{ğ‘ ğ‘— ,ğ‘‡ ğ‘“ (ğ)ğ‘– âˆ’ğ‘ğ‘– }, the expected utility changes by at least
-ğ‘š(`ğ‘– âˆ’ ` ğ‘— ). Since `ğ‘– > ` ğ‘— the expected utility increases. By the
definition ofğ‘š either ğ‘ â€²
ğ‘–
= ğ‘ âˆ—
ğ‘–
or ğ‘ â€²
ğ‘—
= ğ‘ âˆ—
ğ‘—
.
We showed that every vector that violates the structure of ğ‘µ âˆ—
can be modified such that the expected utility increases and at least
one more entry does not violate the structure of ğ‘µ âˆ—
. Repeating this
process at most ğ¾ âˆ’ 1 times yields ğ‘µ âˆ—
and increases the utility. â–¡
2.2 About the Fairness Policy
The fairness policy is comprised of the fairness function ğ‘“ and the
transfer cost _. The function ğ‘“ represents the decision-makerâ€™s
view on how opportunities should be distributed. For example, the
zero function ğ‘“ 0 (ğ)ğ‘–
def
= 0 corresponds to standard Multi-Armed
bandit problem without any constraints. Generalizing this case
for any constant function, e.g., ğ‘“ uni (ğ)ğ‘–
def
= 1
ğ¾
, alludes that the
decision-maker believes that all subpopulations are entitled to the
same share of opportunities irrespective of their expected rewards.
The fairness function can also grow linearly with each expected
reward, for instance ğ‘“ lin (ğ)ğ‘–
def
=
`ğ‘–
ğ¾
. In the most general case, the
number of required opportunities to a subpopulation can also de-
pend on its expected reward relative to the expected rewards of
other subpopulations. For example, ğ‘“ sft (ğ; ğ‘)ğ‘–
def
=
exp
ğ‘`ğ‘–âˆ‘ğ¾
ğ‘—=1 exp
ğ‘`ğ‘—
. Our
modelling and results support these special cases and many other
natural candidates for the fairness function.
Selecting _ complements the decision-makerâ€™s view on revenue
and opportunities. As described in Section 2.1, if the transfer cost
is high the decision-maker will tend to grant the opportunities
explicitly, and would not grant opportunities explicitly only when
the subpopulationsâ€™ expected rewards have big differences. If the
transfer cost is low the decision-maker would derive a higher utility
by supporting subpopulations via CSR and not by directly granting
opportunities. As pointed out earlier, _ can vary between different
subpopulations but for simplicity is assumed equal.
3 NO-REGRET ALGORITHMS
In this section, we present our main algorithmic contribution. We
devise Self-regulated Utility Maximization, which incurs a regret
of ?Ìƒ? (ğ‘‡ 2/3). Before we discuss it, we first demonstrate that classical
MAB algorithms fail miserably on our setting. This is expected
given that such algorithms were not devised for a setting like ours,
28
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Tom Ron, Omer Ben-Porat, and Uri Shalit
but it will serve us later on. Classical MAB algorithms are tuned to
pull sub-optimal arms as little as possible. As shown in Subsection
2.1, it is not always optimal for R-O MAB. If the cost of opportunity
(Î”ğ‘– ) is lower than the transfer cost (_), the optimal algorithm pulls
arm ğ‘– according to the fairness function.
To better illustrate, consider the famous Explore-Then-Commit
(ETC) algorithm. ETC explores all arms for a predetermined number
of rounds (ğ‘ ), and then follows the best preforming arm for the
remaining rounds. We focus on a R-O MAB instance with 2 arms,
ğ = (1, 1
2
), transfer cost _ = 0.6 and constant fairness function
ğ‘“ (ğ)ğ‘– = 1
2
. The optimal algorithm from Subsection 2.1 pulls each
arm
ğ‘‡
2
times and obtains an expected utility of 0.75ğ‘‡ . ETC with
optimized exploration parameter will discover that arm 1 is the
better one relatively fast, and will pick that arm forever; hence, its
utility isğ‘‡ âˆ’_ğ‘‡
2
âˆ’ğ‘œ (1) â‰ˆ 0.7ğ‘‡ . The regret is therefore 0.05ğ‘‡ , which
is linear in the number of rounds ğ‘‡ .
In R-OMAB, we face a unique challenge comparing to the classic
MAB problem. Classical MAB algorithms are aimed at identifying
the optimal arm but do not estimate accurately the expected re-
wards ğ and consequently do not approximate well the reward
gaps (Î”ğ‘– )ğ‘–âˆˆ[ğ¾ ] . As discussed in Section 2.1, the optimal algorithm
depends on the relation between the reward gaps and the transfer
cost; hence, unlike classic MAB, accurate approximation of the re-
ward gaps (Î”ğ‘– )ğ‘–âˆˆ[ğ¾ ] is crucial for our problem. Additionally, ğ‘“ (ğ)
should also be approximated correctly for arms ğ‘– with Î”ğ‘– < _, to
align with the optimal algorithm. These two challenges are singular
to our settings and are reflected in the lower bound.
Algorithm 1, which we term Fairness-Aware-ETC, is a modified
version of ETC, which is aware of the fairness function and the
transfer cost. Fairness-Aware-ETC pulls each arm ğ‘ times, where
ğ‘ is received as an input. After the ğ¾ğ‘ exploration rounds, it
constructs estimates for ğ and ğ‘“ (ğ), which we denote using the
hat notation, i.e., ğ and ğ‘“ (ğ). It then continues optimally with
respect to these estimates (similarly to the optimal algorithm for
the estimated quantities). The number of exploration rounds per
arm ğ‘ balances the tension between exploration and exploitation.
Setting ğ‘ very high ensures that the estimates ğ and ğ‘“ (ğ) are close
to their actual counterparts with high probability, but can allow
little exploitation and hence high regret. Picking ğ‘ too low can
result in wrong estimation of ğ and as a consequence also wrongly
evaluate ğ‘“ .
Theorem 1. Fix any arbitrary instance of R-O MAB, and let ğ‘ =
8ğ¿
2/3ğ‘‡ 2/3
log
1/3ğ‘‡ . Algorithm 1 has a regret of ğ‘‚ (ğ¾ğ¿2/3ğ‘‡ 2/3
log
1/3ğ‘‡ ).
Proof. Wedefine the clean event to be the event that

Ë†Ì€ğ‘–,ğ‘¡ âˆ’ `ğ‘–
 â‰¤
ğ‘Ÿğ‘¡ (ğ‘–) holds for all arms simultaneously. Where ğ‘Ÿğ‘¡ (ğ‘–) =
âˆš
2 logğ‘‡
ğ‘ğ‘–,ğ‘¡
. We
will argue separately the clean event, and the â€œbad eventâ€ â€“ the
complement of the clean event. The regret is -
R(ğ‘‡ ) = R(ğ‘‡ |clean event)P(clean event)+R(ğ‘‡ |bad event)P(bad event).
We now bound the probabilities for the clean event and for
the â€œbad eventâ€ at the end of the exploration phase. That is, af-
ter each arm was pulled ğ‘ times. Using Hoeffdingâ€™s inequality,
P(

Ë†Ì€ğ‘–,ğ‘¡ âˆ’ `ğ‘–
 â‰¤ ğ‘Ÿğ‘¡ (ğ‘–)) â‰¥ 1 âˆ’ 2
ğ‘‡ 4
. Using the union bound, the proba-
bility for the clean event is
P(âˆ€ğ‘– âˆˆ [ğ¾]

Ë†Ì€ğ‘–,ğ‘¡ âˆ’ `ğ‘–
 â‰¤ ğ‘Ÿğ‘¡ (ğ‘–)) â‰¥ 1 âˆ’ 2ğ¾
ğ‘‡ 4
.
Since the â€œbad eventâ€ complements the clean event the probabil-
ity of the â€œbad eventâ€ is at most
2ğ¾
ğ‘‡ 4
. That the regret of the bad event
is bounded by the highest possible regret -ğ‘‡ . Combining the two last
statement together we obtain that R(ğ‘‡ |bad event)P(bad event) <
2ğ¾
ğ‘‡ 3
.
The clean event implies that for all ğ‘– âˆˆ [ğ¾],
| Ë†Ì€ğ‘– âˆ’ `ğ‘– | â‰¤ ğ‘‚
(âˆš
logğ‘‡
ğ‘
)
= ğ‘‚
(
ğ¿âˆ’1/3ğ‘‡âˆ’1/3 log1/3ğ‘‡
)
. Thus, with re-
spect to ğ‘™1 norm, âˆ¥ğ âˆ’ ğ â€²âˆ¥
1
â‰¤ ğ‘‚
(
ğ¾ğ¿âˆ’1/3ğ‘‡âˆ’1/3 log1/3ğ‘‡
)
, since ğ‘“ is
ğ¿-Lipschitz, ğ‘‡ âˆ¥ ğ‘“ (ğ) âˆ’ ğ‘“ (ğ)âˆ¥
1
â‰¤ ğ‘‚
(
ğ¾ğ¿
2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
.
Next, we examine several cases at the end of the exploration
phase and their effect on the regret.
For arm ğ‘– if Î”ğ‘– < _ the optimal algorithm pulls arm ğ‘– exactly
ğ‘‡ ğ‘“ (`)ğ‘– times.
â€¢ If Î”Ì‚ğ‘– < _, Algorithm 1will pull arm ğ‘– ,max {ğ‘,ğ‘‡ ğ‘“ (ğ)ğ‘– } times.
If ğ‘‡ ğ‘“ (ğ)ğ‘– ,ğ‘‡ ğ‘“ (ğ)ğ‘– < ğ‘ the regret is bounded by ğ‘ (Î”ğ‘– +
_) â‰¤ ğ‘‚
(
ğ¿
2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
. Otherwise, the regret is bounded
by (Î”ğ‘– + _)ğ‘‡ |ğ‘“ (ğ)ğ‘– âˆ’ ğ‘“ (ğ) |.
â€¢ If Î”Ì‚ğ‘– > _. Given the clean event it implies that _ âˆ’ Î”ğ‘– â‰¤
ğ‘‚
(âˆš
logğ‘‡
ğ‘
)
. In this situation, Algorithm 1 will not pull arm ğ‘–
anymore. If ğ‘ â‰¥ ğ‘‡ ğ‘“ (ğ)ğ‘– , the regret is be bounded by ğ‘Î” =
ğ‘‚
(
ğ¿
2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
. If ğ‘ < ğ‘‡ ğ‘“ (ğ)ğ‘– , the regret is (_âˆ’Î”ğ‘– )ğ‘ â‰¤
ğ‘‚ (
âˆš
ğ‘ logğ‘‡ ) â‰¤ ğ‘‚ (ğ¿2/3ğ‘‡ 2/3
log
1/3ğ‘‡ ).
For arm ğ‘– if Î”ğ‘– > _ the optimal algorithm does not pull arm ğ‘– at
all.
(1) If Î”Ì‚ğ‘– > _, Algorithm 1 will not pull arm ğ‘– anymore. The
regret is bounded by ğ‘Î”ğ‘– â‰¤ ğ‘‚
(
ğ¿
2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
.
(2) If Î”Ì‚ğ‘– < _. Given the clean event it implies that Î”ğ‘– âˆ’ _ â‰¤
ğ‘‚
(âˆš
logğ‘‡
ğ‘
)
= ğ‘‚
(
ğ¿âˆ’1/3ğ‘‡âˆ’1/3 log1/3ğ‘‡
)
. Algorithm 1 will play
arm ğ‘– , max {ğ‘,ğ‘‡ ğ‘“ (ğ)ğ‘– } times. If ğ‘ â‰¥ ğ‘‡ ğ‘“ (ğ)ğ‘– , the regret
is bounded by ğ‘ (Î”ğ‘– + _) = ğ‘‚
(
ğ¿
2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
. If ğ‘ <
ğ‘‡ ğ‘“ (ğ)ğ‘– â‰¤ ğ‘‡ ğ‘“ (ğ)ğ‘– , the regret is bounded by (Î”ğ‘– âˆ’_)ğ‘‡ ğ‘“ (ğ)ğ‘– â‰¤
ğ‘‚
(
ğ‘‡
2/3
log
1/3ğ‘‡
)
. Otherwise, if ğ‘ < ğ‘‡ ğ‘“ (ğ)ğ‘– and ğ‘‡ ğ‘“ (ğ)ğ‘– â‰¤
ğ‘‡ ğ‘“ (ğ)ğ‘– the regret is bounded by (Î”ğ‘– âˆ’_)ğ‘‡ ğ‘“ (ğ)ğ‘– +_ğ‘‡ (ğ‘“ (ğ)ğ‘– âˆ’
ğ‘“ (ğ)ğ‘– ) â‰¤ ğ‘‚ (ğ¿2/3ğ‘‡ 2/3
log
1/3ğ‘‡ )
The analysis of the cases above implies that after the loop that
starts on Line 1 ends, for every sub-optimal arm ğ‘–ğ‘ âˆ—
ğ‘–
âˆ’ ğ‘ğ‘–
 â‰¤ ğ‘‚ (
ğ¿
2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
+ğ‘‡ |ğ‘“ (ğ)ğ‘– âˆ’ ğ‘“ (ğ)ğ‘– |. Note that
ğ‘‡
ğ¾âˆ‘
ğ‘–=1
|ğ‘“ (ğ)ğ‘– âˆ’ ğ‘“ (ğ)ğ‘– | = ğ‘‡ âˆ¥ ğ‘“ (ğ) âˆ’ ğ‘“ (ğ)âˆ¥1
â‰¤ ğ‘‡ğ¿ âˆ¥ğ âˆ’ ğâˆ¥
1
â‰¤ ğ¾ğ¿2/3ğ‘‡ 2/3
log
1/3ğ‘‡
. Hence the regret in the end of the exploration phase is bounded
by ğ‘‚
(
ğ¾ğ¿
2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
.
29
Corporate Social Responsibility via Multi-Armed Bandits FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
Algorithm 1: Fairness-Aware-ETC
Input: ğ‘ - number of exploration rounds
1 for ğ‘– = 1, . . . ğ¾ do
2 pull arm ğ‘– for ğ‘ rounds
3 for ğ‘– = 1, . . . ğ¾ do
4 if Î”Ì‚ğ‘– < _ then
5 pull arm ğ‘– for max{ğ‘‡ ğ‘“ (ğ)ğ‘– âˆ’ ğ‘, 0} rounds
6 pull an arbitrary arm from argmaxğ‘–âˆˆ[ğ¾ ] Ë†Ì€ğ‘– until the
execution ends
The remaining rounds (less than ğ‘‡ ) are allocated an arbitrary
arm with the highest observed expected reward. Given the clean
event, the reward gap between the optimal arm and the sub-optimal
arm the algorithm commits to is less than ğ‘‚
(
ğ¿âˆ’1/3ğ‘‡âˆ’1/3 log1/3ğ‘‡
)
.
Hence the regret before allocating the remaining rounds is bounded
by ğ‘‚ (ğ‘‡ 2/3
log
1/3ğ‘‡ ).
Putting it all together, The regret for the event case is bounded
by -
R(ğ‘‡ |clean case) â‰¤ ğ‘‚
(
ğ¾ğ¿
2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
The total regret is bounded by -
R(ğ‘‡ ) â‰¤ (1âˆ’2ğ¾
ğ‘‡ 4
)ğ‘‚
(
ğ¾ğ¿
2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
+2ğ¾
ğ‘‡ 4
Â·ğ‘‡ â‰¤ ğ‘‚
(
ğ¾ğ¿
2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
â–¡
Notice that Algorithm 1 is almost data independent. The explo-
ration phase in Lines 1-2 continues even if the estimates of ğ and
ğ‘“ (ğ) are accurate. The predefined exploration length ğ‘ prevents
the algorithm from stopping the exploration early. Such an early
stopping is important after identifying arms with high opportunity
cost or arms that already satisfy the fairness requirements.
3.1 Fairness Aware Black-Box algorithm
In this section, we present a data dependent algorithm addressing
the problems of Algorithm 1 above. If the algorithm is certain (with
high. probability) that the cost of opportunity is higher than the
transfer cost, the decision-maker can stop pulling this arm. If the
fairness function admits low values, the algorithm can satisfy the
fairness requirement within less than ?Ìƒ?
(
ğ‘‡
2/3
)
pulls.
We now explain the course of Algorithm 2. Full version of the
algorithm appears in Section B. The algorithm takes ğ›¼ and ğ›½ , which
we describe shortly, and ğ´ğ¿ğº , a black-box no-regret MAB algo-
rithm as input, whereğ´ğ¿ğº is no-regret with respect to the classical,
rewards-only MAB objective (e.g. UCB1 [34]).
In Lines 1-3 the main variables are initialized: confidence bounds
representing the probable estimates of the reward gaps, i.e.,ğ¿ğ¶ğµ(Î”ğ‘– ),
ğ‘ˆğ¶ğµ(Î”ğ‘– ), and ğ¶ğ‘¡ (Line 3) which is the hyper-cube of probable esti-
mates of ğ. Lines 4-10 consist of four different phases. In the first
phase (Lines 4-5), the reward gaps are approximated up to a factor
of ğ›½ . After this phase, the algorithm knows with high probability
for each arm whether its reward gap is higher or lower than the
transfer cost by more than ğ›½ . To be precise, we care for accurate
approximation of Î”ğ‘– , only if it is close to the transfer cost, _.
The second phase (Lines 6-7), approximates ğ‘“ for arms with low
opportunity cost up to a factor of ğ›¼ . If there is an arm with low
opportunity cost for which the approximation of ğ‘“ is not accu-
rate enough, all the arms are pulled. The term maxğâ€²âˆˆğ¶ğ‘¡ ğ‘“ (ğ
â€²)ğ‘– âˆ’
minğâ€²âˆˆğ¶ğ‘¡ ğ‘“ (ğ
â€²)ğ‘– upper bounds the highest change of ğ‘“ (ğ â€²)ğ‘– inside
the hyper-cube ğ¶ğ‘¡ , and hence also upper bounds our estimation
error of ğ‘“ (?Ì‚?)ğ‘– . To clarify why we pull all arms in Line 7, recall that
ğ‘“ (ğ)ğ‘– also depends on ` ğ‘— , ğ‘— â‰  ğ‘– , in the general case; if our estimate
Ë†Ì€ğ‘— is not accurate, the estimation of ğ‘“ (ğ)ğ‘– might not be accurate as
well. Pulling all arms ensures that all the estimates improve for the
subsequent round, namely, ğ¶ğ‘¡ shrinks in all of its dimensions. In
the third phase (Lines 8-9), we ensure that we pull all arms with low
opportunity cost according to the estimate of ğ‘“ (?Ì‚?)ğ‘– . Lastly, in the
fourth step (Line 10), we invokeğ´ğ¿ğº until the end of the execution.
Next, we discuss the input hyper-parameters, ğ›¼ , ğ›½ and ğ´ğ¿ğº . ğ›¼
is the confidence interval hyper-parameter for the approximation
of ğ‘“ . Setting ğ›¼ to small values implies that arms should be pulled
many times and this can inflict a regret due to over pulling arms.
The approximation error of ğ‘“ can be as big as ğ‘‡ğ›¼ is. The hyper-
parameter ğ›½ is the confidence interval for the approximation of the
reward gaps. If the reward gap is not close to the transfer cost _, it
would be identified almost immediately. Otherwise, Algorithm 2
uses the black-box MAB algorithm ğ´ğ¿ğº . This allows the decision-
maker to devote the fourth and final phase to identifying the best
arm and exploiting its reward.
The only computationally non-trivial step in Algorithm 2 appears
in Line 6: Computing maxğâ€²âˆˆğ¶ğ‘¡ ğ‘“ (ğ
â€²)ğ‘– âˆ’minğâ€²âˆˆğ¶ğ‘¡ ğ‘“ (ğ
â€²)ğ‘– . Finding
the global maximum of a Lipschitz function inside a hyper-cube is
a computationally challenging task. However, due to role ğ‘“ plays
in our setting, we argue that it should have a natural structure.
Indeed, ğ‘“ quantifies a societal requirement and as such should be
easy to grasp: Providing opportunities according to a cumbersome,
hard-to-optimize and unexplainable criteria is likely to be unfair
in and of itself. Consequentially, we shall assume that there is an
oracle that computes the minimal and maximal values ğ‘“ at entry
ğ‘– can obtain in a given hyper-cube. In Subsection A.1, we show
that for the softmax function ğ‘“ sft, implementing such an oracle
boils down to solving a convex optimization problem over a single
variable. As an additional example, the family of monotonically
non-decreasing Cartesian product functions obtain minimum and
maximum on the extreme points of the confidence intervals and
hence are easy to handle. Examples of such functions are linear
functions, exponential functions (e.g., ğ‘“ (ğ)ğ‘– = `ğ‘
ğ‘–
/ğ¾ for ğ‘ â‰¥ 1)
and constant functions. We are ready to state the guarantees of
Algorithm 2.
Theorem 2. Fix any arbitrary instance of R-O MAB, and let
ğ›¼ = ğ¾
2/3ğ¿2/3ğ‘‡âˆ’1/3 log1/3ğ‘‡ , ğ›½ = ğ‘‡âˆ’1/3 log1/3ğ‘‡ . Then, Algorithm 2 has a
regret of ğ‘‚ (ğ¾ 5/3ğ¿2/3ğ‘‡ 2/3
log
1/3ğ‘‡ ).
Proof. We analyze the regret that stems from the different
phases: Approximating the reward gap (Phase 1), approximating
ğ‘“ (Phase 2), granting opportunities (Phase 3), and invoking ğ´ğ¿ğº
(Phase 4).
Lemma 2, which we prove below, guarantees that after each
arm was pulled at most 8ğ‘‡ğ›½ the algorithm is certain (with high
probability.) for every arm if its cost of opportunity is bigger or
30
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Tom Ron, Omer Ben-Porat, and Uri Shalit
smaller than the transfer cost by a factor of ğ›½ . Thus the regret that
stems from this phase is bounded by ğ‘‚ (ğ‘‡ğ¾ğ›½).
The second phase approximates ğ‘“ only for entries with low cost
of opportunity, i.e., ğ¿ğ¶ğµ(Î”ğ‘– ) < _. Lemma 3 shows that after each
arm is pulled at most 8ğ‘‡ğ›¼ times, ğ‘“ is approximated up to a factor of
ğ›¼ in the relevant entries. An immediate consequence of Lemma 3 is
that after at most 8ğ‘‡ğ›¼ rounds,
âˆ‘
ğ‘–âˆˆ[ğ¾ ];ğ¿ğ¶ğµ (Î”ğ‘– ) â‰¤_ maxğâ€²âˆˆğ¶ğ‘¡ ğ‘“ (ğ
â€²)ğ‘–âˆ’
minğâ€²âˆˆğ¶ğ‘¡ ğ‘“ (ğ
â€²)ğ‘– â‰¤ ğ›¼ . Thus the regret from this phase is bounded
by ğ‘‚ (ğ‘‡ğ¾ğ›¼). The regret from the first two phases is bounded by
ğ‘‚ (ğ‘‡ğ¾ max{ğ›¼, ğ›½}).
In the third phase, the algorithm pulls each arm with low oppor-
tunity cost until the condition in Line 9 is met; therefore,
(1) For arm ğ‘– with Î”ğ‘– < _, the regret stems form the approxima-
tion error of ğ‘“ which is bounded by ğ‘‚ (ğ‘‡ğ›¼).
(2) For arm ğ‘– withÎ”ğ‘– > _, if arm ğ‘– is pulled during the third phase.
This happens when the reward gap is very close to the trans-
fer cost and ğ‘‡ğ›½ rounds are not sufficient to approximate Î”ğ‘–
correctly. Ifğ‘ğ‘– â‰¤ 8ğ‘‡ max{ğ›¼, ğ›½}, the regret is also bounded by
this term. Otherwise, assume ğ‘ğ‘– > 8ğ‘‡ max{ğ›¼, ğ›½}. Note that
the regret is bounded byğ‘‡ ğ‘“ (ğ)ğ‘– (Î”ğ‘– âˆ’_) +_ğ‘‡ |ğ‘“ (ğ)ğ‘– âˆ’ ğ‘“ (ğ)ğ‘– |.
The second part of this expression is bounded by the approx-
imation error of ğ‘“ , i.e., ğ‘‚ (ğ‘‡ğ›¼). We bound the first part of
the regret by bounding Î”ğ‘– âˆ’ _. To do so, we now look into
the two possible cases that stopped the loop in Line 5. The
term the clean event is similar to the clean event defined in
Subsection 3.
(a) If ğ‘ˆğ¶ğµ(Î”ğ‘– ) < _ + ğ›½ , given the clean event, _ âˆ’ ğ›½ < _ <
Î”ğ‘– â‰¤ ğ‘ˆğ¶ğµ(Î”ğ‘– ) â‰¤ _ + ğ›½ . This implies that Î”ğ‘– âˆ’ _ â‰¤ 2ğ›½ .
Thus, the first part of the regret is bounded by 2ğ‘‡ğ›½ .
(b) If ğ¿ğ¶ğµ(Î”ğ‘– ) > _ âˆ’ ğ›½ , given the clean event, _ âˆ’ ğ›½ <
ğ¿ğ¶ğµ(Î”ğ‘– ) â‰¤ _ < Î”ğ‘– â‰¤ _ + ğ›½ . This implies that Î”ğ‘– âˆ’ _ â‰¤ 2ğ›½
and therefore the first part of the regret is bounded by
2ğ‘‡ğ›½ .
To summarize, if arm ğ‘– with Î”ğ‘– > _ is pulled in the third
phase, the regret associated with this arm is bounded by
ğ‘‚ (ğ‘‡ (ğ›¼ + ğ›½)).
Eventually, Phase 4 (invoking ğ´ğ¿ğº) contributes the regret of
ğ´ğ¿ğº .
The regret from the approximation phases is bounded by
ğ‘‚
(
ğ¾
5/3ğ¿2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
. The regret from filling the fairness require-
ments is also bounded by this term. Ultimately, the total regret
is bounded by ğ‘‚
(
ğ¾
5/3ğ¿2/3ğ‘‡ 2/3
log
1/3ğ‘‡
)
, assuming that this term is
an upper bound on the regret of ğ´ğ¿ğº with respect to a vanilla
stochastic bandit setting. â–¡
Lemma 2. Fix any arbitrary instance of R-O MAB and let ğ›½ =
ğ‘‡âˆ’1/3 log1/3ğ‘‡ . Then after at most 8ğ‘‡ğ›½ rounds, for every ğ‘– âˆˆ [ğ¾], either
ğ‘ˆğ¶ğµ(Î”ğ‘– ) < _ + ğ›½ or ğ¿ğ¶ğµ(Î”ğ‘– ) > _ âˆ’ ğ›½
Proof. With high probability after pulling each armğ‘‡ğ›½ = ğ‘‡
2/3
log
1/3
times, with high probability
Î”Ì‚ğ‘– âˆ’ Î”ğ‘–  < 2
âˆš
2 logğ‘‡
8ğ‘‡ğ›½
= ğ‘‡âˆ’1/3 log1/3ğ‘‡ =
ğ›½ . Therefore, ğ‘ˆğ¶ğµ(Î”ğ‘– ) âˆ’ ğ¿ğ¶ğµ(Î”ğ‘– ) â‰¤ 2ğ›½ and hence at least one of
the following happens - ğ¿ğ¶ğµ(Î”ğ‘– ) â‰¥ _ âˆ’ ğ›½ orğ‘ˆğ¶ğµ(Î”ğ‘– ) â‰¤ _ + ğ›½ . â–¡
Algorithm 2: Self-regulated Utility Maximization
Input: Black-box bandit algorithm ğ´ğ¿ğº , allowed
approximation error parameters ğ›¼ and ğ›½
1 ğ‘¡ = 1
2 Initialize armsâ€™ data - ğ‘ğ‘– = 0, ğ¿ğ¶ğµ(Î”ğ‘– ) = 0,ğ‘ˆğ¶ğµ(Î”ğ‘– ) = 1 for
all ğ‘– âˆˆ [ğ¾]
3 ğ¶1 = [0, 1]ğ¾ // Hyper-cube of ğ valuesâ€™ in the clean event
4 while âˆƒğ‘– âˆˆ [ğ¾] s.tğ‘ˆğ¶ğµ(Î”ğ‘– ) > _ + ğ›½ and ğ¿ğ¶ğµ(Î”ğ‘– ) < _ âˆ’ ğ›½
do // Phase 1
5 Pull all arms once, update ğ‘¡ , counters, confidence
bounds and ğ¶ğ‘¡
6 while âˆƒğ‘– âˆˆ [ğ¾] s.t. maxğâ€²âˆˆğ¶ğ‘¡ ğ‘“ (ğ
â€²)ğ‘– âˆ’minğâ€²âˆˆğ¶ğ‘¡ ğ‘“ (ğ
â€²)ğ‘– >
ğ›¼ and ğ¿ğ¶ğµ(Î”ğ‘– ) < _ do // Phase 2
7 Pull all arms once, update ğ‘¡ , counters, confidence
bounds and ğ¶ğ‘¡
8 while
âˆƒğ‘– âˆˆ [ğ¾] s.t. ğ¿ğ¶ğµ(Î”ğ‘– ) < _ and ğ‘ğ‘– < ğ‘‡ ğ‘“ (ğ)ğ‘– and ğ‘¡ < ğ‘‡ do
// Phase 3
9 Pull arm ğ‘– the minimal number of times so ğ‘ğ‘– â‰¥ ğ‘‡ ğ‘“ (ğ)ğ‘– ,
update ğ‘¡ and counters.
10 Invoke ğ´ğ¿ğº for the remaining rounds // Phase 4
Lemma 3. Fix any arbitrary instance of R-O MAB and let ğ›¼ =
ğ¾
2/3ğ¿2/3ğ‘‡âˆ’1/3 log1/3ğ‘‡ and a hyper-cubeğ¶ âŠ† [0, 1]ğ¾ . Then after pulling
each arm at most 8ğ‘‡ğ›¼ times, for every ğ‘– âˆˆ [ğ¾], maxğâ€²âˆˆğ¶ ğ‘“ (ğ â€²)ğ‘– âˆ’
minğâ€²âˆˆğ¶ ğ‘“ (ğ â€²)ğ‘– â‰¤ ğ›¼ .
Proof. After pulling each arm 8ğ‘‡ğ›¼ times, with high probabil-
ity for every ğ‘– âˆˆ [ğ¾], | Ë†Ì€ğ‘– âˆ’ `ğ‘– | â‰¤ ğ¾âˆ’1/3ğ¿âˆ’1/3ğ‘‡âˆ’1/3 log1/3ğ‘‡ . That is
|ğ âˆ’ ğ | < ğ¾
2/3ğ¿âˆ’1/3ğ‘‡âˆ’1/3 log1/3ğ‘‡ . Since ğ‘“ is ğ¿-Lipschitz, we obtain
maxğâ€²âˆˆğ¶ ğ‘“ (ğ â€²)ğ‘– âˆ’minğâ€²âˆˆğ¶ ğ‘“ (ğ â€²)ğ‘– â‰¤ ğ¾ 2/3ğ¿2/3ğ‘‡âˆ’1/3 log1/3ğ‘‡ = ğ›¼ . â–¡
3.2 Special Cases
In this section, we present two private cases that achieve better
regret than the worst-case regret of ?Ìƒ? (ğ‘‡ 2/3) presented in Section 3.
First, if ğ‘“ has very loose fairness requirements, i.e., ğ‘“ (ğ)ğ‘– â‰¤ ğ‘‡âˆ’ğ›¾
for all ğ and ğ›¾ â‰¥ 1
2
. In such a case, pulling each arm for ğ‘‚ (ğ‘‡ 1âˆ’ğ›¾ )
rounds and invoking a black-box algorithm for the remaining
rounds achieves a regret of ğ‘‚ (max{ğ¾ğ‘‡ 1âˆ’ğ›¾ ,R(ğ´ğ¿ğº)}). Formally,
let ğ›¾ â‰¥ 1
2
, ğ‘“ âˆˆ Fmin
ğ‘‡,ğ›¾
if for all ğ âˆˆ [0, 1]ğ¾ and for all ğ‘– âˆˆ [ğ¾],
ğ‘“ (ğ)ğ‘– â‰¤ ğ‘‡âˆ’ğ›¾ .
Proposition 1. Fix any R-O MAB instance with horizon ğ‘‡ and
fairness function ğ‘“ âˆˆ Fmin
ğ‘‡,ğ›¾
, pulling all arms ğ‘‚ (ğ‘‡ 1âˆ’ğ›¾ ) and invoking
a black-box bandit algorithm for the remaining rounds achieves a
regret bounded by ğ‘‚ (max{ğ¾ğ‘‡ 1âˆ’ğ›¾ ,R(ğ´ğ¿ğº)}).
Proof. For every arm ğ‘– after it was pulledğ‘‡ 1âˆ’ğ›¾
times,ğ‘‡ ğ‘“ (ğ)ğ‘– â‰¤
ğ‘ğ‘– . Thus, the regret that stems from this phase is bounded by
ğ‘‚
(
ğ¾ğ‘‡ 1âˆ’ğ›¾ )
. The optimal algorithm pulls the optimal arm for the
remaining rounds. Thus any additional regret is a consequence of
ğ´ğ¿ğº and therefore the regret is bounded by ğ‘‚ (R(ğ´ğ¿ğº)). â–¡
The second case is a generalization of ğ‘“ uni which was presented
in Section 2.2. Formally, ğ‘“ âˆˆ F const
if there exists ğ›¾ â‰¤ 1
ğ¾
such that
31
Corporate Social Responsibility via Multi-Armed Bandits FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
Algorithm 3: Constant function utility maximization al-
gorithm
Input: Black-box bandit algorithm ğ´ğ¿ğº
1 for ğ‘– = 1, . . .ğ‘‡ğ›¾ do // Phase 1
2 for ğ‘— = 1 . . . ğ¾ do
3 pull arm ğ‘— if ğ¿ğ¶ğµ(Î” ğ‘— ) â‰¤ _
4 Invoke ğ´ğ¿ğº for the remaining rounds // Phase 2
for all ğ âˆˆ [0, 1]ğ¾ and for all ğ‘– âˆˆ [ğ¾], ğ‘“ (ğ)ğ‘– = ğ›¾ . Algorithm 3 is a
variation of Successive Elimination [34] that achieves a regret of
ğ‘‚ (max{
âˆš
ğ¾ğ‘‡ logğ‘‡,R(ğ´ğ¿ğº)}).
To illustrate the motivation to use Algorithm 3 instead of Al-
gorithm 2 consider the following R-O MAB instance. Fix horizon
ğ‘‡ , number of arms ğ¾ and define ğ‘“ to be ğ‘“ (ğ)ğ‘– = 1
ğ‘‡
for every
ğ âˆˆ [0, 1]ğ¾ and every ğ‘– âˆˆ [ğ¾]. Let _ = 1
2
. Set `1 = 1, `2 = 1+ğ‘‡ âˆ’1/3
2
and set the expected rewards of the other arms arbitrarily. Note
that Î”2 =
1âˆ’ğ‘‡ âˆ’1/3
2
. Thus, the first phase of Algorithm 2 pulls each
armğ‘‚
(
ğ‘‡
2/3
)
and incur a Î©
(
ğ‘‡
2/3
)
regret. Algorithm 3 will pull each
arm exactly once and then invoke the black-box algorithm. Thus,
in this case the regret is bounded by the regret of ğ´ğ¿ğº .
Algorithm 3 leverages the fact that functions in F const
has no ap-
proximation error and thus can combine phases 1-3 in Algorithm 2
to one phase. Note that if an arm is pulled ğ‘‡ğ›¾ times we can stop
pulling it. In the first phase (Lines 1-3) arms are pulled either at
most ğ‘‡ğ›¾ times or until the algorithm is certain that Î”ğ‘– > _. In the
second phase (Line 4), the black box algorithm is invoked and thus
the regret is also bounded by the regret of the black box algorithm.
Proposition 2. For any R-O MAB instance with fairness function
ğ‘“ âˆˆ F const, Algorithm 3 achieves a regret of
ğ‘‚
(
max{
âˆš
ğ¾ğ‘‡ logğ‘‡,R(ğ´ğ¿ğº)}
)
.
Proof. We start by analyzing the regret of the first phase. For
arm ğ‘– with Î”ğ‘– < _, given the clean event, arm ğ‘– is pulled exactly
ğ‘‡ğ›¾ â‰¤ ğ‘‡
ğ¾
times in the first phase, as in the optimal algorithm.
For arm ğ‘– with Î”ğ‘– > _, if Î”ğ‘– âˆ’ _ â‰¤ ğ‘‚
(âˆš
logğ‘‡
ğ‘‡ğ›¾
)
, arm ğ‘– is pulled
ğ‘‡ğ›¾ times and the regret will be bounded by ğ‘‚
(âˆš
ğ‘‡ logğ‘‡
ğ¾
)
. Oth-
erwise, Î”ğ‘– âˆ’ _ > ğ‘‚
(âˆš
logğ‘‡
ğ‘‡ğ›¾
)
. Arm ğ‘– is pulled as long as Î”ğ‘– âˆ’
_ â‰¤ ğ‘‚
(âˆš
logğ‘‡
ğ‘ğ‘–
)
. Thus the regret of the first phase is bounded
by ğ‘‚ (
âˆš
ğ¾ğ‘‡ logğ‘‡ ). The regret that stems from the second phase is
bounded by the regret of ğ´ğ¿ğº . Combining the two together we get
that R( Algorithm 3, ğ´ğ¿ğº) â‰¤ ğ‘‚ (max{
âˆš
ğ¾ğ‘‡ logğ‘‡,R(ğ´ğ¿ğº)}). â–¡
3.3 Instance Dependent Bounds
In this section we analyze the instance dependent regret of Algorithm
2, rather than the general regret given by Theorem 2. We first add
several notations. The instance-dependent bounds revolve around
two main parameters: ğ›¿ğ‘– = |Î”ğ‘– âˆ’ _ | which is the gap between the
reward gap of arm ğ‘– and the transfer cost, and ğ›¿âˆ— = minğ‘–âˆˆ[ğ¾ ] ğ›¿ğ‘–
which is the minimal gap between the reward gap and the trans-
fer cost. The smaller ğ›¿ğ‘– is, the more rounds are required in order
to identify whether the reward gap is smaller or larger than the
transfer cost, and what is the optimal sub-algorithm.
Given an R-O MAB instance, let Bğ‘ be ğ¾ dimensional ball cen-
tered around ğ with radius
âˆš
2 logğ‘‡
ğ‘
. After pulling each arm ğ‘
times, with high probability the observed vector of expected re-
wards vector lies in Bğ‘ . Denote by Bğ‘ğ›¼ the ğ¾ dimensional ball
centered around ğ with the largest radius such that for all ğ‘– âˆˆ [ğ¾],
maxğâ€²âˆˆBğ‘ğ›¼ ğ‘“ (ğ
â€²)ğ‘– âˆ’minğâ€²âˆˆBğ‘ğ›¼ ğ‘“ (ğ
â€²)ğ‘– < ğ›¼ .
Theorem 3. Fix any arbitrary instance of R-O MAB, and let ğ›¼ =
ğ¾
2/3ğ¿2/3ğ‘‡âˆ’1/3 log1/3ğ‘‡ , ğ›½ = ğ‘‡âˆ’1/3 log1/3ğ‘‡ . Then, the regret of Algorithm
2 is bounded by ğ‘‚
(
ğ¾ (min{ğ‘‡ğ›½, logğ‘‡
ğ›¿âˆ—2
} +ğ‘‡ğ›¼ + ğ‘ğ›¼ ) + R(ğ´ğ¿ğº)
)
Proof. In order to analyze the instance dependent bounds of
Algorithm 2 we analyze the instance dependent bound that stems
from each of its phases. The term â€œclean eventâ€ used in this proof
and in the following proofs is similar to the clean event defined in
Subsection 3.
Lemma 4 argues that the first phase ends after each arm was
pulled at most 8min
{
ğ‘‡ğ›½,
logğ‘‡
ğ›¿âˆ—2
}
times and therefore the regret from
this phase is bounded by ğ‘‚ (ğ¾ min
{
ğ‘‡ğ›½,
logğ‘‡
ğ›¿âˆ—2
}
).
Lemma 5 proves that the second phase ends after each arm was
pulled at most 8min {ğ‘‡ğ›¼, ğ‘ğ›¼ } times. Hence, the regret from this
phase is bounded by ğ‘‚ (ğ¾ min {ğ‘‡ğ›¼, ğ‘ğ›¼ }).
The regret from the third phase depends on the approximation
error of ğ‘“ . The approximation error of ğ‘“ on a single entry is bounded
by ğ‘‚ (min {ğ›¼, Yğ›¼ }). We divide the analysis into two cases -
(1) For arm ğ‘– with Î”ğ‘– < _, the regret stems from the approxima-
tion error of ğ‘“ which is bounded by ğ‘‚ (ğ‘‡ min {ğ›¼, Yğ›¼ }).
(2) For arm ğ‘– with Î”ğ‘– > _, given the clean case, if arm ğ‘– is
pulled during the third phase it implies that the reward gap
is very close to the transfer cost. I.e. ğ›¿ğ‘– < ğ›½ . Thus, 8ğ‘‡ğ›½
rounds were not sufficient to approximate Î”ğ‘– correctly and
8ğ‘‡ğ›½ â‰¤ ğ‘ğ‘– . If ğ‘ğ‘– â‰¤ 8ğ‘‡ max {ğ›¼, ğ›½} the regret is bounded by
this term. Otherwise the regret is bounded by ğ‘‡ ğ‘“ (ğ)ğ‘–ğ›¿ğ‘– +
_ğ‘‡ |ğ‘“ (ğ)ğ‘– âˆ’ ğ‘“ (ğ)ğ‘– | â‰¤ ğ‘‚ (ğ‘‡ (ğ›½ +min {ğ›¼, ğœ–ğ›¼ })).
To summarize, the regret resulting from the approximation phases
is bounded by ğ‘‚ (ğ¾ (min{ğ‘‡ğ›½, logğ‘‡
ğ›¿âˆ—2
} + min {ğ‘‡ğ›¼, ğ‘ğ›¼ })). The regret
from the third step is bounded byğ‘‚ (ğ‘‡ğ¾ (min {ğ›¼, Yğ›¼ }+min
{
ğ›½,
logğ‘‡
ğ›¿âˆ—2
}
)).
The regret of the fourth phase depends on the regret of the black-box
algorithm and hence the additional term R(ğ´ğ¿ğº) which completes
the regret analysis. â–¡
Lemma 4. Fix any R-O MAB instance, the first phase of Algorithm
2 ends after each arm is pulled at most 8min{ğ‘‡ğ›½, logğ‘‡
ğ›¿âˆ—2
} times.
Proof. In order for the first phase to endwe need to approximate
all the reward gaps sufficiently well. If ğ›¿âˆ— â‰¤ ğ›½ then given the clean
event all arms will be pulled 8ğ‘‡
2/3
log
1/3ğ‘‡ times in order to obtain
a confidence interval smaller than 2ğ›½ , which ensures that either
ğ¿ğ¶ğµ(Î”ğ‘– ) > _ âˆ’ ğ›½ orğ‘ˆğ¶ğµ(Î”ğ‘– ) < _ + ğ›½ for all ğ‘– âˆˆ [ğ¾].
32
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Tom Ron, Omer Ben-Porat, and Uri Shalit
Otherwise, if ğ›¿âˆ— > ğ›½ , i.e. for all ğ‘– âˆˆ [ğ¾], ğ›¿ğ‘– > ğ›½ . Assuming the
clean event, after pulling each arm
8 logğ‘‡
ğ›¿âˆ—2
times,ğ‘ˆğ¶ğµ(Î”ğ‘– ) â‰¤ Î”ğ‘– +ğ›¿âˆ—
and ğ¿ğ¶ğµ(Î”ğ‘– ) â‰¥ Î”ğ‘– âˆ’ ğ›¿âˆ—. We examine two cases:
(1) If Î”ğ‘– > _, then ğ›¿ğ‘– = Î”ğ‘– âˆ’ _, this implies that Î”ğ‘– = _ + ğ›¿ğ‘– .
ğ¿ğ¶ğµ(Î”ğ‘– ) â‰¥ Î”ğ‘– âˆ’ ğ›¿âˆ— = _ + ğ›¿ğ‘– âˆ’ ğ›¿âˆ— > _ âˆ’ ğ›½ . Therefore arm ğ‘–
will not satisfy the condition in Line 5.
(2) If Î”ğ‘– < _, then ğ›¿ğ‘– = _ âˆ’ Î”ğ‘– , hence Î”ğ‘– = _ âˆ’ ğ›¿ğ‘– . ğ‘ˆğ¶ğµ(Î”ğ‘– ) â‰¤
Î”ğ‘– +ğ›¿âˆ— = _ âˆ’ğ›¿ğ‘– +ğ›¿âˆ— â‰¤ _ + ğ›½ . Therefore arm ğ‘– will not satisfy
the condition in Line 5.
Combining the above together, the first phase ends after each
arm was pulled at most 8min
{
ğ‘‡ğ›½,
logğ‘‡
ğ›¿âˆ—2
}
times. â–¡
Lemma5. Fix any R-OMAB instance. The second phase of Algorithm
2 ends after each arm is pulled at most min {8ğ‘‡ğ›¼, 4ğ‘ğ›¼ } times.
Proof. Assuming the clean case, after pulling each arm 4ğ‘ğ›¼
times,ğ¶4ğ‘ğ›¼ is a ball of radius
âˆš
logğ‘‡
2ğ‘ğ›¼
centered around ğ such that for
every ğ‘– âˆˆ [ğ¾], |`ğ‘– âˆ’ Ë†Ì€ğ‘– | â‰¤
âˆš
logğ‘‡
2ğ‘ğ›¼
. Thus, ğ¶4ğ‘ğ›¼ âŠ‚ Bğ‘ğ›¼ . Therefore,
maxğâ€²âˆˆğ¶4ğ‘ğ›¼
ğ‘“ (ğ â€²)ğ‘– âˆ’minğâ€²âˆˆğ¶4ğ‘ğ›¼
ğ‘“ (ğ â€²)ğ‘– â‰¤ ğ›¼ .
If 4ğ‘ğ›¼ > 8ğ‘‡ğ›¼ , as shown in Lemma 3 the condition in Line 7 is
satisfied trivially. Putting the above observations together, after
each arm is pulled at most min {8ğ‘‡ğ›¼, 4ğ‘ğ›¼ } no arm satisfies the
condition in Line 7 and therefore the second phase ends. â–¡
3.4 Fairness Over the Entire Horizon
Ideally, we would like the impose the fairness requirements or
the CSR payments on every round. This is not always achievable
since we first need to approximate the reward gaps and the fairness
function ğ‘“ well enough. Additionally, Algorithm 2 requires a known
horizon while the horizon is not always known in advance. We
therefore analyze a anytime version of Algorithm 2 which uses the
doubling trick with geometric growth.
Following the proof by Besson andKaufmann [12] ifR(ğ´ğ¿ğº ;ğ‘‡ ) â‰¤
ğ‘ğ‘‡ğ›¾ logğ›¿ ğ‘‡ then the regret of anytime version (denoted by ğ´ğ¿ğº â€²)
with geometric growth achieves a regret of
R(ğ´ğ¿ğº â€²;ğ‘‡ ) â‰¤ ğ‘™ (ğ›¾, ğ›¿,ğ‘‡0, ğ‘)ğ‘ğ‘‡ğ›¾ logğ›¿ ğ‘‡ + ğ‘”(ğ‘‡ ) with increasing func-
tionğ‘”(ğ‘¡) = ğ‘œ (ğ‘¡ğ›¾ ğ‘™ğ‘œğ‘”ğ›¿ğ‘¡). Settingğ‘‡0 = ğ¾2
andğ‘ = 2 attains ğ‘™ (ğ›¾, ğ›¿,ğ‘‡0, ğ‘) <
3.
4 LOWER BOUND
In the previous section, we presented Algorithm 2, which incurs a
regret of ?Ìƒ?
(
ğ‘‡
2/3
)
in the worst case. Here we show that this bound is
asymptotically optimal by designing a family of R-OMAB instances
that can mislead any algorithm.
Theorem 4. Fix time horizon ğ‘‡ , number of arms ğ¾ , and Lipschitz
constant ğ¿. For any algorithm, there exists a R-O MAB instance such
that R(ğ‘‡ ) â‰¥ Î©(ğ‘‡ 2/3).
Proof. We construct a family of R-O MAB instances F that any
algorithm will incur a high regret. For a given ğ¾,ğ‘‡ and ğ¿. Let _ = 1
and ğœ– = ğ‘ Â·ğ‘‡âˆ’1/3. We build F in the following way -
Iğ½ ,ğ‘† =
ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³
`ğ‘– = 1 ğ‘– = ğ‘—
`ğ‘– =
1
2
ğ‘– â‰  ğ‘—, ğ‘– âˆˆ ğ‘†
`ğ‘– =
1+ğœ–
2
ğ‘– â‰  ğ‘—, ğ‘– âˆ‰ ğ‘†
ğ‘“ (ğ)ğ‘– =
ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³
0 `ğ‘– â‰¤ 1
2
ğ¿ (`ğ‘–âˆ’0.5)
ğ¾
1
2
< `ğ‘– â‰¤ 1
2
+ ğœ–
ğ¿ğœ–
ğ¾
1
2
+ ğœ– < `ğ‘– â‰¤ 1
Iğ½ ,ğ‘† describes a vector of expected rewards. The vector is com-
posed of arm ğ‘— with expected reward 1, subset of arms, ğ‘† , with
expected reward
1
2
and the remaining arms have expected reward
of
1+ğœ–
2
. The size of F is ğ¾ Â· 2ğ¾âˆ’1. The fairness function, ğ‘“ is piece-
wise linear with 3 pieces. The first piece, for values less or equal to
1
2
is constant and equal to 0. The second piece, for values between
1
2
to
1
2
+ğœ– is linear that goes from 0 to
ğ¿ğœ–
ğ¾
. The third piece, for values
greater than
1
2
+ ğœ– is constant and equal to
ğ¿ğœ–
ğ¾
. In Claim 1 we prove
that ğ‘“ is ğ¿-Lipschitz and sums to at most one.
Following the optimal algorithm described in Section 2.1, for
every instance in F the optimal algorithm pulls all the sub-optimal
arms according to the fairness function. This means the optimal
algorithm does not pull at all the arms in ğ‘† and pulls
ğ‘‡ğ¿ğœ–
2ğ¾
times
sub-optimal arms not in ğ‘† . The algorithm pulls the optimal arm
the remaining rounds (ğ‘‡ âˆ’ğ‘‡ğ¿ğœ– (ğ¾ âˆ’ 1 âˆ’ |ğ‘† |)/2ğ¾ ). In order to distin-
guish between two instances in F , the decision-maker must pull
each arm at least 1/8ğœ–2 times, otherwise with positive probability
the decision-maker is unable to distinguish between at least two
instances.
We use the following notation for simplicity - I1 = Iğ¾, [ğ¾âˆ’1] ,
Iğ¾,ğ‘– = Iğ¾, [ğ¾âˆ’1]\{ğ‘– } . I1 is an instance where the ğ¾ â€™th arm is the
optimal arm and all the other arms have an expected reward of
1
2
.
Iğ¾,ğ‘– is an instance where the ğ¾ â€™th arm is the optimal arm and all
the other arms expect arm ğ‘– â‰  ğ¾ have an expected reward of
1
2
and
arm ğ‘– has an expected reward of
1+ğœ–
2
.
Fix an algorithmğ´ğ¿ğº . Ifğ´ğ¿ğº pulls each arm at least 1/8ğœ–2 times,
the utility of ğ´ğ¿ğº -
Uğ´ğ¿ğº (I1) â‰¤ (ğ‘‡ âˆ’ (ğ¾ âˆ’ 1)/8ğœ–2) + (ğ¾ âˆ’ 1)/8ğœ–2 Â·
1
2
= ğ‘‡ âˆ’ (ğ¾ âˆ’ 1)ğ‘‡
2/3
16ğ‘2
.
On the other hand, the optimal algorithm will pull only arm ğ¾
and hence, the utility of ğ‘‚ğ‘ƒğ‘‡ is ğ‘‡ . The regret is then - R(I1) =
Uğ‘‚ğ‘ƒğ‘‡ (I1) âˆ’ Uğ´ğ¿ğº (I1) â‰¥ (ğ¾âˆ’1)ğ‘‡
2/3
16ğ‘2
= Î©(ğ‘‡ 2/3).
Otherwise, there exists arm ğ‘– that ğ´ğ¿ğº draws less than 1/8ğœ–2
times. With probability at least 0.01, in the instance that generated
the sequence of rewards ğ‘– âˆ‰ ğ‘† . That is, `ğ‘– = 1+ğœ–
2
. For example
Iğ¾, [ğ¾âˆ’1]\{ğ‘– } . Denote by ğ‘ğ‘– the number of times arm ğ‘– is pulled by
ğ´ğ¿ğº .
33
Corporate Social Responsibility via Multi-Armed Bandits FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
Uğ´ğ¿ğº (Iğ¾,ğ‘– ) â‰¤ (ğ‘‡ âˆ’ ğ‘ğ‘– ) + ğ‘ğ‘–
(
1 + ğœ–
2
)
âˆ’ (ğ‘‡ Â· ğ‘“ (Iğ¾,ğ‘– )ğ‘– âˆ’ ğ‘ğ‘– )
= ğ‘‡ + ğ‘ğ‘–
(
1 + ğœ–
2
)
âˆ’ğ‘‡ Â· ğ‘“ (Iğ¾,ğ‘– )ğ‘–
â‰¤ ğ‘‡ + 1
8ğœ–2
(
1 + ğœ–
2
)
âˆ’ ğ‘‡ğ¿ğœ–
2ğ¾
= ğ‘‡ âˆ’ğ‘‡ 2/3
(
ğ‘ğ¿
2ğ¾
âˆ’ 1
16ğ‘2
)
+ Î˜
(
ğ‘‡
1/3
)
(3)
The optimal algorithm will pull arm ğ‘– , ğ‘‡ğ¿ğœ–/2ğ¾ times and arm ğ¾
the remaining rounds. The utility of ğ‘‚ğ‘ƒğ‘‡ is -
Uğ‘‚ğ‘ƒğ‘‡ (Iğ¾,ğ‘– ) = (1 âˆ’
ğ¿ğœ–
2ğ¾
)ğ‘‡ + ğ‘‡ğ¿ğœ–
2ğ¾
(
1 + ğœ–
2
)
= ğ‘‡ âˆ’ ğ‘‡ğ¿ğœ–
4ğ¾
+ ğ‘‡ğ¿ğœ–
2
4ğ¾
= ğ‘‡ âˆ’ ğ‘‡
2/3ğ‘ğ¿
4ğ¾
+ Î˜(ğ‘‡ 1/3)
Finally, observe that
R(ğ´ğ¿ğº,Iğ¾,ğ‘– ) = Uğ‘‚ğ‘ƒğ‘‡ (Iğ¾,ğ‘– ) âˆ’ Uğ´ğ¿ğº (Iğ¾,ğ‘– )
â‰¥ ğ‘‡ 2/3
(
ğ‘ğ¿
4ğ¾
âˆ’ 1
16ğ‘2
)
+ Î˜(ğ‘‡ 1/3)
= Î©(ğ‘‡ 2/3)
â–¡
Claim 1. ğ‘“ is ğ¿-Lipschitz and sums to at most one.
Proof. ğ‘“ is ğ¿-Lipschitz. ğ‘“ is continuous and piece-wise lin-
ear, hence it is sup |ğ‘“ â€²(ğ‘¥) | âˆ’ ğ¿ğ‘–ğ‘ğ‘ ğ‘â„ğ‘–ğ‘¡ğ‘§. sup |ğ‘“ â€²(ğ‘¥) | = ğ¿
ğ¾
< ğ¿, and
therefore ğ‘“ is ğ¿ âˆ’ ğ¿ğ‘–ğ‘ğ‘ ğ‘â„ğ‘–ğ‘¡ğ‘§.
ğ‘“ sums to at most 1 - notice that
max
ğ¾âˆ‘
ğ‘–=1
ğ‘“ (ğ)ğ‘– â‰¤
ğ¾âˆ‘
ğ‘–=1
max
ğâˆˆ[0,1]ğ¾
ğ‘“ (ğ)ğ‘– = ğ¿ğœ–.
ğ¿ğœ– â‰¤ 1 if ğœ– < 1/ğ¿. In the example above, ğœ– = ğ‘ğ‘‡
âˆ’1/3
, for large
enough ğ‘‡ we obtain ğœ– < 1/ğ¿. â–¡
5 EXPERIMENTS
In this section, we perform an empirical analysis of Algorithm
2. We demonstrate how different fairness policies, i.e., pairs of a
fairness function ğ‘“ and a transfer cost _ influence the course of the
algorithm and the average utility. We consider several R-O MAB
instances with ğ¾ = 6 arms, expected values `ğ‘– = 0.2 + (ğ‘– âˆ’ 1) Â· 0.15
for every ğ‘– âˆˆ [ğ¾], and a varying horizon ğ‘‡ . For fairness functions,
we consider ğ‘“ uni, ğ‘“ lin, and ğ‘“ sft from Subsection 2.2. As for the
transfer cost _, we used 0, 0.4, and 0.8. We used UCB1 as the black
box algorithm ğ´ğ¿ğº in Algorithm 2. Each combination of fairness
function, transfer cost and horizon was executed 200 times on a
standard Mac computer. The entire process took several hours. In
order to compare between the different horizons we present the
average utility, i.e., the cumulative utility divided by the number of
rounds.
Figure 1 demonstrates the behaviour of different fairness func-
tions with respect to varying values of _. As expected, the average
utility decreases as the transfer cost increases. In Figure 1a, we see
that the average utility of all the functions is the same, since ğ´ğ¿ğº
is played across all the rounds. As _ increases, the reward gaps Î”
of more arms go below the transfer cost (see Line 5 in Algorithm 2),
so the decision-maker should pull them ğ‘‡ ğ‘“ (ğ)ğ‘– times (similar to
the optimal algorithm from Subsection 2.1). The expected utility
increases with the horizon, since the proportion of approximation
rounds (Lines 5 and 7) decreases with the horizon. For _ = 0.8, ğ‘“ sft
and ğ‘“ uni use all the rounds for allocating opportunities. The differ-
ence in the average utility stems from the allocation differences.
ğ‘“ uni allocates evenly, while ğ‘“ sft allocates more opportunities as the
arm expected reward grows. Observe that ğ‘“ uni (ğ)ğ‘– > ğ‘“ lin (ğ)ğ‘– for
every ğ‘– âˆˆ [ğ¾] and the described ğ; thus, the utility of ğ‘“ lin is greater
than ğ‘“ uni for _ > 0.
Figure 2 investigates the crux of the execution of Algorithm 2 for
ğ‘“ sft: The proportion of rounds devoted to phase 1 (Line 5), phase
2 (Line 7), phase 3, (Line 9), and phase 4 (invoking ğ´ğ¿ğº , Line 10).
For _ = 0, Algorithm 2 always invokes the black box algorithm
ğ´ğ¿ğº . For _ = 0.4 and _ = 0.8, the proportion of approximation
rounds (phases 1 and 2) decreases as the horizon increases. The
proportion of phase 3 rounds increases as the horizon grows. The
reason is that the proportion of approximation rounds decreases,
while the required number of opportunities grows linearly with
the horizon. For _ = 0.8, the algorithm dedicates all rounds for
opportunities. However, for _ = 0.4, it realizes that for arms 1,2, and
3 (with Î”ğ‘– > _ = 0.4) paying the transfer cost _ per each withheld
opportunities, and hence the remaining rounds are devoted to phase
4.
Figure 3 compares the average utility of Algorithm 1 and Algo-
rithm 2 for ğ‘“ sft. This figure emphasizes that although Algorithm
1 has better asymptotic regret than Algorithm 2, in practice Algo-
rithm 2 achieves better results. In this experiment, even 200,000
rounds are not sufficient for Algorithm 1 to approximate the arms
accurately enough, and therefore the graph for it is flat. Moreover,
the gap between the average rewards is greater when the transfer
cost is low. The reason for this phenomenon is that Algorithm 1
invests many rounds in the exploration of arms which should not
be pulled according to the optimal algorithm, while Algorithm 2
can identify those arms early on and adapt.
Analysis of the pulls distribution and comparison of the average
utility for ğ‘“ lin and ğ‘“ uni can be found in Appendix C.
6 DISCUSSION
We introduced a MAB problem that models decision making from
the perspective of Corporate Social Responsibility and allocation
of opportunities. Our modeling imitates many real-world scenar-
ios where decision-makers are required to maximize their short-
term utility while at the same time upholding fairness principles.
With our framework, commercial companies can incorporate self-
regulation in their algorithmic products, and provide opportunities
as a form of social responsibility. We devised a no-regret algorithm
and showed that its convergence rate is in fact optimal.
We see considerable scope for follow-up work: Self-regulation
for increasing subpopulation welfare can be incorporated in many
34
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Tom Ron, Omer Ben-Porat, and Uri Shalit
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
(a) _ = 0
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
(b) _ = 0.4
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
funi
f lin
fsft
(c) _ = 0.8
Figure 1: Average utility for different transfer costs.
104 5 Â· 104 105 2 Â· 1050
0.25
0.5
0.75
1
Horizon
P
u
ll
s
d
is
tr
ib
u
ti
on
(a) _ = 0
104 5 Â· 104 105 2 Â· 1050
0.25
0.5
0.75
1
Horizon
P
u
ll
s
d
is
tr
ib
u
ti
on
(b) _ = 0.4
104 5 Â· 104 105 2 Â· 1050
0.25
0.5
0.75
1
Horizon
P
u
ll
s
d
is
tr
ib
u
ti
on
1+2 3 4
(c) _ = 0.8
Figure 2: Round distributions per phase for ğ‘“ sft.
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
(a) _ = 0
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
(b) _ = 0.4
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
SRUM
FairETC
(c) _ = 0.8
Figure 3: Average utility Algorithm 1 vs Algorithm 2 for ğ‘“ sft.
other tasks, e.g., in classification or load balancing. Granting oppor-
tunities and investing in CSR can change subpopulationsâ€™ expected
rewards over the long-term. We are interested in the dynamic be-
tween the change in subpopulationsâ€™ expected rewards and the
fairness policy. The cost of corporate social responsibility consid-
ered in this paper is linear in the number of deterred opportunities.
Future work can investigate other forms of cost function. Addi-
tional extensions of this framework can include more complex
MAB scenarios such as contextual bandits or sleeping bandits.
35
Corporate Social Responsibility via Multi-Armed Bandits FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
REFERENCES
[1] Amazon scraps secret AI recruiting tool that showed bias against
women - reuters. URL https://www.reuters.com/article/us-amazon-
com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-
tool-that-showed-bias-against-women-idUSKCN1MK08G.
[2] How to hire with algorithms. URL https://hbr.org/2016/10/how-to-
hire-with-algorithms.
[3] Using mobile to reach the Latin american unbanked | fico. URL https:
//www.fico.com/en/node/8140?file=7900.
[4] F. Abel. We know where you should work next summer: Job recom-
mendations. In Proceedings of the 9th ACM Conference on Recommender
Systems, pages 230â€“230, 2015.
[5] A. Acharya and D. Sinha. Early prediction of students performance
using machine learning techniques. International Journal of Computer
Applications, 107(1), 2014.
[6] S. Agrawal and N. Goyal. Analysis of thompson sampling for the
multi-armed bandit problem. In Conference on learning theory, pages
39â€“1, 2012.
[7] I. Ajunwa and D. Greene. Platforms at work: Automated hiring plat-
forms and other new intermediaries in the organization of work. SP
Vallas, and A. Kovalainen, Work and Labor in the Digital Age, pages
61â€“91, 2019.
[8] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the
multiarmed bandit problem. Machine learning, 47(2-3):235â€“256, 2002.
[9] Y. Bechavod and K. Ligett. Penalizing unfairness in binary classifica-
tion. arXiv preprint arXiv:1707.00044, 2017.
[10] R. Berk. Criminal justice forecasts of risk: A machine learning approach.
Springer Science & Business Media, 2012.
[11] R. Berk, R. Berk, and Drougas. Machine learning risk assessments in
criminal justice settings. Springer, 2019.
[12] L. Besson and E. Kaufmann. What doubling tricks can and canâ€™t do
for multi-armed bandits. arXiv preprint arXiv:1803.06971, 2018.
[13] A. B. Carroll et al. The pyramid of corporate social responsibility: To-
ward the moral management of organizational stakeholders. Business
horizons, 34(4):39â€“48, 1991.
[14] S.-C. Chow andM. Chang. Adaptive design methods in clinical trialsâ€“a
review. Orphanet journal of rare diseases, 3(1):11, 2008.
[15] A. B. Diekman, E. R. Brown, A. M. Johnston, and E. K. Clark. Seek-
ing congruity between goals and roles: A new look at why women
opt out of science, technology, engineering, and mathematics careers.
Psychological science, 21(8):1051â€“1057, 2010.
[16] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical
computer science conference, pages 214â€“226, 2012.
[17] M. C. Fu. Alphago and monte carlo tree search: the simulation op-
timization perspective. In Proceedings of the 2016 Winter Simulation
Conference, pages 659â€“670. IEEE Press, 2016.
[18] A. Fuster, P. Goldsmith-Pinkham, T. Ramadorai, and A. Walther. Pre-
dictably unequal? the effects of machine learning on credit markets.
The Effects of Machine Learning on Credit Markets (November 6, 2018),
2018.
[19] E. Garriga and D. MelÃ©. Corporate social responsibility theories: Map-
ping the territory. Journal of business ethics, 53(1-2):51â€“71, 2004.
[20] M. Hardt, E. Price, N. Srebro, et al. Equality of opportunity in super-
vised learning. In Advances in neural information processing systems,
pages 3315â€“3323, 2016.
[21] M. Joseph, M. Kearns, J. Morgenstern, S. Neel, and A. Roth. Fair
algorithms for infinite and contextual bandits. arXiv preprint
arXiv:1610.09559, 2016.
[22] M. Joseph, M. Kearns, J. H. Morgenstern, and A. Roth. Fairness in learn-
ing: Classic and contextual bandits. In Advances in Neural Information
Processing Systems, pages 325â€“333, 2016.
[23] T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma. Fairness-aware clas-
sifier with prejudice remover regularizer. In Joint European Conference
on Machine Learning and Knowledge Discovery in Databases, pages
35â€“50. Springer, 2012.
[24] A. Lambrecht and C. E. Tucker. Algorithmic bias? an empirical study
into apparent gender-based discrimination in the display of stem career
ads. An Empirical Study into Apparent Gender-Based Discrimination in
the Display of STEM Career Ads (March 9, 2018), 2018.
[25] Y. Liu, G. Radanovic, C. Dimitrakakis, D. Mandal, and D. C. Parkes.
Calibrated fairness in bandits. arXiv preprint arXiv:1707.01875, 2017.
[26] T. Lux, R. Pittman, M. Shende, and A. Shende. Applications of su-
pervised learning techniques on undergraduate admissions data. In
Proceedings of the ACM International Conference on Computing Frontiers,
pages 412â€“417, 2016.
[27] A. Mas-Colell, M. D. Whinston, J. R. Green, et al. Microeconomic theory,
volume 1. Oxford university press New York, 1995.
[28] H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady,
L. Nie, T. Phillips, E. Davydov, D. Golovin, et al. Ad click prediction:
a view from the trenches. In Proceedings of the 19th ACM SIGKDD
international conference on Knowledge discovery and data mining, pages
1222â€“1230, 2013.
[29] Northpointe. Practitionerâ€™s guide to compas core, 2015. URL
https://assets.documentcloud.org/documents/2840784/Practitioner-
s-Guide-to-COMPAS-Core.pdf.
[30] R. Oentaryo, E.-P. Lim, M. Finegold, D. Lo, F. Zhu, C. Phua, E.-Y. Cheu,
G.-E. Yap, K. Sim, M. N. Nguyen, et al. Detecting click fraud in online
advertising: a data mining approach. The Journal of Machine Learning
Research, 15(1):99â€“140, 2014.
[31] V. Patil, G. Ghalme, V. Nair, and Y. Narahari. Achieving fairness in the
stochasticmulti-armed bandit problem. arXiv preprint arXiv:1907.10516,
2019.
[32] A. PÃ©rez-MartÃ­n, A. PÃ©rez-Torregrosa, andM. Vaca. Big data techniques
tomeasure credit banking risk in home equity loans. Journal of Business
Research, 89:448â€“454, 2018.
[33] C. Schumann, Z. Lang, N. Mattei, and J. P. Dickerson. Group fairness
in bandit arm selection. arXiv preprint arXiv:1912.03802, 2019.
[34] A. Slivkins. Introduction to multi-armed bandits. Foundations and
TrendsÂ® in Machine Learning, 12(1-2):1â€“286, 2019. ISSN 1935-8237. doi:
10.1561/2200000068. URL http://dx.doi.org/10.1561/2200000068.
[35] H. R. Varian and H. R. Varian. Microeconomic analysis, volume 3.
Norton New York, 1992.
[36] A. Waters and R. Miikkulainen. Grade: Machine learning support for
graduate admissions. AI Magazine, 35(1):64â€“64, 2014.
[37] J. White. Bandit algorithms for website optimization. " Oâ€™Reilly Media,
Inc.", 2012.
[38] M. B. Zafar, I. Valera, M. G. Rodriguez, and K. P. Gummadi. Fair-
ness constraints: Mechanisms for fair classification. arXiv preprint
arXiv:1507.05259, 2015.
[39] C. Zeng, Q. Wang, S. Mokhtari, and T. Li. Online context-aware rec-
ommendation with time varying multi-armed bandit. In Proceedings of
the 22nd ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 2025â€“2034, 2016.
[40] S. Zhang, W. Xiong, W. Ni, and X. Li. Value of big data to finance:
observations on an internet credit service company in china. Financial
Innovation, 1(1):17, 2015.
[41] J. Zhao, T. Wang, M. Yatskar, V. Ordonez, and K.-W. Chang. Men also
like shopping: Reducing gender bias amplification using corpus-level
constraints. arXiv preprint arXiv:1707.09457, 2017.
36
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Tom Ron, Omer Ben-Porat, and Uri Shalit
A OMITTED PROOFS
A.1 Efficient Oracle for Computing
Equation (4) for Softmax
Proposition 3. Let ğ‘“ sft (ğ)ğ‘– = exp
`ğ‘–âˆ‘ğ¾
ğ‘™=1
exp
`ğ‘™
. For every hyper-cubeğ¶ âŠ†
[0, 1]ğ¾ , the term
max
`âˆˆğ¶
ğ‘“ sft (ğ)ğ‘– âˆ’min
`âˆˆğ¶
ğ‘“ sft (ğ)ğ‘– (4)
can be computed efficiently.
Proof. Let
ğ‘“ sft (ğ) = exp
`ğ‘–
exp
`ğ‘– +âˆ‘ğ‘™â‰ ğ‘— exp`ğ‘™ =
ğ‘¥
ğ‘¥ + ğ‘¦
for ğ‘¥ = exp
`ğ‘–
and ğ‘¦ =
âˆ‘
ğ‘™â‰ ğ‘— exp
`ğ‘™
exp
`ğ‘– . Due to Proposition 4 below,
the function ğ‘”(ğ‘¥,ğ‘¦) def= ğ‘¥
ğ‘¥+ğ‘¦ is monotonically increasing in ğ‘¥ and
monotonically decreasing in ğ‘¦. Consequently,
â€¢ maxğâˆˆğ¶ ğ‘“ sft (ğ)ğ‘– is obtained for `ğ‘– = maxğ¶ [ğ‘–] and ` ğ‘— =
minğ¶ [ ğ‘—] for all ğ‘— âˆˆ [ğ¾], ğ‘— â‰  ğ‘– .
â€¢ minğâˆˆğ¶ ğ‘“ sft (ğ)ğ‘– is obtained for `ğ‘– = minğ¶ [ğ‘–] and for all
ğ‘— âˆˆ [ğ¾], ğ‘— â‰  ğ‘– ` ğ‘— = maxğ¶ [ ğ‘—].
Combining these facts, we proved that Equation 4 can be computed
efficiently.
â–¡
Proposition 4. Let ğ‘”(ğ‘¥,ğ‘¦) = ğ‘¥
ğ‘¥+ğ‘¦ , where ğ‘¥,ğ‘¦ > 0. Then ğ‘” is mono-
tonically increasing in ğ‘¥ and monotonically decreasing in ğ‘¦.
Proof. Since ğ‘” is differentiable, so it is suffice to show that the
partial derivatives are always positive for ğ‘¥ and always negative
for ğ‘¦. Observe that
ğœ•ğ‘”
ğœ•ğ‘¥
=
ğ‘¦
(ğ‘¥ + ğ‘¦)2
,
ğœ•ğ‘”
ğœ•ğ‘¦
=
âˆ’ğ‘¥
(ğ‘¥ + ğ‘¦)2
.
Since ğ‘¦ > 0,
ğœ•ğ‘”
ğœ•ğ‘¥ is always positive; thus, ğ‘” is monotonically increas-
ing in ğ‘¥ . Additionally, since ğ‘¥ > 0,
ğœ•ğ‘”
ğœ•ğ‘¦ is always negative; thus, ğ‘” is
monotonically decreasing in ğ‘¦. â–¡
37
Corporate Social Responsibility via Multi-Armed Bandits FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
B FULL VERSION OF ALGORITHM 2
In this section we present the full version of Algorithm 2, brought
here as Algorithm 4. The algorithm relies heavily on the confidence
intervals of the rewards and of the rewards gaps. The confidence
intervals of the rewards are used to estimate ğ‘“ â€™s variability inside
the hyper-cube of expected rewards. The confidence interval of
the expected reward of arm ğ‘– , given that it was pulled ğ‘ğ‘– times is
[ Ë†Ì€ğ‘– âˆ’
âˆš
2 logğ‘‡
ğ‘ğ‘–
, Ë†Ì€ğ‘– +
âˆš
2 logğ‘‡
ğ‘ğ‘–
]. The confidence intervals of the reward
gaps Î”ğ‘– are used to determine the most probable optimal algorithm
and follow it. They are defined to be ğ¿ğ¶ğµ(Î”ğ‘– ) = maxğ‘— âˆˆ[ğ¾ ] Ë†Ì€ğ‘— +
Ë†Ì€ğ‘– âˆ’ 2
âˆš
2 logğ‘‡
ğ‘ğ‘–
,ğ‘ˆğ¶ğµ(Î”ğ‘– ) = maxğ‘— âˆˆ[ğ¾ ] Ë†Ì€ğ‘— + Ë†Ì€ğ‘– + 2
âˆš
2 logğ‘‡
ğ‘ğ‘–
. Note that
using the formulas above the upper confidence bound can be above
one and the lower confidence bound can be less than zero. This is
not possible in our settings. The bounds are trimmed all through
the algorithm to be between zero and one. Note that in order to
calculate the confidence interval of the reward gaps we use the fact
that in the first two phases all the arms are pulled the same number
of times.
The algorithm starts with initialization phase (Lines 1-5). In
Line 1 the time variable is initialized. We then initialize the armsâ€™
data - the number of pulls is set to zero (Line 3), the confidence
interval of the reward gap is set to [0, 1] (Line 4). Lastly, the hyper-
cube of the rewards, ğ¶1 is initialize with [0, 1]ğ¾ (Line 5).
After the initialization the first phase of the algorithm starts
(Line 6). If there is an arm that the algorithm is not certain with
high probability whether its reward gap is lower or higher than _
by more than ğ›½ , all arms are pulled (Line 7). After arm ğ‘– is pulled
we increase the number of times arm ğ‘– was pulled by one (Line 9),
increase the time by one (Line 10) and update the expected reward
based on the obtained reward (ğ‘Ÿğ‘¡ ) and the previous expected reward
(Line 11). Possibly, the desired approximation is not achieved before
ğ‘‡ steps. In this case the execution is stopped once ğ‘‡ rounds were
played. This is done in Line 12. After pulling all the arms we update
the estimators of the reward gapsÎ”ğ‘– . In Line 14 we find the armwith
the maximal expected reward. Then for each arm, the confidence
interval of the reward gap is calculated (Lines 16-18) and the cube
of probable expected values is updates (Line 19).
The second phase starts in Line 20. If there is an arm with low
opportunity cost, i.e. ğ¿ğ¶ğµ(Î”ğ‘– ) < _, and the variability of ğ‘“ for this
entry in the hyper-cube is high, all arms are pulled once (Line 21).
As in the previous phase, first we pull each arm, update its counter,
the time and the expected reward and then update the estimators.
In Line 26, we assure that even if the desired approximation is not
achieved by ğ‘‡ rounds the execution ends.
In the third phase, which starts in Line 33, we ensure that each
armwith low opportunity cost is pulled a sufficient number of times
with respect to the fairness function ğ‘“ .ğ‘€ğ‘– denotes the number of
additional times arm ğ‘– should be pulled.ğ‘€ğ‘– is the difference between
the number of times arm ğ‘– should be pulled according to the fairness
function (ğ‘‡ ğ‘“ (ğ)ğ‘– ) and the number of times it was already pulled
(ğ‘ğ‘– ) rounded down to an integer. Arm ğ‘– is then pulledğ‘€ğ‘– times and
the counter and time are updated accordingly (Lines 36-38). To be
precise, in order not to pull more than ğ‘‡ times,ğ‘€ğ‘– is set to be the
minimum between ğ‘‡ âˆ’ ğ‘¡ and âŒŠğ‘‡ ğ‘“ (ğ)ğ‘– âˆ’ ğ‘ğ‘– âŒ‹.
Algorithm 4: Self-regulated Utility Maximization
Input: Black-box bandit algorithm ğ´ğ¿ğº , allowed
approximation error parameters ğ›¼ and ğ›½
1 ğ‘¡ = 1
2 for ğ‘– = 1, . . . ğ¾ do // Initialization
3 ğ‘ğ‘– â† 0
4 ğ¿ğ¶ğµ(Î”ğ‘– ) â† 0,ğ‘ˆğ¶ğµ(Î”ğ‘– ) â† 1
5 ğ¶1 [ğ‘–] â† [0, 1]
6 while âˆƒ ğ‘— âˆˆ [ğ¾] s.tğ‘ˆğ¶ğµ(Î” ğ‘— ) > _ + ğ›½ and ğ¿ğ¶ğµ(Î” ğ‘— ) < _ âˆ’ ğ›½
do // Phase 1
7 for ğ‘– = 1, . . . , ğ¾ do
8 Pull arm ğ‘– , receive a reward ğ‘Ÿğ‘¡
9 ğ‘ğ‘– â† ğ‘ğ‘– + 1
10 ğ‘¡ â† ğ‘¡ + 1
11 Ë†Ì€ğ‘–,ğ‘¡ â† ((ğ‘ğ‘– âˆ’ 1) Ë†Ì€ğ‘–,ğ‘¡âˆ’1 + ğ‘Ÿğ‘¡ )/ğ‘ğ‘–
12 if ğ‘¡ > ğ‘‡ then
13 End execution
14 ğ‘—âˆ— â† argmaxğ‘— âˆˆ[ğ¾ ] Ë†Ì€ğ‘—
15 for ğ‘– = 1, . . . , ğ¾ do
16 ğ‘ğ‘– â†
âˆš
2 logğ‘‡
ğ‘ğ‘–
17 ğ¿ğ¶ğµ(Î”ğ‘– ) â† max{0, Ë†Ì€ğ‘—âˆ— âˆ’ Ë†Ì€ğ‘– âˆ’ 2ğ‘ğ‘– }
18 ğ‘ˆğ¶ğµ(Î”ğ‘– ) â† min{1, Ë†Ì€ğ‘—âˆ— âˆ’ Ë†Ì€ğ‘– + 2ğ‘ğ‘– }
19 ğ¶ğ‘¡ [ğ‘–] â† [max{0, Ë†Ì€ğ‘– âˆ’ ğ‘ğ‘– },min{1, Ë†Ì€ğ‘– + ğ‘ğ‘– }]
20 while âˆƒ ğ‘— âˆˆ [ğ¾] s.t. maxğâ€²âˆˆğ¶ğ‘¡ ğ‘“ (ğ
â€²)ğ‘— âˆ’minğâ€²âˆˆğ¶ğ‘¡ ğ‘“ (ğ
â€²)ğ‘— >
ğ›¼ and ğ¿ğ¶ğµ(Î” ğ‘— ) < _ do // Phase 2
21 for ğ‘— = 1, . . . , ğ¾ do
22 Pull arm ğ‘– , receive a reward ğ‘Ÿğ‘¡
23 ğ‘ğ‘– â† ğ‘ğ‘– + 1
24 ğ‘¡ â† ğ‘¡ + 1
25 Ë†Ì€ğ‘–,ğ‘¡ â† ((ğ‘ğ‘– âˆ’ 1) Ë†Ì€ğ‘–,ğ‘¡âˆ’1 + ğ‘Ÿğ‘¡ )/ğ‘ğ‘–
26 if ğ‘¡ > ğ‘‡ then
27 End execution
28 ğ‘—âˆ— â† argmaxğ‘— âˆˆ[ğ¾ ] Ë†Ì€ğ‘—
29 for ğ‘— = 1, . . . , ğ¾ do
30 ğ‘ğ‘– â†
âˆš
2 logğ‘‡
ğ‘ğ‘–
31 ğ¶ğ‘¡ [ğ‘–] â† [max{0, Ë†Ì€ğ‘– âˆ’ ğ‘ğ‘– },min{1, Ë†Ì€ğ‘– + ğ‘ğ‘– }]
32 ğ¿ğ¶ğµ(Î”ğ‘– ) â† Ë†max{0, ` ğ‘—âˆ— âˆ’ Ë†Ì€ğ‘– âˆ’ 2ğ‘ğ‘– }
33 for ğ‘– = 1, . . . ğ¾ do // Phase 3
34 if ğ¿ğ¶ğµ(Î”ğ‘– ) < _ and ğ‘ğ‘– < ğ‘‡ ğ‘“ (ğ â€²)ğ‘– then
35 ğ‘€ğ‘– â† min{ğ‘‡ âˆ’ ğ‘¡, âŒŠğ‘‡ ğ‘“ (ğ)ğ‘– âˆ’ ğ‘ğ‘– âŒ‹}
36 Pull arm ğ‘– ğ‘€ğ‘– times
37 ğ‘ğ‘– â† ğ‘ğ‘– +ğ‘€ğ‘–
38 ğ‘¡ â† ğ‘¡ +ğ‘€ğ‘–
39 Invoke ğ´ğ¿ğº for the remaining rounds // Phase 4
If there are any remaining rounds, ğ´ğ¿ğº is invoked until the end
of the execution (Line 39).
38
FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada Tom Ron, Omer Ben-Porat, and Uri Shalit
C EXPERIMENTS SUPPLEMENTARY RESULTS
In this section we complete the results shown in Section 5. Figures 4 and 6 demonstrate the round distribution for ğ‘“ lin and ğ‘“ uni respectively
and Figures 5 and 7 compares the average utility of Algorithm 1 to 2 with respect to ğ‘“ lin and ğ‘“ uni respectively.
104 5 Â· 104 105 2 Â· 1050
0.25
0.5
0.75
1
Horizon
P
u
ll
s
d
is
tr
ib
u
ti
on
(a) _ = 0
104 5 Â· 104 105 2 Â· 1050
0.25
0.5
0.75
1
Horizon
P
u
ll
s
d
is
tr
ib
u
ti
on
(b) _ = 0.4
104 5 Â· 104 105 2 Â· 1050
0.25
0.5
0.75
1
Horizon
P
u
ll
s
d
is
tr
ib
u
ti
on
1+2 3 4
(c) _ = 0.8
Figure 4: Round distributions per phase for ğ‘“ lin.
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
(a) _ = 0
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
(b) _ = 0.4
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
SRUM
FairETC
(c) _ = 0.8
Figure 5: Average utility Algorithm 1 vs Algorithm 2 for ğ‘“ lin.
104 5 Â· 104 105 2 Â· 1050
0.25
0.5
0.75
1
Horizon
P
u
ll
s
d
is
tr
ib
u
ti
on
(a) _ = 0
104 5 Â· 104 105 2 Â· 1050
0.25
0.5
0.75
1
Horizon
P
u
ll
s
d
is
tr
ib
u
ti
on
(b) _ = 0.4
104 5 Â· 104 105 2 Â· 1050
0.25
0.5
0.75
1
Horizon
P
u
ll
s
d
is
tr
ib
u
ti
on
1+2 3 4
(c) _ = 0.8
Figure 6: Round distributions per phase for ğ‘“ uni.
39
Corporate Social Responsibility via Multi-Armed Bandits FAccT â€™21, March 3â€“10, 2021, Virtual Event, Canada
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
(a) _ = 0
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
(b) _ = 0.4
103 104 105
0.5
0.7
0.9
Horizon
A
v
g.
U
ti
li
ty
SRUM
FairETC
(c) _ = 0.8
Figure 7: Average utility Algorithm 1 vs Algorithm 2 for ğ‘“ uni.
40
