Impossible	Explanations?	Beyond	explainable	AI	in	the	GDPR	
from	a	COVID-19	use	case	scenario	
Ronan	Hamon*	
	European	Commission	
Joint	Research	Centre		
	Ispra,	Italy	
ronan.hamon@ec.europa.eu	
Henrik	Junklewitz*	
European	Commission	
Joint	Research	Centre	
	Ispra,	Italy	
henrik.junklewitz@ec.europa.eu	
		Gianclaudio	Malgieri*	
	Augmented	Law	Institute	
EDHEC	Business	School	
	Lille,	France	
	gianclaudio.malgieri@edhec.edu	
	
Paul	De	Hert	
	Law	Science	Technology	&Society	
	Vrije	Universiteit	Brussel	
	Brussels,	Belgium	
	paul.de.hert@vub.be		
	
Laurent	Beslay	
European	Commission	
Joint	Research	Centre	
	Ispra,	Italy	
laurent.beslay@ec.europa.eu	
	
Ignacio	Sanchez	
European	Commission	
Joint	Research	Centre	
	Ispra,	Italy	
ignacio.sanchez@ec.europa.eu	
	
*	These	authors,	in	alphabetical	order,	have	contributed	equally	to	this	work.	
ABSTRACT	
Can	we	achieve	an	adequate	 level	of	 explanation	 for	 complex	
machine	 learning	 models	 in	 high-risk	 AI	 applications	 when	
applying	the	EU	data	protection	framework?	In	this	article,	we	
address	this	question,	analysing	from	a	multidisciplinary	point	
of	view	the	connection	between	existing	legal	requirements	for	
the	explainability	of	AI	systems	and	the	current	state	of	the	art	
in	the	field	of	explainable	AI.			
We	 present	 a	 case	 study	 of	 a	 real-life	 scenario	 designed	 to	
illustrate	 the	 application	 of	 an	 AI-based	 automated	 decision	
making	process	for	the	medical	diagnosis	of	COVID-19	patients.	
The	scenario	exemplifies	the	trend	in	the	usage	of	increasingly	
complex	 machine-learning	 algorithms	 with	 growing	
dimensionality	 of	 data	 and	model	 parameters.	 Based	 on	 this	
setting,	we	analyse	 the	challenges	of	providing	human	 legible	
explanations	in	practice	and	we	discuss	their	legal	implications	
following	the	General	Data	Protection	Regulation	(GDPR).		
Although	 it	might	appear	 that	 there	 is	 just	one	single	 form	of	
explanation	in	the	GDPR,	we	conclude	that	the	context	in	which	
the	 decision-making	 system	 operates	 requires	 that	 several	
forms	of	explanation	are	considered.	Thus,	we	propose	to	design	
explanations	 in	multiple	 forms,	 depending	on:	 the	moment	 of	
the	disclosure	of	the	explanation	(either	ex	ante	or	ex	post);	the	
audience	of	the	explanation	(explanation	for	an	expert	or	a	data	
controller	and	explanation	for	the	final	data	subject);	the	layer	
of	 granularity	 (such	 as	 general,	 group-based	 or	 individual	
explanations);	 the	 level	 of	 the	 risks	 of	 the	automated	decision	
regarding	 fundamental	 rights	 and	 freedoms.	 Consequently,	
explanations	should	embrace	this	multifaceted	environment.		
Furthermore,	we	highlight	how	the	current	inability	of	complex,	
deep	 learning	 based	machine	 learning	 models	 to	 make	 clear	
causal	links	between	input	data	and	final	decisions	represents	a	
limitation	 for	 providing	 exact,	 human-legible	 reasons	 behind	
specific	decisions.	This	makes	the	provision	of	satisfactorily,	fair	
and	 transparent	 explanations	 a	 serious	 challenge.	 Therefore,	
there	are	cases	where	the	quality	of	possible	explanations	might	
not	 be	 assessed	 as	 an	 adequate	 safeguard	 for	 automated	
decision-making	 processes	 under	 Article	 22(3)	 GDPR.	
Accordingly,	we	suggest	that	further	research	should	focus	on	
alternative	 tools	 in	 the	 GDPR	 (such	 as	 algorithmic	 impact	
assessments	 from	 Article	 35	 GDPR	 or	 algorithmic	 lawfulness	
justifications)	 that	 might	 be	 considered	 to	 complement	 the	
explanations	of	automated	decision-making.	
CCS	CONCEPTS	
Applied	computing	→	Law,	social	and	behavioral	sciences;		
Computing	methodologies	→	Machine	learning	
KEYWORDS	
Explainability,	 AI,	 Machine	 Learning,	 GDPR,	 Black-Box,	
Automated	Decision-Making,	Data	Protection	
ACM	Reference	format:	
Permission	to	make	digital	or	hard	copies	of	all	or	part	of	this	work	
for	personal	or	classroom	use	 is	granted	without	 fee	provided	that	
copies	 are	 not	 made	 or	 distributed	 for	 profit	 or	 commercial	 ad-
vantage	and	that	copies	bear	this	notice	and	the	full	citation	on	the	
first	page.	Copyrights	for	components	of	this	work	owned	by	others	
than	ACM	must	be	honored.	Abstracting	with	credit	is	permitted.	To	
copy	otherwise,	or	republish,	to	post	on	servers	or	to	redistribute	to	
lists,	requires	prior	specific	permission	and/or	a	fee.	Request	permis-
sions	from	Permissions@acm.org.	
FAccT	'21,	March	3–10,	2021,	Virtual	Event,	Canada	
©	2021	Association	for	Computing	Machinery.	
ACM	ISBN	978-1-4503-8309-7/21/03…$15.00	
https://doi.org/10.1145/3442188.3445917	
549
  
 
 
 
 
Ronan	Hamon,	Henrik	Junklewitz,	Gianclaudio	Malgieri	(alph.),	Paul	De	
Hert,	 Laurent	 Beslay	 and	 Ignacio	 Sanchez.	 2021.	 Impossible	
Explanations?	Beyond	explainable	AI	in	the	GDPR	from	a	COVID-19	use	
case	scenario.	In	Proceedings	of	ACM	FaaCT.	ACM,	New	York,	NY,	USA,	2	
pages.	https://doi.org/10.1145/1234567890	
1	 Introduction	
Artificial	intelligence	(AI)	has	become	increasingly	important	in	
many	areas	of	society.	To	ensure	that	its	use	will	benefit	society	
as	 a	whole,	 forms	of	proper	 regulation	are	being	 sought	by	a	
large	 majority	 of	 actors,	 including	 institutions,	 industry	 and	
human	 rights	 organizations.	 AI-based	 automated	 decision-
making	 systems	 are	 now	 deployed	 in	 large-scale,	 digital	 and	
cyber-physical	 settings,	 by	 digital	 companies,	 government	
administration,	and	public	services.	
Automated	decision-making	 systems	 involving	 the	processing	
of	personal	data	of	European	Union	(EU)	citizens	fall	under	the	
scope	of	 the	General	Data	Protection	Regulation	 (GDPR	[12]),	
which	 introduced	measures	 to	 protect	 fundamental	 rights	 of	
data	subjects	in	the	digital	sphere.	Articles	13-15	and	22	(and	
recital	71)	of	 the	GDPR,	 as	 interpreted	by	 the	European	Data	
Protection	 Board	 (the	 EU	 agency	 deputed	 to	 –	 inter	 alia	 -	
interpret	 data	 protection	 law)	 [2],	 promote	 a	 requirement	 of	
transparency	 for	 automated	 decision-making	 systems,	 by	
establishing	 that	 data	 controllers	 using	 such	 systems	need	 to	
provide	meaningful	safeguards,	such	as	meaningful	information	
about	the	logics,	the	significance	and	the	envisaged	effects	of	the	
algorithm;	but	also	a	justification	of	outcomes,	in	order	to	enable	
data	subjects	to	understand	and,	if	deemed	appropriate,	contest	
a	decision	(having	 the	opportunity	 to	express	 their	own	view	
and	have	a	human	 intervention	 in	 the	decision).	Several	 legal	
scholars	reasonably	 argue	 that	 a	 right	 to	 explanation	 can	 be	
inferred	from	those	provisions	[17].			
This	being	said,	explainability	and	transparency	are	thus	crucial	
legal	 requirements	 to	 ensure	 trust	 in	 AI	 systems,	 raising	 the	
question	of	 the	 implementation	of	 such	principles	 in	 systems	
involving	AI	components	[41,	42].	AI-based	systems	are	indeed	
commonly	depicted	as	efficient,	but	opaque	black-box	systems,	
exhibiting	 outstanding	 performances	 through	 complex	
mechanisms	 in	 the	 processing	 of	 large	 volumes	 of	 data.	 The	
question	of		explainability	of	AI	systems	has	been	taken	up	by	
the	scientific	AI	community,	and	a	collection	of	methodologies	
has	 emerged,	 aiming	 at	 encouraging	 explainable	 and	
interpretable	 AI,	 while	 preserving	 the	 same	 level	 of	
performances.	Despite	 	 significant	advances,	 explainable	AI	 is	
still	lacking	a	sound	basis	to	evaluate	its	relevance	from	a	legal	
perspective	 [4],	 in	 particular	 with	 regard	 to	 the	 right	 to	
explanation	derived	from	the	GDPR.	
It	is	obviously	not	sufficient	to	legally	require	to	data	controllers	
to	provide	explanations	for	AI-based	decisions;	it	is	even	more	
important	to	define	what	characteristics	an	explanation	should	
fulfil,	in	coherence	with	the	kind	of	outcomes	technical	tools	can	
provide.	To	that	respect,	understanding	terminology	is	central	
to	create	a	meaningful	link	between	the	scientific	literature	on	
explainable	 AI	 and	 the	 legal	 discussions	 on	 the	 right	 to	
explanation.	 On	 a	 practical	 level,	 this	 discussion	 needs	 to	
become	 interdisciplinary,	between	the	 technological	and	 legal	
dimensions.	 This	 puts	 the	 technical	 terms	 of	 algorithmic	 AI,	
explainability	 and	 interpretability	 into	 the	 focus,	 and	 more	
especially	their	relationship,	to	what	should	be	legally	required	
for	an	explanation	in	an	algorithmic	context.	Furthermore,	this	
discussion	 should	 not	 only	 reflect	 on	 the	 meaning	 of	 an	
explanation	 with	 regard	 to	 technical	 explainability	 or	 being	
valid	 in	 a	 legal	 context,	 but,	 ultimately,	 on	what	 constitutes	 a	
human	understandable	explanation	in	the	most	general	terms	
[1,	6,	20,	22].		
The	fundamental	intention	of	this	study	is	to	explore	to	which	
extent	 current	 and	 future	 AI	 systems	 can	 provide	 adequate	
transparency	 and	 satisfactory	 explanations	 that	 would	 be	
admissible	from	a	legal	point	of	view.	Accordingly,	we	propose	
a	matrix	of	multidimensional	explanation	based	on	the	audience	
of	recipients,	on	the	moment	of	the	explanation,	on	the	level	of	
risk	 of	 the	 decision	 (i.e.,	 multi-layered	 explanation	 with	
increasing	levels	of	granularity	[18]).	
To	 this	end	we	carry	out	 this	analysis	on	a	use	case	scenario,	
which	 is	 based	 on	 a	 high-risk	 automated	 decision-making	
process	in	the	medical	context.	The	growing	integration	of	AI	in	
decision-making	processes	gains	relevance	in	particular	in	light	
of	the	current	COVID-19	pandemic.	Amongst	other	things,	this	
pandemic	has	exacerbated	the	limitations	of	health	systems	to	
handle	 exceptional	 high-stress	 situations,	 in	 many	 places	
partially	overwhelming	the	available	capacity	for	intensive-care	
units	 (ICUs)	 while,	 at	 the	 same	 time,	 shortages	 in	 medical	
supplies	 have	 exposed	 medical	 staff	 to	 the	 disease.	 This	
situation	already	has	led	to	a	surge	of	new	literature	proposing	
to	 automate	 some	 aspects	 of	 the	 medical	 decision-making	
process,	and	in	particular	various	forms	of	automated	triage	to	
determine	admission	to	immediate	intensive	care	[31,	32].	We	
are	precisely	 discussing	 such	 a	 use	 case	 scenario,	 based	on	 a	
deep	learning	based	COVID-19	X-ray	lung	detection	system	used	
in	the	hypothetical	but	not	unrealistic	context	of	an	automated	
emergency	triage.	
This	 paper	 is	 structured	 as	 follows.	 In	 Sections	 2	 and	 3	 we	
introduce	the	legal	and	technical	perspectives	on	explainability	
and	legal	explanations,	respectively.	Section	4	presents	our	use	
case	scenario	on	a	COVID-19	X-ray	detection	together	with	an	
analysis	 both	 from	 technical	 and	 legal	 points	 of	 view.	 We	
conclude	with	a	discussion	and	outlook	 in	Section	5	exposing	
our	doubts	about	the	development	of	satisfactory	explanation	in	
a	 future	 AI-based	 environment,	 suggesting	 the	 use	 of	 some	
possible	alternative	tools.		
2	 Overview:	legal	perspective	
550
  
 
The	 GDPR	[12]	 became	 applicable	 in	 Europe	 in	 2018	 putting	
forward	 a	modernised	 set	 of	 rules	 for	 the	 digital	 age.	 It	 is	 of	
particular	 interest	 for	AI,	as	 it	 introduces	specific	elements	to	
tackle	the	growing	adoption	of	AI	 in	decision-making	systems	
based	on	personal	data	and	thus	constitutes	itself	a	first	step	of	
an	emerging	concept	of	AI	governance.	
Several	of	 the	provisions	of	the	GDPR	relate	to	this	topic.	The	
recital	 71	 of	 the	 regulation	 already	 foresees	 cases	 in	 which	
algorithms	 are	 used	 for	 profiling	 or	 to	 automate	 decision-
making	processes,	and	it	introduces	and	motivates	the	need	to	
introduce	safeguards	for	such	processes.	These	safeguards	aim	
to	protect	against	potential	adverse	consequences	that	profiling	
or	 automatic	 decision-making	 processes	 can	 have	 on	 data	
subjects	[33,	34]	
The	 application	 of	 AI	 to	 personal	 data,	 or	 more	 generically	
automatic	processing,	is	considered	in	the	GDPR	under	different	
circumstances.	The	first	one	is	in	profiling,	which	is	defined	in	
Article	4	as	Any	form	of	automated	processing	of	personal	data	
consisting	of	the	use	of	personal	data	to	evaluate	certain	personal	
aspects	 relating	 to	 a	 natural	 person.	 The	 second	 one	 is	 solely	
automated	decision-making,	which	is	defined	in	[3]	as	the	ability	
to	 make	 decisions	 by	 technological	 means	 without	 human	
involvement.	This	refers	to	the	broader	notion	of	the	application	
of	algorithms	for	the	purposes	of	decision-making,	which	may	
or	may	not	involve	some	form	of	profiling	in	doing	so.	Several	
examples	on	this	topic	can	be	found	in	[3].	
Recital	71	contextualises	the	set	of	data	subject	rights	relevant	
to	 both	 profiling	 and	 automated	 decision-making	 that	 are	
developed	in	the	several	articles	of	the	regulation,	including	the	
right	to	information,	to	obtain	human	intervention,	to	express	
his/her	point	of	view,	to	obtain	an	explanation	of	the	outcome	
of	a	decision	and	to	challenge	such	decision	[33,	34].	
Article	13	 and	 14	 of	 the	 GDPR	 require	 that	 data	 subjects	 are	
informed	 about	 the	 existence	 of	 automated	 decision-making	
processes,	 including	 but	 not	 limited	 to	 profiling.	 Further,	 the	
articles	require	that	data	controllers	provide	data	subjects	with	
information	about	the	underlying	mechanisms	(logics)	behind	
the	automated	decision-making	performed	and	the	significance	
and	 potential	 consequences	 of	 such	 processing.	 The	 right	 of	
access	 (Article	15)	 also	 includes	 similar	 provisions,	 granting	
data	subjects	the	right	to	access	their	personal	data	and	obtain	
such	information	about	its	processing	from	the	data	controllers	
[35].	
The	 aforementioned	 articles	 refer	 respectively	 to	 Article	 22	
where	 additional	 specific	 provisions	 on	 automated	 individual	
decision-making	 are	 introduced.	 Data	 subjects	 have	 the	 right	
not	to	be	subject	to	a	decision	exclusively	based	on	automated	
processing	 if	 such	 decision	 affects	 him/her	 legally	 or	
significantly	 in	 any	 other	 way,	 unless	 any	 of	 the	 exceptions	
foreseen	 in	 Paragraph	 2	 applies	 (necessary	 for	 a	 contract,	
authorised	 by	 Union	 or	Member	 State	 Law	 or	 based	 on	 data	
subject	explicit	consent)	[36].	
Article	22	 emphasises	 the	 requirement	 to	 implement	
appropriate	measures	to	safeguard	the	rights	and	freedoms	of	
data	subjects.	In	those	cases,	where	automated	decision-making	
takes	 place,	 it	 does	 so	 by	 granting	 data	 subjects	 the	 right	 to	
obtain	human	intervention,	to	express	their	point	of	view	and	to	
be	able	to	contest	the	decision	taken.	The	guidelines	released	on	
this	topic	by	the	European	Data	Protection	Board	[2]	state	that	
human	intervention	implies	that	the	human-in-the-loop	should	
refer	to	someone	with	the	appropriate	authority	and	capability	
to	change	the	decision	[33,	40].	
It	 is	 clear	 how	 a	 requirement	 of	 technical	 explainability	 is	
relevant	 for	 these	 envisaged	 safeguards.	 Human	 supervision	
can	only	be	effective	if	the	person	reviewing	the	process	is	in	a	
position	to	assess	the	algorithmic	processing	carried	out.	This	
implies	that	such	processing	should	be	understandable.	Further,	
explainability	is	also	key	to	ensure	that	data	subjects	are	able	to	
express	 their	 point	 of	 view	and	 are	 capable	 of	 contesting	 the	
decision	 [34].	As	 it	 is	 stated	 in	 the	European	Data	Protection	
Board	guidelines,	data	subjects	will	only	be	able	to	do	that	if	they	
fully	understand	how	the	automated	decision	was	made	and	on	
which	bases.	
Even	further,	the	European	Data	Protection	Board	provides	in	
Annex	1	 a	 set	 of	 good	 practice	 recommendations	 for	 data	
controllers,	with	respect	to	the	several	rights	and	provisions	of	
the	 GDPR	 that	 are	 of	 relevance	 for	 profiling	 and	 automated	
decision	 making.	 On	 top	 of	 the	 generic	 transparency	
requirements,	 as	 commented	 in	[3],	 data	 controllers	have	 the	
specific	 duty	 (at	 Articles	 13-15)	 to	 provide	 the	 data	 subjects	
with	meaningful	 information	about	the	logics,	 the	significance	
and	the	consequences	of	automated	decision-making,	but	also	
to	 adopt	 specific	 safeguards	 (right	 to	 contest,	 right	 to	human	
intervention,	 right	 to	 express	 one’s	 view),	 including	 the	
explanation	of	the	decision	reached	(recital	71).	Since	the	GDPR	
entered	into	force,	the	academic	community	has	debated	about	
whether,	 in	 actual	 fact,	 this	 constitutes	 a	 new	 right	 to	
explanation	[17,	35,	38].	
Considering	the	different	levels	of	risk	that	automated	decision-
making	can	imply	for	rights	and	freedoms	of	data	subjects,	some	
scholars	 [18]	proposed	a	Multi-layered	explanation	model:	 for	
low-risk	 scenarios	 a	 general	 layer	 of	 explanation	 about	 the	
algorithmic	functioning	might	help	(inferred	from	Articles	13-
15	 GDPR);	 for	 medium-risk	 scenarios	 a	 group-based	
explanation	 of	 the	 algorithm,	 i.e.	 an	 explanation	 on	 how	 the	
algorithm	 works	 for	 homogeneous	 groups	 of	 subjects	 might	
help	(this	is	inferred	from	the	need	to	understand	the	decision	
before	 being	 able	 to	 exercise	 the	 right	 to	 contest	 a	 decision	
under	 Article	 22(3)	 GDPR);	 for	 high-risk	 scenarios,	 an	
individual	explanation	about	the	decision	reached	in	a	specific	
case	might	be	necessary	(according	to	recital	71).	
551
  
 
 
 
 
3	 Overview:	AI	research	perspective	
While	 nowadays	 many	 automated	 decision	 systems	 are	 still	
based	 either	 on	 non-learning	 expert	 AI	 systems	 or	 on	 more	
standard	 predictive	 machine	 learning	 models,	 this	 is	 quickly	
changing	with	 the	 rapid	uptake	of	 deep	neural	 networks	 and	
more	 availability	 of	 data.	 The	 integration	 of	 AI	 models	 for	
algorithmic	decision-making	in	every-day	products	and	services	
has	 revived	 the	question	of	how	much	 the	outputs	of	 a	given	
algorithm	 are	 understandable	 for	 a	 human,	 or	 even	
fundamentally	 uniquely	 explainable.	 This	 question	 is	 already	
crucial	 for	 a	 reliable	 assessment	 of	 a	 product’s	 security	 and	
safety	when	operating	in	real	environments,	let	alone	to	provide	
meaningful	 legal	 explanations.	 With	 this	 ever-growing	
complexity	of	machine	learning	models	and	training	data,	many	
discussions	 about	 explainability	 based	 on	 more	 classical	
approaches	 and	 standard	 tabular	 data	 sets	 might	 quickly	
become	outdated.		
To	that	end,	the	development	of	the	so-called	field	of	explainable	
AI	 has	 recently	 gained	 a	 lot	 of	 traction,	 in	 order	 to	 provide	
interpretable	elements	alongside	with	model	results.	While	this	
topic	is	not	entirely	new	(early	AI	research	on	expert	systems	in	
the	1980s	already	raised	questions	about	AI	explainability	[1,	
7]),	 discussions	 about	 explainable	 AI	 have	 significantly	
broadened:	 from	 a	 growing	 literature	 of	 technical	 work	 on	
interpretable	models	and	explainable	AI	[14,	24],	to	an	ongoing	
discussion	 about	 the	 precise	 meaning	 and	 definition	 of	
explainability	 and	 interpretability	[7,	 20,	 23],	 to	 more	
procedural	 questions	 about	 the	 evaluation	 of	 existing	
frameworks	[6],	 or	 to	 input	 from	 social	 science	 about	 the	
meaning	of	explanation	[22].	
The	 meaning	 of	 what	 the	 academic	 literature	 refers	 to	 as	
explainability	of	an	AI	model	is	very	different	from	the	meaning	
of	 an	explanation	which	 is	 generally	discussed	 in	other	 social	
contexts	(see	[1,	6,	20,	22]	for	the	ongoing	academic	discussion).	
Since	 precisely	 this	 difference	 of	 meaning	 is	 important	 to	
understand	 for	 an	 interdisciplinary	 discussion	 of	 legal	 and	
technical	dimensions,	we	first	provide	some	background	on	the	
state	of	the	art	of	the	literature	on	explainable	AI.		
Explainability	or	interpretability?	
In	fact,	there	is	no	single,	clear	terminology	agreed	upon	in	the	
scientific	 literature	 to	 describe	 the	 notion	 of	 “how	 well	 the	
results	 of	 a	 particular	 AI	model	 can	 be	 explained”.	 The	most	
generally	 used	 terminology	 is	 captured	 in	 the	 concepts	 of	
explainability	 and	 interpretability.	 In	most	 cases,	both	notions	
are	 actually	 used	 interchangeably,	 often	 (but	 not	 always)	
loosely	 understood	 as	 a	 variant	 of	 how	 well	 a	 human	 could	
understand	 the	 decisions	 of	 an	 autonomous	 algorithmic	
system	[1,	6,	22].	Others	remark	that	interpretability	is	mostly	
employed	in	the	narrower	context	of	machine	learning	models,	
and	 thus	 conclude	 it	 to	be	 a	 subset	of	 the	broader	 field	of	AI	
explainability	[1,	23].	
In	the	review	of	[7],	it	is	pointed	out	that	some	authors	define	
interpretability	 to	 be	 an	 AI	 model-centric	 notion,	 whereas	
explainability	would	be	 a	 subject-centric	 notion.	 In	 this	 sense,	
the	 term	explainability	 takes	on	a	 less	 technical	meaning	and	
would	 be	 of	 larger	 relevance	 to	 legal	 considerations.	We	will	
leave	 this	 discussion	 to	 the	 literature	 and	 for	 all	 purposes	
equate	both	terms	for	the	remainder	of	this	article.	
Interpretable	models	vs.	post-hoc	approaches	
Interpretable	models	are	 fully	or	partially	designed	to	provide	
reliable	and	easy-to-understand	explanations	of	the	prediction	
they	output	[24].	They	are	supposed	to	provide	straightforward	
algorithmic	elements,	which	are	considered	to	be	interpretable	
from	their	very	nature	as	being	simple	enough.	Some	are	simply	
a	number	of	well-known	statistical	algorithms,	which	only	use	a	
few	 parameters	 or	 are	 conceptually	 very	 simple.	 Examples	
include	 linear	 and	 logistic	 regressions,	 decision	 trees,	 or	
generalized	additive	models	such	as	a	spline	fit.	Other	models	
are	specifically	designed	to	incorporate	interpretable	decision-
making	mechanisms	for	a	given	context.	The	problem	is	that	it	
stands	to	reason	as	to	whether	it	is	always	possible	to	design	an	
appropriate	 interpretable	model	 to	 the	desired	 accuracy.	The	
feasibility	 of	 this	 approach	 is	 highly	 debated,	 especially	 in	
application	cases	where	the	most	accurate	solutions	are	usually	
provided	by	complex	models	such	as	deep	neural	networks.	
Post-hoc	 techniques	 are	 used	 to	 extract	 explanations	 from	 a	
model,	which	is	usually	considered	as	a	black	box	either	because	
internal	parameters	of	the	model	are	obfuscated,	or	because	it	
is	 too	 complex	 to	 be	 understood.	Many	 approaches	 from	 the	
literature	 of	 post-hoc	 interpretability	 aim	 to	 train	 an	 openly	
interpretable	 surrogate	 model	 on	 the	 basis	 of	 queries	
forwarded	to	the	model	[14].	
Interpretability	vs.	Accuracy	
The	 problem	 of	 designing	 interpretable	 models	 is	 part	 of	 a	
larger	 discussion,	 in	 which	 it	 is	 debated	 whether	 a	 trade-off	
exists	 in	 ML	 model	 design	 between	 interpretability	 and	
accuracy.	
Usually,	 more	 complex	 models	 are	 employed	 in	 pursuit	 of	
higher	 accuracies	 or	 to	 achieve	 more	 complex	 tasks.	 Making	
those	 models	 more	 interpretable	 in	 turn	 seems	 to	 almost	
inevitably	come	with	a	 loss	 in	these	capabilities.	On	the	other	
hand,	 the	 assumption	 that	 under	 given	 constraints	 better	
results	can	only	be	achieved	with	a	more	complex	model	can	be	
challenged,	 especially	 when	 good	 feature	 engineering	 is	
combined	 with	 simpler,	 but	 robust	 models	[24].	 Yet	 from	
another	 angle,	 the	 very	 notion	 of	 a	 complex	 and	 less	
interpretable	 model	 might	 depend	 on	 the	 point	 of	 view,	
constraints	or	situation	[20].	
In	any	case,	the	trade-off	between	interpretability	and	accuracy	
has	been	a	recurrent	topic	in	the	machine	learning	community	
since	its	beginning.	Nowadays,	the	scientific	discussion	is	built	
552
  
 
on	the	requirements	of	two	desirable	properties	of	systems	that	
are	their	effectiveness,	i.e.	their	ability	to	perform	various	tasks	
with	fewer	mistakes,	and	their	understandability	of	models,	i.e.	
their	 capacity	 to	 provide	 interpretable	 elements	 of	 the	 inner	
mechanisms	involved	in	the	processing	of	data.	The	pursuit	of	
these	 two	 objectives	 has	 actually	 been	 proven	 to	 be	
contradictory	 to	 some	 point,	 as	 interpretable	 methods	 often	
require	constraints	that	may	limit	their	accuracy,	especially	in	
the	case	of	high-dimensional	data.	The	effects	of	this	trade-off	
have	been	particularly	strong	in	recent	years	with	the	advent	of	
deep	 learning	 techniques	 that	 introduced	 models	 involving	
millions	 or	 even	 billions	 of	 parameters,	 with	 little	
understanding	of	the	logics	involved	in	the	processing	of	large	
amount	of	data.	
4	 Case	study:	automated	decision	making	in	
medical	imaging	
To	investigate	in	detail	how	contemporary	technical	discussions	
on	AI	explainability	are	aligned	with	the	legal	discussion	on	the	
right	to	explanation,	we	present	now	a	technical	case	study	of	a	
machine	 learning	 based	 automated	 decision	 systems	 in	 a	
realistic,	high-risk	application,	namely	the	COVID-19	detection	
based	on	X-ray	images.	Analysing	medical	images	is	a	task	that	
requires	 cognitive	 capabilities,	 since	 no	 simple	 rule	 exists	 to	
make	 the	 distinction	 between	 a	 healthy	 lung	 and	 one	 that	
suffers	from	a	pneumonia,	or,	even	more	involved,	to	make	the	
distinction	between	different	types	of	pneumonia.	
This	case	study	highlights	a	bigger	trend	in	machine	learning	to	
provide	 detection	 tools	 that	 not	 only	 achieve	 human	
performances,	but	even	go	beyond	and	are	able	to	capture	weak	
patterns	 in	complex	 images	and	assist	medical	doctors.	 In	the	
COVID-19	 context,	 the	 primary	 goal	 is	 first	 and	 foremost	 to	
replace	the	medical	staff	in	order	to	ease	the	pressure	that	has	
been	put	on	ICU	services	[25],	and	while	this	target	will	 likely	
not	be	achieved	for	the	current	pandemic,	it	is	however	certain	
that	 these	 advances	 will	 trickle	 down	 to	 other	 medical	
applications,	with	possible	extensive	use	of	 such	 tools	 for	 the	
next	health	crises	[27,	30].	
Description	of	the	case	study	
Medical	 imaging	 is	 relevant	 for	 its	potential	high	 impact	on	a	
data	subject’s	rights	and	even	well-being	and	is	highlighted	as	
an	 important	 example	 case	 in	 the	 Article	 29	 Working	 Party	
guidelines	on	automated	decision	making	in	the	GDPR	[2].	The	
medical	 scenario	 is	 chosen	 to	 be	 of	 particular	 relevance	with	
respect	to	the	current	COVID-19	pandemic,	where	we	discuss	a	
“near-future”	 scenario	 of	 automated	 medical	 triage	 for	
classifying	a	symptomatic	patient	to	likely	suffer	from	a	COVID-
19	 induced	 pneumonia	 from	 X-ray	 images.	 It	 should	 be	
highlighted	that,	while	nowadays	medical	 triage	 is	mostly	not	
conducted	in	an	automated	fashion,	we	regard	such	a	scenario	
not	as	hypothetical,	as	the	current	pandemic	already	has	shown	
an	 uptake	 of	 discussion	 around	 such	 systems	 under	 adverse	
hospital	conditions	[32].	
The	objective	of	this	case	study	is	to	highlight	the	complexity	of	
providing	relevant	explanations	in	a	medical	context,	where	the	
inherent	complexity	of	data	and	models	is	a	strong	limitation	to	
the	provision	of	meaningful	explanations.	This	is	due	to	the	two	
following	features:	 first,	medical	data	are	usually	meaningless	
for	 a	 layman,	 requiring	 a	minimal	 set	 of	medical	 knowledge.	
Secondly,	 decision-making	 systems	 are	 relying	 to	 extract	
patterns	 from	 these	 data	 on	 sophisticated	 machine	 learning	
tools,	whose	behaviours	are	themselves	opaque.	
	
Use	case	design	
We	consider	a	machine	learning	based	automated	system	that	
performs	 a	 binary	 classification,	 i.e.	 determines	 whether	 the	
data	 subject	 has	 the	 COVID-19	 disease	 or	 not.	 We	 make	 the	
distinction	 between	 two	 profiles	 of	 actors	 relevant	 for	 the	
scenario,	 for	 which	 we	 showcase	 different	 degrees	 of	
explainability:	
1. Data	subject	 (patient	 for	our	use	case):	user	without	
relevant	technical	nor	domain	knowledge.	They	are	either	
confronted	with	initial	information	about	the	automated	
processing	of	their	data	in	a	situation	ex-ante	to	an	actual	
decision	made	by	the	system	(e.g.,	to	decide	whether	or	
not	they	want	to	allow	the	use	of	the	automated	decision	
system	to	gain	a	benefit	such	as	faster	or	cheaper	handling	
or	just	as	a	mandatory	step	to	be	properly	informed),	or,	
who	contests	a	decision	ex-post,	making	active	use	of	their	
rights	as	a	data	subject;	
2. Expert	 (medical	 staff	 for	 our	 use	 case):	 user	 with	 a	
domain	 expertise	 and	 an	 understanding	 of	 the	
AI	techniques,	 who	 has	 to	 check	 the	 validity	 of	 the	
system’s	decision	 after,	 e.g.,	 the	 exercise	 of	 the	 right	 to	
contest	or	of	the	right	to	human	intervention.	
Explanatory	 factors	 should	 be	 made	 explicit	 to	 the	 user	 and	
presented	with	 the	 right	 level	 of	 simplicity	 depending	on	 the	
level	 of	 expertise.	 The	 trade-offs	 between	 simplicity/efficacy	
and	sobriety/fidelity	in	the	field	of	algorithm	explainability	have	
already	been	discussed	[8].	Here,	we	assume	that	there	are	at	
least	two	different	recipients	for	algorithmic	explanation	in	the	
data	 protection	 field:	 the	 data	 subject	 (where	 simplicity	 and	
sobriety	might	be	considered	more	 important	than	efficacy	of	
fidelity)	and	the	data	controller/processor	(where	efficacy	and	
fidelity	should	prevail	on	simplicity	and	sobriety).	Non-expert	
users	 will	 appreciate	 a	 concise	 description	 of	 the	 system	 in	
simple	 non-technical	 terms,	 with	 positive	 relationships	
between	inputs	of	the	system	and	the	outputs.	The	accuracy	of	
the	explanation	can	be	reduced	to	the	benefit	of	its	clarity.	On	
the	 contrary,	 experts	 are	 likely	 to	 prefer	 more	 detailed	
explanations,	 revealing	 a	 comprehensive	 set	 of	 relationships	
between	inputs	and	outputs,	integrating	confidence	score	of	the	
553
  
 
 
 
 
system.	Illustrations	with	examples	extracted	from	an	external	
dataset	could	also	convey	these	elements.	
We	 order	 the	 scenario	 into	 two	 use	 cases	 on	 explanations	
provided	ex-ante	and	ex-post	to	an	automated	decision.	
Use	case	1:	Explanations	provided	ex-ante	and	confidence	in	the	
system.	 In	 legal	 terms,	 the	 ex-ante	 explanation	 in	 the	 GDPR	
corresponds	 to	 the	 right	 to	 receive	 meaningful	 information	
about	 the	 logic,	 significance	 and	 envisaged	 effects	 of	 the	
automated	decision,	as	stated	at	Articles	13(2)(f),	14(2)(g),	 in	
the	 initial	 phase	 of	 a	 data	 processing	 involving	 automated	
decision-making.	In	this	case,	the	user	might	be	interested	in	the	
behaviour	of	the	decision-making	systems	and	wants	to	receive	
elements	that	will	convince	him	to	use	it.	
Use	case	2:	Explanations	provided	ex-post	and	understanding	a	
decision	for	a	contestation.	The	ex	post	explanation	in	the	GDPR	
corresponds	 to	 the	 right	 to	 receive	 information	 about	 the	
automated	 decision	 reached	 (either	 as	 an	 explanation	 of	 the	
decision	or	as	information	that	enables	the	correct	exercise	of	
the	right	to	contest	the	decision),	as	arises	from	the	combination	
of	Article	22(2)	and	recital	71.	This	use	case	is	ultimately	related	
to	 the	 confidence	 users	 can	 have	 about	 a	 given	 individual	
decision	 made	 by	 the	 system.	 Understanding	 a	 decision	 is	
important	 for	 various	 reasons,	 for	 being	 able	 to	 contest	 a	
decision	that	could	be	or	was	harmful	(see,	indeed,	[2]),	but	also	
in	providing	elements	to	identify	wrongdoing	of	the	system,	or	
on	the	contrary,	confirm	a	decision.	
	
Scenario	description	
The	case	study	relies	on	a	fictitious	yet	plausible	scenario	that	is	
given	below.	
A	patient	with	a	serious	cough,	breathing	problems	and	general	
physical	 discomfort,	 but	 still	 conscious,	 admits	 themself	 to	 the	
emergency	 ward	 of	 a	 hospital	 during	 a	 severe,	 ongoing	 local	
outbreak	of	the	COVID-19	pandemic.	The	hospital	staff	 is	under	
significant	 pressure	 and	 the	 intensive	 care	 unit	 is	 already	
overwhelmed	with	severe	patients.	After	some	initial	tests	and	an	
X-ray	 of	 the	 chest,	 the	ward’s	 nurse	 tells	 the	 patient	 that	 tests	
could	 indicate	 a	 possibility	 of	 a	 COVID-19	 infection.	 Normally,	
such	 patients	 would	 be	 admitted	 after	 decision	 of	 a	 medical	
doctor	to	the	intermediate	care	ward	since	a	pneumonia	induced	
by	COVID-19	is	highly	likely	to	cause	severe	and	rapid	worsening	
of	the	patients.	However,	no	doctor	is	available	for	at	least	several	
hours.	 Instead	 the	 patient	 could	 decide	 to	 let	 an	 automated	
decision	 system	 that	 has	 been	 exceptionally	 authorized	 decide	
from	 the	available	 chest	X-ray	 image,	whether	or	not	he/she	 is	
likely	suffering	from	COVID-19.	
 
1 https://github.com/ieee8023/covid-chestxray-dataset 
We	assume	a	few	basic	parameters	for	the	scenario,	mostly	so	
that	they	have	significance	from	the	legal	perspective:	
• the	situation	does	not	 initially	 involve	an	expert	 for	the	
decision,	 i.e.	we	assume	a	 fully	automated	decision	and	
not	a	system	built	to	aid	a	physician	to	take	a	decision;	
• a	 human	 could	 be	 involved	 in	 the	 loop,	 but	 it	 is	 either	
expensive	 to	do	that,	or	 it	 is	only	considered	ex-post,	 to	
check	the	validity	after	an	eventual	contestation;	
• the	 model	 has	 been	 through	 an	 auditing	 process	 that	
tested	 and	 approved	 it	 according	 to	 some	 unspecified	
protocol,	which	only	guarantees	the	model	performance	
within	 certain	 measured	 parameters	 and	 not	 that	 the	
model	is	generally	unerring.	
For	 this	 case	 study,	we	 trained	 two	 toy	 convolutional	 neural	
networks	(CNN)	models	to	detect,	from	an	X-ray	image,	whether	
or	not	the	patient	is	suffering	from	the	COVID-19	disease	or	not.	
The	first	one	is	based	on	the	COVID-NET	architecture	[29]	and	
is	trained	on	a	large	dataset	including	multiple	sources	for	X-ray	
images.	 The	 second	 model	 leverages	 transfer	 learning	 to	
develop,	 from	 a	 model	 trained	 on	 various	 datasets	 of	 X-ray	
images	[5],	a	detection	system	using	a	subset	of	the	previously	
mentioned	 dataset 1 ,	 that	 also	 includes	 information	 about	
patients	 such	as	 sex	or	 age.	These	 two	models	 are	developed	
solely	to	support	the	discussions	on	explainability	of	AI	systems,	
and	 does	 not	 intend	 to	 provide	 any	 additional	 information	
outside	the	scope	of	this	paper.	
The	decision-making	system	returns	a	probability	that	a	patient	
is	 infected	 based	 on	 an	 X-ray	 image	 of	 the	 chest.	 From	 the	
system	perspective,	 the	 image	 is	viewed	as	an	array	of	pixels,	
each	 pixel	 being	 encoded,	 for	 black	 and	 white	 image,	 as	 a	
numerical	value	between	0	and	255,	 the	 larger	 the	value,	 the	
brighter	the	pixel.	Because	the	number	of	pixels	is	very	large	(in	
this	application,	each	image	has	a	size	of	512	by	512	pixels,	in	
total	 262144	 pixels),	 each	 individual	 pixel	 is	 independently	
irrelevant,	 contrary	 to	 the	 aggregation	 of	 pixels	 that	 forms	
possibly	 complex	 patterns	 that	 may	 be	 of	 interest	 for	 the	
detection.	CNN	models	are	designed	in	such	a	way	that	at	each	
layer	of	the	network,	the	presence	and	the	intensity	of	specific	
patterns	is	scrutinized	over	all	the	image,	and	based	on	this,	a	
score	 is	 derived.	 As	 an	 illustration,	 if	 the	 infection	 is	
characterised	by	the	presence	of	dots	on	the	lungs,	the	model	
will	 learn	 to	 detect	 circular	 shapes,	 and	 the	 intensity	 of	 this	
patterns	 aggregated	 all	 over	 the	 images	 will	 be	 directly	
connected	to	the	value	of	the	final	detection	score.	
554
  
 
Ex-ante	explanations	
In	order	to	decide	whether	to	admit	themself	to	the	automated	
system,	 a	 patient	 is	 given	 an	 information	 sheet	 (see	 below),	
providing	information	about	the	system.	This	information	sheet	
contains	 the	 specifications	 of	 the	 systems,	 including	 the	
information	about	the	development	of	the	system,	the	general	
principle	of	the	algorithm,	and	the	kind	of	data	that	is	used	in	
the	processing.	
Brief	description	of	the	system	brought	to	the	attention	
of	the	patient	before	examination. 
The	system	is	an	automatic	decision-making	system	that	has	
been	 developed	 to	 assess	 the	 probability	 that	 a	 patient	 is	
infected	 by	 the	 SARS-CoV-2	 virus,	 by	 considering	 X-ray	
images	of	the	chest.	The	dataset	used	to	develop	the	system	
is	 composed	of	350	 images,	 including	130	 images	 from	99	
negative	 patients	 and	 220	 images	 from	 166	 patients.	
Additionally,	 180	 images	 have	 been	 used	 for	 testing	
purposes,	including	68	images	from	57	negative	patients,	and	
112	images	from	98	negative	patients. 
Principle:	 the	 system	 is	 based	 on	 an	AI	 system	 based	 on	
Convolutional	Neural	Networks.	It	consists	in	automatically	
extracting	 the	 features	 that	 discriminate	 between	 X-ray	
images	of	the	chest	of	healthy	and	infected	patients. 
Data:	The	system	takes	as	input	the	X-ray	image	of	the	chest	
in	black	and	white.	No	additional	information	is	given	to	the	
system.	An	example		X-ray	image	is	displayed	below. 
 
Figure	1:	Example	of	an	X-Ray	image	of	the	chest	that	is	being	
used	by	the	system	as	an	input.	
Outcome:	a	score	of	 infectiousness	between	0	and	100%.	If	
the	 confidence	 score	 is	 above	60%,	 the	patient	 is	detected	
POSITIVE	to	COVID-19,	otherwise	it	is	detected	NEGATIVE. 
Performances:	The	system	has	achieved	an	accuracy	of	69%	
over	all	patients. 
Ex-post	explanations	
The	 patient	 takes	 the	 automated	 test	 and	 is	 detected	 as	
NEGATIVE	with	a	score	of	71%.	The	detection	is	associated	with	
an	information	sheet	providing	elements	on	how	the	detection	
has	 been	 obtained.	 This	 includes	 the	 performances	 of	 the	
system	 on	 other	 patients	 that	 are	 in	 the	 same	 situation	
(detected	as	NEGATIVE),	that	is	intended	to	help	the	patient	to	
get	an	idea	of	typical	outcomes.	In	addition,	the	patient	receives	
also	group-based	performances	information	based	on	sex	and.	
This	might	help	 the	patient	 identifying	possible	misbehaviour	
specific	to	a	certain	population.	
Personalized	 explanation	 of	 the	 decision	 taken	 by	 the	
system	at	the	attention	of	the	patient	
Results:	 You	 received	 a	 score	 of	 infectiousness	 of	 71%,	
indicating	you	are	NEGATIVE	to	COVID-19	(threshold:	60%).	
Overall	performances:	for	patient	in	the	same	situation	as	you	
(detected	as	POSITIVE),	the	precision	of	the	system	is	69%	
(mean	confidence	score	of	76%).	
Group-based	performances:	for	patients	in	the	same	group	as	
you	(sex	=	’M’	and	(35	<	age	<	50),	the	precision	is	65%	
(confidence	 score	 of	 74%)	 for	 negative	 patients	 and	 54%	
(confidence	score	of	68%)	for	positive	patients.	
Based	upon	these	elements	and	their	own	judgment,	the	patient	
decides	 to	 contest	 the	 decision	 made	 by	 the	 system.	 After	 a	
while,	medical	staff	is	available	to	act	as	a	human-in-the-loop	to	
validate	the	decision.	Here,	we	assume	that	the	medical	doctor	
only	 considers	 the	 outputs	 of	 the	 system	 to	 take	 the	 final	
decision,	 without	 carrying	 out	 his	 own	 analysis.	 The	 system	
provides	a	more	detailed	information	sheet	that	is	divided	into	
two	main	sections.		
First,	 to	 understand	 the	 system	 and	 the	 decision	 made,	 a	
medical	 doctor	 needs	 to	 assess	 his	 general	 confidence	 in	 the	
current	 system.	 The	 general	 information	 provided	 by	 the	
software	 for	 medical	 staff	 contains	 elements	 such	 as	 the	
specifications	and	the	overall	performances	of	the	system,	but	
with	a	greater	level	of	details	taking	into	account	the	expertise	
of	the	medical	staff.	Another	explanatory	tool	aimed	at	informed	
audiences	returns	the	typical	features	that	strongly	activate	one	
of	 the	 outcomes	 of	 the	 model.	 These	 features	 are	 not	 X-ray	
images,	 but	 are	 generated	 to	 reproduce	 the	patterns	 that	 are	
encoded	in	the	model	parameters.	
The	second	section	contains	information	that	is	specific	to	the	
patient,	 including	 feature	 maps	 such	 as	 occlusion	 maps.	 A	
counterfactual	 image	 can	 also	 be	 generated	 from	 the	 X-ray	
image	 of	 the	 patient,	 for	 which	 the	 prediction	 outcome	 is	
changed.	
555
  
 
 
 
 
Specifications	 of	 the	 system	 at	 the	 attention	 of	 the	
medical	staff 
The	 decision-making	 system	 is	 based	 on	 a	 deep	 neural	
network	based	on	the	DenseNet	architecture.	The	model	has	
first	been	trained	on	a	collection	of	X-ray	images	coming	from	
multiple	sources:	RSNA	Pneumonia	Challenge,	NIH	chest	X-
ray8,	PadChest,	CheXpert,	MIMIC-CXR.		
Performances	of	the	detection	system	(in	%). 
	Status	 Accuracy	 Sensitivity	 Specificity	
	Neg.	 76	 67	 69	
Pos.	 76	 81	 79	
Distributed	over	the	true	health	status	of	the	patient.		
 
Sex	\	Age	 	 All	 20-35	 35-50	 50-70	 70+	
All	 	 76	 85	 84	 71	 70	
M	 	 81	 88	 89	 82	 73	
F	 	 65	 82	 85	 55	 62	
Accuracy	distributed	over	sex	and	age. 
Interpretable	elements 
 
Figure	2: occlusion	map	technique	to	indicate	decision-
relevant	areas	in	the	image. 
Figure	2	shows	the	area	of	the	X-Ray	image	that	is	considered	
as	relevant	for	the	automated	detection,	using	an	occlusion	
map	technique.	
Personalized	 explanation	 of	 the	 decision	 taken	 by	 the	
system	on	the	patient	at	the	attention	of	the	medical	staff 
For	 the	 patient,	 the	 system	 returns:	 NEGATIVE	 with	 a	
confidence	score	of	75%.	
Counterfactual 
 
Figure	3:	Artificial	X-ray	image	classified	as	POSITIVE	with	
confidence	100%	by	the	system,	obtained	from	the	original	X-
ray	image	of	the	patient.	
In	 Figure	 3,	 a	 counterfactual,	 artificial	 X-ray	 image	 is	
presented,	 obtained	 from	 the	 original	 X-ray	 image	 of	 the	
patient.	It	is	classified	as	POSITIVE	with	confidence	100%	by	
the	system. 
	
Technical	analysis	
Explaining	 the	 behaviour	 of	 a	model	 could	 consist	 in	making	
explicit	the	patterns	on	which	the	prediction	score	is	based.	In	
practice	however,	 two	substantial	difficulties	are	 	 faced.	First,	
the	number	of	different	patterns	 is	usually	huge.	For	 the	 first	
model	discussed	in	the	case	study,	there	are	4960320	different	
patterns	extracted	by	the	training	procedure,	most	of	them	of	
size	 3	 by	 3	 pixels.	 The	 prediction	 score	 is	 a	 weighted	
combination	of	the	contribution	of	each	of	these	patterns	for	the	
considered	image.	Secondly,	only	a	fraction	of	them	are	directly	
extracted	 from	 the	 original	 image:	 deep	 neural	 networks	
typically	 use	 these	 patterns	 to	 create	 intermediate	 images,	
compressing	the	information	at	different	scales,	the	deepest	the	
layer,	the	more	abstract	the	image.	The	inherent	complexity	of	
both	data	and	models	makes	then	the	derivation	of	explanation	
an	unsolved	challenge,	and	attempts	to	provide	elements	that	
are	 meaningful	 for	 the	 understanding	 of	 a	 decision	 have	 to	
follow	a	circuitous	route.		
The	performances	of	the	system,	if	they	do	not	directly	inform	
about	 the	 logics	 of	 the	 system,	 are	 a	 good	 indication	 that	 the	
system	 is	 working	 correctly,	 provided	 that	 the	 testing	
procedure	 meets	 acceptable	 standards.	 Appropriate	
documentation	 of	 the	 model	 accompanying	 the	 model	 can	
provide	 non-expert	 users	 with	 elements	 to	 understand	 the	
nature	of	the	processing.	After	the	processing,	the	generation	of	
feature	 maps	 is	 a	 classical	 approach	 to	 highlight	 the	 most	
relevant	groups	of	pixels,	and	from	that	understand	the	patterns	
that	have	been	captured	by	the	model.	A	typical	approach,	called	
556
  
 
occlusion	maps,	consists	in	masking	a	portion	of	the	image,	and	
tracking	 the	 evolution	 of	 the	 score.	 The	 resulting	 heat	 map	
indicates	 the	 regions	 with	 features	 characteristic	 to	 the	
infectious	 regions	 (in	 red)	 and	 to	 non-infectious	 regions	 (in	
green).	 With	 this	 type	 of	 information,	 abnormal	 behaviours	
could	be	detected,	e.g.	a	detection	relying	on	regions	with	bone	
tissue.	Similarly,	counterfactual	images	help	to	figure	out	what	
are	 the	 relevant	 patterns	 for	 the	 detection,	 by	 generating	
localized	alterations	of	the	image	that	change	the	outcome	of	the	
detector.	 The	 analysis	 of	 the	 alteration	 gives	 also	 some	
indication	 of	 the	 relevant	 patterns	 at	 stake	 in	 the	 decision-
making	process.	
For	 all	 techniques,	 there	 are	 nonetheless	 no	 guarantee	 that	
these	 regions	 are	 indeed	 the	 ones	 that	 are	 significant,	 nor	 to	
correctly	 assess	 their	 importance	 in	 the	 final	 decision.	
Explainability	 techniques	 are	 themselves	 complicated	
techniques	requiring	fine	parametrization,	and	subject	to	trade-
off	between	accuracy	and	tractability.	
Legal	analysis	
Receiving	an	automated	diagnosis	of	COVID-19	is	considered	to	
fall	under	the	scope	of	Article	22.	It	is	indeed	a	decision	based	
solely	 on	 automated	 processing,	 based	 on	 profiling,	 which	
produces	 legal	 effects	 or	 similarly	 significantly	 effects	 on	 the	
data	subject.	In	particular,	we	can	identify	at	least	two	effects:	
the	 psychological	 effects	 deriving	 from	 a	 diagnosis,	 and	 the	
medical	 consequences	 of	 such	 diagnosis	 (access	 to	 further	
health	cares,	self-quarantine,	etc.).	
Accordingly,	the	data	subject	has	a	right	to	receive	meaningful	
information	about	the	logic,	the	significance	and	the	envisaged	
effects	 of	 the	 automated	decision	when	 she	 first	 answers	 the	
questionnaire	 (see	Article	13(2)(g))	 or	upon	her	 request	 at	 a	
later	stage	(see	Article	15(1)(h)).	In	addition,	we	can	affirm	that	
this	 automated	 decision-making	 system	 might	 be	 allowed	
because	 either	 it	 is	 based	 on	 the	 explicit	 consent	 of	 the	 data	
subject	 (22)(2)(c)	 or	 it	 is	 eventually	 “authorised	by	Union	or	
Member	State	law	to	which	the	controller	is	subject	and	which	
also	lays	down	suitable	measures	to	safeguard	the	data	subject’s	
rights	 and	 freedoms	 and	 legitimate	 interests”	 (22)(2)(b).	
Accordingly,	the	data	controller	(or	the	national	law	eventually	
allowing	 such	 automated	 decision-making	 system)	 needs	 to	
“implement	 suitable	measures	 to	 safeguard	 the	data	 subject’s	
rights	and	freedoms	and	legitimate	interests,	at	least	the	right	
to	obtain	human	 intervention	on	the	part	of	 the	controller,	 to	
express	 his	 or	 her	 point	 of	 view	 and	 to	 contest	 the	 decision”	
(Article	22(3)).	
Given	that	COVID-19	automated	detection/diagnosis	might	be	
considered	a	data	processing	producing	high	risks	for	the	rights	
and	 freedoms	 of	 individuals,	 according	 to	 the	 risk-based	
approach	(Article	24(1)),	the	data	controller	might	be	required	
to	 implement	 higher	 safeguards,	 in	 addition	 to	 the	 three	
safeguards	mentioned	above,	 including	the	right	to	receive	an	
explanation	about	the	decision	reached	(recital	71).	Also,	once	
the	data	subject	contests	the	decision,	the	data	controller	should	
take	some	relevant	steps	to	analyse	the	decision	and	eventually	
take	 a	 new	decision,	 after	 having	understood	 the	 correctness	
and	 reliability	 of	 the	 algorithm	 that	 is	 why	 we	 propose	 an	
expert-based	explanation	that	analyses	the	level	of	confidence	
of	image	prediction.	On	the	other	hand,	the	explanation	that	the	
data	 subject	 (the	patient)	might	 need	 is	 easier	 and	 aiming	 to	
justify	 a	 certain	 decision,	 connecting	 reality	 to	 the	 reached	
decisions.	
The	explanations	given	above	are	composed	of	three	layers	of	
granularity:	general	explanation	of	the	algorithm,	group-based	
explanation	(how	group	of	similar	data	subjects	were	assessed	
by	 the	 algorithm),	 and	 individual	 explanation	 of	 the	 decision	
reached	in	the	single	case	[18].	However,	the	adoption	of	deep	
learning	 image	 analysis	 does	 not	 make	 easy	 such	 an	
explanation:	 there	 is	 no	 direct	 causation	 rules	 in	 such	
automated	decisions,	but	rather	some	correlational	rules.	This	
makes	the	detection	of	biases	or	inaccuracies	more	difficult,	as	
well	as	the	explanatory	narrative.	
5	 Discussion	and	conclusive	remarks	
The	use	case	developed	in	this	article	and	the	surrounding	legal	
discussion	provided	on	the	feasibility	of	legal	requirements	on	
algorithmic	 explanations	 illustrate	 the	 consequences	 of	 the	
current	 trend	 to	 apply	 more	 and	 more	 complex	 machine	
learning	models	with	multidimensional	data	in	a	growing	range	
of	 application	 domains.	 This	 trend	 introduces	 a	 level	 of	
complexity	 that	 is	 hard	 to	 reconcile	 with	 the	 existing	 legal	
requirements,	 especially	 with	 regards	 to	 human	 legibility	 of	
explanations	for	non-expert	data	subjects.	
Our	case	study	explores	 the	strong	 implications	of	employing	
sophisticated	AI	models	for	medical	imaging,	a	task	considered	
as	requiring	human-level	cognitive	capabilities	to	comprehend	
the	complexity	of	data.	Our	analysis	suggests	that	in	this	context,	
getting	 generally	 legible	 explanations	 is	 a	 hard	 challenge,	
considering	the	current	state	of	research	from	the	explainable	
AI	community.	This	is	caused	by	an	inherent	complexity	of	such	
systems,	 including	 a	 large	 number	 of	 stacked	 layers,	 whose	
mechanisms	are	guided	by	training	algorithms	relying	on	large	
volume	 of	 data.	 This	 results	 further	 in	 an	 opaqueness	 of	 the	
inner	mechanisms	at	play	inside	the	mathematical	model.	Even	
more	 pronounced	 and	 obvious	 is	 the	 fact	 that	 the	 employed	
deep	neural	network	is	a	purely	predictive	statistical	procedure.	
As	 such,	 there	 is	 no	 guarantee	 that	 the	 provided	 results	 are	
grounded	 in	 causal	 effect.	 Furthermore,	 the	 provided	
explanations	are	themselves	based	on	statistical	techniques	and	
then	indicating	other	feature	correlations	between	image	pixels	
rather	 than	 clear	 medical	 explanations.	 This	 phenomenon	
appears	despite	the	good	performances	the	system	may	obtain	
in	the	testing	phase.		
557
  
 
 
 
 
The	central	question	we	asked	at	the	outset	was	whether	we	can	
achieve	a	degree	of	explanation	for	complex	machine	learning	
models	 in	high-risk	AI	applications	 that	 is	adequate	 following	
the	applicable	EU	data	protection	framework.	
Our	analysis	of	this	case,	 in	one	of	the	most	sensitive	areas	of	
automated	 decision-making	 (COVID-19	 detection)	 identifies	
several	 legal	 and	 technical	 implications.	 In	 particular,	 we	
contextualized	 in	 a	 practical	 scenario	 a	 reasonably	 fair	 and	
comprehensive	approach	to	explanation.	We	propose	different	
forms	 and	 characteristics	 of	 explanation	 at	 the	 same	 time,	
including	the	moment	of	the	disclosure	of	the	explanation	and	
the	audience	of	the	explanation.	
However,	the	practical	implementation	of	this	model	in	our	use-
case	 has	 also	 shown	 several	 drawbacks	 that	 we	 should	
attentively	 analyse.	 First	 of	 all,	 there	 is	 no	 one-size-fit-all	
explanation	 in	practice:	 each	 form	of	 explanation	 (ex	 ante,	ex	
post,	 expert-based	 or	 subject-based,	 more	 or	 less	 granular)	
strongly	depends	on	the	context	at	issue.	Also,	the	possibility	to	
give	a	 satisfactorily	 fair	and	 transparent	explanation	depends	
also	on	 the	possibility	 to	 show	causal	 link	between	 the	 input	
data	and	the	final	decision.	However,	this	is	not	always	possible:	
while	for	tabular	data-based	decision-making	it	might	be	easier	
to	 give	 adequate	 explanations,	 in	 more	 complex	 AI-based	
decisions	 it	 might	 be	 difficult	 to	 reach	 a	 satisfactory	
transparency	 for	 algorithms.	 In	 particular,	 the	 increasing	
employment	 of	 deep	 learning	 in	 more	 and	 more	 forms	 of	
automated	 decisions	 makes	 it	 difficult	 to	 explain	 exactly	 the	
reasons	of	specific	decisions	taken	(see	the	example	of	COVID-
19	automated	diagnosis).	
In	these	last	cases,	the	quality	of	possible	explanations	might	not	
be	 judged	 adequate	 under	 Article	 22(3)	 GDPR.	If	 we	 adopt	 a	
strict	 approach,	 this	 would	 mean	 that	 either	 more	
technologically	 advanced	 (and	 obscure)	 forms	 of	 decision-
making	should	be	prohibited	because	unexplainable,	or	that	we	
should	tolerate	AI-based	decision-making	systems	that	do	not	
formally	respect	the	transparency	duties	of	the	GDPR.		
Actually,	a	possible	third	way	might	be	to	better	contextualise	
the	right	to	explanation	in	the	broader	picture	of	the	GDPR.	In	
particular,	 Article	 22(3)	 and	 recital	 71,	 when	 addressing	 the	
possible	safeguards	to	reach	accountable	automated	decisions,	
do	not	focus	mostly	on	the	right	to	explanation,	but	on	a	varied	
set	of	tools	(including	right	to	contest,	human	in	the	loop	and	
algorithmic	 auditing).	 Beyond	Article	 22	 (and	 recital	 71),	 the	
GDPR	 contains	 several	 notions	 that	 might	 influence	 the	
interpretation	of	accountability	duties	also	in	case	of	automated	
profiling:	 the	 fairness	 principle,	 the	 lawfulness	 principle,	 the	
risk-based	 approach,	 the	 data	 protection	 impact	 assessment	
model,	etc.	
In	 our	 view,	 in	 cases	 where	 the	 possible	 explanation	 (three-
layered,	audience-based,	moment-based)	is	not	satisfactory,	the	
data	 controller	 should	 anyway	 take	 every	 reasonable	 step	 to	
provide	 such	 an	 explanation,	 but	 at	 the	 same	 time	 other	
complimentary	tools	should	be	implemented	[37].	These	might	
include	 disclosing	 meaningful	 information	 about	 a	 Data	
Protection	 Impact	 Assessment	 (DPIA)	 on	 the	 algorithmic	
decision-making	system	(the	DPIA,	as	mentioned	in	Article	35	
of	the	GDPR,	 is	a	process	to	assess	and	mitigate	the	impact	of	
data	processing	operations	on	fundamental	rights	and	freedoms	
of	data	subjects)	[18]	or	the	result	of	a	justification	effect-based	
test	on	the	algorithm,	where	the	data	controller	explains	why	
the	 algorithm	 (analysed	 on	 the	 aggregated	 final	 effects	 on	
different	 data	 subjects,	 but	 also	 analysed	 in	 its	 purposes,	
intentions,	etc.)	is	not	unfair,	unlawful,	inaccurate,	beyond	the	
purpose	 limitation,	 etc.	 (see	 the	 legibility	 test	 in	[21]).	 These	
alternative	tools	are	already	within	the	GDPR.	Analysing	them	
in	detail	 is	beyond	 the	scope	of	 this	paper,	but	we	encourage	
further	research	on	these	Algorithmic	accountability	elements.	
Moreover,	in	cases	where	satisfactory	explanations	are	not	easy	
to	reach	a	human-mediated	explanation	might	be	helpful.	In	this	
case,	the	expert	understanding	of	AI-based	decisions	might	be	
an	intermediary	for	a	dynamic	explanation	of	the	data	subject.	
REFERENCES	
[1]	 Adadi,	 A.	 and	 Berrada,	 M.	 2018.	 Peeking	 inside	 the	 black-box:	 A	
survey	 on	 Explainable	 Artificial	 Intelligence	 (XAI).	 IEEE	 Access.	 6,	
(2018),	52138–52160.	
[2]	 Article	 29	 Data	 Protection	 Working	 Party	 2018.	 Guidelines	 on	
Automated	individual	decision-making	and	Profiling	for	the	purposes	of	
Regulation	2016/679.	
[3]	 Article	 29	 Data	 Protection	 Working	 Party	 2018.	 Guidelines	 on	
transparency	under	Regulation	2016/679.	
[4]	Arya,	V.	et	al.	2019.	One	Explanation	Does	Not	Fit	All:	A	Toolkit	and	
Taxonomy	of	AI	Explainability	Techniques.	(2019).	
[5]	Cohen,	J.P.,	Dao,	L.,	Morrison,	P.,	Roth,	K.,	Bengio,	Y.,	Shen,	B.,	Abbasi,	
A.,	 Hoshmand-Kochi,	 M.,	 Ghassemi,	 M.,	 Li,	 H.	 and	 Duong,	 T.Q.	 2020.	
Predicting	 COVID-19	 Pneumonia	 Severity	 on	 Chest	 X-ray	 with	 Deep	
Learning.	arXiv:2005.11856	[cs,	eess,	q-bio,	stat].	(May	2020).	
[6]	 Doshi-Velez,	 F.	 and	Kim,	 B.	 2017.	 Towards	 A	 Rigorous	 Science	 of	
Interpretable	 Machine	 Learning.	 arXiv	 e-prints.	 (2017),	
arXiv:1702.08608.	
[7]	Došilović,	F.K.,	Brčić,	M.	and	Hlupić,	N.	2018.	Explainable	artificial	
intelligence:	A	survey.	41st	International	convention	on	information	and	
communication	 technology,	 electronics	 and	 microelectronics	 (MIPRO)	
(2018),	0210–0215.	
[8]	 Dupont,	 L.,	 Fliche,	 O.	 and	 Yang,	 S.	 2020.	 Governance	 of	 Artificial	
Intelligence	in	Finance.	
[9]	European	Commission	2020.	White	Paper:	On	Artificial	Intelligence	-	
A	European	approach	to	excellence	and	trust.	European	Commission.	
[10]	 European	 Commission	 High	 Level	 Expert	 Group	 on	 Artificial	
Intelligence	 2019.	 A	 definition	 of	 AI:	 Main	 capabilities	 and	 scientific	
disciplines.	European	Commission.	
[11]	 European	 Commission	 High	 Level	 Expert	 Group	 on	 Artificial	
Intelligence	 2019.	 Ethics	 Guidelines	 for	 Trustworthy	 AI.	 European	
Commission.	
558
  
 
[12]	 European	 Parliament	 and	 Council	 of	 the	 European	 Union	 2016.	
Regulation	 (EU)	 2016/679	 of	 the	 European	 Parliament	 and	 of	 the	
Council	 of	 27	 April	 2016	 on	 the	 protection	 of	 natural	 persons	 with	
regard	to	the	processing	of	personal	data	and	on	the	free	movement	of	
such	data,	and	repealing	Directive	95/46/EC	(General	Data	Protection	
Regulation).	
[13]	 Gebru,	 T.,	 Morgenstern,	 J.,	 Vecchione,	 B.,	 Wortman	 Vaughan,	 J.,	
Wallach,	 H.,	 Daumeé,	 I.,	 Hal	 and	 Crawford,	 K.	 2018.	 Datasheets	 for	
Datasets.	Proceedings	of	the	5th	workshop	on	fairness,	accountability,	and	
transparency	 in	 machine	 learning,	 stockholm,	 sweden,	 PMLR	 (2018),	
arXiv:1803.09010.	
[14]	Guidotti,	R.,	Monreale,	A.,	Ruggieri,	S.,	Turini,	F.,	Giannotti,	F.	and	
Pedreschi,	 D.	 2019.	 A	 survey	 of	 methods	 for	 explaining	 black	 box	
models.	 ACM	 computing	 surveys	 (CSUR).	 51,	 5	 (2019),	 93.	
DOI:https://doi.org/10.1145/3236009.	
[15]	 Halpern,	 J.Y.	 and	 Pearl,	 J.	 2005.	 Causes	 and	 explanations:	 A	
structural-model	 approach.	 Part	 I:	 Causes.	The	 British	 journal	 for	 the	
philosophy	of	science.	56,	4	(2005),	843–887.	
[16]	 Halpern,	 J.Y.	 and	 Pearl,	 J.	 2005.	 Causes	 and	 explanations:	 A	
structural-model	approach.	Part	II:	Explanations.	The	British	journal	for	
the	philosophy	of	science.	56,	4	(2005),	889–911.	
[17]	Kaminski,	M.E.	2019.	The	right	to	explanation,	explained.	Berkeley	
Tech.	LJ.	34,	(2019),	189.	
[18]	Kaminski,	M.E.	 and	Malgieri,	G.	2020.	Multi-layered	explanations	
from	algorithmic	 impact	assessments	 in	 the	GDPR.	Proceedings	of	 the	
2020	 Conference	 on	 Fairness,	 Accountability,	 and	 Transparency	 (New	
York,	NY,	USA,	2020),	68–79.	
[19]	Lepri,	B.,	Oliver,	N.,	Letouzé,	E.,	Pentland,	A.	and	Vinck,	P.	2018.	Fair,	
Transparent,	and	Accountable	Algorithmic	Decision-making	Processes.	
Philosophy	 &	 Technology.	 31,	 4	 (2018),	 611–627.	
DOI:https://doi.org/10.1007/s13347-017-0279-x.	
[20]	 Lipton,	 Z.C.	 2018.	 The	 mythos	 of	 model	 interpretability.	
Communications	of	the	ACM.	61,	10	(2018),	36–43.	
[21]	Malgieri,	 G.	 and	 Comandé,	 G.	 2017.	Why	 a	 Right	 to	 Legibility	 of	
Automated	 Decision-Making	 Exists	 in	 the	 General	 Data	 Protection	
Regulation.	 International	 Data	 Privacy	 Law.	 7,	 4	 (2017),	 243–265.	
DOI:https://doi.org/10.1093/idpl/ipx019.	
[22]	Miller,	T.	2019.	Explanation	in	artificial	intelligence:	Insights	from	
the	social	sciences.	Artificial	Intelligence.	267,	(2019),	1–38.	
[23]	Murdoch,	W.J.,	Singh,	C.,	Kumbier,	K.,	Abbasi-Asl,	R.	and	Yu,	B.	2019.	
Interpretable	machine	learning:	Definitions,	methods,	and	applications.	
arXiv	e-prints.	(2019),	arXiv:1901.04592.	
[24]	Rudin,	C.	2019.	Stop	explaining	black	box	machine	learning	models	
for	high	stakes	decisions	and	use	interpretable	models	instead.	Nature	
Machine	Intelligence.	1,	5	(2019),	206.	
[25]	Strickland,	E.	2020.	AI	Can	Help	Hospitals	Triage	COVID-19	Patients	
-	IEEE	Spectrum.	IEEE	Spectrum:	Technology,	Engineering,	and	Science	
News.	(2020).	
[26]	 Tobler,	 C.	 2008.	 Limits	 and	 potential	 of	 the	 concept	 of	 indirect	
discrimination.	 Office	 for	 Official	 Publications	 of	 the	 European	
Communities.	
[27]	Toussie,	D.,	Voutsinas,	N.,	Finkelstein,	M.,	Cedillo,	M.A.,	Manna,	S.,	
Maron,	S.Z.,	Jacobi,	A.,	Chung,	M.,	Bernheim,	A.,	Eber,	C.,	Concepcion,	J.,	
Fayad,	Z.	and	Gupta,	Y.S.	2020.	Clinical	and	Chest	Radiography	Features	
Determine	 Patient	 Outcomes	 In	 Young	 and	 Middle	 Age	 Adults	 with	
COVID-19.	 Radiology.	 (2020),	 201754.	
DOI:https://doi.org/10.1148/radiol.2020201754.	
[28]	 Wachter,	 S.,	 Mittelstadt,	 B.	 and	 Russell,	 C.	 2017.	 Counterfactual	
Explanations	without	Opening	the	Black	Box:	Automated	Decisions	and	
the	GPDR.	Harv.	JL	&	Tech.	31,	(2017),	841.	
[29]	 Wang,	 L.	 and	 Wong,	 A.	 2020.	 COVID-Net:	 A	 Tailored	 Deep	
Convolutional	Neural	Network	Design	for	Detection	of	COVID-19	Cases	
from	Chest	X-Ray	Images.	(2020).	
[30]	 Wynants,	 L.	 et	 al.	 2020.	 Prediction	 models	 for	 diagnosis	 and	
prognosis	 of	 covid-19	 infection:	 Systematic	 review	 and	 critical	
appraisal.	BMJ.	369,	(2020).	DOI:	https://doi.org/10.1136/bmj.m1328.	
[31]	Lessmann,	N.	et	al.,	2020.	Automated	Assessment	of	CO-RADS	and	
Chest	 CT	 Severity	 Scores	 in	 Patients	with	 Suspected	 COVID-19	Using	
Artificial	 Intelligence.	 Radiology.	 202439.	 DOI:	
10.1148/radiol.2020202439	
[32]	Wittbold,	K.A.	2020.	How	Hospitals	Are	Using	AI	to	Battle	Covid-19.	
Harvard	 Business	 Review	 Online.	 April	 03	 2020.	
https://hbr.org/2020/04/how-hospitals-are-using-ai-to-battle-covid-
19	
[33]	A.	Roig,	‘Safeguards	for	the	right	not	to	be	subject	to	a	decision	
based	 solely	on	 automated	processing	 (Article	22	GDPR)’,	Eur.	 J.	 Law	
Technol.,	vol.	8,	no.	3	
	
[34]	G.	Malgieri,	2019,	‘Automated	decision-making	in	the	EU	Member	
States:	The	right	to	explanation	and	other	“suitable	safeguards”	in	the	
national	 legislations’,	 Comput.	 Law	 Secur.	 Rev.,	 p.	 105327,	 doi:	
10.1016/j.clsr.2019.05.002	
	
[35]	 Selbst	 A.D.	 and	Powles	 J.,	 2017	 ‘Meaningful	 information	 and	 the	
right	to	explanation’,	Int.	Data	Priv.	Law,	vol.	7,	no.	4,	pp.	233–242,	doi:	
10.1093/idpl/ipx022	
	
[36]	 Edwards	 L.	 and	 Veale	M.,	 2017,	 ‘Slave	 to	 the	 Algorithm?	Why	 a	
“Right	to	an	Explanation”	Is	Probably	Not	the	Remedy	You	Are	Looking	
For’,	Duke	Law	Technol.	Rev.,	vol.	16,	no.	1,	pp.	18–84.	
	
[37]	Edwards	L.	and	Veale	M.,	2018,	 ‘Enslaving	the	Algorithm:	From	a	
“Right	to	an	Explanation”	to	a	“Right	to	Better	Decisions”?’,	IEEE	Secur.	
Priv.,	vol.	16,	no.	3,	pp.	46–54,	2018	
	
[38]	Wachter,	S.,	Mittelstadt,	B.	and	L.	Floridi,	L.	2017,	‘Why	a	Right	to	
Explanation	 of	 Automated	 Decision-Making	 Does	 Not	 Exist	 in	 the	
General	Data	Protection	Regulation’,	Int.	Data	Priv.	Law,	vol.	7,	no.	2,	pp.	
76–99,	doi:	10.1093/idpl/ipx005	
	
[39]	Mantelero	A.,	2018,	‘AI	and	Big	Data:	A	blueprint	for	a	human	rights,	
social	and	ethical	impact	assessment’,	Comput.	Law	Secur.	Rev.,	vol.	34,	
no.	4,	pp.	754–772,	doi:	10.1016/j.clsr.2018.05.017	
	
[40]	Veale	M.	and	Edwards	L.,	‘Clarity,	surprises,	and	further	questions	
in	the	Article	29	Working	Party	draft	guidance	on	automated	decision-
making	and	profiling’,	Comput.	Law	Secur.	Rev.,	vol.	34,	no.	2,	pp.	398–
404,	Apr.	2018,	doi:	10.1016/j.clsr.2017.12.002	
	
[41]	Kroll	J.	et	al.,	2017	‘Accountable	Algorithms’,	Univ.	Pa.	Law	Rev.,	vol.	
165,	no.	3,	p.	633	
	
[42]	 Selbst	 A.D.	 and	 Barocas	 S.,	 2018,	 ‘The	 Intuitive	 Appeal	 of	
Explainable	Machines’,	Fordham	Law	Rev.,	vol.	87,	no.	2018,	p.	1085	
559
