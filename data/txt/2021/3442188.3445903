High Dimensional Model Explanations: An Axiomatic Approach
Neel Patel
University of Southern California
neelbpat@usc.edu
Martin Strobel
National University of Singapore
mstrobel@comp.nus.edu.sg
Yair Zick
University of Massachusetts, Amherst
yzick@umass.edu
ABSTRACT
Complex black-box machine learning models are regularly used
in critical decision-making domains. This has given rise to several
calls for algorithmic explainability. Many explanation algorithms
proposed in literature assign importance to each feature individ-
ually. However, such explanations fail to capture the joint effects
of sets of features. Indeed, few works so far formally analyze high
dimensional model explanations. In this paper, we propose a novel
high dimension model explanation method that captures the joint
effect of feature subsets.
We propose a new axiomatization for a generalization of the
Banzhaf index; our method can also be thought of as an approxima-
tion of a black-box model by a higher-order polynomial. In other
words, this work justifies the use of the generalized Banzhaf index
as a model explanation by showing that it uniquely satisfies a set
of natural desiderata and that it is the optimal local approximation
of a black-box model.
Our empirical evaluation of our measure highlights how it man-
ages to capture desirable behavior, whereas other measures that do
not satisfy our axioms behave in an unpredictable manner.
ACM Reference Format:
Neel Patel, Martin Strobel, and Yair Zick. 2021. High Dimensional Model
Explanations: An Axiomatic Approach. In Conference on Fairness, Account-
ability, and Transparency (FAccT ‚Äô21), March 3‚Äì10, 2021, Virtual Event, Canada.
ACM,NewYork, NY, USA, 11 pages. https://doi.org/10.1145/3442188.3445903
1 INTRODUCTION
Machine learning models are applied in a variety of high-stakes
domains, such as healthcare, insurance, credit decisions and more.
In order to offer high prediction accuracy over high-dimensional
data, one must adopt increasingly complex models. Complex mod-
els are, however, more difficult to understand. Explainability has
been recently identified as an important design criterion, and has
received widespread attention within the ML community; of partic-
ular note is research into generating model explanations. Broadly
speaking, model explanations are based on a labeled dataset as well
as other potential sources of information. Most model explanation
techniques focus on feature-based explanations: for each feature
ùëñ , the model explanation outputs a value ùúôùëñ which signifies the
importance of the ùëñ-th feature in determining model predictions.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAccT ‚Äô21, March 3‚Äì10, 2021, Virtual Event, Canada
¬© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8309-7/21/03. . . $15.00
https://doi.org/10.1145/3442188.3445903
The value ùúôùëñ can be thought of as a score ‚Äî ‚ÄòAlice‚Äôs high income
is highly indicative of her receiving a loan‚Äô ‚Äî or as a recourse
‚Äî ‚Äòhad Alice‚Äôs income been lower by $10,000/year her loan would
have been rejected‚Äô. Either way, the basic premise of attribute-based
model explanations is to explain complex model decisions via a
list of ùëõ numerical values, where ùëõ is the number of data features.
Crucially, this approach fails to capture feature interactions. Features
are often strongly interdependent, especially in complex ML models.
For example: consider a black-box model that predicts sentiments
associatedwith a paragraph of text. In such texts; there can be a high
negative interaction effect between ‚Äúnot‚Äù and ‚Äúbad‚Äù, which attribute-
based model explanations will fail to capture: assigning influence
to ‚Äúbad‚Äù and ‚Äúnot‚Äù individually can be misleading. Consider the
following review:
It isn‚Äôt the greatest scifi flick I‚Äôve every
seen, but it is not a bad movie
The above review is classified as a positive sentence. LIME [27] ‚Äî a
type of feature-based explanation ‚Äî assigns high negative influence
to the words ‚Äúbad‚Äù and ‚Äúnot‚Äù. However, reviews omitting the word
‚Äúbad‚Äù or ‚Äúnot‚Äù will be classified as negative. Explanation methods
that indicate feature interactions, rather than just standalone scores,
offer a much clearer picture of how the model makes its decisions.
The current model explanation landscape offers a wide variety of
model explanation methods; how should we pick the right one? In
this paper, we propose an axiomatic approach: rather than starting
with a candidate solution and then offering post-hoc justifications
for using it, start with the properties one would like to have in a
high-dimensional model explanation, and derive a solution that
satisfies these properties. This is the approach we take in this paper.
We axiomatize a high dimensional model explanation method,
called the Banzhaf Interaction Index, or BII. This method faithfully
captures how feature interactions influence model decision-making.
Not only does our proposed model explanation satisfy a set of
desirable properties, it is the only model explanation that satisfies
all these properties.
1.1 Our Contributions
In this work, we propose a method for high-dimensional expla-
nations for black-box models. Our main goal is to axiomatically
capture higher-order feature interaction. Our main contribution
is twofold: first, we propose a high-dimensional black-box model
explanation method, axiomatically capturing higher-order feature
interaction. Our characterization uses properties that are very nat-
ural in the ML context, such as feature symmetry and effect mono-
tonicity (Section 3). we obtain a new axiomatization of the Banzhaf
interaction index which uniquely satisfies symmetry, limit condition,
general-2-efficiency, and a newly proposed axiom (in the context
of Banzhaf indices): monotonicity. Monotonicity is a rather gen-
eral property which essentially means that the model explanation
401
FAccT ‚Äô21, March 3‚Äì10, 2021, Virtual Event, Canada Neel Patel, Martin Strobel, and Yair Zick
should change in a manner faithful to the underlying data. This
is very fundamental property for an interaction measure: which
states that the net contribution of the subset of features for the
machine learning model ùëì is more than that for the model ùëî; then
the interaction measure for those features for model ùëì should be
more than the interaction measure for those features for model ùëî.
Second, we extend the idea of feature-based model explanations,
which can be thought of as a local linear approximations of black-
box models, to higher-order polynomial approximations. Especially,
we show that our proposed measure can be obtained by approximat-
ing the black-box model by a higher-order polynomial (Section 4).
We evaluate our approach on real datasets. In particular, our
experiments show that feature-based model explanations may fail
to capture feature synergy; moreover, we show that other model
explanations which fail to satisfy our axioms tend to behave in an
undesirable manner (Section 5).
1.2 Related Work
While some model explanations provide record-based explanations
[19], or generate explanations from source code [10], the bulk of the
literature on the topic focuses on feature-based model explanations
[2, 5, 9, 11, 30, 32]. Ancona et al. [3] offer an overview of feature
basedmodel explanations for deep neural networks. The connection
to cooperative game theory has been widely discussed and exploited
in order to generate model explanations [2, 9, 11], with a particular
focus on the Shapley value and its variants [28].
Interaction Indices for Cooperative Games: Two widely accepted
measures of marginal influence from cooperative game theory the
Shapley value [28] and the Banzhaf value [6], are uniquely derived
from a set of natural axioms (see also [35]). These measures do not
capture player interactions; rather, they assign weights to individ-
ual players. Owen [24] proposes the first higher-order solution for a
cooperative game for pairwise players. Grabisch and Roubens [16]
extend it to interaction between arbitrary subsets, and axiomatically
derive the Shapley and the Banzhaf interaction indices. In a recent
paper, Agarwal et al. [1] propose a new axiomatization for interac-
tion among players which is inspired by the Taylor approximation
of a Boolean function.
Feature Interactions: Feature interaction has been studied by dif-
ferent communities. Statistics offers classic ANOVA based feature
interaction analysis [14, 15]. Some recent work in the deep learning
literature discusses feature interaction: Tsang et al. [34] learns in-
teractions by inspecting the inter-layer weight matrices of a neural
network, Tsang et al. [33] construct a generalized additive model
that contains feature interaction information. In another line of
work, Cui et al. [8], Greenside et al. [17] compute the interaction
among features by computing the (expected) Hessian; this can be
thought as an extension of gradient-based influence measures for
neural networks [3]. Datta et al. [11] also propose an influence
measure for a set of features called Set-QII. It essentially measures
the change in model output when we randomly change a fixed set
of features. Lundberg et al. [21] propose the Shapley interaction
index, specifically for tree-based models.
2 PRELIMINARIES
We denote sets with capital lettersùê¥, ùêµ, . . . and use lowercase letters
for functions, scalars, and features. To minimize notation clutter, we
try to omit braces for singletons, pairs etc., e.g. we write ùëì (ùëñ), ùëÜ ‚à™ ùëñ
instead of ùëì ({ùëñ}), ùëÜ ‚à™ {ùëñ} and ùëÜ ‚à™ ùëñ ùëó instead of ùëÜ ‚à™ {ùëñ, ùëó}.
Let ùëÅ = {1, . . . , ùëõ} be the set of features. A black-box model is
a function mapping a set of ùëõ-dimensional input vectors X ‚äÜ Rùëõ
to R. Our objective is to generate a model explanation for a given
point of interest (POI) ¬Æùë• ‚àà Rùëõ ; this explanation should (ideally) offer
stakeholders some insight into the underlying decision-making
process that ultimately resulted in the outcome they receive. In this
work, we are interested in measuring the extent to which features,
and their high-order interactions affect model decisions. In order to
measure feature interaction effects, we adopt the baseline compar-
ison approach [22, 31]; in other words, we assume the existence
of a baseline vector ¬Æùë• ‚Ä≤, to which we compare an input vector ¬Æùë• , in
order to generate a model explanation. For example, in the auto-
matic loan acceptance/rejection domain ¬Æùë• ‚Ä≤ could correspond to an
all-zero vector (e.g. measuring the effect of an applicant having no
money in their bank account, as opposed to the true amount they
have), or a vector of mean values (e.g. comparing the applicant‚Äôs
true income to the average population income).
In order to generate model explanations, we need to formally
reason about the effect of changing features in the POI ¬Æùë• to their
baseline values. Generally speaking, changing a single feature may
have no significant effect on the model prediction. For example,
if ùëì (income = 20ùëò, debt = 90ùëò) = 1, it may well be the case that
changing either the applicant‚Äôs low income (20ùëò) or their high debt
(90ùëò) would not result in them receiving the loan, however, it is
unreasonable to claim that neither had an effect on the outcome.
To formally reason about the joint effect of features, we define a
function measuring their value as a set. Given a set of features
ùëÜ ‚äÜ ùëÅ , a point of interest ¬Æùë• , a baseline ¬Æùë• ‚Ä≤ and a model ùëì , we define
a set function as
ùë£ (ùëÜ, ¬Æùë•, ¬Æùë• ‚Ä≤, ùëì ) = ùëì ( ¬Æùë•ùëÜ , ¬Æùë• ‚Ä≤ùëÅ \ùëÜ ) ‚àí ùëì ( ¬Æùë• ‚Ä≤) . (1)
In other words, the value we assign to a set of features ùëÜ is the
extent to which they cause the model prediction to deviate from the
baseline prediction; as a sanity check, we note that ùë£ (‚àÖ, ¬Æùë•, ¬Æùë• ‚Ä≤, ùëì ) =
0, and ùë£ (ùëÅ, ¬Æùë•, ¬Æùë• ‚Ä≤, ùëì ) = ùëì ( ¬Æùë•) ‚àí ùëì ( ¬Æùë• ‚Ä≤). This formulation induces a
cooperative game, where features correspond to players. We refer
to the game defined in (1) as the feature effect game. We omit ¬Æùë• , ¬Æùë• ‚Ä≤
and ùëì (they will normally be fixed), focusing solely on the set of
features ùëÜ .
We often replace sets of features with a single feature demarcat-
ing the entire set: given a set ùëá ‚äÜ ùëÅ , [ùëá ] denotes a single feature
corresponding to the set. The reduced game w.r.t. the nonempty
ùëá ‚äÜ ùëÅ is defined on the features ùëÅ \ùëá ‚à™ [ùëá ] with the characteristic
function ùë£ [ùëá ] (¬∑, ùëì ) : 2ùëÅ \ùëá‚à™{[ùëá ] } ‚Üí R:
ùë£ [ùëá ] (ùëÜ) =
{
ùë£ (ùëÜ ‚à™ùëá ) [ùëá ] ‚àà ùëÜ
ùë£ (ùëÜ) otherwise
Our objective is to generate high dimensional model explanations,
i.e. functions that assign a value to every subset of features ùëÜ ‚äÜ ùëÅ .
To do so, we define a feature interaction index ùëÜ ‚äÜ ùëÅ for ùë£ as ùêº ùë£ (ùëÜ).
In other words, under the game ùë£ ‚Äî namely the game defined in
402
High Dimensional Model Explanations: An Axiomatic Approach FAccT ‚Äô21, March 3‚Äì10, 2021, Virtual Event, Canada
(1) ‚Äî ùêº ùë£ (ùëÜ) should roughly capture the overall effect that the set of
features ùëÜ has on the value of ùë£ . Going back to the feature effect
game defined in (1), ùêº ùë£ (ùëÜ) should measure the degree to which
switching the value of features in ùëÜ back to their baseline values
affects the prediction for ¬Æùë• . A key idea in our analysis is marginal
effect: consider a single feature ùëñ ‚àà ùëÅ . Its marginal effect on a
set ùëá ‚äÜ ùëÅ \ {ùëñ} equals ùë£ (ùëá ‚à™ ùëñ) ‚àí ùë£ (ùëá ): the extent to which the
value of ùë£ (ùëá ) changes when ùëñ joins ùëá . The marginal effect of ùëñ on ùëá
is denotedùëöùëñ (ùëá ). In the context of the feature importance game,
ùëöùëñ (ùëá, ùë£) is the marginal effect of knowing the feature ùëñ , given that
the values of features in the setùëá are known. This is similar to other
definitions considered in the literature [11, 22]. When considering
a pair of features ùëñ, ùëó ‚àà ùëÅ , how would one define their marginal
effect on a coalition ùëá ‚äÜ ùëÅ ? One very natural definition is to offset
the effect of adding both features to ùëá by the marginal effects of
adding ùëñ and ùëó separately, i.e.
ùëöùëñ ùëó (ùëá, ùë£) = (ùë£ (ùëá ‚à™ ùëñ ùëó) ‚àí ùë£ (ùëá )) ‚àí (ùë£ (ùëá ‚à™ ùëñ) ‚àí ùë£ (ùëá )) ‚àí (ùë£ (ùëá ‚à™ ùëó) ‚àí ùë£ (ùëá ))
=ùë£ (ùëá ‚à™ ùëñ ùëó) ‚àí ùë£ (ùëá ‚à™ ùëñ) ‚àí ùë£ (ùëá ‚à™ ùëó) + ùë£ (ùëá )
We can define the marginal contribution of a general ùëÜ ‚äÜ ùëÅ , in a
similar manner. LetùëöùëÜ (ùëá, ùë£) be:
ùëöùëÜ (ùëá, ùë£) =
‚àëÔ∏Å
ùêø‚äÜùëÜ
(‚àí1) |ùëÜ |‚àí |ùêø |ùë£ (ùëá ‚à™ ùêø) .
This is also known as the ùëÜ discrete derivative of ùë£ at ùëá (echoing the
inclusion-exclusion principle). We note that when ùëá = ‚àÖ,ùëöùëÜ (‚àÖ, ùë£)
is the Harsanyi dividend of ùëÜ , a well-known measure of the synergy
(or surplus) generated by ùëÜ [26]. Forùëá ‚â† ‚àÖ,ùëöùëÜ (ùëá, ùë£) can be thought
of as the added value of having the coalition ùëÜ form, given that the
players in ùëá have already committed to joining. More generally,
ùëöùëÜ (ùëá, ùë£) represents the marginal interaction between features in ùëÜ
within the set of features ùëá ‚à™ ùëÜ .
3 AXIOMATIC CHARACTERIZATION OF
GOOD MODEL EXPLANATIONS
As previously discussed, our objective is to identify high-quality
model explanations. When pursuing quality metrics for a model
explanation, one can take one of two approaches: either show that
the model explanation is the optimal solution to some target (e.g.
minimizes some loss function) [27, 30], or that it satisfies a set of de-
sirable properties [9, 11, 30]. We take both approaches in this work,
resulting in a unique and optimal high-dimensional explanation;
this is the Banzhaf interaction index (BII) [16]. Given a cooperative
game ùë£ , the Banzhaf interaction index for a subset ùëÜ ‚äÜ ùëÅ is
ùêº ùë£BII (ùëÜ) = 1
2ùëõ‚àí|ùëÜ |
‚àëÔ∏Å
ùëá ‚äÜùëÅ \ùëÜ
‚àëÔ∏Å
ùêø‚äÜùëÜ
(‚àí1) |ùëÜ |‚àí |ùêø |ùë£ (ùêø ‚à™ùëá )
=
1
2ùëõ‚àí|ùëÜ |
‚àëÔ∏Å
ùëá ‚äÜùëÅ \ùëÜ
ùëöùëÜ (ùëá, ùë£) (2)
In words, ùêº ùë£BII (ùëÜ) equals ùëÜ ‚Äôs expectedmarginal contribution to a set
ùëá ‚äÜ ùëÅ \ùëÜ , sampled uniformly at random. Grabisch and Roubens [16]
show that BII uniquely satisfies Linearity, Symmetry, Dummy, the
Recursive Property, Generalized 2-Efficiency, and the Limit Condition.
Theorem 3.2 proposes a ‚Äòleaner‚Äô axiomatization of BII , which, as
we argue, is more sensible in the model explanation setting. We
show that BII is the unique measure which satisfies four natural
axioms: Symmetry,Generalized 2-Efficiency, the Limit Condition, and
Monotonicity. The first three axioms are fairly standard assumptions
in identifying ‚Äògood‚Äô solutions, generalized to interaction indices
by [16].
Symmetry (S): for any permutation ùúã over ùëÅ we have that
ùêº ùë£ (ùëÜ) = ùêºùúãùë£ (ùúãùëÜ). Here, ùúãùëÜ equals {ùúã (ùëñ) : ùëñ ‚àà ùëÜ}, and ùúãùë£ is
the game where the value of a coalition ùëá equals ùë£ (ùúã‚àí1ùëá ).
Symmetry is a natural property for any interaction mea-
sure: intuitively, it simply stipulates that features‚Äô interac-
tion value is independent of their identity, and depends only
on their intrinsic coalitional worth.
Generalized 2-Efficiency (GE): for any ùëñ, ùëó ‚àà ùëÅ , and for any
ùëÜ ‚äÜ ùëÅ \ ùëñ ùëó :
ùêº ùë£[ùëñ ùëó ] (ùëÜ ‚à™ [ùëñ ùëó]) = ùêº ùë£ (ùëÜ ‚à™ ùëñ) + ùêº ùë£ (ùëÜ ‚à™ ùëó) .
Intuitively, merging two features into one feature encod-
ing the same information results in no additional influence.
Generalized 2-Efficiency extends the 2-Efficiency axiom pro-
posed by [20] to characterize the Banzhaf value.
Limit Condition (L): if ùëÅ is the set of players of the game
ùë£ then ùêº ùë£ (ùëÅ ) = ùëöùëÅ (‚àÖ, ùë£) =
‚àë
ùêø‚äÜùëÅ (‚àí1)ùëõ‚àí|ùêø |ùë£ (ùêø). In other
words, the interaction value of the set ùëÅ equals exactly the
added value of it forming, given that no subsets of players
have pre-committed themselves to joining.
Next, let us introduce the notion of monotonicity for interaction
indices.
Monotonicity (M): If ‚àÄùëá ‚äÜ ùëÅ \ùëÜ ,ùëöùëÜ (ùëá, ùë£1) ‚â• ùëöùëÜ (ùëá, ùë£2) and
for some ùëá ‚àó ‚äÜ ùëÅ \ ùëÜ strict inequality holds then ùêº ùë£1 (ùëÜ) >
ùêº ùë£2 (ùëÜ). Moreover, if ‚àÄùëá ‚äÜ ùëÅ \ ùëÜ ,ùëöùëÜ (ùëá, ùë£1) =ùëöùëÜ (ùëá, ùë£2) then
ùêº ùë£1 (ùëÜ) = ùêº ùë£2 (ùëÜ). This idea extends the strong monotonicity
axiom proposed by [35], which states that if ùëöùëñ (ùëá, ùë£1) ‚â•
ùëöùëñ (ùëá, ùë£2) for all ùëá ‚äÜ ùëÅ \ ùëñ then ùêº ùë£1 (ùëñ) ‚â• ùêº ùë£2 (ùëñ).
Datta et al. [11] argue that the monotonicity axiom is better
suited for charactering model explanations than the more ‚Äòstandard‚Äô
linearity axiom used in the classic characterization of the Shapley
value [28], as well as the original BII characterization [16].
3.1 Characterization Result
First, we show that the four axioms we propose imply a generalized
version of the dummy property in Lemma 3.1. The main result
of this section is Theorem 3.2 that characterizes our explanation
method uniquely.
Lemma 3.1. If ùêº ùë£ satisfies (S), (GE), (L) and (M) then ifùëöùëÜ (ùëá, ùë£) = 0
‚àÄùëá ‚äÜ ùëÅ \ ùëÜ then ùêº ùë£ (ùëÜ) = 0.
Proof. We begin by showing a weaker claim: if ùëî is a null game
where ùëî(ùëÜ) = 0 for all ùëÜ ‚äÜ ùëÅ , then ùêºùëî (ùëÜ) = 0 for all ùëÜ ‚äÜ ùëÅ .
For a given null game ùëî, we have ùëöùëÜ (ùëá,ùëî) = 0 for all ùëÜ ‚äÜ ùëÅ
and ùëá ‚äÜ ùëÅ \ ùëÜ . By symmetry, ùêºùëî (ùëÜ1) = ùêºùëî (ùëÜ2) for all ùëÜ1, ùëÜ2 ‚äÜ ùëÅ
with |ùëÜ1 | = |ùëÜ2 |, because for any permutation ùúã , ùëî = ùúãùëî. Also, by
the Limit Condition(L), ùêºùëî (ùëÅ ) = ùëöùëÅ (‚àÖ, ùëî) = 0. Similarly, for all
ùëñ1 ‚â† ùëñ2 ‚àà ùëÅ , ùêºùëî [ùëñ1,ùëñ2 ] (ùëÅ \ ùëñ1ùëñ2 ‚à™ [ùëñ1, ùëñ2]) = 0. In fact, this property
holds for all ùëâ ‚äÇ ùëÅ : ùêºùëî [ùëâ ] (ùëÅ \ ùëâ ‚à™ [ùëâ ]) = 0. Now we use (GE)
property for ùêºùëî [ùëâ ] (ùëÅ \ùëâ ‚à™ [ùëâ ]) by sequentially removing all ùëò ‚àà ùëâ
403
FAccT ‚Äô21, March 3‚Äì10, 2021, Virtual Event, Canada Neel Patel, Martin Strobel, and Yair Zick
until it becomes a singleton. First, for all ùëò1 ‚àà ùëâ ;
0 = ùêºùëî [ùëâ ] (ùëÅ \ùëâ ‚à™ [ùëâ ]) = ùêºùëî [ùëâ \ùëò1 ] (ùëÅ \ùëâ ‚à™ [ùëâ \ ùëò1])+
ùêºùëî [ùëâ \ùëò1 ] (ùëÅ \ùëâ ‚à™ ùëò1) = 2ùêºùëî [ùëâ \ùëò1 ] (ùëÅ \ùëâ ‚à™ [ùëâ \ ùëò1])
The second equality holds because of symmetry (S) property for
the game ùëî [ùëâ \ùëò1 ] . Now for ùëò2 ‚àà ùëâ \ ùëò1; we similarly use the (GE)
property to obtain ùêºùëî [ùëâ \ùëò1 ] (ùëÅ \ùëâ ‚à™ [ùëâ \ ùëò1]) = 2ùêºùëî [ùëâ \ùëò1 ] (ùëÅ \ùëâ ‚à™
[ùëâ \ ùëò1ùëò2]). We repeat this argument until only one element is left.
We get 0 = ùêºùëî [ùëâ ] (ùëÅ \ùëâ ‚à™ [ùëâ ]) = 2 |ùëâ |‚àí1ùêºùëî (ùëÅ \ùëâ ‚à™ùëò). This equality
holds for all ùëâ ‚äÜ ùëÅ and all ùëò ‚àà ùëâ . Which shows that for all ùëÜ ‚äÜ ùëÅ ,
ùêºùëî (ùëÜ) = 0.
Now, to prove the second part of the lemma, consider any game
ùë£ . If ùëöùëÜ (ùëá, ùë£) = 0 for all ùëá ‚äÜ ùëÅ \ ùëÜ , then ùëöùëÜ (ùëá, ùë£) = ùëöùëÜ (ùëá,ùëî)
for all ùëá ‚äÜ ùëÅ \ ùëÜ , therefore by the Monotonicity property(M),
ùêº ùë£ (ùëÜ) = ùêºùëî (ùëÜ) = 0, which concludes the proof. ‚ñ°
Before we prove the main theorem, we define important terms
which will be helpful to prove the theorem.
Given a set of features ùëÖ ‚äÜ ùëÅ , we define the primitive game ùëùùëÖ as:
ùëùùëÖ (ùëÜ) =
{
1, if ùëÖ ‚äÜ ùëÜ
0, else
The set BùëÅ of all Boolean functions forms a vector space, and the
set of primitive games PùëÅ = {ùëùùëÖ : ùëÖ ‚äÜ ùëÅ } of all primitive games
is an orthonormal basis of the vector space BùëÅ . Therefore, any
Boolean function ùë£ (In this context a cooperative game ùë£) is uniquely
represented by a linear combination of primitive games. In other
words, given a cooperative game ùë£ : 2ùëÅ ‚Üí R, we can uniquely
write as a linear combination of primitive games:
ùë£ (¬∑) =
‚àëÔ∏Å
ùëÖ‚äÜùëÅ
ùëêùëÖùëù
ùëÖ (¬∑) (3)
The unique decomposition of cooperative games is useful for our
characterization of high-dimensional model explanations. We also
require some technical claims which will be useful for our charac-
terization result (for proofs see supplementary material [25]).
Theorem 3.2. BII is the only high-dimensional model explanation
satisfying (S),(GE),(L) and (M).
Proof. We first show that BII satisfies all the properties. BII
trivially satisfies (S), (L) and (M). To show that it satisfies (GE), take
any ùëÜ ‚äÜ ùëÅ \ ùëñ ùëó
ùêº ùë£BII (ùëÜ ‚à™ ùëñ) = 1
2ùëõ‚àíùë†‚àí1
‚àëÔ∏Å
ùëá ‚äÜùëÅ \(ùëÜ‚à™ùëñ)
ùëöùëÜ‚à™ùëñ (ùëá, ùë£)
=
1
2ùëõ‚àíùë†‚àí1
‚àëÔ∏Å
ùëá ‚äÜùëÅ \(ùëÜ‚à™ùëñ ùëó)
[ùëöùëÜ (ùëá ‚à™ ùëñ, ùë£) ‚àíùëöùëÜ (ùëá, ùë£)]
+ 1
2ùëõ‚àíùë†‚àí1
‚àëÔ∏Å
ùëá ‚äÜùëÅ \(ùëÜ‚à™ùëñ ùëó)
[ùëöùëÜ (ùëá ‚à™ ùëñ ùëó, ùë£) ‚àíùëöùëÜ (ùëá ‚à™ ùëó, ùë£)]
A similar calculation for ùëó shows that
ùêº ùë£BII (ùëÜ ‚à™ ùëó) = 1
2ùëõ‚àíùë†‚àí1
‚àëÔ∏Å
ùëá ‚äÜùëÅ \(ùëÜ‚à™ùëñ ùëó)
[ùëöùëÜ (ùëá ‚à™ ùëó, ùë£) ‚àíùëöùëÜ (ùëá, ùë£)]
+ 1
2ùëõ‚àíùë†‚àí1
‚àëÔ∏Å
ùëá ‚äÜùëÅ \(ùëÜ‚à™ùëñ ùëó)
[ùëöùëÜ (ùëá ‚à™ ùëñ ùëó, ùë£) ‚àíùëöùëÜ (ùëá ‚à™ ùëñ, ùë£)]
Thus, ùêº ùë£BII (ùëÜ ‚à™ ùëñ) + ùêº ùë£BII (ùëÜ ‚à™ ùëó) equals
1
2ùëõ‚àíùë†‚àí2
√ó
‚àëÔ∏Å
ùëá ‚äÜùëÅ \(ùëÜ‚à™ùëñ ùëó)
[ùëöùëÜ (ùëá ‚à™ ùëñ ùëó, ùëì ) ‚àíùëöùëÜ (ùëá, ùëì )] (4)
Equation (4) shows that ùêº ùë£[ùëñ ùëó ] (ùëÜ ‚à™ [ùëñ ùëó]) = ùêº ùë£ (ùëÜ ‚à™ ùëñ) + ùêº ùë£ (ùëÜ ‚à™ ùëó).
BII satisfies the four axioms; to show that it uniquely satisfies
them, we use the fact that ùë£ can be uniquely expressed as the sum
of primitive games;
ùë£ =
‚àëÔ∏Å
ùëÖ‚äÜùëÅ
ùê∂ùëÖùëù
ùëÖ (5)
We define the index Œì of a cooperative game ùë£ to be the minimum
number of terms in the expression of the form (5). We prove the
theorem by induction on Œì. For Œì = 0, in Lemma 3.1, ùêº ùë£ (ùëÜ) = 0 for
all ùëÜ ‚äÜ ùëÅ , which coincides with the Banzhaf interaction index.
If Œì = 1 then ùë£ = ùê∂ùëÖùëù
ùëÖ for some ùëÖ ‚äÜ ùëÅ . Consider ùëÜ ‚äà ùëÖ;
Proposition A1 in the supplementary material [25] implies that
ùëöùëÜ (ùëá, ùë£) = 0 for all ùëá ‚äÜ ùëÅ , which in turn implies ùêº ùë£ (ùëÜ) = 0 =
ùêº ùë£BII (ùëÜ). By Lemma A3 in the supplementary material [25], ùêº ùë£ (ùëÖ) =
ùê∂ùëÖ , which equals ùêº ùë£BII (ùëÖ) by Proposition A2 in the supplementary
material [25]. To complete the proof for the first inductive step,
we need to show that for all ùëÜ ‚äÜ ùëÖ, ùêº ùë£ (ùëÜ) = ùêº ùë£BII (ùëÜ) =
ùê∂ùëÖ
2|ùëÖ |‚àí|ùëÜ | .
If ùëÜ1, ùëÜ2 ‚äÜ ùëÖ and ùë†1 = ùë†2 then by symmetry, ùêº ùë£ (ùëÜ1) = ùêº ùë£ (ùëÜ2): we
can define a permutation ùúã over ùëÅ such that ùëÜ2 bijectively maps
to some ùëÜ1, and all ùëñ ‚àâ ùëÜ1 ‚à™ ùëÜ2 : are invariant. By the Symmetry
property ùêº ùë£ (ùëÜ2) = ùêºùúãùë£ (ùëÜ1); however, ùúãùë£ = ùë£ because ùë£ = ùê∂ùëÖùëù
ùëÖ .
Now, consider any ùëñ1 ‚â† ùëñ2 ‚àà ùëÖ and define ùëÜ := ùëÖ \ {ùëñ1, ùëñ2}; by the
GE property, we can write
ùêº ùë£[ùëñ1ùëñ2 ] (ùëÜ ‚à™ [ùëñ1ùëñ2]) = ùêº ùë£ (ùëÜ ‚à™ ùëñ1) + ùêº ùë£ (ùëÜ ‚à™ ùëñ2)
which implies for anyùëÑ ‚äÇ ùëÖ with |ùëÑ | = |ùëÖ |‚àí1, ùêº ùë£ (ùëÑ) = 1
2 ùêº
ùë£[ùëñ1ùëñ2 ] (ùëÜ‚à™
[ùëñ1ùëñ2]). The reduced game ùë£ [ùëñ1ùëñ2 ] is
ùë£ [ùëñ1ùëñ2 ] (ùëÜ ‚Ä≤) =
{
ùê∂ùëÖ, if ùëÜ ‚à™ [ùëñ1ùëñ2] ‚äÜ ùëÜ ‚Ä≤
0, else
By Lemma A3 in the supplementary material [25], ùêº ùë£[ùëñ1ùëñ2 ] (ùëÜ ‚à™
[ùëñ1ùëñ2]) = ùê∂ùëÖ and ùêº ùë£ (ùëÑ) = ùê∂ùëÖ
2 . This property holds for all ùëá ‚äÇ ùëÅ ,
ùë£ [ùëá ] ; ùêº ùë£[ùëá ] (ùëÅ \ùëá ‚à™ [ùëá ]) = ùê∂ùëÖ . By inductively using the (GE) prop-
erty, in a manner similar to Lemma A3 in the supplementary mate-
rial [25], we show that ùêº ùë£ (ùëÑ) = ùê∂ùëÖ
2|ùëÖ |‚àí|ùëÑ | . By Proposition A2 in the
supplementary material [25], this coincides with Banzhaf interac-
tion index concluding the first inductive step.
To complete the proof, assume that ùêº ùë£ (ùëÜ) coincides with the
Banzhaf interaction index whenever the index of the game ùë£ is at
most Œì = ùõæ . Suppose that ùë£ has an index ùõæ + 1, and expressed as
ùë£ =
ùõæ+1‚àëÔ∏Å
ùëò=1
ùê∂ùëÖùëòùëù
ùëÖùëò
Let ùëÖ =
ùõæ+1‚ãÇ
ùëò=0
ùëÖùëò , and suppose that ùëÜ ‚äà ùëÖ. We define another gameùë§ :
ùë§ =
‚àëÔ∏Å
ùëò :ùëÜ‚äÜùëÖùëò
ùê∂ùëÖùëòùëù
ùëÖùëò
Since ùëÜ ‚äà ùëÖ, the index ofùë§ is strictly smaller than ùõæ + 1. We claim
that for all ùëá ‚äÜ ùëÅ \ ùëÜ ;ùëöùëÜ (ùëá, ùë£) =ùëöùëÜ (ùëá,ùë§). Indeed, consider any
404
High Dimensional Model Explanations: An Axiomatic Approach FAccT ‚Äô21, March 3‚Äì10, 2021, Virtual Event, Canada
ùëá ‚äÜ ùëÅ \ ùëÜ ;ùëöùëÜ (ùëá, ùë£) equals‚àëÔ∏Å
ùêø‚äÜùëÜ
(‚àí1) |ùëÜ |‚àí |ùêø |ùë£ (ùëá ‚à™ ùêø) =
‚àëÔ∏Å
ùêø‚äÜùëÜ
(‚àí1) |ùëÜ |‚àí |ùêø |
ùõæ+1‚àëÔ∏Å
ùëò=1
ùê∂ùëÖùëòùëù
ùëÖùëò (ùëá ‚à™ ùêø) =
ùõæ+1‚àëÔ∏Å
ùëò=1
‚àëÔ∏Å
ùêø‚äÜùëÜ
(‚àí1) |ùëÜ |‚àí |ùêø |ùê∂ùëÖùëòùëù
ùëÖùëò (ùëá ‚à™ ùêø) =
ùõæ+1‚àëÔ∏Å
ùëò=1
ùê∂ùëÖùëòùëöùëÜ (ùëá, ùëùùëÖùëò ) =‚àëÔ∏Å
ùëò :ùëÜ‚äÜùëÖùëò
ùê∂ùëÖùëòùëöùëÜ (ùëá, ùëùùëÖùëò ) =ùëöùëÜ (ùëá,ùë§)
The second-last equality holds by Proposition A1 in the supple-
mentary material [25], hence by induction on Œì and monotonic-
ity(M) ùêº ùë£ (ùëÜ) coincides with BII for all ùëÜ ‚äà ùëÖ.
It remains to show that ùêº ùë£ (ùëÜ) coincides with BII when ùëÜ ‚äÜ ùëÖ.
for any ùëÜ ‚äÜ ùëÖ, consider any ùëñ ‚àà ùëÜ and define ùëÜ ‚Ä≤ := ùëÜ \ ùëñ . Take any
ùëó ‚àà ùëÅ such that ùëó1 ‚àà ùëÖ1 \ ùëÖ2 ‚à™ ùëÖ2 \ ùëÖ1. By the (GE) property, we
can write
ùêº ùë£ (ùëÜ) = ùêº ùë£[ùëñ ùëó1 ] (ùëÜ ‚Ä≤ ‚à™ [ùëñ ùëó1]) ‚àí ùêº ùë£ (ùëÜ ‚Ä≤ ‚à™ ùëó1) (6)
In Equation (6), ùëÜ ‚Ä≤ ‚à™ ùëó1 ‚äà ùëÖ, therefore as previously shown,
ùêº ùë£ (ùëÜ ‚Ä≤‚à™ ùëó1) coincideswith BII for the game ùë£ . Consider the restricted
game ùë£ [ùëñ ùëó1 ] :
ùë£ [ùëñ ùëó ] =
ùõæ+1‚àëÔ∏Å
ùëò=0
ùê∂ùëÖùëòùëù
ùëÖùëò\ùëñ ùëó1‚à™[ùëñ ùëó1 ]
[ùëñ ùëó1 ]
Consider ùëó2 ‚â† ùëó1 ‚àà ùëÖ1 \ ùëÖ2 ‚à™ ùëÖ2 \ ùëÖ1. By the (GE) property,
ùêº ùë£[ùëñ ùëó1 ùëó2 ] (ùëÜ ‚Ä≤ ‚à™ [ùëñ ùëó1 ùëó2]) = ùêº ùë£[ùëñ ùëó1 ] (ùëÜ ‚Ä≤ ‚à™ [ùëñ ùëó1]) + ùêº ùë£[ùëñ ùëó1 ] (ùëÜ ‚Ä≤ ‚à™ ùëó2) (7)
In Equation (7), ùëÜ ‚Ä≤ ‚à™ ùëó2 ‚äà
ùêº+1‚ãÇ
ùëò=1
ùëÖùëò \ ùëñ ùëó1 ‚à™ [ùëñ ùëó1], therefore as we
have shown before, ùêº ùë£[ùëñ ùëó1 ] (ùëÜ ‚Ä≤ ‚à™ ùëó2) = ùêº
ùë£[ùëñ ùëó1 ]
BII (ùëÜ ‚Ä≤ ‚à™ ùëó2). Let us denote
ùëá = ùëÖ1 \ ùëÖ2 ‚à™ ùëÖ2 \ ùëÖ1 and ùëá ‚Ä≤ = ùëñ ‚à™ùëá . By repeating this argument
for all ùëó3, . . . , ùëóùë° ‚àà ùëá and exploiting the (GE) property for each ùëó‚Ñì ,
we can write ùêº ùë£ (ùëÜ) as:
ùêº ùë£ (ùëÜ) = ùêº ùë£[ùëá ‚Ä≤ ] (ùëÜ ‚Ä≤ ‚à™ [ùëá ‚Ä≤]) ‚àí ùêº ùë£ (ùëÜ ‚Ä≤ ‚à™ ùëó1) ‚àí
ùë°‚àëÔ∏Å
ùëô=1
ùêº ùë£[ùëñ ùëó1 ... ùëó‚Ñì ] (ùëÜ ‚Ä≤ ‚à™ ùëó‚Ñì )
(8)
All of the summands in (8) coincide with BII , because ùëÜ ‚Ä≤‚à™ ùëó‚Ñì ‚äà
ùêº+1‚ãÇ
ùëò=1
ùëÖùëò \ ùëñ ùëó1 . . . ùëó‚Ñì‚àí1 ‚à™ [ùëñ ùëó1 . . . ùëó‚Ñì‚àí1] for all ‚Ñì = 1, . . . , ùë° . We can write
the reduced game ùë£ [ùëá ‚Ä≤ ] as
ùë£ [ùëá ‚Ä≤ ] = (ùê∂ùëÖ1 +ùê∂ùëÖ2 )ùëù (ùëÖ1‚à©ùëÖ2\ùëñ)‚à™[ùëá ‚Ä≤ ]
[ùëá ‚Ä≤ ] +
ùõæ+1‚àëÔ∏Å
ùëò=3
ùê∂ùëÖùëòùëù
(ùëÖùëò\ùëá ‚Ä≤)‚à™[ùëá ‚Ä≤ ]
[ùëá ‚Ä≤ ]
Thus, the index of the reduced game ùë£ [ùëá ‚Ä≤ ] is strictly smaller than
ùõæ + 1. By induction, ùêº ùë£[ùëá ‚Ä≤ ] (ùëÜ ‚Ä≤ ‚à™ [ùëâ ]) coincides with BII for the
reduced game ùë£ [ùëá ‚Ä≤ ] . ùêº ùë£ (ùëÜ) can be written as
ùêº ùë£ (ùëÜ) = ùêº ùë£[ùëá ‚Ä≤ ] (ùëÜ ‚Ä≤ ‚à™ [ùëá ‚Ä≤]) ‚àí
ùë°‚àëÔ∏Å
ùëô=1
ùêº ùë£[ùëñ ùëó1 ... ùëó‚Ñì ] (ùëÜ ‚Ä≤ ‚à™ ùëó‚Ñì )
By using the (GE) property inductively, ùêº ùë£BII (ùëÜ) can also be writ-
ten in the same form, which implies that ùêº ùë£ (ùëÜ) coincides with the
Banzhaf interaction index for all, ùëÜ ‚äÜ ùëÖ. ‚ñ°
3.2 Explaining Our Model Explanations
Does the BII measure make sense in the model explanation do-
main? This is purely a function of the strength of the axioms we set
forth. Symmetry is natural enough: if a model explanation depends
on the indices of its features then it fails a basic validity test. The
index in which a feature appears has no bearing on the underlying
trained model (ideally), nor does it affect the outcome.
Recall that Generalized Efficiency requires that model explana-
tions should be invariant under feature merging. In other words -
artificially treating a pair of features as a single entity (while main-
taining the same underlying model) should not have any effect on
how feature behaviors are explained. Interestingly, Shapley values
are not invariant under feature merging, a result shown by Lehrer
[20]. The following examples illustrate what this entails in actual
applications.
Example 3.3. Consider an sentiment analysis task where a model
predict if a movie review was positive. In a preliminary step the
text is parsed by a parser to be machine readable. This can be done
in many different ways. For example the sentence ‚ÄúThis isn‚Äôt a
absolutely terrible movie‚Äù Can be parsed as
| This | isn‚Äôt | a | absolutely | terrible | movie | . |
or as
| This | is | n‚Äôt | a | absolut | ely | terrible | movie | . |.
Generalized Efficiency ensures that the influences of ‚Äúis‚Äù and ‚Äún‚Äôt‚Äù
in the second version add up to the influence of ‚Äúisn‚Äôt‚Äù in the first.
In other words, Generalized Efficiency ensures that the influence of
features generated through different parsers behaves in a sensible
manner.
Example 3.4. Features might be ‚Äúmerged‚Äù in another situation
when features that were readily available during the training of
a model end up being costly to obtain during its deployment. If
additionally these features are highly correlated with other features
they might just be coupled. E.g. generally birds can fly, so the
features is_bird and can_fly may simply be merged at prediction
time1, to make it easier to enter information into a classifier. Again,
Generalized Efficiency ensures that the influence of the merged
feature relates in a natural way to the influence of the original
features.
The Limit condition normalizes the overall influence to be the
discrete derivative of ùë£ (¬∑, ùëì ) with respect to ùëÅ . In other words,
the total influence distributed to sets of features equals the total
marginal effect of reverting features to their baseline values. This is
an interesting departure from other efficiency measures. Shapley-
based measures require efficiency with respect to ùëì ( ¬Æùë•) (or variants
thereof), i.e. the total amount of influence should equal the total
value the classifier takes at the point of interest (or the difference
between the classifier and the baseline value). We require that the
total influence equals the (discretized) rate in which features change
the outcome. This makes BII more similar in spirit to gradient
based model explanations, which are often used for generating
model explanations in several application domains [29].
1The authors are aware of the existence of ostriches, emus, penguins and the fearsome
cassowary.
405
FAccT ‚Äô21, March 3‚Äì10, 2021, Virtual Event, Canada Neel Patel, Martin Strobel, and Yair Zick
Monotonicity is a very natural property in the model explanation
domain: if a set of features has a greater effect on the value ùëì ( ¬Æùë•),
this should be reflected in the amount of influence one attributes
to it. This has already been established in prior works, for Shapley-
based measures [11]. However, this property does not naturally
generalize when using Shapley-based high-dimensional model ex-
planations. Agarwal et al. [1] propose a novel generalization of the
Shapley value to high-dimensional model explanations, which fails
monotonicity for smaller interactions (size of < ùëò) for ùëò-th order
explanations, however, interactions of size ùëò follow monotonicity.
Example 3.5 highlights issues that may arise when monotonicity is
not preserved.
Example 3.5. Given a function ùëìùëê (ùë•1, ùë•2, ùë•3) = ùëêùë•1ùë•2ùë•3 with
ùëê > 0 defined on binary input space (for example, ùëì is the result of an
image classification task where ùë•ùëñ denotes the presence/absence of
particular super-pixel). We assume that the baseline is ¬Æùë• ‚Ä≤ = (0, 0, 0).
Thus, ùë£ (ùëÜ, ¬Æùë•, ¬Æùë• ‚Ä≤, ùëìùëê ) = 0 if {2, 3} ‚äà ùëÜ , resulting in ùë£ ({2, 3}, ùëìùëê ) = 0
and ùë£ (ùëÅ, ùëìùëê ) = ùë£ ({1, 2, 3}, ùëìùëê ) = ùëêùë•1ùë•2ùë•3. What is the interaction
value between 1 and 2? Intuitively {1, 2} offer some degree of in-
teraction that monotonically grows as ùëê increases. Moreover, it is
easy to see that ùë£ (¬∑, ùëìùëê ) ‚â• ùë£ (¬∑, ùëìùëê‚Ä≤) whenever ùëê ‚â• ùëê ‚Ä≤. Set-QII fails
to satisfies the monotoncity, and fails to capture the interaction
between {1, 2} for any ùëê . Set-QII({1, 2}, ùëÜ, ùëìùëê ) = 0 for all ùëê .
Similarly, the Shapley-Taylor interaction index for ùëÜ = {1, 2} and
ùëò = 3 is 0 as it does not follow the monotonicity property, however
for ùëò = 2 it satisfies monotonicity and interaction value for {1, 2} is
ùëê
3 . The BII value for {1, 2} is ùëê .
In Section 4, we show that BII can be interpreted as a poly-
nomial approximation, offering additional intuition as to why our
explanation method is optimal.
4 GEOMETRICAL INTERACTION AND BII
The geometry of model explanations is relatively well understood
for attribute-based methods [22, 27, 30]; Ribeiro et al. [27] view
linear explanation methods as local linear approximations of ùëì (¬∑)
around a point of interest ¬Æùë• . Linear model explanations can be
thought of as functions taking the following form: ùëî( ¬Æùë•) = ùúô0 +‚àëùëÅ
ùëñ=1 ùúôùëñùë•ùëñ , whereùúôùëñ captures the importance of feature ùëñ . Taking this
interpretation, linear model explanations can be ‚Äòobjectively‚Äô bad
explanations: they are a poor local approximation of the underlying
model ùëì as black-box model can be highly non-linear around ¬Æùë•
[30, 33]. In order to better capture the behavior of a black-box
model ùëì , we approximate it using higher-order polynomials. For
better visualization, we first assume that the black-box model ùëì :
{0, 1}ùëÅ ‚Üí R takes a binary input vector mainly referred to as the
humanly understandable feature representation [22, 27]. First, we
start with quadratic approximation of the black-box model ùëì (¬∑),
ùëî( ¬Æùë•) = ùúô0 +
ùëÅ‚àëÔ∏Å
ùëñ=1
ùúôùëñùë•ùëñ +
‚àëÔ∏Å
ùëñ< ùëó
ùúôùëñ, ùëóùë•ùëñùë• ùëó
In the above equation, ùúôùëñ, ùëó captures the interaction between fea-
tures ùëñ and ùëó ; ùúôùëñ , ùúô ùëó capture the importance of ùëñ and ùëó . Thus, it is not
unreasonable to assume that ùúôùëñ, ùëó captures the pure interaction ef-
fect of ùëñ and ùëó : we can delegate the singular effects to ùúôùëñ , having the
resultant coefficient of ùë•ùëñùë• ùëó capture the ‚Äòpure‚Äô interaction between ùëñ
and ùëó . This also connects with the idea of the Statistical Interactions
[14, 15]. For instance, consider a sentiment analysis problem, both
the tokens ‚Äúbad‚Äù and ‚Äúnot‚Äù have negative influence on the predic-
tion. However, when they are present together as ‚Äúnot bad‚Äù, their
influence is positive. In this simple example, it would be desirable
to have ùúô{‚Äúnot‚Äù,‚Äúbad‚Äù} > 0. The idea of higher order interactions can
be extended similarly.
Consider a global polynomial approximation of ùëì (¬∑) by a ùëò-
degree polynomial:
ùëîùëò ( ¬Æùë•) = ùúô0+
‚àëÔ∏Å
ùëÜ‚Ä≤‚äÇùëÅ : |ùëÜ‚Ä≤ |<ùëò
¬©¬≠¬´ùúôùëÜ‚Ä≤
‚àè
ùëó ‚ààùëÜ‚Ä≤
ùë• ùëó
¬™¬Æ¬¨+
‚àëÔ∏Å
ùëÜ‚äÜùëÅ ; |ùëÜ |=ùëò
¬©¬≠¬´ùúôùëÜ
‚àè
ùëó ‚ààùëÜ
ùë• ùëó
¬™¬Æ¬¨ . (9)
Again, to capture interaction among the set of features ùëÜ such that
|ùëÜ | = ùëò , we should remove all internal interaction effects captured
by ùúôùëÜ‚Ä≤ for ùëÜ ‚Ä≤ ‚äÇ ùëÜ as we intend to capture the The polynomial ùëîùëò is
meant to locally approximate ùëì around the POI ¬Æùë• ; what is the best
approximation? Finding the best fitting polynomial of the highest
possible degree seems like a natural objective. However, taking this
approach runs the risk of ignoring lower order feature interactions
and their possible effects; this is shown in Example 4.1.
Example 4.1. Consider the degree 3 polynomial studied in Ex-
ample 3.5, ùëì (ùë•1, ùë•2, ùë•3) = ùëêùë•1ùë•2ùë•3 with the baseline set to (1, 1, 1)
(rather than (0, 0, 0) as was the case in Example 3.5). The best ap-
proximation to ùëì is clearly itself. However, if we do so, then the
interaction coefficients for variable pairs will be zero. This is ar-
guably undesirable: for example, if ¬Æùë• = (0, 0, 1), then ùë•3 has virtually
no impact (it is already set at the baseline). Similarly, ùë•1 and ùë•2
have little individual effect, but do have significant joint effect - it
is only when both are set to 1 that we observe any change in label.
Now we formally define the optimization problem to find the
‚Äúbest‚Äù ùëò-degree polynomial approximation of the black-box model
ùëì (¬∑) globally. Let Pùëò be the set of ùëò-degree polynomials. We are
interested in finding a polynomial ùëîùëò
ùëì
(¬∑) which globally minimizes
the quadratic loss between ùëì ( ¬Æùë•) and ùëî( ¬Æùë•) ‚àà Pùëò for all ¬Æùë• ‚àà 2ùëÅ , i.e.
ùëîùëò
ùëì
( ¬Æùë•) = argmin
ùëî ( ¬∑) ‚ààPùëò
‚àëÔ∏Å
¬Æùë• ‚àà2ùëÅ
[ùëì ( ¬Æùë•) ‚àí ùëî( ¬Æùë•)]2 . (10)
The interaction among features in ùëÜ with |ùëÜ | = ùëò is measured
as the coefficient of
‚àè
ùëñ‚ààùëÜ ùë•ùëñ in the least square approximation of
ùëì (¬∑) with a polynomial of degree ùëò . Theorem 4.2 shows that this
geometrical definition of feature interaction coincides with the
Banzhaf interaction index.
Theorem 4.2. Let ùëîùëò
ùëì
( ¬Æùë•) be the ùëò-degree solution of the optimiza-
tion problem in Equation (10). Then the coefficients of
‚àè
ùëñ‚ààùëÜ ùë•ùëñ for
|ùëÜ | = ùëò in ùëîùëò
ùëì
( ¬Æùë•) is given by ùêº ùë£BII (ùëÜ).
The proof of the Theorem is a direct corollary of [18, Theo-
rem 4.2]. Theorem 4.2 shows that k-th order interaction obtains via
least square approximation of the black-box model using k-th order
polynomial coincides with BII and extend a geometrical argument
for ‚Äúgood" feature-based explanation to feature interactions. More-
over, the least square approximation of the model via polynomial
also resonates with the Statistical interaction among features as we
already discussed. Therefore BII is not only obtained from strong
axiomatic but also derived from an intuitive optimization problem.
406
High Dimensional Model Explanations: An Axiomatic Approach FAccT ‚Äô21, March 3‚Äì10, 2021, Virtual Event, Canada
5 EXPERIMENTAL EVALUATION
In this section, we experimentally illustrate the need for high di-
mensional model explanations and the axioms discussed above.
We consider two-well known benchmark datasets; the UCI Adult
dataset [13] tries to predict whether an individual‚Äôs income ex-
ceeds $50,000/year based on census data, and the IMDB movie re-
view dataset [23] tries to predict the positive/negative sentiment of
movie reviews based on their word content.
For the Adult dataset we train a 3-level decision three (see Fig-
ure 1) which achieves an 84% test accuracy. We use this easily
comprehensible model to show the shortcomings of existing meth-
ods and the importance of our axioms. We also train a random
forest with 50 estimators and a maximum depth of 10 that achieves
a slightly higher accuracy of 86%2.
For the IMDB dataset we train the BERT language model3 [12],
which achieves 89% evaluation accuracy; the BERT-Base model we
use has 110M parameters, making it virtually incomprehensible to
humans.
Like many other model explanation methods, BII assumes the
existence of a baseline, i.e. a vector having ‚Äútypical‚Äù values which
can be used as a reference for the point of interest when creating
the explanation. In the Adult dataset, we assume the baseline for
continuous features to be their median value; categorical features
are one-hot encoded, and the baseline is assumed to be zero. For
the IMDB dataset, each word is a distinct feature, and the baseline
is an empty sentence (i.e. all features set to 0).
5.1 Baseline Explanation Frameworks
We compare BII with an interaction measure based on the Shapley
interaction index (SII) [16] and ametric that falls into the framework
proposed in [11] which we call Set-QII. For all methods we consider
a version using the baseline i.e. the BII and SII values are the
respective game-theoretic measures when ‚Äúplaying‚Äù the game of
setting features to the baseline. Set-QII measures the influence
of a (set of) features by measuring the difference of the function
value on the point of interest and a single another point where the
values of this (set of) feature(s) is set to the baseline. For all three
metrics, we consider model explanations capturing single feature
influences and pairwise feature interactions. We briefly discuss the
computational complexity in the supplementary material [25].
To further illustrate the importance of measuring interactions we
also study the well known LIME method [27] which only generates
individual feature importance.
We also consider a naive approach to create feature interactions
which we call the additive expansion. The additive expansion of a
feature-based explanation methods is computing joint influence
of feature ùëñ and ùëó (interaction among features ùëñ and ùëó ) by simply
adding individual influence of features ùëñ and ùëó , ùêº (ùëñ ùëó) = ùêº (ùëñ) + ùêº ( ùëó).
This is a naive way to compute the joint effect of features when only
individual feature importance is provided. Via additive expansion
we can create a feature-interaction measure from LIME , as well as
2We use keras library to train decision tree and random forest https://github.com/keras-
team/keras.
3We use pertained BERT model for the IMDB dataset available at https://github.com/
artemisart/bert-sentiment-IMDB.
ad-hoc feature-interaction measures from the feature-importance
versions of Banzhaf, Shapley and QII.
5.2 The Importance of Interactions
While seemingly obvious, we begin our empirical investigation
with a simple question: are there instances where high-dimensional
model explanations are needed? In other words, is it possible that
feature-wise explanations offer sufficient explanations in practice?
Our first experiment indicates that high-dimensional explanations
matter. In Figure 2, we visualize interactions for the movie review
described in the Introduction (Section 1). We compute pairwise
interactions using BII among features for the BERT model. We
represent the explanations generated by BII as a symmetric matrix,
where the ùëñ-th diagonal element represents the influence of feature
ùëñ in the decision tree and (ùëñ, ùëó)-th element of the matrix for ùëñ < ùëó
represents the interaction between ùëñ and ùëó .
Comparing Figures 2 and 3, it is immediately clear why feature
importance offers additional insights. The values generated by LIME
are indicative of a negative review (whereas the review we examine
is positive), and look somewhat odd: ‚Äònot‚Äô has a strong positive
influence. On the other hand, the interactions make it clear that
BERT has picked up on the fact that ‚Äònot bad‚Äô indicates a good
review. We compare additional BII feature interactions for the
BERT model and the IMDB classification task in the supplementary
material [25] (Figures 1 to 5). LIME and BII mostly agree for the
individual feature importance; however, BII captures non-trivial
interactions among pairs of words that are impossible to observe
only from feature-based explanation.
In Figure 4, we plot the explanation generated by BII and
LIME for individuals with Marital Status =yes, Capital Gain ‚àà
{5095, 7073} and Education Level < 12 from the adult dataset
for the simple tree classifier (see Figure 1). LIME suggests that
Capital Gain is the most important feature and the other two fea-
tures have minor importance. However, for any coalition of features
(activated features are set to their POI value, and the others to their
baseline value: Marital Status = No and Capital Gain = 0),
changing only Marital Status or Capital Gain to their POI
value does not significantly change the prediction value; but, chang-
ing both of them concurrently to their POI value results in a signif-
icant increase in prediction probability (see Table 1). BII success-
fully captures these insights in Figure 4, and suggests that the high
positive interaction between Capital Gain and Marital Status
is responsible for the positive label. When considering feature inter-
action, there is high positive interaction between Capital Gainand
Marital Status, an insight that linear explanations do not capture,
as seen in Table 1.
Figure 5 summarizes the explanations generated by BII , SII ,
LIME and QII for a married woman with a Capital Gain (CG) of
5178 with Education Level = 13 in the adult test data for the sim-
ple tree classifier (Figure 1). Column (a) is the additive expansion of
the feature-importance versions of these measures and Column (b)
shows the methods that explicitly consider interactions. Both BII
and SII indicate a negative synergy between Education Level
and Capital Gain, whereas all methods indicate positive individ-
ual importance. Hence, additive expansions fail to capture negative
interactions, even for relatively simple models.
407
FAccT ‚Äô21, March 3‚Äì10, 2021, Virtual Event, Canada Neel Patel, Martin Strobel, and Yair Zick
Marital Status
Capital Gain > 7073 Education Level > 12
Education Level > 12 Age > 20 Capital Gain > 5095 Capital Gain > 5095
0.03 0.15 0.00 0.98 0.30 0.98 0.67 1.00
No Yes
No Yes No Yes
No Yes No Yes No Yes No Yes
Figure 1: A three-level decision tree trained on the adult dataset, achieving 84% test accuracy.
Table 1: This example shows the joint importance of the interactions between Capital Gain and Marital Status in the coalition of
{Age, Education Level}, and the monotonicity property in the treeùëá (Figure 1). The prediction probability becomes 1.00when Marital Status
and Capital Gain both join the coalition {Age, Education Level}; however, when the Marital Status or Capital Gain join individually in the
coalition {Age, Education Level}, the prediction probability becomes 0.67 and 0.15 respectively. This shows that the features have synergistic
effect, which is not captured by linear explanations (this holds for other coalitions, not just {Age, Education Level}).
Features POI Base Coalition
{A,EL} {A,EL,MS} {A,EL,CG} {A,EL,MS,CG}
Marital Status (MS) Yes No No Yes No Yes
Capital Gain (CG) 6021 0 0 0 6021 6021
Age (A) 32 37 32 32 32 32
Education Level (EL) 13 10 13 13 13 13
Prediction 1.00 0.03 0.15 0.67 0.15 1.00
5.3 Monotonicity Matters
Having established the importance of measuring feature synergy,
let us turn to evaluating the importance of our proposed ax-
ioms. To do so, we consider a modified version of the tree de-
picted in Figure 1. The output in the second-to-last leaf is changed
from 0.67 to 0.01, i.e. married people with high Education Level
but low Capital Gain are now classified as not having a high
paying job. This change does not affect the prediction of the
two points discussed above. However, this increases the interac-
tion between Capital Gain and Marital Status: having a lower
Capital Gain (or being unmarried) both lead to a negative out-
come. More formally, we denote the cooperative game induced by
the original tree by ùëì and the one induced by the modified tree by
ùëì ‚Ä≤. Under ùëì ‚Ä≤, the joint marginal gains increase:
ùëöMS,CG ({Age, Education Level}, ùëì ‚Ä≤) >
ùëöMS,CG ({Age, Education Level}, ùëì ),
andùëöMS,CG (ùëá, ùëì ‚Ä≤) = ùëöMS,CG (ùëá, ùëì ) for any other coalition ùëá . Ac-
cording to the monotonicity property, the interaction index value
among {Age, Education Level} should be higher under ùëì ‚Ä≤ than
ùëì . This change is reflected in Shapley interactions and BII as both
measures satisfy monotonicity (see Figure 5, column (b) for actual
interaction and column (c) for interaction after the modification).
Set-QII, however, is unchanged; it fails to satisfy monotonicity,
and even for this small example cannot distinguish between the
two trees. Similarly, the interaction between Capital Gain and
Education Number should also increase which is again reflected
by SII and BII ; Set-QII fails to show this (see Figure 5).
5.4 The Effects of 2-Efficiency
Table 2 highlights how failure to satisfy generalized 2-
efficiency can lead to counterintuitive interactions when
features are merged. We calculated the SII values for
Education Level and Education Number for a random for-
est with Marital Status (left) and Capital Gain (right) on some
sampled points; we recalculated them after merging the features to
[Education Level; Education Number]. These features can be
naturally merged as they are in fact identical: Education Number
is just a numerical representation of Education Level.
In some instances, the interaction values of Education Level
and Education Number with other features differ in sign (i.e.
one has positive interaction and another has a negative interac-
tion). In this case, under 2-efficiency, the interaction value of the
merged feature [Education Level; Education Number] should
lie somewhere between the two. In Table 2, we show some of
the points where the Shapley interaction of Marital Status and
Capital Gain with merged feature deviates from the expected in-
teractions. For example, for Point 2, the SII of Education Level
with Marital Status is ‚àí0.32 and Education Number with
Marital Status is 0.123, therefore the interaction after merg-
ing Education Number and Education Level should be less
than 0.123; however, the SII interaction value between
Marital Status and the merged feature is 0.161 ‚Äî more than
408
High Dimensional Model Explanations: An Axiomatic Approach FAccT ‚Äô21, March 3‚Äì10, 2021, Virtual Event, Canada
Table 2: Failure to satisfy generalized 2-efficiency leads to counterintuitive interactions after features aremerged.Wemerge Education Level
and Education Number (which are actually identical); Shapley interaction values with Marital Status (left) and Capital Gain (right) before
and after the features are merged tend to overestimate the expected merged value (as compared to the expected value under generalised
2-efficiency).
Interaction with Marital Status Interaction with Capital Gain
Pt. 1 Pt. 2 Pt. 3 Pt. 4 Pt. 5 Pt. 1 Pt. 2 Pt. 3 Pt. 4 Pt. 5
Education Level (EL) -.068 -.032 -.028 -.066 -.044 -.017 0 .016 .015 -.002
Education Number (EN) .05 .123 .118 .041 .17 .016 -.007 -.009 -.006 -.011
[EL;EN] -.077 .161 .154 -.077 .23 .013 -.013 .017 .019 .014
Expected [EL;EN] -.018 .091 .090 -.025 .126 -.001 -.007 .007 .009 .009
Difference -.059 .070 .064 -.052 -.103 .014 -.006 .010 .010 .005
Negative interaction No interaction Positive interaction
It
It
isn‚Äôt
isn
‚Äôt
the
th
e
greatest
gr
ea
te
st
scifi
sc
ifi
flick
fli
ck
I‚Äôve
I‚Äôv
e
every
ev
er
y
seen,
se
en
,
but
bu
t
it
it
is
is
not
no
t
a
a
bad
ba
d
movie
m
ov
ie
Negative interaction No interaction Positive interaction
Figure 2: Feature importance and interactions for the movie
review given in the introduction generated via BII . BII is
able to pick up on the strong positive interaction between
‚Äúnot‚Äù and ‚Äúbad‚Äù that leads to a positive prediction for this re-
view. Note that the individual importance of ‚Äúnot‚Äù and ‚Äúbad‚Äù
is rather weak - their synergy is assigned importance.
0.123! In other words, SII placed far too much influence on two
identical merged features. We show additional examples in Table 2.
Since BII satisfies generalized 2-efficiency, influence is preserved
under merging and thus matches the expected outcome.
‚àí2 ‚àí1 0 1 2 3 4
every
isn‚Äôt
bad
greatest
not
0.22
‚àí0.31
‚àí0.71
0.98
2.74
Influence
Figure 3: LIME feature importance for the movie review
given in Section 1. The measure assigns significant weight
to the word ‚Äúnot‚Äù, but little weight to ‚Äúbad‚Äù.
BII LIME
CG
CG
0.35 0 0.71
EL
EL
0 0
MS
MS
0.65
Negative Positive
0 0.5
CG
MS
EL
0.63
0.31
9 ¬∑ 10‚àí2
Influence
Figure 4: The explanation generated by BII and LIME for an-
other point in the dataset.LIME fails to capture interactions.
6 DISCUSSION AND FUTUREWORK
Designing provably sound higher-order explanations for machine
learning models in high stake domains is an important problem.
409
FAccT ‚Äô21, March 3‚Äì10, 2021, Virtual Event, Canada Neel Patel, Martin Strobel, and Yair Zick
(a) Additive expansion (b) Actual interactions (c) After monotone shift
Banzhaf
CG
CG
0.28 0.46 1
EL
EL
0.18 0.9
MS
MS
0.72
CG
CG
0.28 ‚àí0.2 0.56
EL
EL
0.18 0.07
MS
MS
0.72
CG
CG
0.44 0.12 0.88
EL
EL
0.02 ‚àí0.24
MS
MS
0.57
Shapley
CG
CG
0.22 0.4 1
EL
EL
0.18 0.96
MS
MS
0.78
CG
CG
0.22 ‚àí0.23 0.63
EL
EL
0.18 0.1
MS
MS
0.78
CG
CG
0.43 0.13 0.9
EL
EL
0.04 ‚àí0.31
MS
MS
0.6
Qii
CG
CG
0.28 0.29 1
EL
EL
0.01 0.74
MS
MS
0.72
CG
CG
0.28 0.6 0.72
EL
EL
0.01 0.83
MS
MS
0.72
CG
CG
0.76 0.6 0.72
EL
EL
0.01 0.83
MS
MS
0.72
Negative interaction No interaction Positive interaction
Figure 5: The explanations for a married woman with a Capital Gain (CG) of 5178 with Education Level = 13 (i.e. Masters) in the adult test
data for the above tree. (a) Linear explanations highlight the importance of Marital Status (MS) and Capital Gain (CG), they fail to show
how these to features interact. (b) The positive interaction between Capital Gain (CG) and Marital Status (MS) highlights that both features
a needed for a positive outcome. (c) After the tree gets modified to strengthen the interaction, the explanation proposed in Set-QII does not
change, failing monotonicity.
Ideally, we want to design explanations we can trust. We offer a
variety of reasons to trust in BII : it uniquely satisfies a set of
natural properties ‚Äî if we believe that these are sensible then our
job is done. That said, the authors believe that a normative approach
is critical in the design of fair, transparent AI solutions. Rather than
debating approaches, one should debate the fundamental properties
they are guaranteed to satisfy. Deriving model explanations from
norms offers an intuitive justification for the chosen interaction
measure, which fosters trust in the explanation method. As shown
in Section 5, some of our fundamental properties are absolutely
critical for the design of sensible explanations. Even if one adheres
to an optimization-based approach to explanation design, we show
that BII is the optimal solution to a natural objective.
While BII certainly satisfies several design criteria, it is not
without issues. The first challenge is computational. Feature-based
explanations are computationally intensive as it is, so it should be
no surprise that computing higher-order interactions comes at a
higher cost. There are, however, good reasons to believe that BII
can be computed efficiently on simpler ML model classes, and that
low error approximations can be found quickly. This, we believe, is
an important direction for future work.
ACKNOWLEDGMENTS
This research was supported by an NRF Research Fellowship R-252-
000-750-733 and the NRF AI Research award is R-252-000-A20-490.
This work was done when the first and the third author were affili-
ated with the National University of Singapore. The authors thank
GAIW 2020, WHI 2020, NeurIPS 2020, and FAccT 2021 reviewers
for their useful comments.
410
High Dimensional Model Explanations: An Axiomatic Approach FAccT ‚Äô21, March 3‚Äì10, 2021, Virtual Event, Canada
REFERENCES
[1] Ashish Agarwal, Kedar Dhamdhere, and Mukund Sundararajan. 2019. A New
Interaction Index inspired by the Taylor Series. arXiv preprint arXiv:1902.05622
(2019). arXiv:1902.05622
[2] Marco Ancona, Enea Ceolini, Cengiz √ñztireli, and Markus Gross. 2017. A unified
view of gradient-based attribution methods for deep neural networks. In NIPS
2017-Workshop on Interpreting, Explaining and Visualizing Deep Learning. 1‚Äì11.
[3] Marco Ancona, Enea Ceolini, Cengiz √ñztireli, and Markus Gross. 2018. Towards
better understanding of gradient-based attribution methods for Deep Neural
Networks. In Proceedings of the 6th International Conference on Learning Repre-
sentations (ICLR). 1‚Äì16.
[4] Yoram Bachrach, Evangelos Markakis, Ariel D Procaccia, Jeffrey S Rosenschein,
and Amin Saberi. 2008. Approximating power indices. In Proceedings of the
7th International Conference on Autonomous Agents and Multi-Agent Systems
(AAMAS). 943‚Äì950.
[5] David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja
Hansen, and Klaus-Robert M√ºller. 2010. How to explain individual classification
decisions. Journal of Machine Learning Research 11 (2010), 1803‚Äì1831.
[6] John F Banzhaf III. 1964. Weighted voting doesn‚Äôt work: A mathematical analysis.
Rutgers Law Review 19 (1964), 317.
[7] Georgios Chalkiadakis, Edith Elkind, and Michael Wooldridge. 2011. Compu-
tational aspects of cooperative game theory. Synthesis Lectures on Artificial
Intelligence and Machine Learning 5, 6 (2011), 1‚Äì168.
[8] Tianyu Cui, Pekka Marttinen, and Samuel Kaski. 2019. Recovering Pairwise
Interactions Using Neural Networks. arXiv preprint arXiv:1901.08361 (2019).
arXiv:1901.08361
[9] Amit Datta, Anupam Datta, Ariel D Procaccia, and Yair Zick. 2015. Influence in
classification via cooperative game theory. In Proceedings of the 24th International
Joint Conference on Artificial Intelligence (IJCAI). 511‚Äì517.
[10] Anupam Datta, Matthew Fredrikson, Gihyuk Ko, Piotr Mardziel, and Shayak Sen.
2017. Use privacy in data-driven systems: Theory and experiments with machine
learnt programs. In Proceedings of the 24th ACM SIGSAC Conference on Computer
and Communications Security (CCS). 1193‚Äì1210.
[11] Anupam Datta, Shayak Sen, and Yair Zick. 2016. Algorithmic transparency via
quantitative input influence: Theory and experiments with learning systems.
In Proceedings of the 37th IEEE Symposium on Security and Privacy (Oakland).
598‚Äì617.
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[13] Dheeru Dua and Casey Graff. 2019. UCI Machine Learning Repository. http:
//archive.ics.uci.edu/ml
[14] Ronald Aylmer Fisher. 1992. Statistical methods for research workers. In Break-
throughs in statistics. Springer, 66‚Äì70.
[15] Andrew Gelman. 2005. Analysis of variance‚Äîwhy it is more important than ever.
Annals of Statistics 33, 1 (2005), 1‚Äì53.
[16] Michel Grabisch and Marc Roubens. 1999. An axiomatic approach to the concept
of interaction among players in cooperative games. International Journal of Game
Theory 28, 4 (1999), 547‚Äì565.
[17] Peyton Greenside, Tyler Shimko, Polly Fordyce, and Anshul Kundaje. 2018. Dis-
covering epistatic feature interactions from neural network models of regulatory
DNA sequences. Bioinformatics 34, 17 (2018), i629‚Äìi637.
[18] Peter L Hammer and Ron Holzman. 1992. Approximations of pseudo-Boolean
functions; applications to game theory. Zeitschrift f√ºr Operations Research 36, 1
(1992), 3‚Äì21.
[19] Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via
influence functions. In Proceedings of the 34th International Conference on Machine
Learning (ICML). 1885‚Äì1894.
[20] Ehud Lehrer. 1988. An axiomatization of the Banzhaf value. International Journal
of Game Theory 17, 2 (1988), 89‚Äì99.
[21] Scott M. Lundberg, Gabriel G. Erion, and Su-In Lee. 2018. Consistent Individu-
alized Feature Attribution for Tree Ensembles. arXiv preprint arXiv:1802.03888
(2018). arXiv:1802.03888
[22] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model
predictions. In Proceedings of the 31st Annual Conference on Neural Information
Processing Systems (NIPS). 4765‚Äì4774.
[23] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,
and Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis.
In Proceedings of the 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies (HLT). 142‚Äì150.
[24] Guillermo Owen. 1972. Multilinear extensions of games. Management Science 18,
5-part-2 (1972), 64‚Äì79.
[25] Neel Patel, Martin Strobel, and Yair Zick. [n.d.]. Online Supplemen-
tary Material: High Dimensional Model Explanation: An Axiomatic Ap-
proach. https://github.com/DataDrivenStrategicCollaborationGroup/High-
Dimensional-Model-Explanations-An-Axiomatic-Approach
[26] Bezalel Peleg and Peter Sudh√∂lter. 2007. Introduction to the Theory of Cooperative
Games. Springer.
[27] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why should
i trust you?: Explaining the predictions of any classifier. In Proceedings of the
22nd International Conference on Knowledge Discovery and Data Mining (KDD).
1135‚Äì1144.
[28] Lloyd S Shapley. 1953. A value for n-person games. Contributions to the Theory
of Games 2, 28 (1953), 307‚Äì317.
[29] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside
convolutional networks: Visualising image classification models and saliency
maps. arXiv preprint arXiv:1312.6034 (2013). arXiv:1312.6034
[30] Jakub Sliwinski, Martin Strobel, and Yair Zick. 2019. A Characterization of
Monotone Influence Measures for Data Classification. In Proceedings of the 33rd
AAAI Conference on Artificial Intelligence (AAAI). 718‚Äì725.
[31] Mukund Sundararajan and Amir Najmi. 2019. The many Shapley values for
model explanation. arXiv preprint arXiv:1908.08474 (2019). arXiv:1908.08474
[32] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution
for deep networks. In Proceedings of the 34th International Conference on Machine
Learning (ICML). 3319‚Äì3328.
[33] Michael Tsang, Dehua Cheng, and Yan Liu. 2017. Detecting statistical interactions
from neural network weights. arXiv preprint arXiv:1705.04977 (2017).
[34] Michael Tsang, Hanpeng Liu, Sanjay Purushotham, Pavankumar Murali, and
Yan Liu. 2018. Neural interaction transparency (NIT): disentangling learned
interactions for improved interpretability. In Proceedings of the 32nd Annual
Conference on Neural Information Processing Systems (NeurIPS). 5804‚Äì5813.
[35] H Peyton Young. 1985. Monotonic solutions of cooperative games. International
Journal of Game Theory 14, 2 (1985), 65‚Äì72.
411
