Fair Algorithms for Learning in Allocation Problems
Hadi Elzayn
University of Pennsylvania
hads@sas.upenn.edu
Shahin Jabbari
University of Pennsylvania
jabbari@cis.upenn.edu
Christopher Jung
University of Pennsylvania
chrjung@seas.upenn.edu
Michael Kearns
University of Pennsylvania
mkearns@cis.upenn.edu
Seth Neel
University of Pennsylvania
sethneel@wharton.upenn.edu
Aaron Roth
University of Pennsylvania
aaroth@cis.upenn.edu
Zachary Schutzman
University of Pennsylvania
ianzachs@seas.upenn.edu
ABSTRACT
Settings such as lending and policing can be modeled by a
centralized agent allocating a scarce resource (e.g. loans or
police officers) amongst several groups, in order to maximize
some objective (e.g. loans given that are repaid, or criminals
that are apprehended). Often in such problems fairness is also
a concern. One natural notion of fairness, based on general
principles of equality of opportunity, asks that conditional on
an individual being a candidate for the resource in question,
the probability of actually receiving it is approximately inde-
pendent of the individual’s group. For example, in lending this
would mean that equally creditworthy individuals in different
racial groups have roughly equal chances of receiving a loan.
In policing it would mean that two individuals committing
the same crime in different districts would have roughly equal
chances of being arrested.
In this paper, we formalize this general notion of fairness
for allocation problems and investigate its algorithmic conse-
quences. Our main technical results include an efficient learn-
ing algorithm that converges to an optimal fair allocation even
when the allocator does not know the frequency of candidates
(i.e. creditworthy individuals or criminals) in each group. This
algorithm operates in a censored feedback model in which
only the number of candidates who received the resource in a
given allocation can be observed, rather than the true number
of candidates in each group. This models the fact that we do
not learn the creditworthiness of individuals we do not give
loans to and do not learn about crimes committed if the police
presence in a district is low.
As an application of our framework and algorithm, we
consider the predictive policing problem, in which the resource
The full technical version of this paper is available at https://arxiv.org/abs/1808.
10549.
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made
or distributed for profit or commercial advantage and that copies bear this
notice and the full citation on the first page. Copyrights for components of this
work owned by others than ACM must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
FAT* ’19, January 29–31, 2019, Atlanta, GA, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287571
being allocated to each group is the number of police officers
assigned to each district. The learning algorithm is trained on
arrest data gathered from its own deployments on previous
days, resulting in a potential feedback loop that our algorithm
provably overcomes. In this case, the fairness constraint asks
that the probability that an individual who has committed a
crime is arrested should be independent of the district in which
they live. We investigate the performance of our learning
algorithm on the Philadelphia Crime Incidents dataset.
CCS CONCEPTS
• Theory of computation → Online learning algorithms;
Machine learning theory;Online learning theory; •Computing
methodologies → Learning from implicit feedback;
KEYWORDS
algorithmic fairness, resource allocation, censored feedback,
online learning
ACM Reference Format:
Hadi Elzayn, Shahin Jabbari, Christopher Jung, Michael Kearns, Seth
Neel, Aaron Roth, and Zachary Schutzman. 2019. Fair Algorithms for
Learning in Allocation Problems. In FAT* ’19: Conference on Fairness,
Accountability, and Transparency (FAT* ’19), January 29–31, 2019, At-
lanta, GA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/
10.1145/3287560.3287571
1 INTRODUCTION
The bulk of the literature on algorithmic fairness has focused
on classification and regression problems (see e.g. [3, 4, 6–
8, 10, 14, 16, 17, 19, 20, 24–26] for a collection of recent work),
but fairness concerns also arise naturally in many resource
allocation settings. Informally, a resource allocation problem
is one in which there is a limited supply of some resource to
be distributed across multiple groups with differing needs.
Resource allocation problems arise in financial applications
(e.g. allocating loans), disaster response (allocating aid), and
many other domains — but the primary example that we will
focus on in this paper is policing. In the predictive policing
problem, the resource to be distributed is police officers, which
can be dispatched to different districts. Each district has a
different crime distribution, and the goal (absent additional
crimes caught.
recent work (see e.g. [11, 12, 21]) has highlighted the extent to
which algorithmic allocation might exacerbate those concerns.
For example, Lum and Isaac [21] show that if predictive polic-
ing algorithms such as PredPol are trained using past arrest
data to predict future crime, then pernicious feedback loops
can arise, which misestimate the true crime rates in certain
districts, leading to an overallocation of police.
munities that Lum and Isaac [21] showed to be overpoliced
on a relative basis were primarily poor and minority, this is
especially concerning from a fairness perspective. In this work,
we study algorithms that avoid this kind of under-exploration
and incorporate quantitative fairness constraints.
In the predictive policing setting, Ensign et al. [11] implic-
itly consider an allocation to be fair if police are allocated
across districts in direct proportion to the district’s crime rate;
generally extended, this definition asks that units of a resource
are allocated according to the group’s share of the total can-
didates for that resource. In our work, we study a different
notion of allocative fairness that has a similar motivation to
the notion of equality of opportunity proposed by Hardt et al.
[14] in classification settings. Informally speaking, it asks that
the probability that a candidate for a resource be allocated a
resource be independent of his group membership. In the pre-
dictive policing setting, it asks that conditional on committing
a crime, the probability that someone is apprehended should
not depend on the district in which they commit the crime.
To illustrate that our notions of fairness do not depend on
whether individuals would prefer to receive or not receive the
resource, we highlight another setting in which allocative fair-
ness is a natural concern: hiring.
to recruit machine learning programmers by advertising on a
social media platform. Many such platforms offer the ability to
advertise to different demographics of users and charge by the
number of times the advertisement is shown to different users
(i.e., the number of impressions); a fixed advertising budget
can then be viewed as a number of impressions to allocate. De-
pending on how well the platform can identify programmers
within each demographic, the ad may be shown to a higher
or lower number of programmers. In this setting, our notion
of allocative fairness asks that the probability a programmer
is exposed to the hiring ad (and thus, receives the opportu-
nity to apply for a job) does not depend on the programmer’s
demographic, and the allocation problem is to maximize the
number of programmers reached via the choice of impressions
across each demographic, subject to fairness constraints.
criminals, including preventing crimes in the first place, fostering healthy com-
munity relations, and generally promoting public safety. But for concreteness
and simplicity we consider the limited objective of apprehending criminals.
in deployed systems, arrest data (rather than 911 reported crime) is used to train
the models.
and with different research questions in mind.
1.1 Our Results
To define the extent to which an allocation satisfies our fair-
ness constraint, we must model the specific mechanism by
which resources deployed to a particular group reach their
intended targets. We study two such discovery models, and we
view the explicit framing of this modeling step as one of the
contributions of our work; the implications of a fairness con-
straint depend strongly on the details of the discovery model,
and specifying such a model is an important step in making
one’s assumptions transparent.
We study two discovery models, which capture two ex-
tremes of targeting ability. In the random discovery model,
regardless of the number of units allocated to a given group,
all individuals within that group are equally likely to be as-
signed a unit, regardless of whether they are a candidate for
the resource or not. In other words, the probability that a can-
didate receives a resource is equal to the ratio of the number
of units of the resource assigned to his group to the size of his
group (independent of the number of candidates in the group).
At the other extreme, in the precision discovery model, units
of the resource are given only to actual candidates within a
group, as long as there is sufficient supply of the resource. That
is, the probability that a candidate receives a resource is equal
to the ratio of the number of units of the resource assigned to
his group to the number of candidates in his group.
In the policing setting, these models can be viewed as two
extremes of police targeting ability for an intervention like
stop-and-frisk. In the random model, police are viewed as stop-
ping people uniformly at random. In the precision model, po-
lice have the omniscient ability to identify individuals with
contraband, and stop only them. Of course, reality lies some-
where in between.
These discovery models have different implications for fair-
ness. In the random model, fairness constrains resources to be
distributed in amounts proportional to group sizes, regardless
of the distribution of candidates, and so is uninteresting from
a learning perspective. On the other hand, the precision model
yields an interesting fairness-constrained learning problem
when the distribution of the number of candidates in each
group must be learned via observation, and what counts as a
‘fair’ allocation depends greatly on these distributions.
We study learning in a censored feedback setting: each
round, the algorithm can choose a feasible deployment of
resources across groups. Then the number of candidates for
the current round in each group is drawn from a fixed, but
unknown group-dependent distribution (which might be not
be independent from the distributions in other groups). The
algorithm does not observe the number of candidates present
in each group, but only the number of candidates that received
the resource. In the policing setting, this corresponds to the
algorithm being able to observe the number of arrests, but not
the actual number of crimes in each of the districts. Thus, the
extent to which the algorithm can learn about the distribution
in a particular group is limited by the number of resources it
deploys there. The goal of the algorithm is to converge to an
optimal fairness-constrained allocation, where here both the
objective value of the solution and the constraints imposed on
it depend on the unknown distributions.
One trivial solution to the learning problem is to sequen-
tially deploy all of one’s resources to each group in turn for
a sufficient amount of time to accurately learn the candidate
distributions. This would reduce the learning problem to an
offline constrained optimization problem, which we show can
be efficiently solved by a greedy algorithm. But this algorithm
is unreasonable: it has a large exploration phase in which it
uses nonsensical deployments, vastly overallocating to some
groups and underallocating to others. A much more realistic,
natural approach is a greedy-style learning algorithm which at
each round uses its current best-guess estimate for the distribu-
tion in each group and deploys an optimal fairness-constrained
allocation according to these estimates. Unfortunately, as we
show, if one makes no assumptions on the underlying distri-
butions, any algorithm that has a guarantee of converging to
a fair allocation must behave like the trivial one, deploying
vast numbers of resources to each group in turn.
This impossibility result motivates us to consider the learn-
ing problem in which the unknown distributions are from a
known parametric family. The natural greedy algorithm uses
an optimal fair deployment at each round given the maximum
likelihood estimates of candidate distributions given its (cen-
sored) observations so far; for concreteness, we consider the
case of the Poisson distribution, and show that it converges to
an optimal fair allocation, but our analysis generalizes for any
single-parameter Lipschitz-continuous family of distributions.
Finally, we conduct an empirical evaluation of our algorithm
on the Philadelphia Crime Incidents dataset, which records
all crimes reported to the Philadelphia Police Department’s
INCT system between 2006 and 2016. We verify that the crime
distributions in each district are in fact well-approximated
by Poisson distributions, and that our algorithm converges
quickly to an optimal fair allocation (as measured according
to the empirical crime distributions in the dataset). We also
systematically evaluate the Price of Fairness, and plot the Pareto
curves that trade off the number of crimes caught versus the
slack allowed in our fairness constraint, for different sizes of
police force, on this dataset. For the random discovery model,
we prove worst-case bounds on the Price of Fairness.
1.2 Further Related Work
Our precision discovery model is inspired by and has tech-
nical connections to Ganchev et al. [13], which models the
dark pool problem from quantitative finance, in which a trader
wishes to execute a specified number of trades across a set of
exchanges of unknown but independently distributed liquidity.
In Ganchev et al. [13], the authors design an optimal allocation
algorithm under the censored feedback of the precision model.
It is straightforward to map their setting onto ours, but they
assume independence between different exchanges, while the
candidate distributions in our setting need not be indepen-
dent. Regardless, we show that their allocation algorithm can
be used to compute an optimal allocation (ignoring fairness)
even when the independence assumption is relaxed (see Re-
mark 1). Later, Agarwal et al. [1] extend the dark pool problem
to an adversarial (rather than distributional) setting. This is
quite closely related to the work of Ensign et al. [12] who also
consider the precision model (under a different name) in an
adversarial predictive policing setting. They provide no-regret
algorithms for this setting by reducing the problem to learn-
ing in a partial monitoring environment. Since their setting is
equivalent to that of Agarwal et al. [1], the algorithms in Agar-
wal et al. [1] can be directly applied to the problem studied by
Ensign et al. [12].
Our desire to study the natural greedy algorithm rather than
an algorithm which uses “unreasonable” allocations during an
exploration phase is an instance of a general concern about
exploration in fairness-related problems [5]. Recent works
have studied the performance of greedy algorithms in different
settings for this reason [2, 18, 23].
Lastly, the term fair allocation appears in the fair division
literature (see e.g. [22] for a survey), but that body of work is
technically quite distinct from the problem we study here.
2 SETTING
We study an allocator who hasV units of a resource and is
tasked with distributing them across a population partitioned
into G groups. Each group is divided into candidates, who
are the individuals the allocator would like to receive the re-
source, and non-candidates, who are the remaining individuals.
We let mi denote the total number of individuals in group
i . The number of candidates ci in group i is a random vari-
able drawn from a fixed but unknown distribution Ci called
the (marginal) candidate distribution. We do not make any
assumptions about the relationship between the candidate
distributions across different groups and in particular these
distributions need not be independent. We use M to denote
the total size of all groups (i.e.,M = Σi ∈[G]mi ). An allocation
v = (v1, . . . ,vG ) is a partitioning of these V units, where
vi ∈ {0, . . . ,V} denotes the units of resources allocated to
group i . Every allocation is bound by a feasibility constraint
which requires that Σi ∈[G]vi ≤ V .
A discovery model disc(vi , ci ) is a (possibly randomized)
function mapping the number of units vi allocated to group
i and the number of candidates ci in group i to the number
of candidates discovered in group i . In the learning setting,
upon fixing an allocation v, the learner will get to observe (a
realization of) disc(vi , ci ) for the realized value of ci for each
group i . Fixing an allocation v, a discovery model disc(·) and
candidate distributions for all groups C = {Ci : i ∈ [G]}, we
define the total expected number of discovered candidates,
χ (v, disc(·),C), as
χ (v, disc(·),C) =
∑
i ∈[G]
E
ci∼Ci
[
disc(vi , ci )
]
, (1)
where the expectation is taken over Ci and any randomization
in the discovery model disc(·). When the discovery model
and the candidate distributions are fixed, we will simply write
χ (v) for brevity. We also use the total expected number of
discovered candidates and (expected) utility exchangeably. We
refer to an allocation that maximizes the expected number
of discovered candidates over all feasible allocations as an
optimal allocation and denote it by w∗.
2.1 Allocative Fairness
For the purposes of this paper, we say that an allocation is
fair if it satisfies approximate equality of candidate discovery
probability across groups. We call this discovery probability
for brevity. This formalizes the intuition that it is unfair if
candidates in one group have an inherently higher probability
of receiving the resource than candidates in another. Formally,
we define our notion of allocative fairness as follows.
Definition 1. Fix a discovery model disc(·) and the candi-
date distributions C. For an allocation v, let
fi (vi , disc(·),Ci ) = E
ci∼Ci
[
disc (vi , ci )
ci
]
,
denote the expected probability that a random candidate from
group i receives a unit of the resource at allocation v (i.e. the
discovery probability in group i). Then for any α ∈ [0, 1], v is
α-fair if
fi (vi , disc(·),Ci ) − fj
(
vj , disc(·),Cj
)  ≤ α ,
for all pairs of groups i and j.
When it is clear from the context, for brevity, wewrite fi (vi )
for the discovery probability in group i . We emphasize that
this definition (1) depends crucially on the chosen discovery
model, and (2) requires nothing about the treatment of non-
candidates. We think of this as aminimal definition of fairness,
in that one might want to further constrain the treatment of
non-candidates — but we do not consider that extension.
Since discovery probabilities fi (vi ) and fj (vj ) are in [0, 1],
the absolute value of their difference is in [0, 1]. By setting
α = 1 we impose no fairness constraints whatsoever on the
allocations, and by setting α = 0 we require exact fairness.
We refer to an allocation v that maximizes χ (v) subject
to α-fairness and the feasibility constraint as an optimal α-
fair allocation and denote it by wα
. In general, χ (wα ) is a
non-increasing quantity in α , since as α diminishes, the utility
maximization problem becomes more constrained.
Remark 1. We note that both the utility and discovery prob-
abilities can be written solely in terms of the marginal candidate
distributions in each of the groups, even when these distribu-
tions are not independent. This is because we have (implicitly)
assumed that the number of candidates discovered in a group
depends only on the number of candidates in the group and the
allocation to that group, regardless of the allocations to and the
number of candidates in other groups. This assumption together
with the linearity of expectation allows us to write the expected
utility as in the right hand side of Equation 1.
3 THE PRECISION DISCOVERY MODEL
We begin by describing the precision model of discovery. Al-
locating vi units to group i in the precision model results in
the discovery of disc(ci ,vi ) ≜ min(ci ,vi ) candidates. This
models the ability to perfectly discover and reach candidates
in a group with resources deployed to that group, limited
only by the number of deployed resources and the number of
candidates present.
The precision model results in censored observations that
have a particularly intuitive form. Recall that in general, a
learning algorithm at each round gets to choose an allocation
v and then observes disc(vi , ci ) for each group i . In the preci-
sion model, this results in the following kind of observation:
when vi is larger than ci , the allocator learns the number of
candidates ci present on that day exactly. We refer to this kind
of feedback as an uncensored observation. When vi is smaller
than ci , all the allocator learns is that the number of candidates
is at least vi . We call this a censored observation.
The rest of this section is organized as follows. In Sec-
tions 3.1 and 3.2 we characterize optimal and optimal fair
allocations for the precision model when the candidate dis-
tributions are known. In Section 3.3 we focus on learning an
optimal fair allocation when these distributions are unknown.
We show that any learning algorithm that is guaranteed to find
a fair allocation in the worst case over candidate distributions
must have the undesirable property that at some point, it must
allocate a vast number of its resources to each group individu-
ally. To bypass this hurdle, in Section 3.4 we show that when
the candidate distributions have a parametric form, a natural
greedy algorithm which always uses an optimal fair allocation
for the current maximum likelihood estimates of the candidate
distributions converges to an optimal fair allocation.
3.1 Optimal Allocation
We first describe how an optimal allocation (absent fairness
constraints) can be computed efficiently when the candidate
distributions Ci are known. In Ganchev et al. [13], the authors
provide an algorithm for computing an optimal allocation
when the distributions over the number of shares present in
each dark pool are known and the trader wishes to maximize
the expected number of traded shares. They assume that the
distributions of shares across different dark pools are inde-
pendent, but our formulation does not require this assump-
tion of independence. Still, we can use the same algorithm as
in Ganchev et al. [13] to compute an optimal allocation in our
setting; this is because, as stated in Remark 1, the utility in
both settings can be written solely in terms of the (marginal)
candidate distributions even when the candidate distributions
are not independent across groups. Here, we present the high
level ideas of their algorithm in the language of our model.
Let Ti (c ) = Prci∼Ci [ci ≥ c] denote the probability that
there are at least c candidates in group i . We refer to Ti (c )
as the tail probability of Ci at c . Recall that the value of the
cumulative distribution function (CDF) of Ci at c is defined to
be
Fi (c ) =
∑
c ′≤c
Prci∼Ci
[
ci = c
′] .
So Ti (c ) can be written in terms of CDF values as Ti (c ) =
1 − Fi (c − 1).
First, observe that the expected total number of candidates
discovered by an allocation in the precision model can be
written in terms of the tail probabilities of the candidate dis-
tributions i.e.
χ (v, disc(·),C) =
∑
i ∈[G]
E
ci∼Ci
[min (vi , ci )] =
∑
i ∈[G]
vi∑
c=1
Ti (c ).
Since the objective function is concave (as Ti (c ) is a non-
increasing function in c for all i), a greedy algorithm which
iteratively allocates the next unit of the resource to a group in
argmax
i ∈[G]
(
Ti
(
vti + 1
)
− Ti
(
vti
))
,
where vti is the current allocation to group i in the t th round
achieves an optimal allocation.
3.2 Optimal Fair Allocation
We next show how to compute an optimal α-fair allocation
in the precision model when the candidate distributions are
known and do not need to be learned.
To build intuition for how the algorithm works, imagine
that the group i has the highest discovery probability in wα
,
and the allocation wα
i to that group is somehow known to
the algorithm ahead of time. The constraint of α-fairness then
implies that the discovery probability for each other group j
inwα
must satisfy fj ∈ [fi −α , fi ]. This in turn implies upper
and lower bounds on the feasible allocations wα
j to group j.
The algorithm is then simply a constrained greedy algorithm:
subject to these implied constraints, it iteratively allocates
units so as to maximize their marginal probability of reaching
another candidate. Since the group i maximizing the discovery
probability inwα
and the corresponding allocationwα
i are not
known ahead of time, the algorithm simply iterates through
each possible choice of i .
Algorithm 1 Computing an optimal fair allocation in the
precision model
Input: α , C andV .
Output: An optimal α-fair allocation wα
.
wα ← 0⃗.
χmax ← 0.
for i ∈ [G] do
v← 0⃗.
for vi ∈ {0, . . .V} do
Set vi in v and compute fi (vi ).
ubi ← vi .
lbi ← vi .
for j , i, j ∈ [G] do
Update lbj and ubj using fi (vi ), α and Cj .
vj ← lbj .
if Σi ∈[G]vi > V then
continue.
Vr = V − Σi ∈[G]vi
for t = 1, . . . ,Vr do
j∗ ∈ argmax
j ∈[G]
(
Tj (vj + 1) − Tj (vj )
)
s.t. vj < ubj .
vj∗ ← vj∗ + 1.
χ (v) = Σi ∈[G]Σ
vi
c=1Ti (c ).
if χ (v) > χmax then
χmax ← χ (v).
wα ← v.
return wα
.
Pseudocode is given in Algorithm 1. We prove that Algo-
rithm 1 returns an optimal α-fair allocation in Theorem 1. We
defer all the omitted proofs and details to the full version.
Theorem 1. Algorithm 1 computes an optimal α -fair alloca-
tion for the precision model in time O (GV (GV +M )).
3.3 Learning Fair Allocations Generally
Requires Brute-Force Exploration
In Sections 3.1 and 3.2 we assumed the candidate distributions
were known. When the candidate distributions are unknown,
learning algorithms intending to converge to optimal α-fair
allocations must learn a sufficient amount about the distribu-
tions in question to certify the fairness of the allocation they
finally output. Because learners must deal with feedback in the
censored observation model, this places constraints on how
they can proceed. Unfortunately, as we show in this section,
if candidate distributions are allowed to be worst-case, this
will force a learner to engage in “brute-force exploration” —
the iterative deployment of a large fraction of the resources to
each subgroup in turn. This is formalized in Theorem 2.
Theorem 2. Define m∗ = maxi ∈[G]mi to be the size of
the largest group and assume mi > 6 for all i and G ≥ 2.
Let α ∈ [0, 1/(2m∗)), δ ∈ (0, 1/2), and A be any learning
algorithm for the precision model which runs for a finite number
of rounds and outputs an allocation. Suppose that there is some
group i for which A has not allocated at leastmi/2 units for
at least k ln(1/δ )/(αmi ) rounds upon termination, where k is
an absolute constant. Then there exists a candidate distribution
such that, with probability at least δ , A outputs an allocation
that is not α-fair.
Sketch of the Proof. Let i denote a group in which A
has not allocated at leastmi/2 units for at leastk ln(1/δ )/(αmi )
rounds upon its termination and let v denote an arbitrary al-
location. We will design two candidate distributions for group
i which have true discovery probabilities that are at least 2α
apart given vi , but which are indistinguishable given the ob-
servations of the algorithm with probability at least δ . If theA
cannot distinguish between Ci and C
′
i , it cannot distinguish
between fi and f ′i , and thus cannot guarantee whether group
i’s discovery probability is indeed within α of every other
group’s discovery probability.
To design these candidate distributions, consider distribu-
tions Ci and C
′
i which satisfy the following four conditions.
(1) Ci and C
′
i agree on all values less thanmi/2 − 2.
(2) The total mass of both distributions belowmi/2 − 2 is
1 − 2αmi .
(3) The remaining 2αmi mass of Ci is on the valuemi/2−1.
(4) The remaining 2αmi mass of C′i is on the valuemi .
Distinguishing between Ci and C
′
i requires at least one un-
censored observation beyondmi/2 − 2. However, conditioned
on allocating at leastmi/2 units, the probability of observing
an uncensored observation is at most 2αmi . So to distinguish
between Ci and C
′
i with confidence 1−δ , and therefore to guar-
antee an α-fair allocation, a learning algorithm must allocate
at leastmi/2 units to group i for k ln(1/δ )/(αmi ) rounds. □
Recall that we used m∗ to denote the size of the largest
group. When m∗ > 2V , then Theorem 2 implies that no
algorithm can guarantee α-fairness for sufficiently small α .
Moreover, even when m∗ ≤ 2V , Theorem 2 shows that in
general, if we want algorithms that have provable guarantees
for arbitrary candidate distributions, it is impossible to avoid
something akin to brute-force search (recall that there is a
trivial algorithm which simply allocates all resources to each
group in turn, for sufficiently many rounds to approximately
learn the CDF of the candidate distribution, and then solves
the offline problem). In the next section, we circumvent this by
giving an algorithm with provable guarantees, assuming that
the candidate distributions have a known parametric form.
3.4 Poisson Distributions and Convergence
of the MLE
In this section, we assume that all the candidate distributions
have a particular and known parametric form but that the
parameters of the these distributions are not known to the
allocator. Concretely, we assume that the candidate distribu-
tion for each group is Poisson
λ∗ = (λ∗
G
) for the true underlying parameters of the
candidate distributions; this choice appears justified, at least
in the predictive policing application, as the candidate dis-
tributions in the Philadelphia Crime Incidents dataset are
well-approximated by Poisson distributions (see Section 4 for
further discussion). This assumption allows an algorithm to
learn the tails of these distributions without needing to rely
on brute-force search, thus circumventing the limitation given
in Theorem 2. Indeed, we show that (a small variant of) the
natural greedy algorithm incorporating these distributional
assumptions converges to an optimal fair allocation.
For simplicity, we assume a parametric form on the mar-
ginal candidate distribution in each of the groups. We could
have equivalently assumed that the candidates across groups
are drawn from amultivariate Poisson distribution to highlight
the (potential) correlation between candidates distributions.
However, since for a given multivariate Poisson distribution
the marginal distribution on each group is itself a Poisson
distribution [15], we made our parametric assumption directly
on these marginal distributions.
At a high level, in each round, our algorithm uses Algo-
rithm 1 to calculate an optimal fair allocation with respect to
the current maximum likelihood estimates of the group dis-
tributions; then, it uses the new observations it obtains from
this allocation to refine these estimates for the next round.
This is summarized in Algorithm 2. The algorithm differs from
this pure greedy strategy in one respect, to overcome the fol-
lowing subtlety: there is a possibility that Algorithm 1, when
operating on a preliminary estimate for the candidate distri-
butions, will suggest sending zero units to some group, even
when the optimal allocation for the true distributions sends
some units to every group. Such a deployment would result
in the algorithm receiving no feedback for the zero-allocated
distribution to satisfy the bounded support condition. However, the distinction
will not be important for the analysis, and so to minimize technical overhead,
we perform the analysis assuming an untruncated Poisson.
group that round. If this suggestion is followed and a lack
of feedback is allowed to persist indefinitely, the algorithm’s
parameter estimate for the zero-allocated group will also stop
updating — potentially at an incorrect value. In order to avoid
this problem and continue making progress in learning, our
algorithm chooses another allocation in this case. As we show,
any allocation that allocates positive resources to all groups
will suffice; in particular, our algorithm simply repeats the
allocation from the previous round.
Algorithm 2 Learning an optimal fair allocation
Input: α ,V and T (total number of rounds).
Output: An allocation vT+1 and estimates to parameters
{λTi }.
v1 ← (⌊(V/G)⌋, . . . , ⌊(V/G)⌋).
for rounds t = 1, . . . ,T do
if ∃i such that vti == 0 then
vt ← vt−1.
Observe oti = min{vti , c
t
i } for each group.
for i = 1, . . . ,G do
Update history ht+1i with oti and v
t
i .
ˆλti ← argmaxλ∈[λmin,λmax]
ˆL (ht+1i , λ).
vt+1 ← Algorithm 1(α , {C ( ˆλti )},V ).
return vT+1 and {λTi }.
Notice that Algorithm 2 chooses an allocation at every
round which is fair with respect to its estimates of the param-
eters of the candidate distributions; hence, asymptotic con-
vergence of its output to an optimal α-fair allocation follows
directly from the convergence of the estimates to true param-
eters. However, we seek a stronger, finite sample guarantee,
as stated in Theorem 3.
Theorem 3. Let ϵ,δ > 0. Suppose that the candidate distribu-
tions are Poisson distributions with unknown parameters in the
vector λ∗, where λ∗ lies in the known interval [λmin, λmax]
G .
Suppose we run Algorithm 2 for t > ˜O (ln(G/δ )/(η(ϵ ))2) ≜
Tmax rounds, where η(·) is some distribution specific function to
get an allocation v̂ and estimated parameters ˆλi for all groups i .
Then with probability at least 1 − δ
(1) For all i in [G], | ˆλi − λ∗i | ≤ ϵ .
(2) Let D = maxi ∈[G] DTV (C (λ∗i ),C (
ˆλi )) where DTV de-
notes the total variation distance between two distribu-
tions. Then v̂
• is (α + 4D)-fair.
• has utility at most 4DGV smaller than the utility
of an optimal (α − 4D)-fair allocation i.e. χ (v̂) ≥
χ (wα−4D ) − 4DGV .
Remark 2. Theorem 3 implies that in the limit, the allocation
from Algorithm 2 will converge to an optimal α-fair allocation.
As t → ∞, ˆλi
p
→ λ∗i for all i , meaning D → 0 and more
importantly, v̂ will be α-fair and optimal.
To prove Theorem 3, we first show that any sequence of
allocations selected by Algorithm 2 will eventually recover the
true parameters. There are two conceptual difficulties here:
the first is that standard convergence results typically leverage
the assumption of independence, which does not hold in this
case as Algorithm 2 computes adaptive allocations which de-
pend on the allocations in previous rounds; the second is the
censoring of the observations. Despite these difficulties, we
give quantifiable rates with which the estimates converge to
the true parameters. Next, we show that computing an optimal
α-fair allocation using the estimated parameters will result
in an allocation that is (α + 4D)-fair with respect to the true
candidate distributions where D denotes the maximum total
variation distance between the true and estimated Poisson
distributions across all groups. Finally, we show that this allo-
cation also achieves a utility that is comparable to the utility
of an optimal (α − 4D)-fair allocation.
Remark 3. Although we assumed Poisson distributions in this
section, all our results hold for any single-parameter Lipschitz-
continuous distribution whose parameter is drawn from a com-
pact set. However, the convergence rate of Theorem 3 depends on
the quantity η(ϵ ) which depends on the family of distributions
used to model the candidate distributions.
4 EXPERIMENTS
In this section, we apply our allocation and learning algorithms
for the precision model to the Philadelphia Crime Incidents
dataset, and complement the theoretical convergence guaran-
tee of Algorithm 2 to an optimal fair allocation with empirical
evidence suggesting fast convergence in practice. We also
study the trade-off between fairness and utility in the dataset.
4.1 Experimental Design
The Philadelphia Crime Incidents dataset
crimes reported to the Police Department’s INCT system be-
tween 2006 and 2016. The crimes are divided into two types.
Type I crimes include violent offenses such as aggravated as-
sault, rape, and arson among others. Type II crimes include
simple assault, prostitution, gambling and fraud. For simplicity,
we aggregate all crime of both types, but in practice, a police
department would of course treat different categories of crime
differently. We note as a caveat that these crimes are reported
and may not represent the entirety of committed crimes.
To create daily crime frequencies in Figure 1, we first cal-
culate the daily counts of criminal incidents in each of the
21 geographical police districts in Philadelphia by grouping
together all the crime reports with the same date; we then
normalize these counts to get frequencies.
Figure 1 represents a police district. The horizontal axis of the
subfigure corresponds to the number of reported incidents in a
day and the vertical axis represents the frequency of each num-
ber on the horizontal axis. These frequencies approximate the
true (marginal) distributions of the number of reported crimes
in each of the districts in Philadelphia. Therefore, throughout
districts-units/index.html. The dataset however contains 25 districts; we re-
moved Districts 77 and 92, which correspond to the PHL airport and urban
parks, as well as 4 and 23, which were dissolved in 2010.
Figure 1: Frequencies of the number of reported crimes
in each district in the Philadelphia Crime Incidents
dataset. The red curves display the best Poisson fit to
the data.
this section we take these frequencies as the ground truth can-
didate distributions for the number of reported incidents in
each of the districts.
Figure 1 shows that crime distributions in different districts
can be quite different; e.g., the average number of daily re-
ported incidents in District 15 is 43.5, which is much higher
than the average of 11.35 in District 1. Despite these differ-
ences, each of the crime distributions can be approximated
well by a Poisson distribution. The red curves overlayed in
each subfigure correspond to the Poisson distribution obtained
via maximum likelihood estimation on data from that district.
Throughout, we refer to such distributions as the best Poisson
fit to the data. districts as the resource to be distributed, the
ground truth crime frequencies as candidate distributions, and
aim to maximize the sum of the number of crimes discovered
under the precision model of discovery.
4.2 Results
We can quantify the extent to which fairness degrades utility
in the dataset through a notion we call Price of Fairness (PoF
henceforth). In particular, given the ground truth crime dis-
tributions and the precision model of discovery, for a fairness
level α , we define PoF(α ) = χ (w∗)/χ (wα ). The PoF is simply
the ratio of the expected number of crimes discovered by an
optimal allocation to the expected number of crimes discov-
ered by an optimal α-fair allocation. Since χ (w∗) ≥ χ (wα )
for all α , the PoF is at least one. Furthermore, the PoF is mono-
tonically non-increasing in α . We can apply the algorithms
given in Sections 3.1 and 3.2 respectively for computing opti-
mal unconstrained, and optimal fair allocations with the with
ground truth distributions as input and numerically compute
the PoF. This is illustrated in Figure 2. The x axis corresponds
to different α values and the y axis displays 1/PoF(α ). Each
curve corresponds to a different number of total police officers
denoted byV . Because feasible allocations must be integral,
there can sometimes be no feasible α-fair allocation for small
α . Since the PoF in these cases is infinite we instead opt to
display the inverse, 1/PoF, which is always bounded in [0, 1].
Higher values of inverse PoF are more desirable.
Figure 2: Inverse PoF plots for the Philadelphia Crime
Incidents dataset. Smaller values indicate greater sacri-
fice in utility to meet the fairness constraint.
Figure 2 shows a diverse set of utility/fairness trade-offs
depending on the number of available police officers. It also
illustrates that the cost of fairness is rather low inmost regimes.
For example, in the worst case, with only 50 police officers (the
black curve) (which is much smaller than the average number
of daily reported crimes: 563.88), the inverse PoF is 1 for α ≥
0.1, which corresponds to a 10% difference in the discovery
probability across districts. When we increase the number of
available police officers to 400 (the magenta curve), tolerating
only a 4% difference in the discovery probability across districts
is sufficient to guarantee no loss in the utility. Figure 2 also
shows that for any fixedα , the inverse PoF(α ) tends to increase
as the number of police increases (i.e. the cost of fairness
decreases).
a less costly constraint when resources are in greater supply.
Finally, we observe a thresholding phenomenon in Figure 2; in
each curve, increasing α beyond a threshold will significantly
increase the inverse PoF. This is due to discretization effects,
since only integral allocations are feasible.
We next turn into analyzing the performance of Algorithm 2
in practice. We run the algorithm instantiated to fit Poisson
distributions, but use observations from the ground truth dis-
tribution at each round. As we have shown in Figure 1,the
ground truth is well approximated by a Poisson distribution.
We measure the performance of Algorithm 2 as follows.
First, we fix a police budgetV and unfairness budgetα and run
Algorithm 2 for 2000 rounds using the dataset as the ground
truth. That is, we simulate each round’s crime count realiza-
tions in each of the districts as being sampled from the ground
truth distributions, and return censored observations under the
precision model to Algorithm 2 according to the algorithm’s
allocations and the drawn realizations. The algorithm returns
is between 0.03 and 0.04, the inverse PoF decreases as V increases from 100 to
200. This occurs because only integral allocations are feasible, so achieving a
particular fairness level may require leaving some resources unallocated until
significantly more resources become available; increasing V in this regime
improves the utility of an optimal allocation while leaving the utility of an
optimal fair allocation unchanged.
an allocation after termination and we can measure the ex-
pected number of crimes discovered and fairness violation (the
maximum difference in discovery probabilities over all pairs
of districts) of the returned allocation using the ground truth
distributions. Varying α while fixingV allows us to trace out
the Pareto frontier of the utility/fairness trade-off for a fixed
police budget. Similarly, for any fixed V and α , we can run
Algorithm 1 (the offline algorithm for computing an optimal
fair allocation) with the ground truth distributions as input
and trace out a Pareto curve by varying α . We refer to these
two Pareto curves by the learned and optimal Pareto curves,
respectively.
can compare the learned and optimal Pareto curves.
Figure 3: Pareto frontier of expected crimes discovered
versus fairness violation.
In Figure 3, each curve corresponds to a police budget. The x
andy axes represent the expected number of crimes discovered
and fairness violation for allocations on the Pareto frontier,
respectively. In our simulations we variedα between 0 and 0.15.
For each police budgetV , the ‘x’ s connected by the dashed
lines show the learning Pareto frontier. Similarly, the circles
connected by solid lines show the optimal Pareto frontier. We
point out that while it is possible for the fairness violations
in the learned Pareto curves to be higher than the level of α
set as an input to Algorithm 2, the fairness violations in the
optimal Pareto curves are always bounded by α .
The disparity between the optimal and learned Pareto curves
are due to the fact that the learning algorithm has not yet
fully converged. This can be attributed to the large number
of censored observations received by Algorithm 2, which are
significantly less informative than uncensored observations.
Censoring happens frequently because the number of police
used in every case plotted is less than the daily average of
563.88 crimes across all the districts in the dataset — so it is
unavoidable that in any allocation, there will be significant
censoring in at least some districts.
Figure 3 shows that while the learning curves are domi-
nated by the optimal curves, the performance of the learning
instead of the ground truth distributions. These curves look very similar to the
optimal Pareto curves.
algorithm approaches the performance of the offline optimal
allocation as V increases. Again, this is because increasing
V generally decreases the frequency of censoring. We study
theV = 500 regime in more detail, to explore the empirical
rate of convergence. In Figure 4, we study the round by round
performance of the allocation computed by Algorithm 2 in a
single run with the choice ofV = 500 and α = 0.05.
Figure 4: The per round expected number of crimes dis-
covered and fairness violation of Algorithm 2.V = 500
and α = 0.05.
In Figure 4, the x axis labels progression of rounds of the
algorithm. The y axis measures the fairness violation (left)
and expected number of crimes discovered (right) of the allo-
cation deployed by the algorithm, as measured with respect
to the ground truth distributions. The black curves represent
Algorithm 2. For comparison we also show the same quan-
tities for an offline optimal fair allocation as computed with
respect to the ground truth (red line), and an offline optimal
fair allocation as computed with respect to the best Poisson
fit to the ground truth (blue line). Note that in the limit, the
allocations chosen by Algorithm 2 are guaranteed to converge
to the blue baselines — but not the red baseline, because the
algorithm is itself learning a Poisson approximation to the
ground truth. The disparity between the red and blue lines
quantifies the degradation in performance due to using Pois-
son approximations, rather than due to non-convergence of
the learning process.
Figure 4 shows that Algorithm 2 converges to the Poisson
approximation baseline well before the termination time of
2000, and substantially before the convergence bound guar-
anteed by our theory. Examining the estimated Poisson pa-
rameters used internally by Algorithm 2 reveals that although
the allocation has converged to an optimal fair allocation, the
estimated parameters have not yet converged to the parame-
ters of the best Poisson fit in any of the districts. In particular,
Algorithm 2 systematically underestimates the parameters in
all of the districts: the correlation coefficient between the true
and estimated parameters is 0.9975.
We see also in Figure 4 that convergence to the optimum
expected number of discovered crimes occurs more quickly
than convergence to the target fairness violation level. This
is also apparent in Figure 3 where the learning and optimal
Pareto curves are generally similar in terms of the maximum
number of crimes discovered, while the fairness violations are
higher in the learning curves.
5 THE RANDOM DISCOVERY MODEL
Finally, we consider the random model of discovery. In the
random model, when vi units are allocated to a group with ci
candidates, the number of discovered candidates is a random
variable corresponding to the number of candidates that ap-
pear in a uniformly random sample of vi individuals from a
group of sizemi . Equivalently, whenvi units are allocated to a
group of sizemi with ci candidates, the number of candidates
discovered by disc(·) is a random variable disc(vi , ci ) ≜ oi ,
where oi is drawn from the hypergeometric distribution with
parametersmi , ci and vi . Furthermore, the expected number
of candidates discovered when allocating vi units to group i
is E[disc(vi , ci )] = vi E[ci ]/mi .
For simplicity, throughout this section, we assumemi ≥ V
for all i . This assumption can be completely relaxed.Moreover,
let µi = E[ci ]/mi denote the expected fraction of candidates in
group i . Without loss of generality, for the rest of this section,
we assume µ1 ≥ µ2 ≥ . . . ≥ µG .
5.1 Optimal Allocation
In this section, we characterize optimal allocations. Note that
the expected number of candidates discovered by the alloca-
tion choice vi ≤ mi in group i is simply vi µi . This suggests a
simple algorithm to compute w∗: allocating every unit of the
resource to group 1. More generally, let G∗ = {i | µi = µ1}
denote the subset of groups with the highest expected number
of candidates. An allocation is optimal if and only if it only
allocates all resources to groups in G∗.
5.2 Properties of Fair Allocations
We next discuss the properties of fair allocations in the ran-
dom discovery model. First, we point out that the discovery
probability can be simplified as
fi (vi ) = E
ci∼Ci
[
civi/mi
ci
]
=
vi
mi
.
So an allocation is α-fair in the random model if |vi/mi −
vj/mj | ≤ α for all groups i and j. Therefore, fair allocations
(roughly) distribute resources in proportion to the size of the
groups, essentially ignoring the candidate distributions within
each group.
5.3 Price of Fairness
Recall that PoF quantifies the extent to which constraining
the allocation to satisfy α-fairness degrades utility. While in
Section 4 we study the PoF on the Philadelphia Crime Incidents
dataset, we can define a worst-case variant as follows.
Definition 2. Fix the random model of crime discovery and
let α ∈ [0, 1]. We define the PoF as
PoF(α ) = max
C
χ (w∗,C)
χ (wα ,C)
.
where C ranges over all possible candidate distributions.
We can fully characterize this worst-case PoF in the random
discovery model.
Theorem 4. The PoF in the random discovery model is
PoF(α ) =


1, V
m1
≤ α ,
M
m1+α (M−m1 )
, V
m1
> α .
The PoF in the random model can be as high as M/m1 in
the worst case. If all groups are identically sized, this grows
linearly with the number of groups.
6 CONCLUSION AND FUTURE
DIRECTIONS
Our presentation of allocative fairness provides a family of
fairness definitions, modularly parameterized by a “discov-
ery model”. What counts as “fair” depends a great deal on
the choice of discovery model, which makes explicit what
would otherwise be unstated assumptions about the process
of tasks like policing. The random and precision models of
discovery studied in this paper represent two extreme points
of a spectrum. In the predictive policing setting, the random
model of discovery assumes that officers have no advantage
over random guessing when stopping individuals for further
inspection. The precision model assumes they can oracularly
determine offenders, and stop only them. An interesting direc-
tion for future work is to study discovery models that lie in
between these two.
We have also made a number of simplifying assumptions
that could be relaxed. For example, we assumed the candidate
distributions are stationary — fixed independently of the ac-
tions of the algorithm. Of course, the deployment of police
officers can change crime distributions. Modeling this kind
of dynamics, and designing learning algorithms that perform
well in such dynamic settings would be interesting. Finally,
we have assumed that the same discovery model applies to
all groups. One friction to fairness that one might reasonably
conjecture is that the discovery model may differ between
groups — being closer to the precision model for one group,
and closer to the random model for another. We leave the
study of these extensions to future work.
Acknowledgements
We thank Sorelle Friedler for giving a talk at Penn which ini-
tially inspired this work. We also thank Carlos Scheidegger,
Kristian Lum, Sorelle Friedler, and Suresh Venkatasubrama-
nian for helpful discussions at an early stage of this work.
Finally, we thank Richard Berk and Greg Ridgeway for help-
ful discussions about predictive policing. We finally thank the
anonymous reviewer for helpful comments regarding Figure 2.
REFERENCES
[1] Alekh Agarwal, Peter Bartlett, and Max Dama. 2010. Optimal allocation strate-
gies for the dark pool problem. In Proceedings of the 13th International
Conference on Artificial Intelligence and Statistics, pages 9–16.
[2] Hamsa Bastani, Mohsen Bayati, and Khashayar Khosravi. 2017. Exploiting the
natural exploration in contextual bandits. CoRR, abs/1704.09011.
[3] Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael
Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. 2017. A convex frame-
work for fair regression. CoRR, abs/1706.02409.
[4] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron
Roth. 2018. Fairness in criminal justice risk assessments: The state of the art.
Sociological Methods & Research.
[5] Sarah Bird, Solon Barocas, Kate Crawford, Fernando Diaz, and Hanna
Wallach. 2016. Exploring or exploiting? social and ethical implications of au-
tonomous experimentation in AI.
[6] Toon Calders, Asim Karim, Faisal Kamiran, Wasif Ali, and Xiangliang
Zhang. 2013. Controlling attribute effect in linear regression. In Proceedings of
13th International Conference on Data Mining, pages 71–80.
[7] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. 2017.
Fair clustering through fairlets. In Proceedings of the 31th Annual Conference
on Neural Information Processing Systems, pages 5029–5037.
[8] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017.
Algorithmic decision making and the cost of fairness. In Proceedings of the
23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pages 797–806.
[9] Cynthia Dwork and Christina Ilvento. 2019. Fairness under composition. In
Proceedings of the 10th Innovations in Theoretical Computer Science.
[10] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and
Richard Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd
Innovations in Theoretical Computer Science, pages 214–226.
[11] Danielle Ensign, Sorelle Friedler, Scott Neville, Carlos Scheidegger, and
Suresh Venkatasubramanian. 2018. Runaway feedback loops in predictive polic-
ing. In Conference on Fairness, Accountability and Transparency, pages
160–171.
[12] Danielle Ensign, Sorelle Frielder, Scott Neville, Carlos Scheidegger, and
Suresh Venkatasubramanian. 2018. Decision making with limited feedback. In
Proceedings of the 29th Conference on Algorithmic Learning Theory, pages
359–367.
[13] Kuzman Ganchev, Michael Kearns, Yuriy Nevmyvaka, and Jennifer Wort-
man Vaughan. 2009. Censored exploration and the dark pool problem. In
Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence,
pages 185–194.
[14] Moritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of opportunity in
supervised learning. In Proceedings of the 30th Annual Conference on Neural
Information Processing Systems, pages 3315–3323.
[15] David Inouye, Eunho Yang, Genevera Allen, and Pradeep Ravikumar.
2017. A review ofmultivariate distributions for count data derived from the poisson
distribution. Wiley Interdisciplinary Reviews: Computational Statistics, 9.
[16] Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, and
Aaron Roth. 2017. Fairness in reinforcement learning. In Proceedings of the 34th
International Conference on Machine Learning, pages 1617–1626.
[17] Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. 2016.
Fairness in learning: classic and contextual bandits. In Proceedings of the
30th Annual Conference on Neural Information Processing Systems, pages
325–333.
[18] Sampath Kannan, Jamie Morgenstern, Aaron Roth, Bo Waggoner, and
Zhiwei Steven Wu. 2018. A smoothed analysis of the greedy algorithm for
the linear contextual bandit problem. In Proceedings of the 32nd Annual
Conference on Neural Information Processing Systems.
[19] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2017. Inherent
trade-offs in the fair determination of risk scores. In Proceedings of the 8th
Innovations in Theoretical Computer Science, pages 43:1–43:23.
[20] Lydia Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. 2018.
Delayed impact of fair machine learning. In Proceedings of the 35th Inter-
national Conference on Machine Learning, pages 3156–3164.
[21] Kristian Lum and William Isaac. October 2016. To predict and serve? Significance, pages
14–18.
[22] Ariel Procaccia. 2013. Cake cutting: Not just child’s play. Communications of
the ACM, 56(7):78–87
[23] Manish Raghavan, Aleksandrs Slivkins, Jennifer Wortman Vaughan, and
Zhiwei Steven Wu. 2018. The externalities of exploration and how data diversity
helps exploitation. In Proceedings of the 31st Conference On Learning Theory,
pages 1724–1738
[24] Blake Woodworth, Suriya Gunasekar, Mesrob Ohannessian, and Nathan
Srebro. 2017. Learning non-discriminatory predictors. In Proceedings of the 30th
Conference on Learning Theory, pages 1920–1953
[25] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Kr-
ishna Gummadi. 2017. Fairness beyond disparate treatment & disparate impact:
Learning classification without disparate mistreatment. In Proceedings of
the 26th International Conference on World Wide Web, pages 1171–1180.
[26] Richard Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia
Dwork. 2013. Learning fair representations. In Proceedings of the 30th Interna-
tional Conference on Machine Learning, pages 325–333.
