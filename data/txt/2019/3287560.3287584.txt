A Moral Framework for Understanding Fair ML through Economic Models of Equality of Opportunity
Hoda Heidari
ETH Z√ºrich
hheidari@inf.ethz.ch
Michele Loi
University of Z√ºrich
michele.loi@uzh.ch
Krishna P. Gummadi
MPI-SWS
gummadi@mpi-sws.org
Andreas Krause
ETH Z√ºrich
krausea@ethz.ch
ABSTRACT
We map the recently proposed notions of algorithmic fairness to
economic models of Equality of opportunity (EOP)‚Äîan extensively
studied ideal of fairness in political philosophy. We formally show
that through our conceptual mapping, many existing definition of
algorithmic fairness, such as predictive value parity and equality
of odds, can be interpreted as special cases of EOP. In this respect,
our work serves as a unifying moral framework for understand-
ing existing notions of algorithmic fairness. Most importantly, this
framework allows us to explicitly spell out the moral assumptions
underlying each notion of fairness, and interpret recent fairness
impossibility results in a new light. Last but not least and inspired
by luck egalitarian models of EOP, we propose a new family of mea-
sures for algorithmic fairness.We illustrate our proposal empirically
and show that employing a measure of algorithmic (un)fairness
when its underlying moral assumptions are not satisfied, can have
devastating consequences for the disadvantaged group‚Äôs welfare.
CCS CONCEPTS
‚Ä¢ Computing methodologies ‚Üí Supervised learning; Batch
learning; ‚Ä¢ Applied computing ‚Üí Economics; Sociology;
KEYWORDS
Equality of Opportunity (EOP), Fairness for Machine Learning,
Rawlsian and Luck Egalitarian EOP, Statistical Parity, Equality of
Odds, Predictive Value Parity
ACM Reference Format:
Hoda Heidari, Michele Loi, Krishna P. Gummadi, and Andreas Krause. 2019.
A Moral Framework for Understanding Fair ML, through Economic Models
of Equality of Opportunity. In FAT* ‚Äô19: Conference on Fairness, Account-
ability, and Transparency (FAT* ‚Äô19), January 29‚Äì31, 2019, Atlanta, GA, USA.
ACM,NewYork, NY, USA, 10 pages. https://doi.org/10.1145/3287560.3287584
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
FAT* ‚Äô19, January 29‚Äì31, 2019, Atlanta, GA, USA
¬© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6125-5/19/01. . . $15.00
https://doi.org/10.1145/3287560.3287584
1 INTRODUCTION
Equality of opportunity (EOP) is a widely supported ideal of fair-
ness, and it has been extensively studied in political philosophy
over the past 50 years [1, 7, 9, 10, 22, 27]. The concept assumes the
existence of a broad range of positions, some of which are more
desirable than others. In contrast to equality of outcomes (or posi-
tions), an equal opportunity policy seeks to create a level playing
field among individuals, after which they are free to compete for
different positions. The positions that individuals earn under the
condition of equality of opportunity reflect their merit or deserv-
ingness, and for that reason, inequality in outcomes is considered
ethically acceptable [24].
Equality of opportunity emphasizes the importance of personal
(or native) qualifications, and seeks to minimize the impact of cir-
cumstances and arbitrary factors on individual outcomes [7, 9, 10,
22]. For instance within the context of employment, one (narrow)
interpretation of EOP requires that desirable jobs are given to those
persons most likely to perform well in them‚Äîe.g. those with the
necessary education and experience‚Äîand not according to arbitrary
factors, such as race or family background. According to Rawls‚Äôs
(broader) interpretation of EOP, native talent and ambition can jus-
tify inequality in social positions, whereas circumstances of birth
and upbringing such as sex, race, and social background can not.
Many consider the distinction between morally acceptable and
unacceptable inequality the most significant contribution of the
egalitarian doctrine [26].
Prior work in economics has sought to formally characterize
conditions of equality of opportunity to allow for its precise mea-
surement in practical domains (see e.g. [12, 25]). At a high level,
in these models an individual‚Äôs outcome/position is assumed to
be affected by two main factors: his/her circumstance c and effort
e . Circumstance c is meant to capture all factors that are deemed
irrelevant, or for which the individual should not be held morally
accountable; for instance c could specify the socio-economic status
he/she is born into. Effort e captures all accountability factors‚Äî
those that can morally justify inequality. (Prior work in economics
refers to e as effort for the sake of concreteness, but e summarizes
all factors for which the individual can be held morally accountable;
the term ‚Äúeffort" should not be interpreted in its ordinary sense
here.) For any circumstance c and any effort level e , a policy œï in-
duces a distribution of utility among people of circumstance c and
effort e . Formally, an EOP policy will ensure that an individual‚Äôs
final utility will be, to the extent possible, only a function of their
effort and not their circumstances.
While EOP has been traditionally discussed in the context of
employment practices, its scope has been expanded over time to
other areas, including lending, housing, college admissions, and
beyond [29]. Decisions made in such domains are increasingly auto-
mated andmade throughAlgorithmic Data Driven DecisionMaking
systems (A3DMs). We argue, therefore, that it is only natural to
study fairness for A3DMs through the lens of EOP. In this work, we
draw a formal connection between the recently proposed notions of
fairness for supervised learning and economic models of EOP. We
observe that in practice, predictive models inevitably make errors
(e.g. the model may mistakenly predict that a credit-worthy appli-
cant won‚Äôt pay back their loan in time). Sometimes these errors
are beneficial to the subject, and sometimes they cause harm. We
posit that in this context, EOP would require similar individuals
(in terms of what they can be held accountable for) to have the
same prospect of receiving this benefit/harm, irrespective of their
irrelevant characteristics.
More precisely, we assume that a person‚Äôs features can be par-
titioned into two sets: those for which we consider it morally ac-
ceptable to hold him/her accountable, and those for which it is
not so. We will broadly refer to the former set of attributes as the
individual‚Äôs accountability features, and the latter, as their arbitrary
or irrelevant features. Note that there is considerable disagreement
on the criteria to determine what factors should belong to each
category. Roemer [23] for instance proposes that societies decide
this democratically. We take a neutral stance on this issue and
leave it to domain experts and stake-holders to reach a resolution.
Throughout, we assume this partition has been identified and is
given.
We distinguish between an individual‚Äôs actual and effort-based
utility when subjected to algorithmic decision making. We assume
an individual‚Äôs advantage or total utility as the result of being
subject to A3DMs, is the difference between their actual and effort-
based utility (Section 2). Our main conceptual contribution is to
map the supervised learning setting to economic models of EOP by
treating predictive models as policies, irrelevant features as individ-
ual circumstance, and effort-based utilities as effort (Figure 1). We
show that using this mapping many existing notions of fairness for
classification, such as predictive value parity [18] and equality of
odds [14], can be interpreted as special cases of EOP. In particular,
equality of odds is equivalent to Rawlsian EOP, if we assume all
individuals with the same true label are equally accountable for
their labels and have the same effort-based utility (Section 3.1).
Similarly, predictive value parity is equivalent to luck egalitarian
EOP if the predicted label/risk is assumed to reflect an individual‚Äôs
effort-based utility (Section 4). In this respect, our work serves as a
unifying framework for understanding existing notions of algorith-
mic fairness as special cases of EOP. Importantly, this framework
allows us to explicitly spell out the moral assumptions underlying
each notion of fairness, and interpret recent fairness impossibility
results [18] in a new light.
Last but not least, inspired by Roemer‚Äôs model of egalitarian EOP
we present a new family of measures for algorithmic (un)fairness,
applicable to supervised learning tasks beyond binary classifica-
tion. We illustrate our proposal on a real-world regression dataset,
and compare it with existing notions of fairness for regression. We
Policy ùùì Predictive model h 
Effort e Effort-based utility d
Circumstance c Irrelevant features z
Utility u Actual - effort-based utility (a - d) 
Economic Models of EOP Fair Machine Learning
Figure 1: Our proposed conceptual mapping between Fair
ML and economic literature on EOP.
empirically show that employing the wrong measure of algorith-
mic fairness‚Äîwhen the moral assumptions underlying it are not
satisfied‚Äîcan have devastating consequences on the welfare of the
disadvantaged group.
We emphasize that our work is not meant to advocate for any par-
ticular notion of algorithmic fairness, rather our goal is to establish‚Äî
both formally and via real-world examples‚Äîthat implicit in each
notion of fairness is a distinct set of moral assumptions about de-
cision subjects; therefore, each notion of fairness is suitable only
in certain application domains and not others. By making these
assumptions explicit, our framework presents practitioners with a
normative guideline to choose the most suitable notion of fairness
specifically for every real-world context in which A3DMs are to be
deployed.
1.1 Equality of Opportunity: An Overview
Equality of opportunity has been extensively debated among po-
litical philosophers. Philosophers such as Rawls [22], Dworkin [9],
Arneson [1], and Cohen [7] contributed to the egalitarian school of
thought by proposing different criteria for making the cut between
arbitrary and accountability factors. The detailed discussion of their
influential ideas is outside the scope of this work, and the interested
reader is referred to excellent surveys by Arneson [2] and Roemer
and Trannoy [26].
In this section, we briefly mention several prominent interpre-
tations of EOP and discuss their relevance to A3DMs. Following
Arneson [3], we recount three main conceptions of equality of
opportunity:
‚Ä¢ Libertarian EOP: A person is morally at liberty to do what
she pleases with what she legitimately owns (e.g. self, busi-
ness, etc.) as long as it does not infringe upon other people‚Äôs
moral rights (e.g. the use of force, fraud, theft, or damage
on persons or property of another individual is considered a
violation of their rights). Other than these restrictions, any
outcome that occurs as the result of people‚Äôs free choices on
their legitimate possessions is considered just. In the context
of A3DMs and assuming no gross violations of individuals‚Äô
data privacy rights, this interpretation of EOP leaves the en-
terprise at total liberty to implement any algorithm it wishes
through Economic Models of Equality of Opportunity FAT* ‚Äô19, January 29‚Äì31, 2019, Atlanta, GA, USA
for decision making. The algorithm can utilize all available
information, including individuals‚Äô sensitive features such as
race or gender, to make (statistically) accurate predictions.
‚Ä¢ Formal EOP: Also known as ‚Äúcareers open to talents", for-
mal EOP require desirable social positions to be open to all
who possess the attributes relevant for the performance of
the duties of the position (e.g. anyone who meets the formal
requirements of the job) and wish to apply for them [25].
The applications must be assessed only based on relevant
attributes/qualifications that advances the morally innocent
goals of the enterprise. Direct discrimination based on factors
deemed arbitrary (e.g. race or gender) is therefore prohibited
under this interpretation of EOP. Formal EOP would permit
differences in people‚Äôs circumstances‚Äîe.g. their gender‚Äîto
have indirect, but nonetheless deep impact on their prospects.
For instance, if women are less likely to receive higher edu-
cation due to prejudice against female students, as long as
a hiring algorithm is blind to gender and applies the same
educational requirement to male and female job applicants,
formal equality of opportunity is maintained. In context of
A3DMs, Formal EOP is equivalent to the removal of the sen-
sitive feature information from the learning pipeline. In the
fair ML community, this is sometimes referred to as ‚Äúfairness
through blindness".
‚Ä¢ Substantive EOP: Substantive EOPmoves the starting point
of the competition for desirable positions further back in
time, and requires not only open competition for desirable
positions, but also fair access to the necessary qualifications
for the position. This implies access to qualifications (e.g.
formal requirements for a job) should not to be affected by
arbitrary factors, such as race gender or social class. The
concept is closely related to indirect discrimination: if the
A3DM indirectly discriminates against people with a certain
irrelevant feature (e.g. women or African Americans) this
may be an indication that the irrelevant/arbitrary feature has
played a role in the acquisition of the requirements. When
there are no alternative morally acceptable explanations for
it, indirect discrimination is often considered in violation of
substantive EOP.
Our focus in this work is on substantive EOP, and in particular, on
two of its refinements, called Rawlsian EOP and Luck Egalitarian
EOP.
Rawlsian EOP. According to Rawls, those who have the same
level of talent or ability and are equally willing to use them must
have the same prospect of obtaining desirable social positions, re-
gardless of arbitrary factors such as socio-economic background [22].
This Rawlsian conception of EOP has been translated into pre-
cise mathematical terms as follows [19]: let c denote circumstance,
capturing factors that are not considered legitimate sources of in-
equality among individuals. Let scalar e summarize factors that are
viewed as legitimate sources of inequality. For the sake of brevity,
the economic literature refer to e as ‚Äúeffort", but e is meant to sum-
marize all factors an individual can be held morally accountable
for.1 Let u specify individual utility, which is a consequence of ef-
fort, circumstance, and policy. Formally, let Fœï (.|c, e ) specify the
cumulative distribution of utility under policy œï at a fixed effort
level e and circumstance c . Rawlsian/Fair EOP requires that for
individuals with similar effort e , the distribution of utility should
be the same‚Äîregardless of their circumstances:
Definition 1 (Rawlsian Eqality of Opportunity (R-EOP)).
A policy œï satisfies Rawlsian EOP if for all circumstances c, c ‚Ä≤ and all
effort levels e ,
Fœï (.|c, e ) = Fœï (.|c ‚Ä≤, e ).
Note that this conception of EOP takes an absolutist view of effort:
it assumes e is a scalar whose absolute value is meaningful and can
be compared across individuals. This view requires effort e to be
inherent to individuals and not itself impacted by the circumstance
c or the policy œï.
Luck Egalitarian EOP. Unlike fair EOP, luck egalitarian EOP
offers a relative view of effort, and allows for the possibility of cir-
cumstance c and implemented policy œï impacting the distribution
of effort e . In this setting, Roemer [24] argues that ‚Äúin comparing
efforts of individuals in different types [circumstances], we should
somehow adjust for the fact that those efforts are drawn from distri-
butions which are different". As the solution he goes on to propose
‚Äúmeasuring a person‚Äôs effort by his rank in the effort distribution of
his type/circumstance, rather than by the absolute level of effort he
expends".
Formally, let Fc,œïE be the effort distribution of type c under pol-
icy œï. Roemer argues that ‚Äúthis distribution is a characteristic of
the type c , not of any individual belonging to the type. Therefore,
an inter-type comparable measure of effort must factor out the
goodness or badness of this distribution". Roemer declares two in-
dividuals as having exercised the same level of effort if they sit at
the same quantile or rank of the effort distribution for their corre-
sponding types. More precisely, let the indirect utility distribution
function Fœï (.|c,œÄ ) specify the distribution of utility for individuals
of type c at the œÄ th quantile (0 ‚â§ œÄ ‚â§ 1) of Fc,œïE . Equalizing opportu-
nities means choosing the policy œï to equalize utility distributions,
Fœï (.|c,œÄ ), across types at fixed levels of œÄ :2
Definition 2 (Luck Egalitarian Eqality of Opportunity
(e-EOP)). A policyœï satisfies Luck Egalitarian EOP if for all œÄ ‚àà [0, 1]
and any two circumstances c, c ‚Ä≤:
Fœï (.|c,œÄ ) = Fœï (.|c ‚Ä≤,œÄ ).
To better understand the subtle difference between Rawlsian
EOP and luck egalitarian EOP, consider the following example:
suppose in the context of employment decisions, we consider years
of education as effort, and gender as circumstance. Suppose Alice
1Note that in Rawls‚Äôs formulation of EOP, talent and ambition are treated as a legiti-
mate source of inequality, even when they are independent of a person‚Äôs effort and
responsibility. The mathematical formulation proposed here includes talent, ability and
ambition all in the scalar e . Whether natural talent should be treated as a legitimate
source of inequality is a subject of controversy. As stated earlier, throughout this work
we assume such questions have been already answered through a democratic process
and/or deliberation among stakeholders and domain experts.
2Note that in Roemer‚Äôs original work, utility is assumed to be a deterministic function
of c, e, œï . Here we changed the definition slightly to allow for the possibility of
non-deterministic dependence.
and Bob both have 5 years of education, whereas Anna and Ben
have 3 and 7 years of education, respectively. Rawlsian EOP would
require Alice and Bob to have the same employment prospects, so
it would ensure that factors such as sexism wouldn‚Äôt affect Alice‚Äôs
employment chances, negatively (compared to Bob). Luck egali-
tarian EOP goes a step further and calculates everyone‚Äôs rank (in
terms of years of education) among all applicants of their gender. In
our example, Alice is ranked 1st and Anna is ranked 2nd. Similarly,
Bob is ranked 2nd and Ben is ranked 1st. A luck egalitarian EOP
policy would ensure that Alice and Ben have the same employment
prospects, and may indeed assign Bob to a less desirable position
than Alice‚Äîeven though they have similar years of education.
Next, we will discuss the above two refinements of substantive
EOP in the context of supervised learning.
2 SETTING
As a running example in this section, we consider a business owner
who uses A3DM to make salary decisions so as to improve business
productivity/revenue. We assume a higher salary is considered to
be more desirable by all employees. An A3DM is designed to predict
the salary that would improve the employee‚Äôs performance at the
job, using historical data. This target variable, as we will shortly
formalize, does not always coincide with the salary the employee
is morally accountable/qualified for.
We consider the standard supervised learning setting. A learning
algorithm receives a training data set T = {(xi ,yi )}ni=1 consisting
of n instances, where xi ‚àà X specifies the feature vector for in-
dividual i and yi ‚àà Y , the true label for him/her (the salary that
would improve his/her performance). Unless otherwise specified,
we assume Y = {0, 1} and X = Rk . Individuals are assumed to be
sampled i.i.d. from a distribution F . The goal of a learning algorithm
is to use the training data T to fit a model (or pick a hypothesis)
h : X ‚Üí Y that accurately predicts the label for new instances. Let
H be the hypothesis class consisting of all the models the learning
algorithm can choose from. A learning algorithm receives T as the
input; then utilizes the data to select a model h ‚àà H that minimizes
some empirical loss, L (T ,h). We denote the predicted label for an
individual with feature vector x by yÃÇ (i.e. yÃÇ = h(x)).
Consider an individual who is subject to algorithmic decision
making in this context. To discuss EOP, we begin by assuming
that his/her observable attributes, x, can be partitioned into two
disjoint sets, x = ‚ü®z,w‚ü©, where z ‚àà Z denotes the individual‚Äôs
observable characteristics for which he/she is considered morally
not accountable‚Äîthis could include sensitive attributes such as race
or gender, as well as less obvious attributes, such as zip code. We
refer to z as morally arbitrary or irrelevant features. Let w ‚àà W
denote observable attributes that are deemed morally acceptable to
hold the individual accountable for; in the running example, this
could include the level of job-related education and experience.
We refer to w as accountability or relevant features. We emphasize
once again that determining what factors should belong to each
category is entirely outside the scope of this work. We assume
throughout that a resolution has been previously reached in this
regard‚Äîthrough the appropriate process‚Äîand is given to us.
Let d ‚àà [0, 1] specify the individual‚Äôs effort-based utility‚Äîthe
utility he/she should receive solely based on their accountability
factors (e.g. the salary an employee should receive based on his/her
years of education and job-related experience. Note that this may
be different from their actual salary). Effort-based utility d is not
directly observable, but we assume it is estimated via a function
–¥ : X √óY √óH ‚Üí R+, such that
d = –¥(x,y,h).
Function–¥ links the observable information, x,y, andh, to the effort-
based utility, d . Let a ‚àà [0, 1] be the actual utility the individual
receives subsequent to receiving prediction yÃÇ (e.g. the utility they
get as the result of their predicted salary). We assume there exists
a function f : X √óY √óH ‚Üí R+ that estimates a:
a = f (x,y,h).
Throughout, for simplicity we assume higher values of a and d
correspond to more desirable conditions.
Let u be the advantage or overall utility the individual earns as
the result of being subject to predictive model h. For simplicity and
unless otherwise specified, we assume u has the following simple
form:
u = a ‚àí d . (1)
That is, u captures the discrepancy between an individual‚Äôs actual
utility (a) and their effort-based utility d . With this formulation, an
individual‚Äôs utility is 0 when their actual and effort-based utilities
coincide (i.e. u = 0 if a = d).
We consider the predictive advantage u to be the currency of
equality of opportunity for supervised learning. That is, u is what
we hope to equalize across similar individuals (similar in terms of
what they can be held accountable for). Our moral argument for this
choice is as follows: the predictive model h inevitably makes errors
in assigning individuals to their effort-based utilities‚Äîthis could be
due to the target variable not properly reflecting effort-based utility,
the prediction being used improperly, or simply a consequence of
generalization. Sometimes these errors are beneficial to the subject,
and sometimes they cause harm. Advantage u precisely captures
this benefit/harm. EOP in this setting requires that all individuals,
who do not differ in ways for which they can be held morally
accountable, have the same prospect of earning the advantage u‚Äî
regardless of their irrelevant attributes. As an example, let‚Äôs assume
the true labels in the training data reflects individuals‚Äô effort-based
utilities (as we will shortly argue, this assumption is not always
morally acceptable, but for now let‚Äôs ignore this issue). In this
case, a perfect predictor‚Äîone that correctly predicts the true label
for every individual‚Äîwill distribute no predictive advantage, but
such predictor almost never exists in real world applications. The
deployed predictive model almost always distributes some utility
among decision-subjects through the errors it makes. A fair model
(with EOP rationale) would give all individuals with similar true
labels the same prospect of earning this advantage‚Äîregardless of
their irrelevant attributes.
Our main conceptual contribution is to map the above setting to
that of economicmodels of EOP (Section 1.1).We treat the predictive
model h as a policy, arbitrary features z as circumstance, and the
effort-based utilities d as effort (Figure 1). In the next Section, we
show that through our proposed mapping, most existing statistical
notions of fairness can be interpreted as special cases of EOP.
through Economic Models of Equality of Opportunity FAT* ‚Äô19, January 29‚Äì31, 2019, Atlanta, GA, USA
3 EOP FOR SUPERVISED LEARNING
In this Section, we show that many existing notions of algorithmic
fairness, such as statistical parity [11, 16, 17], equality of odds [14],
equality of accuracy [5], and predictive value parity [18, 30, 31],
can be cast as special cases of EOP. The summary of our results in
this Section can be found in Table 1. To avoid any confusion with
the notation, we define random variables X,Y to specify the feature
vector and true label for an individual drawn i.i.d. from distribution
F . Similarly given a predictive model h, random variables YÃÇ =
h(X),Ah ,Dh ,U h specify the predicted label, actual utility, the effort-
based utility, and advantage, respectively, for an individual drawn
i.i.d. from F . When the predictive model in reference is clear from
the context, we drop the superscript h for brevity.
Before we formally establish a connection between algorithmic
fairness and EOP, we shall briefly overview the Fair ML litera-
ture and remind the reader of the precise definition of previously-
proposed notions of fairness. Existing notions of algorithmic fair-
ness can be divided into two distinct categories: individual- [8, 28]
and group-level fairness. Much of the existing work on algorithmic
fairness has been devoted to the study of group (un)fairness, also
called statistical unfairness or discrimination. Statistical notions
of fairness require that given a classifier, a certain fairness met-
ric is equal across all (protected or socially salient) groups. More
precisely, assuming z ‚àà Z specifies the group each individual be-
longs to, statistical parity seeks to equalize the percentage of people
receiving a particular outcome across different groups:
Definition 3 (Statistical Parity). A predictive model h satis-
fies statistical parity if ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄyÃÇ ‚àà Y :
P(X,Y )‚àºF [h(X) = yÃÇ |Z = z] = P(X,Y )‚àºF [h(X) = yÃÇ |Z = z‚Ä≤].
Equality of odds requires the equality of false positive and false
negative rates across different groups:
Definition 4 (Eqality of Odds). A predictive model h satisfies
equality of odds if ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄy, yÃÇ ‚àà Y :
P(X,Y )‚àºF [YÃÇ = yÃÇ |Z = z,Y = y] = P(X,Y )‚àºF [YÃÇ = yÃÇ |Z = z‚Ä≤,Y = y].
Equality of accuracy requires the classifier to make equally ac-
curate predictions across different groups:
Definition 5 (Eqality of Accuracy). A predictive model h
satisfies equality of accuracy if ‚àÄz, z‚Ä≤ ‚àà Z :
E(X,Y )‚àºF [(YÃÇ ‚àí Y )2 |Z = z] = E(X,Y )‚àºF [(YÃÇ ‚àí Y )2 |Z = z‚Ä≤].
Predictive value parity (which can be thought of as a weaker
version of calibration [18]) requires the equality of positive and
negative predictive values across different group:
Definition 6 (Predictive Value Parity). A predictive model h
satisfies predictive value parity if ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄy, yÃÇ ‚àà Y :
P(X,Y )‚àºF [Y = y |Z = z, YÃÇ = yÃÇ] = P(X,Y )‚àºF [Y = y |Z = z‚Ä≤, YÃÇ = yÃÇ].
3.1 Statistical Parity, Equality of Odds and
Accuracy as Rawlsian EOP
We begin by translating Rawlsian EOP into the supervised learning
setting using the mapping proposed in Figure 1. Recall that we
proposed replacing e with effort-based utility d , and circumstance c
with vector of irrelevant features z. In order for the definition of
Rawlsian EOP to be morally acceptable, we need d to not be affected
by z and the model h. In other words, it can only be a function of w
and y. Let Fh (.) specify the distribution of utility across individuals
under predictive model h. We define Rawlsian EOP for supervised
learning as follows:
Definition 7 (R-EOP for supervised learning). Suppose d =
–¥(w,y). Predictive model h satisfies Rawlsian EOP if for all z, z‚Ä≤ ‚àà Z
and all d ‚àà [0, 1],
Fh (.|Z = z,D = d ) = Fh (.|Z = z‚Ä≤,D = d ).
In the binary classification setting, if we assume the true la-
bel Y reflects an individual‚Äôs effort-based utility D, Rawlsian EOP
translates into equality of odds across protected groups:3
Proposition 1 (Eqality of Odds as R-EOP). Consider the
binary classification task where Y = {0, 1}. Suppose U = A ‚àí D,
A = h(X) = YÃÇ (i.e., the actual utility is equal to the predicted label)
and D = –¥(W,Y ) where –¥(W,Y ) = Y (i.e., effort-based utility of an
individual is assumed to be the same as their true label). Then the
conditions of R-EOP are equivalent to those of equality of odds.
Proof. Recall that R-EOP requires that ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄd ‚àà D, and
for all possible utility levels u:
P(U ‚â§ u |Z = z,D = d ) = P(U ‚â§ u |Z = z‚Ä≤,D = d ).
ReplacingU with (A‚àíD),D withY ,Awith YÃÇ , the above is equivalent
to
‚àÄz, z‚Ä≤ ‚àà Z,‚àÄy ‚àà {0, 1},‚àÄu ‚àà {0,¬±1} :
P[YÃÇ ‚àí Y ‚â§ u |Z = z,Y = y] = P[YÃÇ ‚àí Y ‚â§ u |Z = z‚Ä≤,Y = y]
‚áî ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄy ‚àà {0, 1},‚àÄu ‚àà {0,¬±1} :
P[YÃÇ ‚â§ u + y |Z = z,Y = y] = P[YÃÇ ‚â§ u + y |Z = z‚Ä≤,Y = y]
‚áî ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄy ‚àà {0, 1},‚àÄyÃÇ ‚àà {0, 1} :
P[YÃÇ = yÃÇ |Z = z,Y = y] = P[YÃÇ = yÃÇ |Z = z‚Ä≤,Y = y]
where the last line is identical to the conditions of equality of odds
for binary classification. ‚ñ°
The important role of the above proposition is to explicitly spell
out the moral assumption underlying equality of odds as a measure
of fairness: by measuring fairness through equality of odds, we
implicitly assert that all individuals with the same true label have
the same effort-based utility. This can clearly be problematic in
practice: true labels don‚Äôt always reflect/summarize accountability
factors. At best, they are only a reflection of the current state of
affairs‚Äîwhich itself might be tainted by past injustices. For these
reasons, we argue that equality of odds can only be used as a
valid measure of algorithmic fairness (with an EOP rationale) once
the validity of the above moral equivalency assumption has been
carefully investigated and its implications are well understood in
the specific context it is utilized in.
Other statistical definitions of algorithmic fairness‚Äînamely sta-
tistical parity and equality of accuracy‚Äîcan similarly be thought of
as special instances of R-EOP. See Table 1. For example statistical
3Note that Hardt et al. [14] referred to a weaker measure of algorithmic fairness (i.e.
equality of true positive rates) as equality of opportunity.
Notion of fairness Effort-based utility D Actual utility A Notion of EOP
Accuracy Parity constant (e.g. 0) (YÃÇ ‚àí Y )2 Rawlsian
Statistical Parity constant (e.g. 1) YÃÇ Rawlsian
Equality of Odds Y YÃÇ Rawlsian
Predictive Value Parity YÃÇ Y egalitarian
Table 1: Interpretation of existing notions of algorithmic fairness for binary classification as special instances of EOP.
parity can be interpreted as R-EOP if we assume all individuals
have the same effort-based utility.4
Proposition 2 (Statistical Parity as R-EOP). Consider the
binary classification task where Y = {0, 1}. Suppose U = A ‚àí D,
A = YÃÇ and D = –¥(W,Y ) where –¥(W,Y ) is a constant function (i.e.,
effort-based utility of all individuals is assumed to be the same). Then
the conditions of R-EOP is equivalent to statistical parity.
Proof. Without loss of generality, suppose –¥(X,Y ,h) ‚â° 1, i.e.
all individuals effort-based utility 1. Recall that R-EOP requires that
‚àÄz, z‚Ä≤ ‚àà Z,‚àÄd ‚àà D,‚àÄu ‚àà R :
P(U ‚â§ u |Z = z,D = d ) = P(U ‚â§ u |Z = z‚Ä≤,D = d ).
Replacing U with (A ‚àí D), D with 1, and A with YÃÇ , the above is
equivalent to
‚àÄz, z‚Ä≤ ‚àà Z,‚àÄd ‚àà {1},‚àÄu ‚àà {0,‚àí1} :
P[YÃÇ ‚àí D ‚â§ u |Z = z,D = 1] = P[YÃÇ ‚àí D ‚â§ u |Z = z‚Ä≤,D = 1]
‚áî ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄu ‚àà {0,‚àí1} :
P[YÃÇ ‚â§ u + 1|Z = z] = P[YÃÇ ‚â§ u + 1|Z = z‚Ä≤]
‚áî ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄyÃÇ ‚àà {0, 1} :
P[YÃÇ = yÃÇ |Z = z] = P[YÃÇ = yÃÇ |Z = z‚Ä≤]
where the last line is identical to the conditions of statistical parity
for binary classification. ‚ñ°
As a real-world example where statistical parity can be applied,
consider the following: suppose the society considers all patients to
have the same effort-based utility‚Äîwhich can be enjoyed by access
to proper clinical examinations. Now suppose that undergoing an
invasive clinical examination has utility 1 if one has the suspected
diseases and -1 otherwise, whereas avoiding the same clinical inves-
tigation has utility 1 if one does not have the suspected disease, and
-1 otherwise. For all subjects, the effort-based utility is the same
(the maximum utility, let us suppose). In other words, all people
with a disease deserve the invasive clinical investigation and all
people without the disease deserve to avoid it. Consider a policy of
giving clinical investigation to all the people without the disease
and to no people without the disease. This would achieve an equal
distribution of effort-based utility (D) and distribute no advantage
U . Such policy, however, could only be achieved with a perfect
accuracy predictor. For an imperfect accuracy predictor, R-EOP
would require the distribution of (negative, in this case) utility (U)
to give the same chance to African Americans and white patients
with (without) the disease to receive (avoid) an invasive clinical
exam.
4Statistical parity can be understood as equality of outcomes as well, if we assume YÃÇ
reflects the outcome.
Proposition 3 (Eqality of Accuracy as R-EOP). Consider
the binary classification task where Y = {0, 1}. SupposeU = A ‚àí D,
A = (YÃÇ ‚àí Y )2 and D = –¥(W,Y ) where –¥(W,Y ) ‚â° 0 (i.e., effort-based
utility of all individuals are assumed to be the same and equal to 0).
Then the conditions of R-EOP is equivalent to equality of accuracy.
Proof. Recall that R-EOP requires that‚àÄz, z‚Ä≤ ‚àà Z,‚àÄd ‚àà D,‚àÄu ‚àà
R :
P(U ‚â§ u |Z = z,D = d ) = P(U ‚â§ u |Z = z‚Ä≤,D = d ).
ReplacingU with (A‚àíD), D with 0, andAwith (YÃÇ ‚àíY )2, the above
is equivalent to ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄd ‚àà {0},‚àÄu ‚àà {0, 1} :
P[(YÃÇ‚àíY )2‚àíD ‚â§ u |Z = z,D = d] = P[(YÃÇ‚àíY )2‚àíD ‚â§ u |Z = z‚Ä≤,D = d]
We can then write:
‚áî ‚àÄz, z‚Ä≤,‚àÄu : P[(YÃÇ ‚àí Y )2 = u |Z = z] = P[(YÃÇ ‚àí Y )2 = u |Z = z‚Ä≤]
‚áî ‚àÄz, z‚Ä≤ ‚àà Z : E[(YÃÇ ‚àí Y )2 |Z = z] = E[(YÃÇ ‚àí Y )2 |Z = z‚Ä≤]
where the last line is identical to the conditions of equality of
accuracy for binary classification. ‚ñ°
The critical moral assumption underlying equality of accuracy
as a measure of fairness (with EOP rationale) is that errors reflect
the advantage distributed by the predictive model among decision
subjects. This exposes the fundamental ethical problem with adopt-
ing equality of accuracy as a measure of algorithmic fairness: it
fails to distinguish between errors that are beneficial to the subject
and those that are harmful. For example, in the salary prediction
example, equality of accuracy would make no distinction between
an individual who earns a salary higher than what they deserve,
and someone who earns lower than their effort-based/deserved
salary.
Max-min distribution vs. strict equality. At a high level, R-EOP
prescribes equalizing advantage distribution across persons with
the same effort-based utility. Some egalitarian philosophers have
argued that we can remain faithful to the spirit (though not the
letter) of EOP by delivering a max-min distribution of advantage,
instead of a strict egalitarian one [24]. The max-min distribution
deviates from equality only when this makes the worst off group
better off. Even though this distribution permits inequalities that do
not reflect accountability factors, it is considered a morally superior
alternative to equality, if it improves the utility of least fortunate.5
The max-min distribution addresses the ‚Äúleveling down" objection
to equality: the disadvantaged group may be more interested in
5The idea that inequalities are justifiable only when they result from a scheme arranged
to maximally benefit the worst off position is expressed through the Difference Principle
by John Rawls in his theory of ‚Äújustice as fairness" [21].
through Economic Models of Equality of Opportunity FAT* ‚Äô19, January 29‚Äì31, 2019, Atlanta, GA, USA
maximizing their absolute level of utility, as opposed to their relative
utility compared to that of the advantaged group.
3.2 Predictive Value Parity as Egalitarian EOP
Note that predictive value parity (equality of positive and negative
predictive values across different groups) can not be thought of
as an instance of R-EOP, as it requires the effort-based utility of
an individual to be a function of the predictive model h (as we
will shortly show, it assumes D = h(X)). This is in violation of
the absolutist view of Rawlsian EOP. In this Section, we show that
predictive value parity can be cast as an instance of luck egalitarian
EOP.
We first specialize Roemer‚Äôs model of Egalitarian EOP to the
supervised learning setting. Recall that egalitarian EOP allows the
effort-based utility to be a function of the predictive model h, that is
D = f (X,Y ,h). When this is the case, following the argument put
forward by Roemer we posit that the distribution of effort-based
utility for a given type z (denoted by F z,hD ) is a characteristic of the
type z, not something for which any individual belonging to the
type can be held accountable. Therefore, an inter-type comparable
measure of effort-based utility must factor out the goodness or
badness of this distribution. We consider two individuals as being
equally deserving if they sit at the same quantile or rank of the
distribution of D for their corresponding type.
More formally, let the indirect utility distribution function, de-
noted by Fh (.|z,œÄ ), specify the distribution of utility for individuals
of type z at the œÄ th quantile (0 ‚â§ œÄ ‚â§ 1) of effort-based utility dis-
tribution, F z,hD . Equalizing opportunities means choosing the pre-
dictive model h to equalize the indirect utility distribution across
types, at fixed levels of œÄ :
Definition 8 (e-EOP for supervised learning). Suppose d =
f (x,y,h). Predictive model h satisfies egalitarian EOP if for all œÄ ‚àà
[0, 1] and z, z‚Ä≤ ‚àà Z,
Fh (.|Z = z,Œ† = œÄ ) = Fh (.|Z = z‚Ä≤,Œ† = œÄ ). (2)
Next, we show that predictive value parity can be thought of
as a special case of e-EOP, where the predicted label/risk h(X) is
assumed to reflect the individual‚Äôs effort-based utility, and the true
label Y reflects his/her actual utility.
Proposition 4 (predictive value parity as e-EOP). Consider
the binary classification task where Y = {0, 1}. SupposeU = A ‚àí D,
A = Y and D = –¥(X,Y ,h) where –¥(X,Y ,h) = h(X) = YÃÇ (i.e., effort-
based utility of an individual under h is assumed to be the same as
their predicted label). Then the conditions of e-EOP are equivalent to
those of predictive value parity.
Proof. Recall that e-EOP requires that ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄœÄ ‚àà [0, 1],
and ‚àÄu ‚àà R :
P[U ‚â§ u |Z = z,Œ† = œÄ ] = P[U ‚â§ u |Z = z‚Ä≤,Œ† = œÄ ].
Note that since D = YÃÇ and in the binary classification, YÃÇ can only
take on two values, there are only two ranks/quantiles possible in
terms of the effort-based utility‚Äîcorresponding to YÃÇ = 0 and YÃÇ = 1.
So the above condition is equivalent to ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄyÃÇ ‚àà {0, 1},‚àÄu ‚àà
{0,¬±1} :
P[U ‚â§ u |Z = z, YÃÇ = yÃÇ] = P[U ‚â§ u |Z = z‚Ä≤, YÃÇ = yÃÇ].
ReplacingU with (A‚àíD),D with YÃÇ ,AwithY , the above is equivalent
to
‚àÄz, z‚Ä≤ ‚àà Z,‚àÄyÃÇ ‚àà {0, 1},‚àÄu ‚àà {0,¬±1} :
P[Y ‚àí YÃÇ ‚â§ u |Z = z, YÃÇ = yÃÇ] = P[Y ‚àí YÃÇ ‚â§ u |Z = z‚Ä≤, YÃÇ = yÃÇ]
‚áî ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄyÃÇ ‚àà {0, 1},‚àÄu ‚àà {0,¬±1} :
P[Y ‚â§ u + yÃÇ |Z = z, YÃÇ = yÃÇ] = P[Y ‚â§ u + yÃÇ |Z = z‚Ä≤, YÃÇ = yÃÇ]
‚áî ‚àÄz, z‚Ä≤ ‚àà Z,‚àÄyÃÇ ‚àà {0, 1},‚àÄy ‚àà {0, 1} :
P[Y = y |Z = z, YÃÇ = yÃÇ] = P[Y = y |Z = z‚Ä≤, YÃÇ = yÃÇ]
where the last line is identical to predictive value parity. ‚ñ°
Note that there are two assumptions needed to cast predictive
value parity as an instance of e-EOP: 1) the predicted label/risk
h(x) reflects the individual‚Äôs effort-based utility; and 2) the true
label Y reflects his/her actual utility. The plausibility of such moral
assumptions must be critically evaluated in a given context before
predictive parity can e employed as a valid measure of fairness.
Next, we discuss the plausibility of these assumptions through
several real-world examples.
Plausibility of assumption 1. The choice of predicted label, h(X),
as the indicator of effort-based utility, may sound odd at first. How-
ever, there are several real-world settings in which this assumption
is considered appropriate. Consider the case of driving under influ-
ence (DUI): the law considers all drivers equally at risk of causing
an accident‚Äîdue to the consumption of alcohol or drugs‚Äîequally
accountable for their risk and punishes them similarly, even though
only some of them will end up in an actual accident, and the rest
won‚Äôt. In this context, the potential/risk of causing an accident‚Äîas
opposed to the actual outcome‚Äîjustifies unequal treatment, because
we believe differences in actual outcomes among equally risky in-
dividuals is mainly driven by arbitrary factors, such as brute luck.
Arguably, such factors should never specify accountability.
Can assumptions 1 and 2 hold simultaneously? The following is
an example in which ssumptions 1 and 2 hold simultaneously (in
particular, the true label Y specifies the actual utility an individual
receives subsequent to being subject to automated decision mak-
ing). Consider the students of a course, offered online and open to
students from all over the world. The final assessment of students
enrolled in the course includes an essential oral exam. The oral
exam is very challenging and extremely competitive. The instruc-
tors hold an exam session every month. Every student is allowed
to take the oral exam, but since resources for oral examinations
are limited, to discourage participation without preparation, the
rule is that, if a student fails the exam, he/she has to wait one
year before taking the exam again. Suppose that students belong
to one of the two groups: African Americans and Asians. African
American and Asian students study in different ways, with different
cognitive strategies. As a result, an African American student with
0.9 passing score may correspond to a very different feature vector
compared to an Asian student with a 0.9 passing score. Suppose a
predictive model is used to predict the outcome of the oral exam for
individual students, based on the student‚Äôs behavioral data. (The
online learning platform records data on how students interact
with the course materials.) Students are given a simple ‚Äúpass/fail"
prediction to help them make an informed choice about when to
take the exam. In this example, we argue that both assumptions
underlying predictive value parity are satisfied:
(1) A = Y : Passing the exam is a net utility, not passing the exam
is a net disutility (due to the one year delay). Also, being
predicted to pass per se has no utility associated with it.
(2) D = YÃÇ . It is plausible to consider students morally respon-
sible for their chances of success, because the predictions
are calculated based on how they have studied the course
material.
In this example, a fair predictor (with EOP rationale) should satisfies
predictive value parity. That means: students who are predicted to
pass, should be equally likely to pass the exam, irrespective of their
race.
On Recent Fairness Impossibility Results. Several papers have re-
cently shown that group-level notions of fairness, such as predictive
value parity and equality of odds, are generally incompatible with
one another and cannot hold simultaneously [13, 18]. Our approach
confers a moral meaning to these impossibility results: they can be
interpreted as contradictions between fairness desiderata reflect-
ing different and irreconcilable moral assumptions. For example
predictive value parity and equality of odds make very different
assumptions about the effort-based utility d : Equality of odds as-
sumes all persons with similar true labels are equally accountable
for their labels, whereas predictive value parity assumes all persons
with the same predicted label/risk are equally accountable for their
predictions. Note that depending on the context, usually only one
(if any) of these assumptions is morally acceptable. We argue, there-
fore, that unless we are in the highly special case where Y = h(X),
it is often unnecessary‚Äîfrom a moral standpoint‚Äîto ask for both
of these fairness criteria to be satisfied simultaneously.
4 EGALITARIAN MEASURES OF FAIRNESS
In this section, inspired by Roemer‚Äôs model of egalitarian EOP we
present a new family of measures for algorithmic fairness. Our
proposal is applicable to supervised learning tasks beyond binary
classification, and to utility functions beyond the simple linear form
specified in Equation 1. We illustrate our proposal empirically, and
compare it with existing notions of (un)fairness for regression. Our
empirical findings suggest that employing a measure of algorithmic
(un)fairness when its underlying assumptions are not met, can have
devastating consequences on the welfare of decision subjects.
4.1 A New Family of Measures
For supervised learning tasks beyond binary classification (e.g. mul-
ticlass classification or regression), the requirement of equation 2
becomes too stringent, as there will be (infinitely) many quantiles
to equalize utilities over. The problem persists even if we relax the
requirement of equal utility distributions to maximizing the mini-
mum expected utility at each quantile. More formally, let vz (œÄ ,h)
specify the expected utility of individuals of type z at the œÄ th quan-
tile of the effort-based utility distribution. For œÄ ‚àà [0, 1], we say
that a predictive model hœÄ satisfies egalitarian EOP at the œÄ -slice
of the population, if:
hœÄ ‚àà arg max
h‚ààH
min
z‚ààZ
vz (œÄ ,h).
Assuming we are concerned only with the œÄ -slice, then hœÄ would
be the equal-opportunity predictive model. Unfortunately, when
we move beyond binary classification, we generally cannot find
a model that is simultaneously optimal for all ranks œÄ ‚àà [0, 1].
Therefore, we need to find a compromise. Following Roemer, we
define the e-EOP predictive model as follows:
h‚àó ‚àà arg max
h‚ààH
min
z‚ààZ
‚à´ 1
That is, we consider h‚àó to be an e-EOP predictive model if it maxi-
mizes the expected utility of theworst off group (i.e.
‚à´ 1
0 vz (œÄ ,h)dœÄ ).6
Replacing the expectation with its in-sample analogue, our pro-
posed family of e-EOP measures can be evaluated on the data set T
as follows:
F (h,T ) = min
z‚ààZ
‚àë
i ‚ààT :zi=z
u (xi ,yi ,h)
where u (xi ,yi ,h) is the utility an individual with feature vector xi
and true label yi receives when predictive model h is deployed; and
nz is the number of individuals in T whose arbitrary features value
is z ‚àà Z. (The arbitrary features value z specifies the (intersectional)
group each individual belongs to. We usem to denote the number
of such (intersectional) groups. For simplicity in our illustration,
we refer to these groups as G1, ¬∑ ¬∑ ¬∑ ,Gm .)
To guarantee fairness, we propose the following in-processing
method: maximize the expected utility of the worst off group, sub-
ject to error being upper bounded (by œµ).
max
h‚ààH
F (h,T )
s.t. L (T ,h) ‚â§ œµ (4)
Note that if the loss functionL is convex and F is concave in model
parameters, Optimization 4 is convex and can be solved efficiently.
We remark that our notion of fairness does not require us to
explicitly specify the effort-based utility D, since it only compares
the overall expected utility of different groups with one another‚Äî
without the need to explicitly compare the utility obtained by indi-
viduals at a particular rank of D across different groups. Further-
more, the utility function, u (x,y,h), does not have to be restricted
to take the simple linear form specified in Equation 1.
4.2 Illustration
Next, we illustrate our proposal on the Crime and Communities data
set [20]. The data consists of 1994 observations, each corresponding
to a community/neighborhood in the United States. Each commu-
nity is described by 101 features, specifying its socio-economic, law
enforcement, and crime statistics extracted from the 1995 FBI UCR.
Community type (e.g. urban vs. rural), average family income, and
6Roemer in fact proposes two further alternatives: in the first solution, the objective
function for each œÄ -slice of the population is assumed to beminz‚ààZ v z (œÄ , h)‚Äîwhich
is then weighted by the size of the slice. In the second solution, he declares the
equal opportunity policy to be the average of the policies hœÄ . Roemer expresses no
strong preference for any of these alternatives, other than the fact that computational
simplicity sometimes suggests one over the others [24]. This is in fact the reasoning
behind our choice of Equation 3.
through Economic Models of Equality of Opportunity FAT* ‚Äô19, January 29‚Äì31, 2019, Atlanta, GA, USA
(a) (b) (c)
Figure 2: NRD, PRD, and average utility of the disadvantaged group as a function of œµ (the upperbound onmean squared error).
The notion of fairness enforced on algorithmic decisions can have a devastating impact on the welfare of the disadvantaged
group.
the per capita number of police officers in the community are a
few examples of the explanatory variables included in the dataset.
The target variable (Y ) is the ‚Äúper capita number of violent crimes".
We train a linear regression model, Œ∏ ‚àà Rk , on this dataset to pre-
dict the per capita number of violent crimes for a new community.
We hypothesize that crime predictions can affect the law enforce-
ment resources assigned to the community, the value of properties
located in the neighborhood, and business investments drawn to it.
We preprocess the original dataset as follows: we remove the
instances for which target value is unknown. Also, we remove fea-
tures whose values are missing for more than 80% of instances. We
standardize the data so that each feature has mean 0 and variance
1. We divide all target values by a constant so that labels range
from 0 to 1. Furthermore, we flip all labels (y ‚Üí 1 ‚àí y), so that
higher y values correspond to more desirable outcomes. We assume
a neighborhood belongs to the protected group (G1) if the majority
of its residents are non-Caucasian, that is, the percentage of African
American, Hispanic, and Asian residents of the neighborhood com-
bined, is above 50%. This divides the training instances into two
groups G0,G1. We include this group membership information as
the (sensitive) feature z in the training data (zi = 1[i ‚àà G1]).
For simplicity, we assume the utility functionu has the following
functional dependence on x and Œ∏ :u (z,y, yÃÇ); that is,u‚Äôs dependence
on x andŒ∏ are through z and yÃÇ = Œ∏ .x, respectively. For communities
belonging toG0 andG1, we assumeu (z,y, yÃÇ) = f (z,y, yÃÇ)‚àí–¥(z,y, yÃÇ)
is respectively defined as follows:
‚Ä¢ For a majority-Caucasian neighborhood,
u (0,y, yÃÇ) = (1 + 0.5yÃÇy) ‚àí (0.5yÃÇ).
‚Ä¢ For a minority-Caucasian neighborhood,
u (1,y, yÃÇ) = (1 + 3yÃÇy + 2yÃÇ) ‚àí (y).
At a high level, neighborhoods in both groups enjoy a high utility if
their predicted and actual crime rates are low, simultaneously (note
that the absolute value of utility derived from this case is higher for
the minority). The minority-Caucasian group further benefits from
low crime predictions (regardless of actual crime rates). We assume
the effort-based utility for the minority group, is one minus the
actual crime rate (y), and for the majority group, it is proportional
to one minus the predicted crime rate (0.5yÃÇ). Note that these utility
functions are made up for illustration purposes only, and do not
reflect any deep knowledge of how crime and law enforcement
affect the well-being of a neighborhood‚Äôs residents.
To illustrate our proposal, we solve the following convex opti-
mization problem for different values of œµ :
max
œÉ ,Œ∏
œÉ
s.t.
‚àë
i ‚ààG0
‚àí0.5Œ∏ .xi + 0.5(Œ∏ .xi )yi + 1 ‚â• œÉ
‚àë
i ‚ààG1
2Œ∏ .xi + 3(Œ∏ .xi )yi ‚àí yi + 1 ‚â• œÉ
n‚àë
i=1
(Œ∏ .xi ‚àí yi )2 + Œª‚à•Œ∏ ‚à•1 ‚â§ œµ (5)
We choose the value of Œª by running a 10-fold cross validation on
the data set. For each value of œµ (Mean Squared Error), we measure
the following quantities via 5-fold cross validation:
‚Ä¢ Positive residual difference [6] is the equivalent of false
positive rate in regression, and is computed by taking the
absolute difference of mean positive residuals across the two
groups:

‚àë
i ‚ààG1
max{0, (yÃÇi ‚àí yi )} ‚àí
‚àë
i ‚ààG0
max{0, (yÃÇi ‚àí yi )}

.
In the above, n+–¥ is the number of individuals in group –¥ ‚àà
{0, 1} who get a positive residual, i.e. yÃÇi ‚àí yi ‚â• 0.
‚Ä¢ Negative residual difference [6] is the equivalent of false
negative rate in regression, and is computed by taking the
absolute difference of mean negative residuals across the
two groups.
‚Ä¢ Average utility of the disadvantaged group is computed by
taking the average utility of all individuals in the test data
set:
min
Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥
Ô£≥
‚àë
i ‚ààG0
u (xi ,yi ,h),
‚àë
i ‚ààG1
u (xi ,yi ,h)
Ô£ºÔ£¥Ô£¥Ô£ΩÔ£¥Ô£¥
Ô£æ
.
Figure 2 shows the results of our simulations. Blue curves corre-
spond to our proposal (Optimization 5). As evident in Figures 2a
and 2b, positive and negative residual difference increase with œµ ,
while the average utility increases (see Figure 2c).
To compare our proposal with existing measures of (un)fairness
for regression, we utilize the in-processing method of Heidari et al.
[15]. The method enforces an upperbound on
‚àë
i (yÃÇi ‚àí yi ), and has
been shown to control the positive and negative residual differ-
ence across the two groups. More precisely, we solve the following
optimization problem for different values of œµ :
max
Œ∏
‚àë
i ‚ààT
Œ∏ .xi ‚àí yi s.t.
n‚àë
i=1
(Œ∏ .xi ‚àí yi )2 + Œª‚à•Œ∏ ‚à•1 ‚â§ œµ (6)
Red curves in Figure 2 correspond to this baseline. As evident in
Figures 2a and 2b, by enforcing a lower bound on
‚àë
i (yÃÇi ‚àí yi ),
positive and negative residual difference go to 0 very quickly‚Äîas
expected. However, the trained model performs very poorly in
terms of average utility of the disadvantaged group.
5 CONCLUSION
Our work makes an important contribution to the rapidly growing
line of research on algorithmic fairness‚Äîby providing a unifying
moral framework for understanding existing notions of fairness
through philosophical interpretations and economic models of EOP.
We showed that the choice between statistical parity, equality of
odds, and predictive value parity can be mapped systematically to
specific moral assumptions about what decision subjects morally
deserve. Determining accountability features and effort-based util-
ity is arguably outside the expertise of computer scientists, and has
to be resolved through the appropriate process with input from
stakeholders and domain experts. In any given application domain,
reasonable people may disagree on what constitutes factors that
people should be considered morally accountable for, and there
will rarely be a consensus on the most suitable notion of fairness.
This, however, does not imply that in a given context all existing
notions of algorithmic fairness are equally acceptable from a moral
standpoint.
ACKNOWLEDGMENT
H. Heidari and A. Krause acknowledge support from CTI grant
no. 27248.1 PFES-ES. K. P. Gummadi is supported in part by the
European Research Council (ERC) Advanced Grant for the project
‚ÄúFoundations for Fair Social Computing", funded under the Euro-
pean Union‚Äôs Horizon 2020 Framework Programme (grant agree-
ment no. 789373). Michele Loi is supported by the CANVAS project,
funded under the European Union‚Äôs Horizon 2020 Research and
Innovation Programme (grant agreement no. 700540).
REFERENCES
[1] Richard J. Arneson. 1989. Equality and equal opportunity for welfare. Philosophi-
cal Studies: An International Journal for Philosophy in the Analytic Tradition 56, 1
(1989), 77‚Äì93.
[2] Richard J. Arneson. 2015. Equality of Opportunity. In the Stanford Encyclopedia
of Philosophy (summer 2015 ed.), Edward N. Zalta (Ed.). Metaphysics Research
Lab, Stanford University.
[3] Richard J. Arneson. 2018. Four Conceptions of equal opportunity. (2018).
[4] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth.
2017. Fairness in Criminal Justice Risk Assessments: The State of the Art. arXiv
preprint arXiv:1703.09207 (2017).
[5] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy
disparities in commercial gender classification. In Proceedings of the Conference
on Fairness, Accountability and Transparency. 77‚Äì91.
[6] Toon Calders, Asim Karim, Faisal Kamiran,Wasif Ali, and Xiangliang Zhang. 2013.
Controlling attribute effect in linear regression. In Proceedings of the International
Conference on Data Mining. IEEE, 71‚Äì80.
[7] Gerald A. Cohen. 1989. On the currency of egalitarian justice. Ethics 99, 4 (1989),
906‚Äì944.
[8] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through awareness. In Proceedings of the Innovations in
Theoretical Computer Science Conference. ACM, 214‚Äì226.
[9] Ronald Dworkin. 1981. What is equality? Part 1: Equality of welfare. Philosophy
& Public Affairs 10, 3 (1981), 185‚Äì246.
[10] Ronald Dworkin. 1981. What is equality? Part 2: Equality of resources. Philosophy
& Public Affairs 10, 4 (1981), 283‚Äì345.
[11] Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and
Suresh Venkatasubramanian. 2015. Certifying and removing disparate impact.
In Proceedings of the International Conference on Knowledge Discovery and Data
Mining. ACM, 259‚Äì268.
[12] Marc Fleurbaey. 2008. Fairness, responsibility, and welfare. Oxford University
Press.
[13] Sorelle A. Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. 2016.
On the (im)possibility of fairness. arXiv preprint arXiv:1609.07236 (2016).
[14] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in
supervised learning. In Proceedings of the 30th Conference on Neural Information
Processing Systems. 3315‚Äì3323.
[15] Hoda Heidari, Claudio Ferrari, Krishna P. Gummadi, and Andreas Krause. 2018.
Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision
Making. In Proceedings of the 32nd Conference on Neural Information Processing
Systems.
[16] Faisal Kamiran and Toon Calders. 2009. Classifying without discriminating. In
Proceedings of the 2nd International Conference on Computer, Control and Commu-
nication. IEEE, 1‚Äì6.
[17] Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. 2011. Fairness-aware
learning through regularization approach. In Proceedings of the International
Conference on Data Mining Workshops. IEEE, 643‚Äì650.
[18] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2017. Inherent
trade-offs in the fair determination of risk scores. In In proceedings of the 8th
Innovations in Theoretical Computer Science Conference.
[19] Arnaud Lefranc, Nicolas Pistolesi, and Alain Trannoy. 2009. Equality of opportu-
nity and luck: Definitions and testable conditions, with an application to income
in France. Journal of Public Economics 93, 11-12 (2009), 1189‚Äì1207.
[20] M. Lichman. 2013. UCI Machine Learning Repository: Communities and Crime
Data Set. http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime.
[21] John Rawls. 1958. Justice as fairness. The philosophical review 67, 2 (1958),
164‚Äì194.
[22] John Rawls. 1971. A theory of justice. Harvard university press.
[23] John E. Roemer. 1993. A pragmatic theory of responsibility for the egalitarian
planner. Philosophy & Public Affairs (1993), 146‚Äì166.
[24] John E. Roemer. 2002. Equality of opportunity: A progress report. Social Choice
and Welfare 19, 2 (2002), 455‚Äì471.
[25] John E. Roemer. 2009. Equality of opportunity. Harvard University Press.
[26] John E. Roemer and Alain Trannoy. 2015. Equality of opportunity. In Handbook
of income distribution. Vol. 2. Elsevier, 217‚Äì300.
[27] Amartya Sen. 1979. Equality of What? The Tanner Lecture on Human Values
(1979).
[28] Till Speicher, Hoda Heidari, Nina Grgic-Hlaca, Krishna P. Gummadi, Adish Singla,
Adrian Weller, and Muhammad Bilal Zafar. 2018. A Unified Approach to Quanti-
fying Algorithmic Unfairness: Measuring Individual and Group Unfairness via
Inequality Indices. In Proceedings of the International Conference on Knowledge
Discovery and Data Mining.
[29] Wikipedia. 2018. Equal opportunity. https://en.wikipedia.org/wiki/Equal_
opportunity.
[30] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P
Gummadi. 2017. Fairness beyond disparate treatment & disparate impact: Learn-
ing classification without disparate mistreatment. In Proceedings of the 26th
International Conference on World Wide Web. 1171‚Äì1180.
[31] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P.
Gummadi. 2017. Fairness Constraints: Mechanisms for Fair Classification. In
Proceedings of the 20th International Conference on Artificial Intelligence and
Statistics.
