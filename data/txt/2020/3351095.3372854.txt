Towards a more representative politics in the ethics of computer science
Jared Moore
jared@jaredmoore.org
University of Washington
ABSTRACT
Ethics curricula in computer science departments should include a
focus on the political action of students. While ‘ethics’ holds signifi-
cant sway over current discourse in computer science, recent work,
particularly in data science, has shown that this discourse elides
the underlying political nature of the problems that it aims to solve.
In order to avoid these pitfalls—such as co-option, whitewashing,
and assumed universal values—we should recognize and teach the
political nature of computing technologies, largely through science
and technology studies. Education is an essential focus not just in-
trinsically, but also because computing students end up joining the
companies which have outsize impacts on our lives. At those com-
panies, students both have a responsibility to society and agency
beyond just engineering decisions, albeit not uniformly. I propose
that we move away from strict ethics curricula and include exam-
ples of and calls for political action of students and future engineers.
Through such examples—calls to action, practitioner reflections,
legislative engagement, direct action—we might allow engineers to
better recognize both their diverse agencies and responsibilities.
CCS CONCEPTS
• Social and professional topics→ Codes of ethics; Comput-
ing education; Accreditation; Socio-technical systems.
KEYWORDS
politics, civics, activism, science and technology studies
ACM Reference Format:
Jared Moore. 2020. Towards a more representative politics in the ethics
of computer science. In Conference on Fairness, Accountability, and Trans-
parency (FAT* ’20), January 27–30, 2020, Barcelona, Spain. ACM, New York,
NY, USA, 11 pages. https://doi.org/10.1145/3351095.3372854
1 INTRODUCTION
Ethics is a hot word in computer science. Companies, organiza-
tions, and universities have begun to use the term in principles,
declarations, and promotional material. Lately, this has occurred in
the space of ‘artificial intelligence (AI) ethics.’ Ethics commitments
have bled over into curricula as well. For example, in spring 2019
the Mozilla Foundation awarded grants to a number of computer
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00
https://doi.org/10.1145/3351095.3372854
science programs in order to increase the extent of ethics education
[71]. Furthermore, a recent list shows more than 200 university
courses that claim to fall in the category of tech ethics [38].
Clearly, something is afoot. At first glance, such attention might
have those of us who argue that values are embedded in technology
celebrating. The situation is not so clear. Such developments in
ethics may actually be missing the original point of the critiques (as
say [46]). Companies continue to develop computing technologies
that arguably undermine common normative frameworks such as
distributive justice and human rights, while still using the language
of ethics.1
Take Amazon, for example. It appears to allow deployment of
facial recognition technology on the U.S.-Mexico border [28]. The
topics usually covered in discussions of AI ethics fail to capture the
significance of such situations. The question of whether facial recog-
nition technology ‘fairly’ identifies people across racial groups2
appears moot when any contribution to the further criminalization
of asylum seekers and exclusion from economic opportunity flouts
principles of distributive justice.
Let us assume that someone has come to the conclusion that
such a use of facial recognition technology, one that further harms
asylum seekers, is unethical under a given theory which appears
reasonable given broad interpretations of any class of ethical frame-
works. The question then remains of what to do about it. This is
particularly relevant given that ethics in computing seems to be
restricted to applied engineering questions or ‘microethics.’ Applied
in this sense, ethics would dictate how to change the design of a
system, but not how to change structural systems of oppression in
which such a system operates.
Of course, the description of the action of companies and engi-
neers in relation to philosophical definitions of ethics is useful—
whether, for example, Amazon’s use of facial recognition technol-
ogy on the U.S. border would be permissible in the strict egalitari-
anism distributive justice. Nevertheless, such an examination, if the
only one conducted, would squelch the important political nature
of the case, being: how do we operationalize our conclusion? Fur-
thermore, current CS ethics curricula appear to be mainly applied
and professional in nature. That is to say, current ethics education
may not even go so far as to consider the structure of society.
Here, I do not necessarily mean to endorse distributive justice,
but rather to highlight a difference between what are construed as
chiefly ethical and political issues. Our interest, as would be that
of anyone concluding an ethical chain of reasoning, should be to
promote the world in which we find resolution of the ethical issue.
This promotion is inherently political.
1For an examination of distributive justice see [62].
2See discussions of mathematical fairness in the literature, e.g. [60].
Essentially, current debates in computer science are political
and not just ethical ones. They involve questions of not just ‘what
values does this technology assume,’ but also ‘what kind of society
does this technology create,’ ‘how can I create the kind of society
in which I want to live.’ The recognition and discussion of moral
principles—ethics—of course are necessary but may not be sufficient
for this.3
It is not apparent whether the current teaching of ethics in com-
puter science covers the politics of technology in both design and
action. In particular, the focus on applied ethics blunts the political
nature of the situations in which computer scientists find them-
selves. Furthermore, ethics carries its own history and should have
us ask: whose ethical theories are we using and whom might they
leave out? Relevant are not only ethical theories, but also their pro-
fessional realization. While scholars have historically found that
both ethics codes and classes appear to have little impact on the
actions of engineers (as [26] discuss) this may be due to the limits
of the framing of such ethics.
Nevertheless, despite the limits of ethics as they exist in computer
science, practitioners have begun to act. Reporting from summer
2019 details the actions of Stanford computing students to resist the
big tech companies, who the students see as perpetuating injustice
by supporting groups such as the U.S. Immigration and Customs
Enforcement agency [42]. At Google in fall 2018, thousands of
workers walked out to protest the company’s sexual harassment
policies. That came a year after tech workers organized to oppose
building infrastructure to support the Trump administration’s Mus-
lim ban [52]. The Tech Worker’s Coalition aims to organize both
gig workers and software engineers to advance more just policies.
Google has terminated contracts with the U.S. Department of De-
fense (DoD) due to pressure from employees, with some speaking
up under the hashtag #TechWontBuildIt [82]. Companies such as
Amazon [28] and Wayfair [43] have seen similar pressure. Amazon
has also faced pressure with regard to climate justice [56] and has
responded with increased environmental commitments [11].
After the 2016 U.S. presidential election, it was widely acknowl-
edged that ‘fake news’ and targeted ads significantly contributed
to the outcome of the election. While the C.E.O Mark Zuckerberg
initially said this was “a pretty crazy idea” [92], he later changed
his mind after facing pressure from his employees. The editor of
Wired, investigating this case, found that “the place that you can
put the most pressure on executives comes from engineers” [87].
Other commentators have begun to say the same [75]. Clearly tech
employees have agency beyond being ‘just an engineer’ [45].
Many turn to technology company executives to answer for
ethical issues because executives, supposedly, have the agency to
make changes. We can see this, for example, in the furor over
Mark Zuckerberg’s U.S. Congressional testimony [96]. However,
other scholars, as will be discussed, also advocate for employees to
embrace their own obligations to society.
I explore how computer scientists might explore their agencies
through ethics education. Of course, agency is intersectional, not
uniform, and may be explored through means other than education.
3Such political implications are quite related to economic ones as [37] explore in
human-computer interaction. See also [99] for more examples of the capital model of
modern data systems and [94] for a theoretical analysis of them.
To study how tech employees might develop their ethical reason-
ing, I created and taught a course on ethics to computer scientists
at the University of Washington. In line with this paper, the course
aimed for students to develop both a broader conception of the
dilemmas in current computing technologies and a more expan-
sive framework for their own ethical responsibilities. A colleague
and I then re-made the course to further focus on the structural
and political implications of computing technologies and included
previous and potential actions of stakeholders of such systems.4
While questions of ethics in computer science are relevant inter-
nationally, this paper focuses primarily on the U.S. because it is the
political system I know best. I would greatly appreciate collabora-
tion with scholars knowledgeable of other political systems.
In this paper, I argue that ethics curricula in computer science
departments should cover social activism and politics as extensions
of classical discussions of ethics. This argument proceeds in four
parts. First, I review literature on the topics of ethics, civics, and
employee responsibility. Second, I offer an argument for why we
should cover political engagement in addition to ethics. Third, I ex-
plore how to implement such a change and offer recommendations.
Fourth, I consider critiques one might have for my approach.
2 LITERATURE REVIEW
Here I conduct a literature review that consists of three parts. First,
I review ethics in computer science and education. Second, I review
current critiques of ethics. Third, I establish the responsibilities of
engineers and review discussions of civics. In conducting this re-
view, I used the ACM digital library, IEEExplore, and Google Scholar
with terms such as “ethics,” “civics”, “politics,” and “activism.”
2.1 Ethics in Engineering
Ethics is “the branch of knowledge or study dealing with moral
principles” [10]. Computer science already engages with some ver-
sions of ethics. Most university degree programs offer courses or
components on ethics. The Accreditation Board of Engineering and
Technology (ABET) requires at least an awareness of ethics.
2.1.1 In Computer Science. Let us begin with Moor, whose widely
cited paper, “What is Computer Ethics?,” established, through ex-
ample, computer ethics as its own discipline. He advocates for the
need to conceptualize the nature of computer programs and the
realities they engender. He uses ethics both in the individual and
societal sense. Indeed, “computer ethics [is not] the rote application
of ethical principles to a value-free technology” [68, p. 268].
Stahl et al. comprehensively survey literature on ethics in com-
puting through 2012 [85]. They appropriately focus on parallel
discussions in other fields and offer a discussion of the history of
ethics and what its use means. Nonetheless, they presuppose bene-
fits from computing technology. They contend ethics will “ensure
that computing can realize its potential benefits” [85, p. 1] and
mention “the growing importance of computing as an enabler of
economic activities” [85, p. 27].
2.1.2 In Computer Science Education. Skirpan et al. review com-
puter science ethics classes [83]. They find tension between stand-
alone and integrated ethics classes. In one sense, stand-alone classes
4Syllabus: https://courses.cs.washington.edu/courses/cse492e/20wi/
may more deeply examine social issues and retain instructors who
can teach ethical theory, but students might intuit that the material
is irrelevant to them, especially when those courses are offered from
departments besides computer science. Integrated courses suffer
a converse fate—they may not delve deep enough, and instructors
may not have the background or desire to teach ethical or social
components. Skirpan et al. detail a case-study of one course that
attempts to integrate core concepts and ethics—to good results.
Such teaching may actually increase engagement (a trait [26] find
ethics courses often lack).
Similarly, Saltz et al. [78] offer a good recent history of comput-
ing ethics in addition to a study of the inclusion of ethics in the
top 20 U.S. computer science programs. They look specifically at
courses on machine learning (ML). I follow them in arguing that
“to act responsibly, ML engineers must adopt perspectives and com-
petencies that go beyond complexity analysis and usability, and
into histories, social sciences, and morality” [78, p. 3]. Nonetheless,
I find flaw in their method of evaluation in line with my argument.
They excluded courses which “focused on high-level societal ethi-
cal considerations beyond the possible control of the organization
supporting the data science effort” [78, p. 8]. As I will discuss below,
this exclusion criteria removes the agency of engineers in pushing
for extra-organizational forms of change.
Recently, [95] show, through a case study, how ethics might be
better embedded throughout computer science curricula, even by
involving other departments. Indeed, they say that theirmodules are
most effective when related to the technical material students have
already learned (which begins to address the tension found in [83]).
This is a move in the right direction, but still does not acknowledge
the political implications of such a restriction to technical material.
2.1.3 In Engineering Education. Through observational case stud-
ies, Colby and Sullivan [26] examine undergraduate engineering
ethics education in the United States. They use engineering codes
(e.g. from ABET) to frame the goals of university curricula. They
build on the role of moral and civic learning as important for un-
dergraduates to engage politically in a democracy [25]. In their
study of undergraduate engineering ethics, Colby and Sullivan
take a similar approach to this paper by defining ethics broadly—
as including an obligation to society. They identify a number of
particularly relevant areas for growth such as the apparent lack
of desire of engineers to engage in social issues as compared to
other undergraduates. Their proposed approaches include using
active pedagogies, engaging faculty, and increasing institutional
intentionality [26, pp. 335-336].
Gray et al., through a study abroad program for user experience
designers, explicate the kind of cross-cultural learning that Colby
and Sullivan note engineers may lack. Indeed, the concept of iden-
tity and empathy arise in what Gray et al. describe of as promoting
“digital civics.” Their study abroad experience “promoted identity
transitions that more fully account for design as a social good” [44,
p. 74].
Monteiro et al. conduct a case study of engineering courses in
Portugal, exploring how engineers’ mindsets place importance on
ethics and civics education. Invariably, they find that “the engineer
is seen more as an executor of technical orders than as a socially
involved actor” [67, p. 168]. Interestingly for this paper, they al-
most always use ethics and civics in conjunction—suggesting a
connection between theoretical reasoning and political action.
Drawing on science and technology studies in the context of
scientific literacy, Hodson advocates for a focus on civic science lit-
eracy which encompasses “the knowledge, skills, attitudes, and val-
ues necessary for making decisions on matters such as energy pol-
icy, use of natural resources, environmental protection, and moral-
ethical issues raised by technological innovations” [49, p. 197]. He
establishes a number of levels of engagement with such literacy—
from appreciating social impacts of technology to taking action
about those impacts—whichwould inform a curriculum. It is through
such political engagement that a curriculum can allow students to
“learn how to participate in sociopolitical action,” and “to encourage
others to participate, too” [49, p. 204].
Bucciarelli and Drew, in proposing a new undergraduate degree
program to better enmesh liberal arts with engineering, focus on
how current engineering programs fail to teach civics. Indeed, ABET
requires no civics courses and, instead, their ethics requirement
often boils down to an “ethics lecture...on how to avoid negligence;
there is little said about virtue ethics, or social/civic responsibility
of the individual as member of a profession” [22, p. 105]. In their
proposal, as in this one, they hope to demonstrate the compatibil-
ity of “education in science and technology (engineering)...with
education in the humanities and social sciences, in particular with
learning civics” [22, p. 106].
This is akin to what Schuler evokes as a necessary part of engi-
neering culture: that—civic—engagement with the world be brought
to “greater levels of visibility and appreciation and practice” [81].
Hekert uses the language of macro and microethics to describe a
focus on, respectively, the social or the individual and professional.
Like Moor, he makes the case that engineering ethics education
focuses too much on microethics, eliding societal concerns. As I
do, he advocates for a greater coverage of science and technology
studies and corresponding curriculum revision [48].
2.1.4 Social justice. Such a visibility of social implications has in-
creasingly come in the form of social justice. Leydens and Lucena
find engineering curriculum detached from literature like science
and technology studies, a lack of focus on the macroethical or soci-
etal concerns, little coverage of sustainability or inclusivity, and no
theoretical grounding, such as in social justice. They advocate for a
number of criteria to judge curricula, among which is, “(3) Acknowl-
edging political agency/mobilizing power” [63, p. 32]. They find
that contextual listening and appropriate teaching of, “this mapping
of power reveals the degree to which citizens in a community are
agentic—that is, how much agency they have in shaping their own
future.” [63, 26].
Riley [76] and Nieusma et al. [72] both provide further grounding
for this unification of engineering and social justice. Such a frame-
work is useful because social justice “tempers ‘technical’ impera-
tives by directing attention to social power imbalances surrounding
technology decision making as well as inequitable material out-
comes” [72, p. 10]. I follow after their suggestions in advocating for
educational interventions.
Karwat et al. seek “to fundamentally redefine contemporary
engineering practice by exposing the political and value-based
nature of engineering; by applying socio-ecological learning to
technological design; by imbuing a different sense of responsibility
in engineers; and by moving the scope of engineering beyond solely
technological development” [57, p. 91]. In line with the suggestions
of [80] and in a comprehensive compendium of prior literature, he
operationalizes this by advocating for practitioners’ self-reflection
[58]. Importantly, he focuses on how this act of reflection is more
representative, it acknowledges the inherent politics at play.
2.2 Critiques of Ethics
Recent work demonstrates the insufficiency of ethics in computer
science. I cover general critiques, those related to ethics codes, and
those stemming from practitioners.
Wagner examines how technology companies, particularly in the
scope of AI, use ethics as an escape from regulation. He proposes a
number of criteria to make ethics principles more robust, verifiable,
not-arbitrary, and legally operational [93].
Metcalf et al. examine how the tech industry operationalizes
ethics through ‘owners,’ or employees with that as their focus.
This use of ethics, “strains an already broad term that in some
contexts means an open-ended philosophical investigation into
moral conditions of human experience and, in other contexts, means
the bureaucratized expectations of professional behavior” [66, p. 1].
Furthermore, they cite [33] and [16] in arguing how the term “ethics”
locates the problem in individuals and technical systems as opposed
to in structures or societal systems.
Benjamin, quoted above, expands on her point, noting how care
“gets limited to questions of ethics and safety rather than extending
to issues of politics and democracy” [16, p. 208]. Her book offers
a “race conscious orientation to emerging technology not only as
a mode of critique but as a prerequisite for designing technology
differently” [16, p. 35].
Hoffman explores the history of antidiscrimination with regard
to fairness and big data, surfacing a few limits from the focus on
bad actors, single axis thinking, and choice of a limited set of goods.
These limits also plague ethics. She suggests that we should: pay
attention to institutional order, not only focus on disadvantage, and
take a more intersectional approach [51].
2.2.1 Regarding Ethics Codes. Ethics often find footing in the codes
of professional organizations, like the IEEE (the Institute of Electri-
cal and Electronics Engineers) [3] or the ACM (the Association of
Computing Machinery) [6].
In a 1983 survey of engineers, Luegenbiehl assails ethics codes
in engineering as not being used in practice—they’re designed for
cover as opposed to as actual resources [64].
In a way responding to Luegenbiehl, Jin and Drozdenko find
that ethics codes do play a role in informing organizational culture
(at least codes correlated with culture) and that acting in a socially
responsible manner may be related to success at an organization,
the success of the organization in general, or both [54].
Metcalf examines the history of ethics codes in fields such as
computing, medicine, and journalism [65]. He finds that they need
to be improved in terms of revisability, target populations, univer-
salism, and reactive versus proactive creation.
Saltz et al. [79] study a variety of ethics codes and relate them
to data science ethics. They find that there are unique challenges
with data science ethics as particularly related to education.
Greene et al. find the current ethics codes of AI and ML groups
co-opt the language of critics into a technologically deterministic
framework [46]. Such determinism assumes that technology has to
occur, that it determines outcomes such as societal improvement.
They cite [12] in noting the lack of a moral background in the ethics
principles examined. That is to say, principles assume a normative
backing (e.g. what fairness means and how it relates to outcomes)
that is not universally accepted by practitioners, but the principles
project a kind of completeness and therefore elide the space for the
debate over this moral background.
Stark and Hoffman [86] study the ethics codes of computing
societies and of metaphorically related professional societies. While
they note that ethics codes are useful in educating professionals
and instilling norms, they find that these codes are limited. For
example, the computing ethics codes mostly examine employees
and their organizations and do not often include recognition of
other groups which may be affected. While codes often include
clauses about an engineer’s responsibility to the public good, they
fail to evoke exactly what this means. Stark and Hoffman thus
note, like [46], that “conversations around professional ethics in
data science and related fields such as ML/AI are a necessary but
absolutely insufficient conditions for the kinds of progressive, just
and equitable social outcomes we seek for the world” [86, p. 20].
2.2.2 From practitioners. Many technical practitioners have advo-
cated for ethics and the inherently political nature of their work.
Greenmakes the case that data science is inherently political [45].
Ethics codes and courses fail to create mechanisms for accountabil-
ity, artificially divide technology and society, and lack normative
underpinning. It is this lack of clarity in ethics that should open
the door for political discussion: “data scientists must recognize
themselves as political actors engaged in normative constructions
of society and, as befits political work, evaluate their efforts accord-
ing to the material downstream impacts on people’s lives” [45, p. 1].
Following a refutation of common arguments against the inher-
ent political nature of data scientists, Green offers steps for how
to realize these politics: through practitioners’ interest, reflection,
applications, and practice.
Rogaway presents how cryptography is inherently political with
an intrinsically moral dimension. He specifically calls out the im-
plicit and overt politics present in cryptographic work and argues
that practitioners have an ethic of responsibility [77].
Agre, an AI researcher writing in the 1990s walks the line be-
tween a critical perspective and that of a practitioner, evoking some
may bristle at critique. Nonetheless, “critical analysis...legitimizes
moral and ethical discussion and encourages connections with
methods and concepts from other fields” [13, p. 149].
Moore explores whether, given these critiques, AI technologies
can be deemed as ‘good’ at all, particularly in light of the growing
trend of the use of terms like “AI for social good” [70].
That technologies are political is not a new argument. From
2007, [55] mentions a similar danger of co-option of language and
the kind of technological determinism involved.5 Even earlier, in
2000, mainstream computer ethics suffered from two problems: they
“focus[ed] too narrowly on publicly recognized moral dilemmas”
and tended “to downplay computer technology itself as an object
of moral analysis” [20, pg. 11]. I add a third problem to mainstream
computer ethics: they assume too narrow a scope.
2.3 Civics
Here I explore engineers’ obligations and the use of “civics” in
computer science. While defined as “the study of government and
the state with particular emphasis on the rights and duties of the
citizen” [8], as it is used “civics” relates to the kind of societal and
political engagement that I advance in the argument of this paper.
2.3.1 Regarding Engineers’ Obligations. Discussions of the scope
of engineers are often limited to the technical capacity from an
employer’s view. That is to say, professional and applied ethics
in computing, which make up the majority of the discourse, view
engineers as being concerned only with the technical aspects of
the products on which they work. Alternatively, one might view
the engineer as responsible more generally to society and to the
systems with which they interact—a position more in line with
Moor’s original formulation of computer ethics.
Pirtle and Szajnfarber [73] do just this. They make the case
for the engineer’s engagement with democratic responsibilities by
examining ideals of engineering and science in philosophy.
Furthermore, engineers, and all of those with specialized skills,
owe a debt to society for their training, as Schön demonstrates. Part
of the social contract of specialization is using those skills for the
betterment of all. Indeed, he suggests professional curricula should
reflect the constrictions of practitioners’ action, such development
particularly in applied sciences, and reflection of practitioners on
their organizational settings [80, p. 321].
2.3.2 In Computer Science. In engineering, and particularly in
computer science and its sub-field human-computer interaction,
civics takes on a number of meanings. Civics in this literature is
used with regard to the development of explicit “civic tech,” “digital
civics” or the new part of civil society which exists online, and, in
the sense of education, the culture of engineers.6
Civic tech. So-called “civic tech” has arisen as an alternative to
the capital-driven domain of Silicon Valley. The rise of big data
prompted discussion of how regulation might encourage “Data-
Driven Innovation” in the building of civic infrastructure—the com-
putational tools produced by various forms of government [47].
From interviews with government officials, hackers, and commu-
nity groups in Atlanta, [18] evokes various understandings of civic
tech. Through a series of speculative prototypes, [34] elicit themes
for civic tech like “Mediated Civics, Computational Civics, and Prox-
ied Civics.” They take a pluralistic view of civics, which I follow.
To them, civics are “the structures, practices, and experiences of
public life” including the state and extending to activism and civil
society [34, pg. 1]. In their work on relational service models, [91]
use the term civics similarly.
5[94, p. 27], [16, p. 40], and [23] offer a more in-depth exploration of technological
determinism such as the difference between hard and soft determinism.
6See https://civictech.guide/ for a list of some projects.
Digital media may also promote learning of civics concepts,
which is the use of civics in [4, 39, 61, 90]. In this sense, the politics
of civic engagement are related to the work of computer scientists.
In this section, I have reviewed literature on ethics in computer
science literature, professional ethics codes, ethics education, cri-
tiques of ethics in computer science, and civics in computer science.
We see that current ethics courses face tension in whether to be in-
tegrated in the curriculum or stand-alone, how to engage engineers,
and how to broaden the narrow frame of professional ethics. The
political and societal engagement offered by liberal arts and civic
education present these tensions in an alternative light, particularly
given recent critiques of ethics in computing.
3 POLITICS AND ETHICS
My argument rests on a number of normative commitments. I
ground my argument in social justice as operationalized by the
capabilities approach.7 I take a social constructivist approach and
complicate a simple narrative of technological determinism (e.g.
I disagree with [85] and [78] who excluded from their analysis
courses covering extra-organizational implications of computing
technologies), particularly as Wajcman evokes [94, p. 27].
I follow actor-network theory [21], in emerging the agency of
students and future engineers in their associations with each other,
their employers, technological artifacts, etc. I use the term politics
both in terms of civic participation and theory, following Winner
who describes politics as the, “arrangements of power and authority
in human associations as well as the activities that take place within
those arrangements” [97, p. 123]. I follow [66] in defining ethics
primarily “as social phenomena and not as primarily philosophical
abstractions” [66, p. 4], which accords with ethics as both personal
and societal [48, 68]. I call for greater political engagement in ethics
to increase the representation of other histories. So does Hoffman
with regard to antidiscrimination [51]. Brey [20] calls for computer
technology itself to be an object of moral analysis. Benjamin [16,
p. 208] quotes [84] as saying “we have to decentralize our idea of
where solutions and decisions happen, where ideas come from.”
This argument proceeds in five parts. First, I establish the neces-
sity, but also limits, of ethics. Second, I make the case that computing
professionals are responsible to society more generally. Third, I
present the role of the university as not just a knowledge creator
but also to encourage students’ action. Fourth, I incorporate current
critiques of ethics in computing to argue for practitioners’ politi-
cal engagement. Fifth, I cover the actual political engagement and
agency of computing professionals. Following these, I argue that
computing ethics education should cover political engagement to a
greater degree.
First, as is widely established, ethics are necessary for engineers.
They are required for engagement with values but are not suffi-
cient to lead to action. At the same time, as [26] find, engineers
commonly perceive their role as limited. This is often called the
‘engineering mindset.’ Computer science students often limit their
opportunities to shape or influence the world as occurring only
through technology or the accumulation of capital. Along these
7[45] cites Collins in defining social justice as “an organized, long-term effort to
eliminate oppression and empower individuals and groups within a just society” [27].
[70] expands this in terms of the capability approach as do [63].
lines, [45] examines three very common responses to the politics
of AI technologies. This is to say, current ethics education is both
limited in scope and appears not to address engineers’ perceived
lack of agency.
Second, engineers have a responsibility to society beyond a strict
interpretation of their roles in applied ethics. I argue that they are
public servants and have a responsibility to society. Interestingly,
the case to make some major tech companies such as Facebook util-
ities would strengthen this argument. If we assume that computing
technologies, like the internet, social media, peer to peer messaging,
e-commerce, are essential to modern life and part of civil society
then engineers play a unique role in those organizations, regardless
of whether they are employed privately or publicly.
In those civil roles, computing professionals possess some agency
over products, companies, their workplaces, and beyond. As has
been argued by [69], consider Facebook, which has approximately
two billion user accounts and about forty-five thousand employ-
ees [9]. While clearly not proportional, this equates to about one
employee for fifty thousand users. Of course, these employees do
not possess total sway over the direction of the companies, but nei-
ther do they have no agency. Given the actions of tech employees
already, one wonders how to encourage engineers to realize their
agency. While, current discussion of ethics focuses on major deci-
sions of companies and research centers, I propose we also focus
on those who end up becoming integral to technology companies.
Third, given practitioners’ responsibility, universities should act
to encourage it. Some may think that political engagement is sepa-
rate from the knowledge-creation ambit of universities. A social con-
structivist position shows us that we produce knowledge through
interactions with each other—that knowledge does not exist by
itself. In this sense, knowledge is already produced and acted upon
as partially determined by an agent’s normative positions; neither
knowledge nor science is value free. Therefore, a university acting
to refrain from covering the use of and arguments about knowledge
in the world would have, as [45] and others have made clear, already
adopted a political position—a conservative one.
Fourth, a greater focus on the political nature of the organizations
and problems which computer scientists face will better address
the problems raised about it. The restriction just to ethical issues
such as the trolley problems [88] drowns out concern over the
realization of ethically less ambiguous areas—such as the treatment
of marginalized populations [53]. Along these lines, Green calls
for data scientists to recognize the politics in their work and cast
off the current use of ethics [45]. Greene et al. [46] find a lack of
grounding and co-option in prominent AI ethics principles. Wagner
finds that industry groups use ethics as a means to avoid regulation
and do not provide means of assessment on those ethics [93]. Stark
and Hoffman discuss how norms around equality and justice must
supplant “processual” professional ethics [86]. Metcalf et al. [66]
and Benjamin [16] find that the current presentation of ethics is too
narrow. Likewise, ethics courses for engineers are lacking—Colby
and Sullivan [26] find engineers less willing to engage in social
issues and Bucciarelli et al. [22] discuss how these courses fail to
be meaningful for students.
At the same time, there is a growing of interest in civic tech. For
human-computer interaction students at least, cultural experiences
increase desire to engage with civic tech [44]. This complements
Colby and Sullivan’s [26] other finding that engineers engage more
when presented with active pedagogies and when part of institu-
tions with clear missions—that which might have been lacking for
students in the technical track found by Monteiro. This dovetails
[49]’s call that science literacy include “sociopolitical action” as sim-
ilarly evoked by [81]. Likewise, a focus on political action appears to
show that the engineering mindset may not be recalcitrant as [26]
find directly and in anecdotal evidence from my own experience.
All of this work demonstrates that there is more room to engage
with politics in computer science. Such engagement might build on
classic approaches like Value-Sensitive Design [41] to acknowledge
the inherent political nature of technologies while, at the same
time, presenting engineers with tangible solutions—working on
problems according to their theory of change and advocating for the
computing discipline to transform and not just recognize structural
problems.
Fifth, the politics of computer science better corresponds with
what is happening in the real world. As mentioned in the intro-
duction, software engineers and computing professionals already
exercise their ethical values politically.
These exercises have come through direct action, general advo-
cacy bodies, intra-organizational advocacy, and a growth of more
socially oriented problem areas such as ‘civic tech’. Examples of
direct action include the GoogleWalkout and the #TechWontBuildIt
movement. General advocacy bodies include the Tech Worker’s
Coalition, the defunct Computer Professionals for Social Respon-
sibility, and various not-for-profits like, more recently, the Center
for Human Technology and older organizations like the Electronic
Frontier Foundation. Intra-organizational efforts include how em-
ployees of Facebook convinced Zuckerberg to change the newsfeed
[87]. Civic tech and public interest technology have begun to in-
crease in prevalence as discussed in the literature review.
Ethics, as it exists in computing, treats professionals with limited
agency, as they may themselves express. Still, these professionals
are responsible to society beyond a strict understanding of their pro-
fessional duties. Indeed, many critiques have assailed the missing
politics in the use of ethics in computing. Nevertheless, computing
professionals have begun to act extra-organizationally and in line
with their responsibility. Therefore, I argue that we move towards
addressing political engagement to a greater degree in the ethical
education of computer professionals. Otherwise, we reify a politi-
cally conservative, as opposed to representative, conception of the
ethics of computing.
4 RECOMMENDATIONS
In this section, I offer recommendations to incorporate the politics
of computing in ethics education. First, I model how one might
measure success of my proposal. A greater focus on politics might
support ethics education through a number of mechanisms. I offer
recommendations from bottom-up, second, and top-down, third.
In terms of open pedagogical questions in ethics and computing,
I agree with the integrated curricula approach and the suggestions
of [26, pp. 335-336] for increased instructor engagement, active
pedagogies, and increased institutional intentionality. These might
be expanded in light of the recommendations of [45]: through prac-
titioners’ interest, reflection, applications, and practice.
4.1 Signs of Success
A greater coverage of politics in computer science ethics curricula
might succeed in a number of forms. In linewithmy aforementioned
normative commitments, I cover how those changes might occur
in both courses and in students.
4.1.1 Ethics Courses. First, the constitution of ethics courses might
change. Courses might assign more critical perspectives to reduce
the focus on dilemmas and employ discussion-based methods (e.g.
using [1] as a reference). They might focus on action and provide
examples of students and engineers who have acted politically in
contemporary tech companies.
Critical Perspectives. Literature from a number of disciplines can
explicate the political nature of computing. Some of these disci-
plines include critical data studies, science and technology studies,
surveillance studies, race and ethnic studies, sociology, feminism,
and the philosophy of science, some of which human-computer
interaction covers.
For example, one might cover the shifts in power in our datafied
society and the political implications of modern AI systems. To do
so, as Hoffman does in her course,8 one might start out with theory,
such as Foucault’s coverage of the Panopticon [40], weave through
Desrosières’ discussion of statistics [32], and tie these together with
Deleuze’s analysis on the transition of power in society [31]. Then,
to bring these ideas into the big data age, one might employ Bowker
and Star’s book on classification [19], Dwork et al.’s summary and
argument to expand our concern from just privacy [36], and Barocas
and Nissenbaum’s more practitioner-focused piece on privacy [15].
An ethics course might also present a unit on facial recognition
as an emerging technology, addressing its technical underpinnings,
connection to theory, structural implications, and previous actions.
To this end, Arcas et al. [14], rebuking the prediction of criminality
from photos of faces, serves as a good introduction. They begin with
the history of measuring facial features to determine criminality in
Italy, technically describe how machine learning works— regard-
ing parameters, training data, and overfitting—review how biases
emerge in such systems, and present the flaws and racism inherent
to measurement of facial features. This might then be paired with
Keyes’ work on the trans-exclusive and gender-essentialist nature
of automated gender recognition [59] which explicitly considers
the “ethical underpinnings of part of the field.” Then, one might use
Crawford and Joler’s [30] examination of various facial recognition
training data sets—images and their labels—through their histories
and taxonomies to argue that politics arise at every level of a com-
puter vision task. Students might better realize the societal and
political effects of [30]’s argument through news coverage such as
[74]. Crawford’s call to action against the use of facial recognition
technology [29] reinforces that connection.
Furthermore, consider Barocas’s course, “Ethics and Policy in
Data Science” at Cornell which called on students to draft responses
to the Consumer Financial Protection Bureau’s “Request for Infor-
mation Regarding Use of Alternative Data andModeling Techniques
in the Credit Process.”9 One might do something similar with facial
8See her syllabus: https://static1.squarespace.com/static/5b8ab61f697a983fd6b04c38/t/
5c367df0898583acb0e1eee0/1547075056600/Hoffmann-INSC_598A_Syllabus.pdf
9See his syllabus: https://www.onlineethics.org/File.aspx?id=45373
recognition technology. For example, Washington State is currently
considering privacy legislation which could cover government use
of facial recognition technology.10 Students could be directed to
draft public comments11 with regard to what they learn in the class,
engaging with the technology like Hoffman does [50].
4.1.2 Students. Second, students might act differently. We might
see students engage with and adopt more critical language. For
example, they might use terms such as “values” or “sociotechnical.”
They might discuss the political nature of technical decisions and
artifacts (e.g. in enforcing a gender binary). Students might identify
their own normative commitments (e.g. “I’m more in favor of a pro-
actionary principle than a pro-cautionary one”). Lastly, students
might then act with their values in mind—by even engaging in
activism. Such actions might include:
(1) Appealing for accountability from organizational leadership:
(a) through letters, conversation, etc.;
(b) by voting, if available;
(c) by asking about companies’ positions in hiring processes.
(2) Exercising discretion in their choice of work:
(a) by exploring different research courses;
(b) by not applying to certain companies.
(3) Engaging with technological issues publicly:
(a) with their peers, colleagues, family, and communities;
(b) by writing on social, or other, media.
(4) Engaging in direct action:
(a) by participating in strikes;
(b) by showing up to protests.
4.2 Bottom-up
It seems unlikely outside of significant lobbying efforts that univer-
sities will soon offer, if even require, a recognition of the political
nature of computing technologies. Until, and in order to reach
that point, those of us who would have computer science under-
graduates recognize and explore their political voices regarding
technology have a few options.
First, we can begin to use terms like politics in addition to terms
like ethics. This is in contrast to the current use of ethics in computer
science. If computer science ethics courses already address politics
as I discuss in this paper (as opposed to, for example, teaching ethics
in the sense of professional ethics codes or limiting discussions
to dilemmas), the change to describe ethics courses as relating
to politics might alone address the limits of ethics in computing.
Nonetheless, that seems unlikely. Thus, using the term politics
would also imply the adoption of the practices associated with
it—like discussions of responsibility to society, engagement with
critical literature, voting, debates, and a focus on action.
Second, we might focus on examples of activism in the tech world
in our own courses. Students are accustomed to solving problems
and may grow frustrated when presented with problems to which
they have no solution. Indeed, the ‘engineering mindset’ appears
at odds with the kind of open-ended philosophy of traditional
ethics. Examples of positive action of tech workers might allow
students to recognize that their ‘solution space’ extends to political
10https://app.leg.wa.gov/billsummary?BillNumber=5376&Year=2019
11Submit comments at https://app.leg.wa.gov/pbc/bill/5376
action. Of course, examples will not motivate all students, but their
use is nevertheless more in line with the telos of most accepted
normative frameworks. That is, most students will not face the
trolley problem in practice (see [53]), but they will likely end up
working for organizations with histories of oppression.
Third, we should employ discussion-based methods (e.g. see [1]).
Many undergraduate computer science programs offer large lecture
style courses—often to hundreds of students. Even recent efforts
in ethics and computer science, like the grants from the Mozilla
Foundation [71], appear to bolster this large lecture approach. It
appears unlikely that lecturing alone will allow students to en-
gage with the inherently political and explorative nature of these
problems—there’s not a correct answer. While classes with fewer
than 20 students would be ideal to promote dialog, larger classes
might approximate this by breaking up into discussion groups, em-
ploying activities such as think-pair-share, using online platforms
for students to engage with each other, etc.
Fourth, we can cite literature the better explicates the political
nature of computer science, in line with the above discussion of
critical perspectives.
4.3 Top-down
Accreditation and grant requirements, given their historical use,
appear the most effective mechanisms to change department and
university policies on ethics courses for computer scientists.
4.3.1 Accreditation. Accreditation dictates which courses com-
puter science departments offer. In the United States, ABET (the
Accreditation Board of Engineering and Technology) controls most
accreditation, but this primarily applies to students seeking com-
puter engineering rather than computer science degrees. (Changing
the requirements for computer science degrees would more likely
occur on the university level.)
Since the 2017-2018 accreditation cycle, the ABET requirements
for student outcomes in computer engineering programs have
changed in a manner that appears to make them more limited.
The total number of required student outcomes reduced from nine
to five. While the outcomes previously required that students attain
“(e) An understanding of professional, ethical, legal, security and
social issues and responsibilities” and “(g) An ability to analyze the
local and global impact of computing on individuals, organizations,
and society” [5] by 2018 - 2019, these had become, under the head-
ing student outcomes, “4. Recognize professional responsibilities and
make informed judgments in computing practice based on legal and
ethical principles” and, under the heading curriculum, courses must
cover “3. Local and global impacts of computing solutions on indi-
viduals, organizations, and society” [7]. Furthermore, the outcomes
now use language that centers, to a greater degree, computing and
the organizations in which students will work. Students should
produce solutions that are “computing-based...in the context of the
program’s discipline.” Their responsibilities and ethics are now rela-
tive to “professional” commitments. Both “social” and “issues” have
disappeared from the new criteria, suggesting a perceived lack of
importance. Notably, even [78], who conducted a review of ABET
and how the top 20 U.S. computer science programs covered ethics,
failed to comment on the implications of the 2018 - 2019 ABET
changes. Still, while unnecessarily vague, the fact that curricula
must include a focus on “local and global impacts” at least allows
for the inclusion of a greater political awareness of computing
technologies, as I suggest above.
Despite the drawbacks of recent changes, future accreditation
criteria might include language to make more stringent the ethics
requirement or, even better, the political nature of technologies.
4.3.2 Grants. Grants appear the most likely top-down mechanism
to create change. While limited, the use of ethics in computer
science demonstrates scholarly engagement which might imply
interest in the expansion of curricula. For example, the Mozilla
Foundation [71] recently awarded 3.5 million dollars among 17
schools. A foundation might do something similar to explicate the
political nature of computing more specifically. In another example,
the 2000s saw Congress, cognizant of the risks of the nanotech-
nology, mandate “the integration of research on societal, ethical
and environmental concerns with nanotechnology research and
development” [2]. An act might do the same for computer science.
5 LIMITATIONS
In the scope of advancing amore representative treatment of politics
in computer science education one might levy a number of critiques.
I address eight of these. These critiques are relevant but are exactly
why we should forward the politics in computer science education—
politics are already present, but currently not discussed enough.
First, one might object to my argument on the grounds that
(computer) science should be apolitical. This is similar to arguments
that science should be value-free or objective which [35] clearly
refutes—values arise in all aspects of science.
Second, and closely related, is the critique that the political en-
gagement of engineers is contrary to what their position should
be—that is, largely subservient to management. As discussed in the
literature review, this is also not the case. Computer scientists or
engineers are as much members of society as anyone else and thus
have the right and responsibility to act as such.
Third, one might argue that even if politics are embedded in tech-
nology and computer scientists have a role in engaging with those
politics, it is not the role of the university to teach such engagement.
I present an argument against this in section three. Nonetheless,
readers might interpret me as insinuating a (deterministic) causal
link between education and the political action of students. This
is not my intention. To that end, one might conduct a systematic
analysis to describe a relationship between, if it exists, computer
science programs and the normative classifications of companies
for which their graduates work.
Fourth, even if universities are allowed to act as such, one might
argue that to do so would endanger the support computer science
departments currently receive from technology companies. This
is likely the case but appears necessary in focusing on normative
outcomes. Indeed, to not teach future tech employees of their polit-
ical roles is itself a political position and perhaps even an unethical
one if these departments support the oppressive actions of tech
companies by furnishing subservient graduates.
Fifth, and more practically, instructors in computer science de-
partments might not be interested, available, or capable of teaching
such classes. As [78, p. 7] cite a response to a 1996 article, one com-
puter science professor, reacting to a report on ethics, said “ethics
is ‘not computer science’ and that it was ‘difficult to imagine a
computer scientist teaching these things.’”
This raises serious concerns for scaling classes which address pol-
itics (or even ethics). Clearly, the ability to teach set-theory does not
qualify a professor to lead a discussion on Foucault. Nevertheless,
computer science departments possess the funds and this dearth
of instructors might conveniently solve issue six: that focusing on
computer science departments encourages academic silo-ing and
silences critical scholars whose careers are predicated on paying
attention to such issues around technology. The lack of teaching
ability in computer science departments might be an opportunity
for educators and interdisciplinary engagement. Still, such a shift
might open the door for students to stop paying attention—‘it’s just
the ethics instructor, not real work.’ Effective endorsement from
authority figures in computer science, co-teaching, and sufficient
technical coverage (e.g. a lecture on how facial recognition works)
might overcome such responses.
Seventh, one might argue that my presentation of agency is
limited. Of course, my discussion does not fully address the compli-
cations of agency. Intersecting identities, such as of citizenship or
economic status, gender, sexuality, and race, complicate a simple
presentation of agency. For example, consider a female-identifying
first-generation college student whose parents were born outside
of the United States and a male-identifying student whose parents
both hold doctorates and pay for his tuition outright. Even if both
are offered jobs as a “Software Engineer I” at a large cloud com-
puting company with military contracts in a major Western U.S.
city and both hold similar normative and political commitments,
these two will possess a very different sort of agency. While both
are offered six-figure salaries, the man appears to face fewer risks
in turning the job down. As stated in the recommendation section,
students have a variety of means through which to act politically
besides just ‘talking with their feet.’ Still, neither students, tech em-
ployees, nor the communities for whom they claim to act, might be
expected to fulfill just one presentation of a reflective practitioner.
Their means are manifold.
Eighth, and most importantly, one might argue that greater politi-
cal engagement is still whitewashing. In that sense, such instruction
could lead to greater harms. The proposal to encourage more cov-
erage of politics in ethics curricula might even backfire. Instructors
might interpret it as just one more box to check. Students, given an
outlet for their moral action, might then do nothing outside of the
classroom. I am most concerned by this critique. Nonetheless, by
maintaining a focus on others’ action outside of the classroom as
models for students and with engaged instructors with exposure to
the literature, the potential for such a backfire seems less likely.
It is not my intention to indicate that re-naming or changing
of curricula will engender structural transformation. If instructors
did start teaching political agency as part of ethics classes, it would
still be just a bandage. Instead, this is part of a greater campaign to
recognize the political nature of computer science. Ideally this leads
to and works with the kind of activism, legislation, and behavior
change necessary for the application of a given ethical theory.
Lastly, political exercise carries risk. Such risk is not reason
enough to avoid showing students their responsibility and agen-
cies but is worth noting. For example, organizers of the walkout
against sexual harassment at Google in the fall of 2018 have faced
retaliation [89] and since left [24]. In December 2019, four Google
union organizers were fired for their efforts [17]. These risks are not
uniform and complicate an intersectional understanding of agency.
6 CONCLUSION
In this paper, I have identified a problem with the ethics curricula
in computer science and suggested a solution through greater in-
corporation of politics. I began by reviewing literature on ethics in
computing. Then I offered a systematic argument for why such an
incorporation of politics would be appropriate. Then I attempted to
operationalize this argument through recommendations. Cognizant
of critiques, I finished by covering limitations to my approach.
Stark and Hoffman [86] aptly quote Langdon Winner as testify-
ing to Congress to deplore, “those who conduct research about the
ethical dimensions of emerging technology to gravitate toward the
more comfortable, even trivial questions involved, avoiding issues
that might become a focus of conflict” [98].
This work aims to support the increased proliferation of ethics
courses. It also focuses on the bigger picture political questions
at stake. Ethics education in computer science has many ways to
improve before even reaching the point of advocacy.
As Ethan Zuckerman, a prominent MIT computer scientist, has
said, “My guess is that courses that force us to have these sorts
of arguments are critical to unpacking the intricacies of emerging
technologies and their implications. To be clear, there’s the field
of science and technology studies, which makes these questions
central to its debates” [100].
Values, and discussions about them, under-gird the work of
computer scientists. Recent uses of ethics, particularly regarding
technologies like artificial intelligence, appear to ameliorate the
concerns of computing technologies, but in practice, ‘ethics’ does
not go far enough to recognize the inherently political nature of
these technologies. While past works on ethics in computing have
focused on research, regulation, and executives, practitioners, too,
have a role. Indeed, some tech employees have already begun to act
beyond their organizationally prescribed scope. Given the issues
with ethics as it is used in computer science and students’ likely
receptivity to action-based teaching, we should teach the politics
of computing. Doing so would provide students opportunities to
explore not only values in technologies and normative frameworks,
but also means for redressal.
ACKNOWLEDGMENTS
This article would not have been possible without the feedback
from three anonymous reviewers, support of the University of
Washington School of Computer Science, comments from scholars,
such as Joyce Havstad, at the Conference on Values in Medicine,
Science, and Technology, and the students in the ethics courses I
have taught. In particular, I would like to thank Dan Grossman,
Anna Hoffman, Johan Michalove, Alan Borning, Zachary Pirtle,
David Tomblin, and Meg Young for their ideas, conversation, and
suggestions.
REFERENCES
[1] [n. d.]. 0. Discussion Leading Guidelines. https://teachingcommons.
stanford.edu/resources/teaching-resources/teaching-strategies/how-lead-
discussion/discussion-leading-guidelines
[2] [n. d.]. 0. History. https://cns.asu.edu/about/history
[3] [n. d.]. 0. IEEE Code of Ethics. https://www.ieee.org/about/corporate/governance/
p7-8.html
[4] W. Lance Bennett. 2007. Civic Identities, Online Technologies: From Designing Civics Curriculum
to Supporting Civic Experiences. In Civic Life Online, W. Lance Bennett (Ed.).
The MIT Press. https://doi.org/10.7551/mitpress/7893.003.0007
[5] -. 2016. Criteria for Accrediting Computing Programs, 2017-2018. Technical Report.
ABET. https://www.abet.org/accreditation/accreditation-criteria/criteria-for-
accrediting-computing-programs-2017-2018/
[6] -. 2018. ACM Code of Ethics and Professional Conduct. https://www.acm.org/
about-acm/acm-code-of-ethics-and-professional-conduct
[7] -. 2018. Criteria for Accrediting Computing Programs, 2019 - 2020. Technical Report.
ABET. https://www.abet.org/accreditation/accreditation-criteria/criteria-for-
accrediting-computing-programs-2019-2020/
[8] -. 2019. civics, n. https://www.oed.com/view/Entry/33574
[9] -. 2019. Company Info. https://about.fb.com/company-info/
[10] -. 2019. ethic, n. and adj. https://www.oed.com/view/Entry/64755
[11] -. 2019. Sustainability: Thinking Big. https://sustainability.aboutamazon.com/
[12] Gabriel Abend. 2016. Themoral background: an inquiry into the history of business
ethics. Vol. 60. Princeton University Press.
[13] Philip E. Agre. 1997. Lessons Learned in Trying to Reform AI. Social sci-
ence, technical systems, and cooperative work: Beyond the great divide (1997),
131. https://web.archive.org/web/20040203070641/http://polaris.gseis.ucla.edu/
pagre/critical.html
[14] Blaise Aguera y Arcas. 2017. Physiognomy’s New Clothes. https://medium.
com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a
[15] Solon Barocas and Helen Nissenbaum. 2014. Big data’s end run around
procedural privacy protections. Commun. ACM 57, 11 (Oct. 2014), 31–33.
https://doi.org/10.1145/2668897
[16] Ruha Benjamin. 2019. Race after technology: Abolitionist tools for the new jim
code. John Wiley & Sons.
[17] Laurence Berland, Paul Duke, Rebecca Rivers, and Sophie Wald-
man. 2019. Google fired us for organizing. We’re fighting back.
https://medium.com/@GoogleWalkout/google-fired-us-for-organizing-
were-fighting-back-d0daa8113aed
[18] Kirsten Boehner and Carl DiSalvo. 2016. Data, Design and Civics: An Exploratory
Study of Civic Tech. In Proceedings of the 2016 CHI Conference on Human Factors
in Computing Systems (CHI ’16). ACM, New York, NY, USA, 2970–2981. https:
//doi.org/10.1145/2858036.2858326
[19] Geoffrey C. Bowker and Susan Leigh Star. 1999. Sorting things out: classification
and its consequences. MIT Press, Cambridge, Mass.
[20] Philip Brey. 2000. Disclosive Computer Ethics. SIGCAS Comput. Soc. 30, 4 (Dec.
2000), 10–16. https://doi.org/10.1145/572260.572264
[21] Attila Bruni and Maurizio Teli. 2007. Reassembling the social—An introduction
to actor network theory. Management Learning 38, 1 (2007), 121–125.
[22] Louis L. Bucciarelli and David E. Drew. 2015. Liberal studies in engineering - a
design plan. Engineering Studies 7, 2-3 (Sept. 2015), 103–122. https://doi.org/10.
1080/19378629.2015.1077253
[23] Paul E Ceruzzi. 2005. Moore’s law and technological determinism: reflections
on the history of technology. Technology and Culture 46, 3 (2005), 584–593.
[24] Google Walkout For Real Change. 2019. Onward! Another #GoogleWalk-
out Goodbye. https://medium.com/@GoogleWalkout/onward-another-
googlewalkout-goodbye-b733fa134a7d
[25] Anne Colby, Elizabeth Beaumont, Thomas Ehrlich, and Josh Corngold. 2007.
Educating for Democracy: Preparing Undergraduates for Responsible Political
Engagement. John Wiley & Sons. Google-Books-ID: AGe6iRZLWg0C.
[26] Anne Colby and William M. Sullivan. 2008. Ethics Teaching in Undergraduate
Engineering Education. Journal of Engineering Education 97, 3 (July 2008),
327–338. https://doi.org/10.1002/j.2168-9830.2008.tb00982.x
[27] Patricia Hill Collins. 2002. Black feminist thought: Knowledge, consciousness, and
the politics of empowerment. Routledge.
[28] Kate Conger. 2018. AmazonWorkers Demand Jeff Bezos Cancel Face Recognition
Contracts With Law Enforcement. https://gizmodo.com/amazon-workers-
demand-jeff-bezos-cancel-face-recognitio-1827037509
[29] Kate Crawford. 2019. Halt the use of facial-recognition technology until it is
regulated. Nature 572 (Aug. 2019), 565–565. https://doi.org/10.1038/d41586-
019-02514-7
[30] Kate Crawford and Trevor Paglen. 2019. Excavating AI: The Politics of Training
Sets for Machine Learning. https://www.excavating.ai
[31] Gilles Deleuze. 1992. Postscript on the Societies of Control. October 59 (1992),
3–7.
[32] Alain Desrosières. 2002. The politics of large numbers: A history of statistical
reasoning. Harvard University Press.
[33] Catherine D’Ignazio and Lauren Klein. 2019. Data Feminism. MIT Press Open.
https://bookbook.pubpub.org/data-feminism
[34] Carl DiSalvo, Tom Jenkins, and Thomas Lodato. 2016. Designing Speculative
Civics. In Proceedings of the 2016 CHI Conference on Human Factors in Computing
Systems (CHI ’16). ACM, New York, NY, USA, 4979–4990. https://doi.org/10.
1145/2858036.2858505 event-place: San Jose, California, USA.
[35] Heather Douglas. 2009. Science, policy, and the value-free ideal. University of
Pittsburgh Pre.
[36] Cynthia Dwork and Deirdre K Mulligan. 2013. It’s Not Privacy, and It’s Not
Fair. Stanford Law Review 66 (2013), 6.
[37] Hamid Ekbia and Bonnie Nardi. 2015. The Political Economy of Computing:
The Elephant in the HCI Room. interactions 22, 6 (Oct. 2015), 46–49. https:
//doi.org/10.1145/2832117
[38] Casey Fiesler. 2019. Tech Ethics Curricula: A Collection of Syl-
labi. https://medium.com/@cfiesler/tech-ethics-curricula-a-collection-of-
syllabi-3eedfb76be18
[39] Marcus Foth, Martin Tomitsch, Christine Satchell, and M. Hank Haeusler. 2015.
From Users to Citizens: Some Thoughts on Designing for Polity and Civics. In
Proceedings of the Annual Meeting of the Australian Special Interest Group for
Computer Human Interaction (OzCHI ’15). ACM, New York, NY, USA, 623–633.
https://doi.org/10.1145/2838739.2838769 event-place: Parkville, VIC, Australia.
[40] Michel Foucault. 2012. Discipline and punish: The birth of the prison. Vintage.
[41] Batya Friedman, Peter H. Kahn, Alan Borning, and Alina Huldtgren. 2013.
Value Sensitive Design and Information Systems. In Early engagement and
new technologies: Opening up the laboratory. Springer, Dordrecht, 55–95. https:
//doi.org/10.1007/978-94-007-7844-3_4
[42] April Glaser. 2019. The Techlash Has Come to Stanford. Slate Magazine (Aug.
2019). https://slate.com/technology/2019/08/stanford-tech-students-backlash-
google-facebook-palantir.html
[43] April Glaser. 2019. Why the Wayfair Walkout Is Different. https:
//slate.com/technology/2019/06/wayfair-walkout-trump-protest-border-
detention-beds.html
[44] Colin M. Gray, Austin L. Toombs, Marlo Owczarzak, and Christopher Watkins.
2019. Digital civics goes abroad. Interactions 26, 2 (Feb. 2019), 74–77. https:
//doi.org/10.1145/3301661
[45] Ben Green. 2018. Data Science as Political Action: Grounding Data Science in a
Politics of Justice. arXiv:1811.03435 [cs] (Nov. 2018). http://arxiv.org/abs/1811.
03435 arXiv: 1811.03435.
[46] Daniel Greene, Anna Lauren Hoffmann, and Luke Stark. 2019. Better, Nicer,
Clearer, Fairer: A Critical Assessment of the Movement for Ethical Artificial
Intelligence and Machine Learning. https://doi.org/10.24251/HICSS.2019.258
[47] J. Hemerly. 2013. Public Policy Considerations for Data-Driven Innovation.
Computer 46, 6 (June 2013), 25–31. https://doi.org/10.1109/MC.2013.186
[48] Joseph R. Herkert. 2005. Ways of thinking about and teaching ethical problem
solving: Microethics and macroethics in engineering. Science and Engineering
Ethics 11, 3 (Sept. 2005), 373–385. https://doi.org/10.1007/s11948-005-0006-3
[49] Derek Hodson. 2010. Science Education as a Call to Action. Canadian Journal
of Science, Mathematics and Technology Education 10, 3 (Aug. 2010), 197–206.
https://doi.org/10.1080/14926156.2010.504478
[50] Anna Hoffman. 2019. The privacy risks of unchecked facial-recognition technol-
ogy. https://www.seattletimes.com/opinion/the-privacy-risks-of-unchecked-
facial-recognition-technology/
[51] Anna Lauren Hoffmann. 2019. Where fairness fails: data, algorithms, and the
limits of antidiscrimination discourse. Information, Communication & Society
22, 7 (June 2019), 900–915. https://doi.org/10.1080/1369118X.2019.1573912
[52] Leigh Honeywell. 2016. neveragain.tech. http://neveragain.tech/
[53] Abby Everett Jaques. 2019. Why the moral machine is a monster. University of
Miami School of Law, 10.
[54] K. Gregory Jin and Ronald G. Drozdenko. 2010. Relationships among Per-
ceived Organizational Core Values, Corporate Social Responsibility, Ethics, and
Organizational Performance Outcomes: An Empirical Study of Information
Technology Professionals. Journal of Business Ethics 92, 3 (March 2010), 341–359.
https://doi.org/10.1007/s10551-009-0158-1
[55] Deborah G. Johnson. 2007. Ethics and Technology ’in the Making’: An Essay
on the Challenge of Nanoethics. NanoEthics 1, 1 (March 2007), 21–30. https:
//doi.org/10.1007/s11569-007-0006-7
[56] Amazon Employees for Climate Justice. 2019. Open letter to Jeff Be-
zos and the Amazon Board of Directors. https://medium.com/
@amazonemployeesclimatejustice/public-letter-to-jeff-bezos-and-the-
amazon-board-of-directors-82a8405f5e38
[57] Darshan MA Karwat, Walter E Eagle, Margaret S Wooldridge, and Thomas E
Princen. 2015. Activist engineering: changing engineering practice by deploying
praxis. Science and engineering ethics 21, 1 (2015), 227–239.
[58] Darshan M. A. Karwat. 2019. Self-reflection for Activist Engineering. Science
and Engineering Ethics (Oct. 2019). https://doi.org/10.1007/s11948-019-00150-y
[59] Os Keyes. 2018. The Misgendering Machines: Trans/HCI Implications of Au-
tomatic Gender Recognition. Proceedings of the ACM on Human-Computer
Interaction 2, CSCW (Nov. 2018), 1–22. https://doi.org/10.1145/3274357
[60] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2016. Inherent
Trade-Offs in the Fair Determination of Risk Scores. arXiv:1609.05807 [cs, stat]
(Sept. 2016). http://arxiv.org/abs/1609.05807 arXiv: 1609.05807.
[61] Matthias Korn. 2013. Situating Engagement: Ubiquitous Infrastructures for In-Situ
Civic Engagement. Ph.D. Dissertation. Datalogisk Institut, Aahus Universitet.
https://www.forskningsdatabasen.dk/en/catalog/2389300983
[62] Julian Lamont and Christi Favor. 2017. Distributive Justice. In The Stanford
Encyclopedia of Philosophy (winter 2017 ed.), Edward N. Zalta (Ed.). Metaphysics
Research Lab, Stanford University. https://plato.stanford.edu/archives/win2017/
entries/justice-distributive/
[63] Jon A. Leydens and Juan C. Lucena. 2018. Engineering justice: transforming
engineering education and practice. John Wiley & Sons ; IEEE Press, Hoboken,
NJ : Piscataway, NJ. OCLC: ocn945390599.
[64] Heinz C. Luegenbiehl and Public Interest Enterprises, Inc. 1983. Codes of Ethics
and the Moral Education of Engineers:. Business and Professional Ethics Journal
2, 4 (1983), 41–61. https://doi.org/10.5840/bpej1983244
[65] Jacob Metcalf. 2014. Ethics Codes: History, Context, and Challenges. Council
for Big Data, Ethics, and Society (Nov. 2014), 15.
[66] Jacob Metcalf, Emanuel Moss, and danah boyd. 2019. Owning Ethics:
Corporate Logics, Silicon Valley, and the Institutionalization of Ethics.
Social Research: An International Quarterly 86, 2 (2019), 449–476.
https://datasociety.net/output/owning-ethics-corporate-logics-silicon-
valley-and-the-institutionalization-of-ethics/
[67] Fátima Monteiro, Carlinda Leite, and Cristina Rocha. 2017. The influence
of engineers’ training models on ethics and civic education component in
engineering courses in Portugal. European Journal of Engineering Education 42,
2 (March 2017), 156–170. https://doi.org/10.1080/03043797.2016.1267716
[68] James H. Moor. 1985. What Is Computer Ethics?*. Metaphilosophy 16, 4 (1985),
266–275. https://doi.org/10.1111/j.1467-9973.1985.tb00173.x
[69] Jared Moore. 2018. Tech employees can make up for executives. TechCrunch
(Sept. 2018). http://techcrunch.com/2018/09/27/tech-employees-can-make-up-
for-executives/
[70] Jared Moore. 2019. AI for Not Bad. Frontiers in Big Data 2 (2019), 32.
[71] Mozilla. 2018. Announcing a Competition for Ethics in Computer Science,
with up to $3.5 Million in Prizes. https://blog.mozilla.org/blog/2018/10/10/
announcing-a-competition-for-ethics-in-computer-science-with-up-to-3-5-
million-in-prizes
[72] Dean Nieusma and Rensselaer Polytechnic Institute. 2011. Engineering, Social
Justice, and Peace: Strategies for Pedagogical, Curricular, and Institutional
Reform. (2011), 12.
[73] Zachary Pirtle and Zoe Szajnfarber. 2017. On Ideals for Engineering in Demo-
cratic Societies. In Philosophy and Engineering. Springer, 99–112.
[74] Naomi Rea. 2019. How ImageNet Roulette, a Viral Art Project That Exposed
Facial Recognition’s Biases, Is Changing Minds About AI. https://news.artnet.
com/art-world/imagenet-roulette-trevor-paglen-kate-crawford-1658305
[75] Lyel Resner. 2019. Why the Tech Worker Activist Movement Is Just Begin-
ning. https://medium.com/@lyelr/why-the-tech-worker-activist-movement-
is-just-beginning-782f268ddc89
[76] Donna Riley. 2008. Engineering and Social Justice. Synthesis Lectures on Engi-
neers, Technology, and Society (2008), 14.
[77] Phillip Rogaway. 2015. The Moral Character of Cryptographic Work. (Dec.
2015), 48.
[78] Jeffrey Saltz, Michael Skirpan, Casey Fiesler, Micha Gorelick, Tom Yeh, Robert
Heckman, Neil Dewar, and Nathan Beard. 2019. Integrating Ethics within
Machine-learning Courses. ACM Transactions on Computing Education 19, 4
(Aug. 2019), 1–26. https://doi.org/10.1145/3341164
[79] Jeffrey S. Saltz, Neil I. Dewar, and Robert Heckman. 2018. Key Concepts for
a Data Science Ethics Curriculum. In Proceedings of the 49th ACM Technical
Symposium on Computer Science Education - SIGCSE ’18. ACM Press, Baltimore,
Maryland, USA, 952–957. https://doi.org/10.1145/3159450.3159483
[80] Donald A. Schön. 1987. Educating the reflective practitioner. (1987).
[81] Doug Schuler. 2001. Computer Professionals and the Next Culture of Democracy.
Commun. ACM 44, 1 (Jan. 2001), 52. https://link.galegroup.com/apps/doc/
A69372350/AONE?u=gainstoftech&sid=AONE&xid=7c08aba3
[82] Scott Shane and Daisuke Wakabayashi. 2018. ’The Business of War’:
Google Employees Protest Work for the Pentagon - The New York
Times. https://www.nytimes.com/2018/04/04/technology/google-letter-ceo-
pentagon-project.html
[83] Michael Skirpan, Nathan Beard, Srinjita Bhaduri, Casey Fiesler, and Tom Yeh.
2018. Ethics Education in Context: A Case Study of Novel Ethics Activities
for the CS Classroom. In Proceedings of the 49th ACM Technical Symposium on
Computer Science Education - SIGCSE ’18. ACM Press, Baltimore, Maryland, USA,
940–945. https://doi.org/10.1145/3159450.3159573
[84] Stina Soderling. 2019. Emergent Strategy: Shaping Change, Changing
Worlds/And the Spirit Moved Them: The Lost Radical History of America’s
First Feminists. Anarchist Studies 27, 1 (2019), 100–103.
[85] Bernd Carsten Stahl, Job Timmermans, and Brent Daniel Mittelstadt. 2016. The
Ethics of Computing: A Survey of the Computing-Oriented Literature. Comput.
Surveys 48, 4 (Feb. 2016), 1–38. https://doi.org/10.1145/2871196
[86] Luke Stark and Anna Lauren Hoffmann. 2019. Data Is the New What? Popular
Metaphors & Professional Ethics in Emerging Data Culture. Journal of Cultural
Analytics (2019). https://doi.org/10.22148/16.036
[87] Nicholas Thompson. 2018. A Reckoning at Facebook. https://www.wnycstudios.
org/story/reckoning-facebook-1
[88] Judith Jarvis Thomson. 1984. The trolley problem. Yale LJ 94 (1984), 1395.
[89] Nitasha Tiku. 2019. Google Walkout Organizers Say They’re Facing Retaliation.
Wired (April 2019). https://www.wired.com/story/google-walkout-organizers-
say-theyre-facing-retaliation/?mbid=social_twitter_onsiteshare
[90] Zeynep Tufekci. 2014. Engineering the public: Big data, surveillance and com-
putational politics. First Monday 19, 7 (July 2014). https://doi.org/10.5210/fm.
v19i7.4901
[91] Vasillis Vlachokyriakos, Clara Crivellaro, Christopher A. Le Dantec, Eric Gordon,
Pete Wright, and Patrick Olivier. 2016. Digital Civics: Citizen Empowerment
With and Through Technology. In Proceedings of the 2016 CHI Conference Ex-
tended Abstracts on Human Factors in Computing Systems (CHI EA ’16). ACM,
New York, NY, USA, 1096–1099. https://doi.org/10.1145/2851581.2886436 event-
place: San Jose, California, USA.
[92] Fred Vogelstein and Nicholas Thompson. 2018. Inside Facebook’s Two Years of
Hell. Wired (Feb. 2018). https://www.wired.com/story/inside-facebook-mark-
zuckerberg-2-years-of-hell/
[93] Ben Wagner. 2018. Ethics as an Escape from Regulation: From ethics-washing
to ethics-shopping? Being Profiling. Cogitas Ergo Sum (2018).
[94] Judy Wajcman. 2015. Pressed for time: The acceleration of life in digital capitalism.
University of Chicago Press.
[95] Kate Vredenburgh Jeff Behrends Lily Hu Alison Simmons Jim Barbara J. Grosz
Waldo, David Gray Grant. 2019. Embedded EthiCS: Integrating Ethics Across
CS Education. Commun. ACM 62, 8 (Aug. 2019), 54–61. https://cacm.acm.org/
magazines/2019/8/238345-embedded-ethics/fulltext
[96] Chloe Watson. 2018. The key moments from Mark Zuckerberg’s testimony
to Congress. The Guardian (April 2018). https://www.theguardian.com/
technology/2018/apr/11/mark-zuckerbergs-testimony-to-congress-the-key-
moments
[97] Langdon Winner. 1980. Do artifacts have politics? Daedalus (1980), 121–136.
[98] Langdon Winner. 2003. The Societal Implications of Nanotechnology. http:
//homepages.rpi.edu/~winner/testimony.htm
[99] Shoshana Zuboff. 2019. The age of surveillance capitalism: The fight for a human
future at the new frontier of power. Profile Books.
[100] Ethan Zuckerman. 2019. Training the next generation of ethical
techies. http://www.ethanzuckerman.com/blog/2019/08/14/training-the-next-
generation-of-ethical-techies/
