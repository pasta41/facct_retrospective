Garbage In, Garbage Out? Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From?
R. Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai, Jie Qiu, Rebekah Tang, and Jenny Huang
Berkeley Institute for Data Science, University of California, Berkeley
ABSTRACT
Many machine learning projects for new application areas involve
teams of humans who label data for a particular purpose, from
hiring crowdworkers to the paper’s authors labeling the data them-
selves. Such a task is quite similar to (or a form of) structured
content analysis, which is a longstanding methodology in the so-
cial sciences and humanities, with many established best practices.
In this paper, we investigate to what extent a sample of machine
learning application papers in social computing — specifically pa-
pers from ArXiv and traditional publications performing an ML
classification task on Twitter data — give specific details about
whether such best practices were followed. Our team conducted
multiple rounds of structured content analysis of each paper, mak-
ing determinations such as: Does the paper report who the labelers
were, what their qualifications were, whether they independently
labeled the same items, whether inter-rater reliability metrics were
disclosed, what level of training and/or instructions were given to
labelers, whether compensation for crowdworkers is disclosed, and
if the training data is publicly available. We find a wide divergence
in whether such practices were followed and documented. Much of
machine learning research and education focuses on what is done
once a “gold standard” of training data is available, but we discuss
issues around the equally-important aspect of whether such data is
reliable in the first place.
CCS CONCEPTS
• Information systems → Content analysis and feature se-
lection; • Computing methodologies→ Supervised learning
by classification; • Social and professional topics → Project
and people management.
KEYWORDS
machine learning, data labeling, human annotation, content analy-
sis, training data, research integrity, meta-research
ACM Reference Format:
R. Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai, Jie Qiu, Rebekah Tang,
and Jenny Huang. 2020. Garbage In, Garbage Out? Do Machine Learning
Application Papers in Social Computing Report Where Human-Labeled
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FAT* ’20, January 27–30, 2020, Barcelona, Spain
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6936-7/20/01. . . $15.00
https://doi.org/10.1145/3351095.3372862
Training Data Comes From?. In Conference on Fairness, Accountability, and
Transparency (FAT* ’20), January 27–30, 2020, Barcelona, Spain. ACM, New
York, NY, USA, 18 pages. https://doi.org/10.1145/3351095.3372862
1 INTRODUCTION
1.1 Garbage In, Garbage Out?
Machine learning (ML) has become widely used in many academic
fields, as well as across the private and public sector. Supervised
machine learning is particularly prevalent, in which training data is
collected for a set of entities with known properties (a “ground truth”
or “gold standard”), then used to create a classifier that will make
predictions about new entities of the same type. One of the earliest
applications of supervised ML is in spam detection [e.g. 11, 48].
Humans would label e-mails as spam or non-spam, then classifiers
trained on these data would be used to predict if future e-mails were
spam or non-spam. As Brunton details in his history of spam [8],
spam is a messy concept, especially when trying to define it globally
at scale. Like most problems of classification, we typically rely on an
“I know it when I see it” standard, which often breaks down at edge
cases [7]. One challenge in spam detection is in operationalization:
not just defining spam, but also creating a systematic procedure
to measure it. Then, assuming a fuzzy social concept like spam is
sufficiently defined and operationalized, humans are typically still
needed to label cases according to this definition and procedure.
“Garbage In, Garbage Out” is a longstanding aphorism in com-
puting about how flawed input data or instructions will produce
flawed outputs [1, 39]. SupervisedML requires high-quality training
data to produce high-quality classifiers. However, contemporary
ML research and education tends to focus less on obtaining and
validating such a training dataset, with such considerations often
passed over in major textbooks [e.g. 15, 20, 28]. The predominant
focus is typically on what is done with the training data to produce
a classifier, with heavy emphasis on mathematical foundations and
routine use of clean and tidy “toy” datasets. The process of creating
a “gold standard” or “ground truth” dataset is routinely black-boxed.
Many papers in ML venues are expected to use a standard, public
training dataset, with authors comparing various performance met-
rics on the same dataset. While such a focus on what is done to a
training dataset may be appropriate for theoretically-oriented basic
research in ML, this is not the case for supervised ML applications.
1.2 Study overview
All approaches of producing a training dataset involve some form of
human judgment, albeit at varying levels of granularity. In this pa-
per, we investigate and discuss a wide range of issues and concerns
around the curation of human-labeled or human-annotated data, in
which one or more individuals make discrete assessments of items.
We report from a study in which a team of six labelers systemati-
cally examined a corpus of supervised machine learning application
papers in social computing, specifically those that classified tweets
from Twitter for various purposes. For each paper, we recorded
what the paper does or does not state about the training data used to
produce the classifier presented in the paper. The bulk of the papers
we examined were a sample of preprints or postprints published
on ArXiV.org, plus a smaller set of published papers sampled from
Scopus. We determined whether such papers involved an original
classification task using supervised ML, whether the training data
labels were produced from human annotation, and if so, the source
of the human-labeled dataset (e.g. the paper’s authors, Mechanical
Turk, recruited experts, no information given, etc.). For all papers
in which an original human-labeled dataset was produced, we then
made a series of further determinations, including if definitions
and/or examples were given to labelers, if labelers independently
labeled the same items, if inter-rater reliability metrics were pre-
sented, if compensation details for crowdworkers were reported, if
a public link to the dataset was available, and more.
As our research project was a human-labeling project studying
other human-labeling projects, we took care in our own practices.
We only have access to the paper reporting about the study and not
the actual study itself, and many papers either do not discuss such
details at all or without sufficient detail to make a determinations.
For example, many papers did note that the study involved the cre-
ation of an original human-labeled dataset, but did not specify who
labeled it. Other papers specified such details about who labeled the
cases, but did not specify whether labelers independently labeled
the same items. For some of our items, one of the most common
labels we gave was “no information” — which is a concerning issue,
given how crucial such information is in understanding the validity
of the training dataset and by extension, the validity of the classifier.
2 LITERATURE REVIEW AND MOTIVATION
2.1 A different kind of “black-boxing” in
machine learning
In the introduction, we noted training data is frequently black-
boxed in machine learning research and applications. We use the
term “black-boxed” in a different way than it is typically invoked
in and beyond the FAT* community, where often refers to inter-
pretability. In that sense, “black-boxing” means that even for experts
who have access to the training data and code which created the
classifier, it is difficult to understand why the classifier made each
decision. In social science and humanities work on “black-boxing”
of ML (and other “algorithmic” systems), there is often much elision
between issues of interpretability and intentional concealment, as
Burrell [9] notes. A major focus is on public accountability [e.g.
45], where many problematic issues can occur behind closed doors.
This is even the case with relatively simple forms of analytics and
automation — such as if-then statements, linear regressions, or
rule-based expert systems [13, 62].
In contrast, the issue we refer to around training data being
black-boxed is more about what is and is not taken for granted in
the practice of developing a classifier. This use of the term is closer
to how it was used by Latour andWoolgar in an ethnographic study
of scientific research laboratories [34]. Their use of the term was
about scientific lab equipment, where a mass spectrometer would
typically be implicitly trusted to turn samples into signals. However,
when the results showed something drastically unexpected, it could
either be a problem with the machine or a fundamental scientific
breakthrough. Scientists and technicians would have to “open up
the black box,” changing their relationship to the equipment and
run various tests and diagnostics to determine if the problem was
with the equipment or the prevailing theory.
In this view, black-boxing is a relational concept, rather than an
objective property. It is a particular orientation that people may or
may not have to the same social-technical systems they routinely
work with and rely upon. Correspondingly, “opening up the black
box” is not so much about digging into technical or internal details
per se, but more of a gestalt shift in whether the output of a system
is implicitly taken for granted or open for further investigation.
In this view, black-boxing is not inherently problematic; in fact,
it is a necessary aspect of modern life and especially of globally-
distributed ‘big science.’ However, the question is about who is
obligated to suspend disbelief and who gets to be more skeptical
about the inputs and outputs — questions that are also being raised
in conversations about scientific reproducibility and replicability.
Scholarship in this same tradition around Science and Technology
Studies, ethnomethodology, anthropology, and related qualitative
fields has also extensively discussed and analyzed processes of
knowledge production that involve human judgment, particularly
discussing standardization and formal operationalization [7, 22, 33].
2.2 Content analysis
Creating human-labeled training datasets for machine learning
often looks like content analysis, a well-established methodology
in the humanities and the social sciences (particularly literature,
communication studies, and linguistics), which also has versions
used in the life, ecological, and medical sciences. Content analysis
has taken many forms over the past century, from more positivist
methods that formally establish structural ways of evaluating con-
tent to more interpretivist methods that embrace ambiguity and
multiple interpretations, such as grounded theory [19]. The inter-
section of ML and interpretivist approaches is outside of the scope
of this article, but it is an emerging area of interest [e.g. 43].
Today, structured content analysis (also called “closed coding”)
is used to turn qualitative or unstructured data of all kinds into
structured and/or quantitative data, including media texts, free-
form survey responses, interview transcripts, and video recordings.
Projects usually involve teams of “coders” (also called “annotators”,
“labelers”, or “reviewers”), with human labor required to “code”, “an-
notate”, or “label” a corpus of items. (Note that we use such terms
interchangeably in this paper.) In one textbook, content analysis is
described as a “systematic and replicable” [52, p. 19] method with
several best practices: A “coding scheme” is defined, which is a
set of labels, annotations, or codes that items in the corpus may
have. Schemes include formal definitions or procedures, and often
include examples, particularly for borderline cases. Next, coders are
trained with the coding scheme, which typically involves interac-
tive feedback. Training sometimes results in changes to the coding
scheme, in which the first round becomes a pilot test. Then, anno-
tators independently review at least a portion of the same items
throughout the entire process, with a calculation of “inter-annotator
agreement” or “inter-rater reliability.” Finally, there is a process of
“reconciliation” for disagreements, which is sometimes by majority
vote without discussion and other times discussion-based.
Structured content analysis is a difficult, complicated, and labor-
intensive process, requiring many different forms of expertise on
the part of both the coders and those who manage them. Histori-
cally, teams of students have often performed such work. With the
rise of crowdwork platforms like Amazon Mechanical Turk, crowd-
workers are often used for content analysis tasks, which are often
similar to other kinds of common crowdworking tasks. Google’s
reCAPTCHA [66] is a Turing test in which users perform anno-
tation tasks to prove their humanness — which initially involved
transcribing scanned phrases from books, but now involves image
labeling for autonomous vehicles. There are major qualitative data
analysis software tools that scaffold the content analysis process to
varying degrees, such as MAXQDA or NVivo, which have support
for inter-annotator agreement metrics. There have also been many
new software platforms developed to support more micro-level
annotation or labeling at scale, including in citizen science, lin-
guistics, content moderation, and more general-purpose use cases
[5, 10, 23, 35, 42, 49]. For example, the Zooniverse [59] provides
a common platform for citizen science projects across different
domain application areas, which let volunteers make judgements
about items, which are aggregated and reconciled in various ways.
2.3 Meta-research and methods papers in
linguistics and crowdsourcing
Our paper is also in conversation with various meta-research and
standardization efforts in linguistics, crowdsourcing, and other re-
lated disciplines. Linguistics and Natural Language Processing have
long struggled with issues around standardization and reliability
of linguistic tagging. Linguistics researchers have long developed
best practices for corpus annotation [e.g. 26], including recent work
about using crowdworkers [53]. Annotated corpus projects often
release guidelines and reflections about their process. For exam-
ple, the Linguistic Data Consortium’s guidelines for annotation of
English-language entities (version 6.6) is 72 single-spaced pages
[12]. A universal problem of standardization is that there are often
too many standards and not enough enforcement. As [3] notes,
33-81% of linguistics/NLP papers in various venues do not even
mention the name of the language being studied (usually English). A
meta-research study found only 1 in 9 qualitative papers in Human-
Computer Interaction reported inter-rater reliability metrics [36].
Another related area are meta-research and methods papers
focused on identifying or preventing low-effort responses from
crowdworkers — sometimes called “spam” or “random” responses,
or alternatively ”fraudsters” or ”cheaters.” Rates of “self-agreement”
are often used, determining if the same person labels the same
item differently at a later stage. One paper [41] examined 17 crowd-
sourced datasets for sentiment analysis and found none had self-
agreement rates (Krippendorf’s alpha) above 0.8, with some lower
than 0.5. Another paper recommends the self-agreement strategy
in conjunction with asking crowdworkers to give a short expla-
nation of their response, even if the response is never actually
examined. [61]. One highly-cited paper [51] proposes a strategy
in which crowdworkers are given some items with known labels
(a gold/ground truth), and those who answer incorrectly are suc-
cessively given more items with known labels, with a Bayesian
approach to identifying those who are answering randomly.
2.4 The data documentation movements
Our paper is also in conversation with two related movements in
computationally-supported knowledge production that have sur-
faced issues around documentation. First, we see connections with
the broader open science and reproducibility movements. Open
science is focused on a range of strategies, including open ac-
cess research publications, educational materials, software tools,
datasets, and analysis code [14]. The reproducibility movement is
deeply linked to the open science movement, focusing on getting
researchers to release everything that is necessary for others to
perform the same tasks needed to get the exact same results [30, 68].
This increasingly includes pushing for high standards for releasing
protocols, datasets, and analysis code. As more funders and journals
are requiring releasing data, the issue of good documentation for
data and protocols is rising [18, 21]. There are also intersecting
literatures on systems for capturing information in ML data flows
and supply chains [17, 55, 60], as well as supporting data clean-
ing [32, 56]. These issues have long been discussed in the fields
of library and information science, particularly in Research Data
Management [6, 38, 54, 57].
A major related movement is in and around the FATML field,
with many recent papers proposing training data documentation in
the context of ML. Various approaches, analogies, and metaphors
have been taken in this area, including “datasheets for datasets” [16],
”model cards” [40], “data statements” [3], “nutrition labels” [25], a
“bill of materials” [2], “data labels” [4], and “supplier declarations
of conformity” [24]. Many go far beyond the concerns we have
raised around human-labeled training data, as some are also (or
primarily) concerned with documenting other forms of training
data, model performance and accuracy, bias, considerations of ethics
and potential impacts, and more. We discuss how our findings relate
to this broader emerging area more in the concluding discussion.
3 DATA AND METHODS
3.1 Data: machine learning papers performing
classification tasks on Twitter data
Our goal was to find a corpus of papers that were using original
human annotation or labeling to produce a new training dataset for
supervised ML. We restricted our corpus to papers whose classifiers
were trained on data from Twitter, for various reasons: First, we did
attempt to produce a broader corpus of supervised ML application
papers, but found our search queries in academic search engines
would either 1) be so broad that most papers were non-applied /
theoretical papers or papers re-using public pre-labeled datasets;
or 2) that the results were so narrow they excluded many canonical
papers in this area, which made us suspect that they were non-
representative samples. Sampling to papers using Twitter data has
strategic benefits for this kind of initial study. Data from Twitter
is of interest to scholars from a variety of disciplines and topical
interest areas, in addition to those who have an inherent interest
in Twitter as a social media site. As we detail in appendix section
7.1.1, the papers represented political science, public health, NLP,
sentiment analysis, cybersecurity, content moderation, hate speech,
information quality, demographic profiling, and more.
We drew the main corpus of ML application papers from ArXiV,
the oldest and most established “preprint” repositories, originally
for researchers to share papers prior to peer review. Today, ArXiV
is widely used to share both drafts of papers that have not (yet)
passed peer review (“preprints”) and final versions of papers that
have passed peer review (often called “postprints”). Users submit
to any number of disciplinary categories and subcategories. Sub-
category moderators perform a cursory review to catch spam, bla-
tant hoaxes, and miscategorized papers, but do not review papers
for soundness or validity. We sampled all papers published in the
Computer Science subcategories of Artificial Intelligence (cs.AI),
Machine Learning (cs.LG), Social and Information Networks (cs.SI),
Computational Linguistics (cs.CL), Computers and Society (cs.CY),
Information Retrieval (cs.IR), and Computer Vision (CS.CV), the
Statistics subcategory of Machine Learning (stat.ML), and Social
Physics (physics.soc-ph). We filtered for papers in which the title
or abstract included at least one of the words “machine learning”,
“classif*”, or “supervi*” (case insensitive). We then filtered to papers
in which the title or abstract included at least “twitter” or “tweet”
(case insensitive), which resulted in 494 papers. We used the same
query on Elsevier’s Scopus database of peer-reviewed articles, se-
lecting 30 randomly sampled articles, which mostly selected from
conference proceedings. One paper from the Scopus sample was
corrupted, so only 29 papers were examined.
ArXiV is likely not a representative sample of all ML publications.
However, we chose it because ArXiV papers are widely accessi-
ble to the public, indexed in Google Scholar and other scholarly
databases, and are generally considered citeable publications. The
fact that many ArXiV papers are not peer-reviewed and that papers
posted are not likely representative samples of ML research is worth
considering when reflecting on the generalizability of our findings.
However, given that such papers are routinely discussed in both
academic literature and the popular press means that issues with
their reporting of training data is just as crucial. Sampling from
ArXiv also lets us examine papers at various stages in the peer-
review cycle, breaking out preprints not (yet) published, preprints
of later published papers, and postprints of published works. The
appendix details both corpora, including an analysis of the topics
and fields of papers (in 7.1.2), an analysis of the publishers and
publication types (e.g. an early preprint of a journal article, a final
postprint of a conference proceeding, a preprint never published)
and publishers (in 7.1.3 and 7.1.2). The final dataset can be found
on GitHub and Zenodo.1
3.2 Labeling team, training, and workflow
Our labeling team included one research scientist who led the
project (RSG) and undergraduate research assistants, who worked
for course credit as part of an university-sponsored research expe-
rience program (KY, YY, MD, JQ, RT, and JH). The project began
with five students for one semester, four of whom continued on
the project for the second semester. A sixth student replaced the
student who did not continue. All students had some coursework
1https://doi.org/10.5281/zenodo.3564844 and https://github.com/staeiou/gigo-fat2020
in computer science and/or data science, with a range of prior expe-
rience in machine learning in both a classroom and applied setting.
Students’ majors and minors included Electrical Engineering &
Computer Science, Data Science, Statistics, and Linguistics.
The labeling workflow was that each week, a set of papers were
randomly sampled each week from the unlabled set of 494 ArXiV
papers in the corpus. For two weeks, the 30 sampled papers from
Scopus were selected. The five students independently reviewed
and labeled the same papers each week, using a different web-based
spreadsheet to record labels. The team leader synthesized labels
and identified disagreement. The team met in person each week to
discuss cases of disagreement, working to build a consensus about
the proper label (as opposed to purely majority vote). The team
leader facilitated these discussions and had the final say when a
consensus could not be reached. The papers labeled for the first
two weeks were in a training period, in which the team worked on
a different set of papers not included in the dataset. In these initial
weeks, the team learned the coding schema and the reconciliation
process, which were further refined.
3.3 Second round verification and
reconciliation
After 164 papers were labeled by five annotators, we conducted
a second round of verification. This was necessary both because
there were some disagreements in labeling and changes made to the
coding schema (discussed in appendix 7.2.2). All labels for all 164
papers were independently re-examined by at least two of the six
team members. Annotators were given a summary of the original
labels in the first round and were instructed to review all papers,
being mindful of how the schema and instructions had changed.
We then aggregated, reconciled, and verified labels in the same way
as in the first round. For papers where there was no substantive
disagreement on any question between those who re-examined it
in the second round, the paper’s labels were considered to be final.
For papers where there was any substantive disagreement on any
question, the paper was either discussed to consensus in the same
manner as in the first round or decided by the team leader. The
final schema and instructions are in the appendix, section 7.4.
Finally, we cleaned up issues with labels around implicit or blank
values using rule-based scripts. We learned our process involved
some ambiguities around whether a subsequent value needed to
be filled in. For example, if a paper was not using crowdworkers,
then the instructions for our schema were that the question about
crowdworker compensation was to remain blank. However, we
found we had cases where “reported crowdworker compensation”
was “no” for papers that did not use crowdworkers. This would be
concerning had we had a “yes” for such a variable, but found no
such cases. We recoded questions about pre-screening for crowd-
work platforms (implied by using crowdworkers in original human
annotation source) and the number of human annotators.
We measured interrater reliability metrics using mean percent
total agreement, or the proportion of cases where all labelers ini-
tially gave the same label. This is a more stringent metric than
Fleiss’s kappa and Krippendorf’s alpha, and our data does not fit
the assumptions for those widely-used metrics. IRR rates for round
one were relatively low: across all questions, the mean percent total
agreement was 66.67%, with the lowest question having a rate of
38.2%. IRR rates for round two were quite higher: the mean percent
total agreement across all questions was 84.80% and the lowest
agreement score was 63.4% (for “used external human annotation”,
which we discuss later). We are confident about our labeling process,
especially because these individual ratings were followed by an
expert-adjudicated discussion-based reconciliation process, rather
than simply counting majority votes. We detail more information
and reflection about interrater reliability in appendix section 7.2.1.
3.4 Raw and normalized information scores
We quantified the information about training data in papers, devel-
oping a raw and normalized information score, as different studies
demanded different levels of information. For example, our question
about whether inter-annotator agreement metrics were reported
is only applicable for papers involving multiple annotators. Our
questions about whether prescreening was used for crowdwork
platforms or whether crowdworker compensation was reported
is only relevant for projects using crowdworkers. However, some
kinds of information are relevant to all papers that involve original
human annotation: who the annotators are (annotation source),
annotator training, formal instructions or definitions were given,
the number of annotators involved, whether multiple annotators
examined the same items, or a link to a publicly-available dataset.
For raw scores, papers involving original human annotation
received one point each for reporting the six itemsmentioned above.
In addition, they received one point per question if they included
information for each of the two questions about crowdworkers if
the project used crowdworkers, and one point if they reported inter-
annotator metrics if the project used multiple annotators per item.
For the normalized score, the raw score was divided by the highest
possible raw score.2 We only calculated scores for papers involving
original human annotation. Finally, we conducted an analysis of
information scores by various bibliometric factors, which required
determining such factors for all papers. For all ArXiV papers, we
determined whether the PDF was a pre-print not (yet) published
in another venue, a post-print identical in content to a published
version, or a pre-print version of a paper published elsewhere with
different content. For all Scopus papers and ArXiV post-prints, we
also determined the publisher. We detail these in appendix 7.1.2.
4 FINDINGS
4.1 Original classification task
The first question was whether the paper was conducting an orig-
inal classification task using supervised machine learning. Our
keyword-based process of generating the corpus included many
papers not in this scope. However, defining the boundaries of super-
vised ML and classification tasks is difficult, particularly for papers
that are long, complex, and ambiguously worded. We found that
some papers claimed to be using ML, but when we examined the
details, these did not fall under our definition. We defined machine
learning broadly, using a common working definition in which
machine learning includes any automated process that does not
exclusively rely on explicit rules, in which the performance of a
2By 6 if neither crowdworkers nor multiple annotators were used, by 7 if multiple
annotators were used, by 8 if crowdworkers were used, and by 9 if both were used.
task increases with additional data. This includes simple linear re-
gressions, for example, and there is much debate about if and when
simple linear regressions are a form of ML. However, as we were
also looking for classification tasks, linear regressions were only
included if it is used to make a prediction in a set of defined classes.
We defined an “original” classifier to mean a classifier the authors
made based on new or old data, which excludes the exclusive use
of pre-trained classifiers or models.
Table 1: Original classification task
Count Proportion
Yes 142 86.59%
No 17 10.37%
Unsure 5 3.05%
As table 1 shows, the overwhelming majority of papers in our
dataset were involved in an original classification task. We placed
5 papers in the “unsure” category — meaning they did not give
enough detail for us to make this determination, or that they were
complex boundary cases. One of the “unsure” cases clearly used
labels from human annotation, and so we answered the subsequent
questions, which is why the counts in Table 2 add up to 143 (as well
as some other seeming disparities in later questions).
4.2 Labels from human annotation
One of the major issues we had to come to a consensus around was
whether a paper used labels from human annotation. We observed
a wide range of cases in which human judgment was brought to
bear on the curation of training data. Our final definition required
that “the classifier [was] at least in part trained on labeled data
that humans made for the purpose of the classification problem.”
We decided on a working definition that excluded many “clever
uses of metadata” from this category, but did allow some cases
of “self-annotation” from social media, which were typically the
most borderline cases on the other side. For example, one case
from our examples we decided was human annotation used specific
politically-inflected hashtags to automatically label tweets as for or
against a position, for use in stance detection (e.g. #ProChoice ver-
sus #ProLife). However, these cases of self-annotation would all be
considered external human annotation rather than original human
annotation, and so the subsequent questions about the annotation
process would be not applicable. Another set of borderline cases
involved papers where no human annotation was involved in the
curation of the training dataset that was used to build the classifier,
but human annotation was used for validation purposes. We did
not consider these to involve human annotation as we originally
defined it in our schema, even though the same issues arise with
equal significance for the validity of such research.
Table 2: Labels from human annotation
Count Proportion
Yes 93 65.04%
No 46 32.17%
Unsure 4 2.79%
4.3 Used original human annotation and
external human annotation
Our next two questions were about whether papers that used hu-
man annotation used original human annotation, which we defined
as a process in which the paper’s authors obtained new labels from
humans for items. It is common in ML research to re-use public
datasets, and many of papers in our corpus did so. We also found
10 papers in which external and original human annotation was
combined to create a new training dataset. For these reasons, we
modified our schema to ask separate questions for original and ex-
ternal human annotation data, to capture all three cases (using only
original, only external, or both). Tables 3 and 4 show the breakdown
for both questions. We only answered the subsequent questions
about the human annotation process for the papers producing an
original human annotated dataset.
Table 3: Used original human annotation
Count Proportion
Yes 72 75.00%
No 21 21.88%
Unsure 3 3.13%
Table 4: Used external human annotation data
Count Proportion
No 61 63.54%
Yes 32 33.33%
Unsure 3 3.13%
4.4 Original human annotation source
Our next question asked who the annotators were, for the 74 papers
that used original human annotation. The possible options were:
the paper’s authors, Amazon Mechanical Turk, other crowdwork-
ing platforms, experts/professionals, other, and no information. We
took phrases like “we labeled” (with no other details) to be an im-
plicit declaration that the paper’s authors did the labeling. If the
paper discussed labelers’ qualifications for the task beyond an aver-
age person, we labeled it as “experts / professionals.” For example,
some of our boundary cases involved recruiting students to label
sentiment. One study involved labeling tweets with both English
and Hindi text and noted that the students were fluent in both lan-
guages – which we considered to be in the “experts / professionals”
category. Another paper we included in this category recruited stu-
dents to label tweets with emojis, noting that the recruited students
“are knowledgeable with the context of use of emojis.”
As table 5 shows, we found a diversity of approaches to the
recruitment of human annotators. The plurality of papers involved
the paper’s authors doing the annotationwork themselves. The next
highest category was “no information,” which was found in almost
a quarter of the papers using original human annotation. Experts /
professionals was far higher than we expected, although we took
any claim of expertise for granted. Crowdworkers constituted a far
smaller proportion than we expected, with Amazon Mechanical
Turk and other platforms collectively comprising about 15% of
papers. Almost all of the other crowdworking platforms specified
were CrowdFlower/FigureEight, with one paper using oDesk.
Table 5: Original human annotation source
Count Proportion
Paper’s authors 22 29.73%
No information 18 24.32%
Experts / professionals 16 21.62%
Amazon Mechanical Turk 3 4.05%
Other crowdwork 8 10.81%
Other 7 9.46%
4.5 Number of human annotators
Our instructions for the question about the number of human anno-
tators was not precise and had one of the lower levels of inter-rater
reliability. If the paper included information about the number of
human annotators, the instructions were to put such a number,
leaving the field blank for no information. Most of the disagree-
ment was from differences around how papers report the number of
annotators used. For example, some papers specified the total num-
ber of humans who worked on the project annotating items, while
others only specified how many annotators were used per item
(particularly for those using crowdworkers), and a few reported
both. Some involved a closed set of annotators who all examined the
same set of items, similar to how our team operated. Other papers
involved an open set of annotators, particularly drawn from crowd-
working platforms, but had a consistent number of annotators who
reviewed each item. Due to these inconsistencies, we computation-
ally re-coded responses into the presence of information about the
number of human annotators. These are both important aspects
to discuss, although it is arguably more important to discuss the
number of annotators who reviewed each item. In general, having
more annotators review each item provides a more robust way of
determining the validity of the entire process, although this also
requires caluclating inter-annotator agreement metrics.
Table 6: Number of annotators specified
Count Proportion
Yes 41 55.40%
No 33 44.60%
As table 6 shows, a slim majority of papers using original human
annotation specified the number of annotators involved in some
way. Based on our experiences, we typically noticed that papers
discussing the number of annotators often fell into two categories:
1) a small closed team (more often 2-3, sometimes 4-6) that were
either the papers’ authors or recruited directly by the authors, who
tended to perform the same amount of work for the duration of
the project; or 2) a medium to large (25-500) open set of annotators,
typically but not necessarily recruited through a crowdworking
platform, who each performed highly variable amounts of work.
4.6 Formal definitions and instructions
Our next question was about whether instructions or guidelines
with formal definitions or examples are reportedly given to annota-
tors. Formal definitions and concrete examples are both important,
as they help annotators understand how the researchers have oper-
ationalized the concept in question and determine edge cases. With
no or ambiguous definitions/examples, there could be fundamental
misunderstandings that are not captured by inter-annotator agree-
ment metrics, if all annotators make the same misunderstandings.
We defined two levels: giving no instructions beyond the text of
a question, then giving definitions for each label and/or concrete
examples. The paper must describe or refer to instructions given
(or include them in supplemental materials), otherwise, we catego-
rized it "No Information". Some borderline cases involved authors
labeling the dataset themselves, where the paper presented a formal
definition, but only implied that it informed the labeling – which
we took to be a formal definition. As table 7 shows, the plurality of
papers did not provide enough information to make a determina-
tion (it is rare for authors to say they did not do something), but
43.2% provided definitions or examples.
Table 7: Formal instructions
Count Proportion
No information 35 47.30%
Instructions w/ formal definitions/examples 32 43.24%
No instructions beyond question text 7 9.46%
4.7 Training for human annotators
We defined training for human annotators to involve some kind of
interactive process in which the annotators have the opportunity to
receive some kind of feedback and/or dialogue about the annotation
process. We identified this as a distinct category from both the qual-
ifications of the annotators and the instructions given to annotators,
which are examined in other questions. Training typically involved
some kind of live session or ongoing meeting in which annotators’
progress was evaluated and/or discussed, where annotators had
the chance to ask questions or receive feedback on why certain
determinations did or did not match definitions or a schema. We
used our own team’s process as an example of this, and found sev-
eral papers that used a similar roundtable process, which went into
detail about interactions between team members. Cases in which
the paper only specified that annotators were given a video or a
detailed schema to review were not considered training details, as
this was a one-way process and counted as definitions/instructions.
Table 8: Training for human annotators
Count Proportion
No information 63 85.14%
Some training details 11 14.86%
The overwhelming majority of papers did not discuss such is-
sues, as table 8 shows, with 15% of papers involving a training
session. Because we had a quite strict definition for what consti-
tutes training (versus what many may think of around “trained
annotators”), this is expected. We also are not all that concerned
with this low number, as there are many tasks that likely do not
require specialized training — unlike our project, which required
both specific expertise in an area and with our complicated schema.
4.8 Pre-screening for crowdwork platforms
Crowdwork platforms let employers pre-screen or test for traits,
skills, or performance metrics, which significantly narrows the
pool of crowdworkers. For example, “project-specific pre-screening”
involves offering a sample task with known outcomes: if the crowd-
worker passed, they would be invited to annotate more items. 5 of
the 11 papers using crowdworkers reported using this approach.
Platforms also often have location-based screening (e.g. US-only),
which 2 papers reported using. Some crowdwork platforms have a
qualification for workers who have a positive track record based
on total employer ratings (e.g. AMT Master). Platforms also offer
generic skills-based tests for certain kinds of work (e.g. Crowd-
Flower’s Skill Tests). These last two qualifications were in our
coding schema, but no papers reported using them.
Table 9: Prescreening for crowdwork platforms
Count Proportion
Project-specific prescreening 5 45.0%
Location qualification 2 18.0%
No information 4 36.0%
4.9 Multiple annotator overlap and reporting
inter-annotator agreement
Our next two questions were about using multiple annotators to
review the same items (multiple annotator overlap) and whether
inter-annotator agreement metrics were reported. Having multiple
independent annotators is typically a foundational best practice in
structured content analysis, so that the integrity of the annotations
and the schema can be evaluated (although see [36]). For multiple
annotator overlap, our definitions required papers state whether all
or some of the items were labeled by multiple labelers, otherwise
“no information” was recorded. Then, for papers that did multi-
ple annotator overlap, we examined whether any inter-annotator
agreement metric was reported. We did find one paper that did
not explicitly state that multiple labelers overlapped, but did report
inter-annotator agreement metrics. This implicitly means that at
least some of the items were labeled by multiple labelers, but for
consistency, we keep the “no information” label for this case. We
did not record what kind of inter-annotator metric was used, such
as Cohen’s kappa or Krippendorff’s alpha, but many different met-
rics were used. We also did not record what the exact statistic was,
although we did notice a wide variation in what was considered an
acceptable or unacceptable score for inter-annotator agreement.
Table 10: Multiple annotator overlap
Count Proportion
No information 34 45.95%
Yes for all items 31 41.89%
Yes for some items 6 8.11%
No 3 4.05%
Table 11: Reported inter-annotator agreement
Count Proportion
Yes 26 70.27%
No 11 29.73%
For multiple annotator overlap, table 10 shows that just under
half of all papers that involved an original human annotation task
did not provide explicit information one way or the other about
whether multiple annotators reviewed each item. This includes the
one paper that reported inter-annotator agreement metrics, but did
not specify whether overlap was for all items or some items. Only
three papers explicitly stated that there was no overlap among
annotators, and so it is quite likely that the papers that did not
specify such information did not engage in such a practice. For the
37 papers that did involve some kind of multiple annotator over-
lap, the overwhelming majority of this subsample (84%) involved
multiple annotation of all items, rather than only some items. We
also found that for papers that did involve some kind of multiple
overlap, the large majority of them ( 70%) did report some metric
of inter-annotator agreement, as table 11 indicates.
4.10 Reported crowdworker compensation
Crowdworking is often used because of the low cost, which can
be far below minimum wage in certain countries. Researchers and
crowdworkers have been organizing around issues related to the
exploitation of crowdworkers in research, advocating ethical prac-
tices including fair pay [58]. We examined all papers involving
crowdworkers for any indication of compensation, and found zero
mentioned compensation. We did find that some papers using other
sources of human annotation (e.g. students) discussed compensa-
tion for annotators, but this was not in our original schema.
4.11 Link to dataset available
Our final question was about whether the paper contained a link
to the dataset containing the original human annotated training
dataset. Note that this question was only answered for papers in-
volving some kind of original or novel human annotation, and
papers that were exclusively re-using an existing open or public
dataset were left blank to avoid double-counting. We did not follow
such links or verify that such data was actually available. As table
12 shows, the overwhelming majority of papers did not include
such a link, with 8 papers (10.81%) using original human-annotated
training datasets linking to such data. Given the time, labor, exper-
tise, and funding in creating original human annotated datasets,
authors may be hesitant to release such data until they feel they
have published as many papers as they can.
Table 12: Link to dataset available
Count Proportion
No 66 89.19%
Yes 8 10.81%
5 PAPER INFORMATION SCORES
The raw and normalized information scores (see section 3.4 for
methodology) were calculated for all papers that involved original
human annotation. As previously discussed, our corpora represent a
likely non-representative sample of ML research, even if bounded to
social computing. Our relatively small sample sizes combined with
the number of multiple comparisons would mean that thresholds
for statistical significance would need to be quite high. Instead,
we present these results to help provide an initial framework and
limited results on this issue, intended to help inform a broader
and more systematic evaluation the ML literature. We do observe
quite varying ranges and distributions of information scores, which
does give evidence to the claim that there is substantial and wide
variation in the practices around human annotation, training data
curation, and research documentation.
5.1 Overall distributions of information scores
Figure 1 shows histograms for raw and normalized information
scores, which both suggest a bimodal distribution, with fewer pa-
pers at the both extremes and the median. This suggests that there
are roughly two populations of researchers, with one centered
around raw scores of 1-2 and normalized scores of 0.25 and one
centered around raw scores of 5 and normalized scores of 0.7. The
normalized information score ranged from 0 to 1, with 6 papers hav-
ing a normalized score of 0 and only 1 paper with a score of 1. The
raw information score ranged from 0 to 7, with no paper receiving
a full score of 8 or 9, which would have required a study involving
crowdworkers, multiple overlap, and open datasets. Overall, the
mean normalized information score was 0.441, with a median of
0.429 and a standard deviation of 0.261. The mean raw score was
3.15, with a median of 3.0 and a standard deviation of 2.05.
Figure 1: Histograms of raw and normalized information
scores for all papers involving original human annotation.
5.2 Information scores by corpus and
publication type
Figure 2 shows two boxplots3 of normalized information scores
that are based on different intersecting categories of publication
type and status. The left figure compares scores in four categories:
all papers in the Scopus sample (non-ArXived), ArXiv preprints
that were never (or are not yet) published, and ArXiv preprints
that were either postprints or preprints of a traditional publication.
The category with the lowest median score are papers from the
Scopus sample, which is followed closely by ArXiv preprints never
published, although preprints never published had a much larger
IQR and standard deviation. Postprints of publications had a similar
IQR and standard deviation as preprints never published, but a
much higher median score. Preprints of publications had a similar
median score as postprints, but with a much smaller IQR and stan-
dard deviation. The righthand figure plots publication types for the
combined corpora. Conference proceedings and ArXiv preprints
never published have somewhat similar medians and IQRs, with
journal articles having a higher median of 0.5 and a much narrower
IQR. While we hesitate to draw generalizable conclusions, we see
these findings indicating a wide range of factors potentially at play.
3The main box is the inter-quartile range (IQR), or the 25th & 75th percentiles. The
middle red line is the median, the green triangle is the mean, and the outer whiskers
are 5th & 95th percentiles.
Figure 2: Boxplots of normalized information scores by type
of paper. Top: scores by corpus and preprint/postprint status.
Bottom: scores from both corpora by publication type.
5.3 Information scores by publisher
Figure 3 shows boxplots for normalized information scores by pub-
lisher, split between papers sampled from ArXiv and Scopus. The
boxplots are ordered by the median score per publisher. In papers
in the ArXiv corpus, those that were pre- or post-prints of papers
published by the professional societies Association for Comput-
ing Machinery (ACM) or Association of Computational Linguistics
(ACL) tied for the highest median scores of 0.667, with similar IQRs.
These were followed by Springer and Elsevier, with respective
medians 0.625 and 0.603 and narrower IQRs. ArXiv preprints not
published elsewhere had a median score of 0.381 and the highest
IQR and standard deviation (0.289), suggesting that it represents a
wide range of papers. The publishers at the lower end of the scale
included AAAI, with a median of 0.444 and a narrower IQR, and
IEEE, with a median of 0.226 and the second-highest IQR and stan-
dard deviation (0.327). Curiously, papers from the Scopus corpus
show different results per-publisher, with the median scores of all
publishers lower in the Scopus corpus than in the ArXiv corpus.
However, given the small number of papers in the Scopus sample,
we hesitate to draw general conclusions from this finding, and leave
this for future research.
Figure 3: Boxplots of normalized information scores by pub-
lisher and corpus, ordered by median score.
6 CONCLUDING DISCUSSION
6.1 Findings
In the sample of ML application publications using Twitter data we
examined, we found a wide range in levels of documentation about
methodological practices in human annotation. While we hesitate
to overly generalize our findings to ML at large, these findings do
indicate concern, given how crucial the quality of training data is
and the difficulty of standardizing human judgment. Yet they also
give us hope, as we found a number of papers we considered to
be excellent cases of reporting the processes behind their datasets.
About half of the papers using original human annotation engaged
in some form of multiple overlap, and about 70% of the papers that
did multiple overlap reported metrics of inter-annotator agreement.
The distribution of annotation information scores was roughly
bimodal, suggesting two distinct populations of those who provide
substantially more and less information about training data in their
papers. We do see preliminary evidence that papers in our sample
published by certain publishers/venues tended to have papers with
far more information than others (e.g. ACM and ACL at the top end,
followed closely by journal publishers Springer and Elsevier, with
IEEE and AAAI proceedings at the lower end). Preprints exclusively
published on ArXiv also had the widest range of scores.
6.2 Implications
Based on our findings and experiences in this project, we believe
human annotation should be considered a core aspect of the re-
search process, with as much attention, care, and concern placed
on the annotation process as is currently placed on performance-
based metrics like F1 scores. Our findings — while preliminary,
descriptive, and limited in scope — tell us that there is much room
for improvement. This paper also makes steps towards more large-
scale and systematic analyses of the research landscape, as well as
towards standards and best practices for researchers and reviewers.
Institutions like journals, funders, and disciplinary societies have
a major role to play in solutions to these issues. Most publica-
tions have strict length maximums, and many papers we scored
highly spent at least a page or more describing their process. Re-
viewer expectations are crucial in any discussion of the reporting
of methodological details in research publications. It could be that
some authors did include such details, but were asked to take it out
and add other material instead. Authors have incentives to be less
open about the messiness inherent in research, as this may open
them up to additional criticism.We see many parallels here to issues
around reproducibility and open science, which are increasingly
being tackled by universal requirements from journals and funders,
rather than relying on individuals to change norms. Such research
guidelines are common, including the COREQ standard for qualita-
tive data analysis reporting [63], a requirement by some journals. A
number of proposed standards have been created around datasets
for ML [2–4, 16, 24, 25, 40], which are often framed as potential
ways to mitigate bias and improve transparency and accountability.
Several of these are broader proposals around reporting informa-
tion about ML classifiers and models, which include various aspects
beyond our study. In fact, given the recent explosion of propos-
als for structured disclosure or transparency documents around
ML, the Partnership on AI has recently created the “ABOUT ML”
working group to arrive at a common format or standard.4
It is important to frame this issue as one of research validity and
integrity: what kind of information about training data is needed
for researchers, reviewers, and readers to have confidence in the
model or classifier? As we observed in our discussions, we became
skeptical about papers that did not adequately describe their human
annotation processes. However, human annotation is a broad and
diverse category of analytical activity, encompassing a wide range
of structured human judgment brought to bear on items, some far
more straightforward or complex. We saw the wide range papers
that were engaged in various forms of annotation or labeling, even
though we bounded our study to papers using data from Twitter.
One important distinguishing factor is the difficulty of the task and
the level of specific knowledge needed to complete it, which can
vary significantly. Another key distinction may be between when
there is expected to be only one ‘right’ answer and when there
might be many valid answers.
Most importantly, wewould not want a straightforward checklist
to overdetermine issues of research integrity. A number of papers
we read were missing details we thought were crucial for under-
standing that study, but would not make sense for a majority of
papers we examined. If a checklist was created, it should not be
seen as an end in itself. The classic principle of scientific replica-
bility could be a useful heuristic: does the paper provide enough
information about the labeling process such that any reader could
(with sufficient resources and access to the same kind of human
annotators) conduct a substantively identical human annotation
process on their own? We also see a role for technical solutions
to help scaffold adherence to these best practices. For example,
major qualitative data analysis platforms like MAXQDA or NVivo
have built-in support for inter-annotator agreement metrics. Sev-
eral crowdsourcing and citizen science platforms for data labeling
are built to support reconciliation for disagreements. Automated
workflow, pipeline, and provenance tracking is an increasing topic
in ML, although these can focus more on model building and tuning,
taking data as given. We recommend such projects include human
annotation as a first-class element, with customization as needed.
Finally, our own experience in this human annotation project
studying human annotation projects has shown us the costs and
benefits of taking an intensive, detailed, collaborative, and multi-
stage approach to human annotation. On one side, we believe that
after going through such a long process, we have not only better
data, but also a much better contextual understanding of our object
of study. Yet on the other hand, even though struggling over the
labels and labeling process is an opportunity, our time- and labor-
intensive process did have a direct tradeoff with the number of
items we were able to annotate. These issues and tradeoffs are
important for ML researchers to discuss when designing their own
projects and evaluating others.
6.3 Limitations and future work
Our study has limitations, as we only examined a sample of publi-
cations in the ML application space. First, we only examined papers
that performing a classification task on tweets, which is likely not
4https://www.partnershiponai.org/tag/about-ml/
a representative sample of ML application publications. We would
expect to find different results in different domain application areas.
Papers in medicine and health may have substantially different
practices around reporting training data, due to strict reporting
standards in clinical trials and related areas. We also generally ex-
amined papers that are posted on ArXiV (in addition to 30 papers
sampled from Scopus) and ArXiV is likely to not be a representative
sample of academic publications. ArXiV papers are self-submitted
and represent a range of publication stages, from drafts not sub-
mitted to review, preprints in peer review, and postprints that have
passed peer review. Future work should examine different kinds of
stratified random samples to examine differences between various
publishers, publication types, disciplines, topics, and other factors.
Our study only examined a set of the kinds of issues that scholars
and practitioners in ML are examining when they call for greater
transparency and accountability through documentation of datasets
and models. We have not recorded information about what exactly
the rates of inter-annotator agreement are. In particular, we did
not record information about the reconciliation or adjudication
process for projects which involve multiple overlap (e.g. majority
rule, talking to consensus), which we have personally found to be a
crucial and difficult process. Other questions we considered but did
not include were: the demographics of the labelers, the number of
labelers (total and per item), compensation beyond crowdworkers,
whether instructions or screenshot of the labeling interface was
included, and whether labelers had the option to choose “unsure”
(vs. being forced to choose a label). We leave this for future work,
but also found that each additional question made it more difficult
for labelers. We also considered but did not have our team give a
holistic score indicating their confidence in the paper (e.g. a 1-5
score, like those used in some peer reviewing processes).
Our study also has limitations that any human annotation project
has, and we gained much empathy around the difficulties of human
annotation. Our process is not perfect, and as we have analyzed
our data, we have identified cases that make us want to change our
schema even further or reclassify boundary cases. In future work,
we would also recommend using a more structured and constrained
system for annotation to capture the text that annotators use to
justify their answers to various questions. ML papers are very long
and complex, such that our reconciliation and adjudication process
was very time-consuming. Finally, we only have access to what the
publications say about the work they did, and not the work itself.
Future work could improve on this through other methods, such as
ethnographic studies of ML practitioners.
APPENDIX
See the supplementary materials for the appendix.
ACKNOWLEDGMENTS
This work was funded in part by the Gordon & Betty Moore Founda-
tion (Grant GBMF3834) and Alfred P. Sloan Foundation (Grant 2013-
10-27), as part of the Moore-Sloan Data Science Environments grant
to UC-Berkeley. This workwas also supported by UC-Berkeley’s Un-
dergraduate Research Apprenticeship Program (URAP). We thank
many members of UC-Berkeley’s Algorithmic Fairness & Opacity
Group (AFOG) for providing invaluable feedback on this project.
REFERENCES
[1] Charles Babbage. 1864. Passages from the Life of a Philosopher. Longman, Green,
Longman, Roberts, and Green, London.
[2] Iain Barclay, Alun Preece, Ian Taylor, and Dinesh Verma. 2019. Towards Trace-
ability in Data Ecosystems using a Bill of Materials Model. arXiv preprint
arXiv:1904.04253 (2019). https://arxiv.org/abs/1904.04253
[3] Emily M Bender and Batya Friedman. 2018. Data statements for NLP: Toward
mitigating system bias and enabling better science. Transactions of the ACL 6
(2018), 587–604. https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00041
[4] Elena Beretta, Antonio Vetrò, Bruno Lepri, and Juan Carlos De Martin. 2018.
Ethical and Socially-Aware Data Labels. In Annual International Symposium on
Information Management and Big Data. Springer, 320–327.
[5] Kalina Bontcheva, Hamish Cunningham, Ian Roberts, Angus Roberts, Valentin
Tablan, Niraj Aswani, andGenevieve Gorrell. 2013. GATETeamware: a web-based,
collaborative text annotation framework. Language Resources and Evaluation 47,
4 (Dec. 2013), 1007–1029. https://doi.org/10.1007/s10579-013-9215-6
[6] Christine L Borgman. 2012. The conundrum of sharing research data. Journal of
the American Society for Information Science and Technology 63, 6 (2012), 1059–
1078.
[7] Geoffrey C Bowker and Susan Leigh Star. 1999. Sorting Things Out: Classification
and its Consequences. The MIT Press, Cambridge, MA.
[8] Finn Brunton. 2013. Spam: A shadow history of the Internet. The MIT Press,
Cambridge, MA.
[9] Jenna Burrell. 2016. How the machine ‘thinks’: Understanding opacity in machine
learning algorithms. Big Data & Society 3, 1 (2016). https://doi.org/10.1177/
Crowdsourcing for Labeling Machine Learning Datasets. In Proceedings of the
2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). ACM, New
York, NY, USA, 2334–2346. https://doi.org/10.1145/3025453.3026044 event-place:
Denver, Colorado, USA.
[11] WilliamW Cohen et al. 1996. Learning Rules that Classify E-mail. In AAAI Spring
Symposium on Machine Learning in Information Access, Vol. 18. AAAI, New York,
25. http://www.aaai.org/Papers/Symposia/Spring/1996/SS-96-05/SS96-05-003.
pdf
[12] Linguistic Data Consortium. 2008. ACE (Automatic Content Extraction) English
annotation guidelines for entities version 6.6. https://www.ldc.upenn.edu/sites/
www.ldc.upenn.edu/files/english-entities-guidelines-v6.6.pdf
[13] Virginia Eubanks. 2018. Automating inequality: How high-tech tools profile, police,
and punish the poor. St. Martin’s Press.
[14] Benedikt Fecher and Sascha Friesike. 2014. Open Science: One Term, Five Schools
of Thought. In Opening Science: The Evolving Guide on How the Internet is
Changing Research, Collaboration and Scholarly Publishing, SÃűnke Bartling and
Sascha Friesike (Eds.). Springer International Publishing, Cham, 17–47. https:
//doi.org/10.1007/978-3-319-00026-8_2
[15] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. 2009. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). Springer,
New York.
[16] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan,
HannaWallach, Hal Daumeé III, and Kate Crawford. 2018. Datasheets for Datasets.
arXiv preprint arXiv:1803.09010 (2018).
[17] Gharib Gharibi, Vijay Walunj, Rakan Alanazi, Sirisha Rella, and Yugyung Lee.
2019. Automated Management of Deep Learning Experiments. In Proceedings
of the 3rd International Workshop on Data Management for End-to-End Machine
Learning (DEEM’19). ACM, New York, NY, USA, 8:1–8:4. https://doi.org/10.1145/
3329486.3329495 event-place: Amsterdam, Netherlands.
[18] Yolanda Gil, CÃľdric H. David, Ibrahim Demir, Bakinam T. Essawy, Robinson W.
Fulweiler, Jonathan L. Goodall, Leif Karlstrom, Huikyo Lee, Heath J. Mills, Ji-
Hyun Oh, Suzanne A. Pierce, Allen Pope, Mimi W. Tzeng, Sandra R. Villamizar,
and Xuan Yu. 2016. Toward the Geoscience Paper of the Future: Best practices for
documenting and sharing research from data to software to provenance. Earth
and Space Science 3, 10 (2016), 388–415. https://doi.org/10.1002/2015EA000136
[19] Barney G Glaser, Anselm L Strauss, and Elizabeth Strutzel. 1968. The discovery
of grounded theory; strategies for qualitative research. Nursing research 17, 4
(1968), 364.
[20] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. The
MIT Press, Cambridge, MA. http://www.deeplearningbook.org.
[21] Alyssa Goodman, Alberto Pepe, AlexanderW. Blocker, Christine L. Borgman, Kyle
Cranmer, Merce Crosas, Rosanne Di Stefano, Yolanda Gil, Paul Groth, Margaret
Hedstrom, David W. Hogg, Vinay Kashyap, Ashish Mahabal, Aneta Siemigi-
nowska, and Aleksandra Slavkovic. 2014. Ten Simple Rules for the Care and
Feeding of Scientific Data. http://dx.plos.org/10.1371/journal.pcbi.1003542. PLoS
Computational Biology 10, 4 (Apr 2014), e1003542. https://doi.org/10.1371/journal.
pcbi.1003542
[22] Charles Goodwin. 1994. Professional Vision. American Anthropologist 96, 3 (sep
1994), 606–633. https://doi.org/10.1525/aa.1994.96.3.02a00100
[23] Aaron Halfaker and R Stuart Geiger. 2019. ORES: Lowering Barriers with Partici-
patory Machine Learning in Wikipedia. arXiv preprint arXiv:1909.05189 (2019).
https://arxiv.org/pdf/1909.05189.pdf
[24] Michael Hind, Sameep Mehta, Aleksandra Mojsilovic, Ravi Nair,
Karthikeyan Natesan Ramamurthy, Alexandra Olteanu, and Kush R Varshney.
2018. Increasing Trust in AI Services through Supplier’s Declarations of Confor-
mity. arXiv preprint arXiv:1808.07261 (2018). https://arxiv.org/pdf/1808.07261
[25] SarahHolland, AhmedHosny, SarahNewman, Joshua Joseph, and Kasia Chmielin-
ski. 2018. The dataset nutrition label: A framework to drive higher data quality
standards. arXiv preprint arXiv:1805.03677 (2018). https://arxiv.org/abs/1805.
tion: a new methodological challenge for corpus linguistics. International Journal
of Translation 22, 1 (2010), 13–36.
[27] John D. Hunter. 2007. Matplotlib: A 2D Graphics Environment. Computing in
Science & Engineering 9, 3 (2007), 90–95. https://doi.org/10.1109/MCSE.2007.55
arXiv:https://aip.scitation.org/doi/pdf/10.1109/MCSE.2007.55
[28] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An
introduction to statistical learning. Springer, New York.
[29] Eric Jones, Travis Oliphant, Pearu Peterson, et al. 2001. SciPy: Open source
scientific tools for Python. http://www.scipy.org/
[30] Justin Kitzes, Daniel Turek, and Fatma Deniz. 2018. The Practice of Reproducible
Research : Case Studies and Lessons from the Data-Intensive Sciences. University
of California Press, Oakland. 337 pages. http://practicereproducibleresearch.org
[31] Thomas Kluyver, Benjamin Ragan-Kelley, Fernando Pérez, Brian Granger,
Matthias Bussonnier, Jonathan Frederic, Kyle Kelley, Jessica Hamrick, Jason
Grout, Sylvain Corlay, Paul Ivanov, Damián Avila, Safia Abdalla, and Carol
Willing. 2016. Jupyter Notebooks: A Publishing format for Reproducible Com-
putational Workflows. In Positioning and Power in Academic Publishing: Players,
Agents and Agendas, F. Loizides and B. Schmidt (Eds.). IOS Press, Amsterdam, 87
– 90. https://doi.org/10.3233/978-1-61499-649-1-87
[32] Sanjay Krishnan, Michael J. Franklin, Ken Goldberg, Jiannan Wang, and Eu-
gene Wu. 2016. ActiveClean: An Interactive Data Cleaning Framework For
Modern Machine Learning. In Proceedings of the 2016 International Conference
on Management of Data (SIGMOD ’16). ACM, New York, NY, USA, 2117–2120.
https://doi.org/10.1145/2882903.2899409 event-place: San Francisco, California,
USA.
[33] Bruno Latour. 1999. Circulating Reference: Sampling the Soil in the Amazon
Forest. In Pandora’s Hope. Harvard University Press, Cambridge, Mass.
[34] Bruno Latour and Steve Woolgar. 1979. Laboratory Life: The Social Construction
of Scientific Facts. Sage Publications, Beverly Hills.
[35] Kazuaki Maeda, Haejoong Lee, Shawn Medero, Julie Medero, Robert Parker,
and Stephanie M. Strassel. 2008. Annotation Tool Development for Large-Scale
Corpus Creation Projects at the Linguistic Data Consortium.. In Proceedings of the
Sixth International Conference on Language Resources and Evaluation (LREC’08),
Vol. 8. http://www.lrec-conf.org/proceedings/lrec2008/pdf/775_paper.pdf
[36] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and
Inter-rater Reliability in Qualitative Research: Norms and Guidelines for CSCW
and HCI Practice. Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 72 (Nov.
2019), 23 pages. https://doi.org/10.1145/3359174
[37] Wes McKinney. 2010. Data Structures for Statistical Computing in Python.
In Proceedings of the 9th Python in Science Conference, Stéfan van der Walt and
Jarrod Millman (Eds.). 51–56. http://conference.scipy.org/proceedings/scipy2010/
mckinney.html
[38] N. Medeiros and R.J. Ball. 2017. Teaching Integrity in Empirical Economics: The
Pedagogy of Reproducible Science in Undergraduate Education. In Undergraduate
Research and the Academic Librarian: Case Studies and Best Practices, M.K. Hensley
and S. Davis-Kahl (Eds.). Association of College & Research Libraries, Chicago.
https://scholarship.haverford.edu/cgi/viewcontent.cgi?article=1189
[39] WD Mellin. 1957. Work with new electronic ‘brains’ opens field for army math
experts. The Hammond Times 10 (1957), 66.
[40] Margaret Mitchell, SimoneWu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019.
Model cards for model reporting. In Proceedings of the Conference on Fairness,
Accountability, and Transparency. ACM, 220–229.
[41] Igor Mozetič, Miha Grčar, and Jasmina Smailović. 2016. Multilingual Twitter
Sentiment Classification: The Role of Human Annotators. PLOS ONE 11, 5 (may
2016), e0155036. https://doi.org/10.1371/journal.pone.0155036
[42] Hiroki Nakayama, Takahiro Kubo, Junya Kamura, Yasufumi Taniguchi, and Xu
Liang. 2018. doccano: Text Annotation Tool for Human. https://github.com/
doccano/doccano Software available from https://github.com/doccano/doccano.
[43] Laura K Nelson. 2017. Computational grounded theory: A methodological frame-
work. Sociological Methods & Research (2017).
[44] Anton Oleinik, Irina Popova, Svetlana Kirdina, and Tatyana Shatalova. 2014.
On the choice of measures of reliability and validity in the content-analysis of
texts. Quality & Quantity 48, 5 (Sept. 2014), 2703–2718. https://doi.org/10.1007/
s11135-013-9919-0
[45] Frank Pasquale. 2015. The Black Box Society: The Secret Algorithms That Control
Money and Information. Harvard University Press, Cambridge.
[46] Fernando Pérez and Brian E. Granger. 2007. IPython: a System for Interactive
Scientific Computing. Computing in Science and Engineering 9, 3 (May 2007),
21–29. https://doi.org/10.1109/MCSE.2007.53
[47] Project Jupyter, Matthias Bussonnier, Jessica Forde, Jeremy Freeman, Brian
Granger, Tim Head, Chris Holdgraf, Kyle Kelley, Gladys Nalvarte, Andrew
Osheroff, M Pacer, Yuvi Panda, Fernando Perez, Benjamin Ragan Kelley, and
Carol Willing. 2018. Binder 2.0 - Reproducible, Interactive, Sharable Environ-
ments for Science at Scale. In Proceedings of the 17th Python in Science Confer-
ence, Fatih Akici, David Lippa, Dillon Niederhut, and M Pacer (Eds.). 113 – 120.
https://doi.org/10.25080/Majora-4af1f417-011
[48] Jefferson Provost. 1999. Naive Bayes vs. Rule-Learning in Classification of Email.
Technical Report AI-TR-99-284. University of Texas at Austin, Artificial Intelli-
gence Lab. http://www.cs.utexas.edu/ftp/AI-Lab/tech-reports/UT-AI-TR-99-284.
pdf
[49] MartÃŋn PÃľrez-PÃľrez, Daniel Glez-PeÃśa, Florentino Fdez-Riverola, and
AnÃąlia LourenÃğo. 2015. Marky: A tool supporting annotation consistency in
multi-user and iterative document annotation projects. Computer Methods and
Programs in Biomedicine 118, 2 (Feb. 2015), 242–251. https://doi.org/10.1016/j.
cmpb.2014.11.005
[50] David Quarfoot and Richard A. Levine. 2016. How Robust Are Multirater Inter-
rater Reliability Indices to Changes in Frequency Distribution? The American
Statistician 70, 4 (Oct. 2016), 373–384. https://doi.org/10.1080/00031305.2016.
annotators for crowdsourced labeling tasks. Journal of Machine Learning Research
13, Feb (2012), 491–518.
[52] Daniel Riff, Stephen Lacy, and Frederick Fico. 2013. Analyzing media messages:
Using quantitative content analysis in research. Routledge, New York.
[53] Marta Sabou, Kalina Bontcheva, Leon Derczynski, and Arno Scharl. 2014. Cor-
pus Annotation through Crowdsourcing: Towards Best Practice Guidelines. In
Proceedings of the Ninth International Conference on Language Resources and Eval-
uation (LREC’14). European Language Resources Association (ELRA), Reykjavik,
Iceland, 859–866. http://www.lrec-conf.org/proceedings/lrec2014/pdf/497_Paper.
pdf
[54] Andrew Sallans and Martin Donnelly. 2012. DMP Online and DMPTool: Different
Strategies Towards a Shared Goal. International Journal of Digital Curation 7, 2
(2012), 123–129. https://doi.org/10.2218/ijdc.v7i2.235
[55] Sebastian Schelter, Joos-Hendrik BÃűse, Johannes Kirschnick, Thoralf Klein,
and Stephan Seufert. 2017. Automatically tracking metadata and provenance of
machine learning experiments. In Machine Learning Systems workshop at NIPS.
[56] Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biess-
mann, and Andreas Grafberger. 2018. Automating Large-scale Data Qual-
ity Verification. Proc. VLDB Endow. 11, 12 (Aug. 2018), 1781–1794. https:
//doi.org/10.14778/3229863.3229867
[57] Alan A Schreier, Kenneth Wilson, and David Resnik. 2006. Academic research
record-keeping: Best practices for individuals, group leaders, and institutions.
Academic medicine: journal of the Association of American Medical Colleges 81, 1
(2006), 42. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3943904/
[58] M Six Silberman, Bill Tomlinson, Rochelle LaPlante, Joel Ross, Lilly Irani, and
Andrew Zaldivar. 2018. Responsible research with crowds: pay crowdworkers at
least minimum wage. Commun. ACM 61, 3 (2018), 39–41.
[59] Robert Simpson, Kevin R. Page, and David De Roure. 2014. Zooniverse: Ob-
serving the World’s Largest Citizen Science Platform. In Proceedings of the 23rd
International Conference on World Wide Web (WWW ’14 Companion). ACM, New
York, NY, USA, 1049–1054. https://doi.org/10.1145/2567948.2579215
[60] Jatinder Singh, Jennifer Cobbe, and Chris Norval. 2019. Decision Provenance:
Harnessing Data Flow for Accountable Systems. IEEE Access 7 (2019), 6562–6574.
https://doi.org/10.1109/ACCESS.2018.2887201
[61] Guillermo Soberón, Lora Aroyo, Chris Welty, Oana Inel, Hui Lin, and Manfred
Overmeen. 2013. Measuring crowd truth: Disagreement metrics combined with
worker behavior filters. In CrowdSem 2013 Workshop.
[62] Guy Stuart. 2004. Databases, Felons, and Voting: Bias and Partisanship of the
Florida Felons List in the 2000 Elections. Political Science Quarterly 119, 3 (sep
2004), 453–475. https://doi.org/10.2307/20202391
[63] A. Tong, P. Sainsbury, and J. Craig. 2007. Consolidated criteria for reporting
qualitative research (COREQ): a 32-item checklist for interviews and focus groups.
International Journal for Quality in Health Care 19, 6 (sep 2007), 349–357. https:
//doi.org/10.1093/intqhc/mzm042
[64] S. van der Walt, S. C. Colbert, and G. Varoquaux. 2011. The NumPy Array: A
Structure for Efficient Numerical Computation. Computing in Science Engineering
13, 2 (March 2011), 22–30. https://doi.org/10.1109/MCSE.2011.37
[65] Guido van Rossum. 1995. Python Library Reference. https://ir.cwi.nl/pub/5009/
05009D.pdf
[66] Luis Von Ahn, Benjamin Maurer, Colin McMillen, David Abraham, and Manuel
Blum. 2008. recaptcha: Human-based character recognition via web security
measures. Science 321, 5895 (2008), 1465–1468.
[67] Michael Waskom, Olga Botvinnik, Drew O’Kane, Paul Hobson, Joel Ostblom,
Saulius Lukauskas, David C Gemperline, Tom Augspurger, Yaroslav Halchenko,
John B. Cole, Jordi Warmenhoven, Julian de Ruiter, Cameron Pye, Stephan Hoyer,
Jake Vanderplas, Santi Villalba, Gero Kunter, Eric Quintero, Pete Bachant, Marcel
Martin, Kyle Meyer, Alistair Miles, Yoav Ram, Thomas Brunner, Tal Yarkoni,
Mike Lee Williams, Constantine Evans, Clark Fitzgerald, Brian, and Adel Qalieh.
2018. Seaborn: Statistical Data Visualization Using Matplotlib. https://doi.org/
10.5281/zenodo.592845
[68] Greg Wilson, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt,
and Tracy K. Teal. 2017. Good enough practices in scientific computing. http:
//dx.plos.org/10.1371/journal.pcbi.1005510. PLOS Computational Biology 13, 6
(Jun 2017), e1005510. https://doi.org/10.1371/journal.pcbi.1005510
