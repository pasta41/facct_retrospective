Centering Disability Perspectives in Algorithmic Fairness, 
Accountability, & Transparency 
Alexandra Reeve Givens 
 Georgetown University Institute for Tech Law & Policy 
 Washington, D.C., USA 
 alexandra.givens@law.georgetown.edu 
Meredith Ringel Morris 
 Microsoft Research 
 Redmond, WA, USA 
 merrie@microsoft.com 
ABSTRACT 
It is vital to consider the unique risks and impacts of algorithmic 
decision-making for people with disabilities. The diverse nature of 
potential disabilities poses unique challenges for approaches to 
fairness, accountability, and transparency. Many disabled people 
choose not to disclose their disabilities, making auditing and 
accountability tools particularly hard to design and operate. 
Further, the variety inherent in disability poses challenges for 
collecting representative training data in any quantity sufficient to 
better train more inclusive and accountable algorithms.  
This panel highlights areas of concern, present emerging research 
efforts, and enlist more researchers and advocates to study the 
potential impacts of algorithmic decision-making on people with 
disabilities. A key objective is to surface new research projects 
and collaborations, including by integrating a critical disability 
perspective into existing research and advocacy efforts focused on 
identifying sources of bias and advancing equity. 
In the technology space, discussion topics will include methods to 
assess the fairness of current AI systems, and strategies to develop 
new systems and bias mitigation approaches that ensure fairness 
for people with disabilities. For example, how do today’s 
currently-deployed AI systems impact people with disabilities? If 
developing inclusive datasets is part of the solution, how can 
researchers ethically gather such data, and what risks might 
centralizing data about disability pose? What new privacy 
solutions must developers create to reduce the risk of deductive 
disclosure of identities of people with disabilities in 
“anonymized” datasets? How can AI models and bias mitigation 
techniques be developed that handle the unique challenges of 
disability, i.e., the “long tail” and low incidence of many types of 
disability – for instance, how do we ensure that data about 
disability are not treated as outliers? What are the pros and cons of 
developing custom/personalized AI models for people with 
disabilities versus ensuring that general models are inclusive?  
In the law and policy space, the framework for people with 
disabilities requires specific study. For example, the Americans 
with Disabilities Act (ADA) requires employers to adopt 
“reasonable accommodations” for qualified individuals with a 
disability. But what is a “reasonable accommodation” in the 
context of machine learning and AI? How will the ADA’s unique 
standards interact with case law and scholarship about algorithmic 
bias against other protected groups? When the ADA governs what 
questions employers can ask about a candidate’s disability, and 
HIPAA and the Genetic Information Privacy Act regulate the 
sharing of health information, how should we think about 
inferences from data that approximate such questions? 
Panelists will bring varied perspectives to this conversation, 
including backgrounds in computer science, disability studies, 
legal studies, and activism. In addition to their scholarly expertise, 
several panelists have direct lived experience with disability. The 
session format will consist of brief position statements from each 
panelist, followed by questions from the moderator, and then open 
questions from and discussion with the audience. 
Moderator: Alexandra Reeve Givens, Georgetown Institute for 
Technology Law & Policy; Alexandra is the Executive Director of 
Georgetown Law’s Institute for Technology Law & Policy and 
the Vice chair of the Christopher and Dana Reeve Foundation. 
Panelists: Jeffrey P. Bigham, Carnegie Mellon University & 
Apple; Jeff is a professor in CMU’s Human-Computer Interaction 
Institute, and also directs a Machine Learning + Accessibility 
research group at Apple. Lydia X.Z. Brown, Georgetown 
Institute for Technology Law & Policy; Lydia is a disability 
justice advocate, organizer, educator, attorney, strategist, and 
writer. Shaun Kane, University of Colorado Boulder; Shaun 
directs the Superhuman Computing Lab at CU Boulder. Meredith 
Ringel Morris, Microsoft Research; Merrie leads the Ability 
research team at Microsoft. Karen Nakamura, University of 
California, Berkeley; Karen is a cultural anthropologist and chair 
of the Berkeley Disability Lab and the Haas Institute for a Fair 
and Inclusive Society. Maria Skoularido, University of 
Cambridge; Maria is a doctoral candidate in machine learning, as 
well as the founder and chair of the advocacy and support group 
{Dis}Ability in AI. 
CCS CONCEPTS 
• Computing methodologies~Artificial intelligence 
KEYWORDS 
AI FATE, algorithmic bias, disability studies, accessibility 
 
Permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for third-party components of this 
work must be honored. For all other uses, contact the owner/author(s). 
FAT* '20, January 27–30, 2020, Barcelona, Spain 
© 2020 Copyright held by the owner/author(s). 
ACM ISBN 978-1-4503-6936-7/20/01…$15.00 
https://doi.org/10.1145/3351095.3375686 
684
