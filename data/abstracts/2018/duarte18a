Proceedings of Machine Learning Research 81:1–1, 2018 Conference on Fairness, Accountability, and Transparency
Mixed Messages? The Limits of Automated Social Media
Content Analysis [Extended Abstract]∗
Natasha Duarte natasha@cdt.org
Emma Llanso emma@cdt.org
Anna Loup aloup@usc.edu
Center for Democracy & Technology
Editors: Sorelle A. Friedler and Christo Wilson
Abstract
Governments and companies are turning to
automated tools to make sense of what peo-
ple post on social media. Policymakers rou-
tinely call for social media companies to
identify and take down hate speech, ter-
rorist propaganda, harassment, fake news
or disinformation. Other policy proposals
have focused on mining social media to in-
form law enforcement and immigration de-
cisions. But these proposals wrongly as-
sume that automated technology can ac-
complish on a large scale the kind of nu-
anced analysis that humans can do on a
small scale. Todays tools for analyzing so-
cial media text have limited ability to parse
the meaning of human communication or
detect the intent of the speaker.
A knowledge gap exists between data sci-
entists studying natural language process-
ing (NLP) and policymakers advocating
for wide adoption of automated social me-
dia analysis and moderation. Policymakers
must understand the capabilities and limits
of NLP before endorsing or adopting auto-
mated content analysis tools, particularly
for making decisions that affect fundamen-
tal rights or access to government benefits.
Without proper safeguards, these tools can
facilitate overbroad censorship and biased
enforcement of laws or terms of service.
This paper draws on existing research to
explain the capabilities and limitations of
text classifiers for social media posts and
other online content. It is aimed at helping
researchers and technical experts address
the gaps in policymakers knowledge about
what is possible with automated text anal-
∗ Full paper: https://cdt.org/insight/
mixed-messages-for-fat-conference-2018/
ysis. We provide an overview of how NLP
classifiers work and identify five key limi-
tations of these tools that must be commu-
nicated to policymakers:
1. NLP classifiers require domain-specific training
and cannot be applied with the same reliabil-
ity across different domains. Policies should not
rely on the use of a one-size-fits-all tool.
2. Decisions based on automated social media con-
tent analysis risk further marginalizing and dis-
proportionately censoring groups that already
face discrimination. NLP tools can amplify so-
cial bias reflected in language and are likely to
have lower accuracy for minority groups who are
underrepresented in training data.
3. Accurate text classification requires clear, con-
sistent definitions of the type of speech to be
identified. Policy debates around content mod-
eration and social media mining tend to lack
such precise definitions.
4. The accuracy achieved in NLP studies does not
warrant widespread application of these tools to
social media content analysis and moderation.
5. Text filters remain easy to evade and fall far
short of humans ability to parse meaning from
text. Human review of flagged content remains
essential for avoiding over-censorship.
The paper concludes with recommenda-
tions for NLP researchers to bridge the
knowledge gap between technical experts
and policymakers, including (1) Clearly de-
scribe the domain limitations of NLP tools;
(2) Increase development of non-English
training resources; (3) Provide more detail
and context for accuracy measures; and (4)
Publish more information about definitions
and instructions provided to annotators.
Keywords: NLP, content moderation
c© 2018 N. Duarte, E. Llanso & A. Loup.
