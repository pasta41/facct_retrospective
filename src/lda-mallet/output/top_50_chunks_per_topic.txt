---------------------------------------------------------------------------
TOPIC 0: user-study

---------------------------------------------------------------------------

0.9006 less with the decision of the system compared to the postgraduates Following the same line of thought we examined whether there is a difference in the perception of appropriateness of the factors considered for the systems decision between undergraduates and postgraduates Similar to above in their responses regarding Case A and B we did not have any significant differences In Case C median engagement score was statistically significantly lower in undergraduates than in postgraduates U z p indicating that undergraduates considered the factors used in the system for making the decision less appropriate compared to the postgraduates Finally there is a marginal statistical difference between undergraduates and postgraduates in their indication of whether the decision-making process was fair in Case C Median engagement score was statistically significantly lower in undergraduates than in postgraduates U z p indicating that undergraduates considered the decision-making process less fair compared to the postgraduates QUALITATIVE FINDINGS For Q participants were asked whether they had sufficient information The free-text responses that simply stated a yes/no were excluded from the qualitative analysis To analyse participants free-text responses we used content analysis by coding responses for the themes mentioned in relation to the concepts in question Two researchers

0.8919 were found Median engagement score was moderately statistically significantly higher in postgraduates than in undergraduates U z p indicating that postgraduates understood the process that the system is following in making a decision better compared to undergraduates There was no significant difference between the two groups with respect to the other parameters Since we had indications from the previous analysis where the different cases in Scenario perceived differently we wanted to see whether there is a difference between undergraduates and postgraduates in the perception of sufficiency of information provided In Case A and Case C we did not find any significant differences between the two groups In Case B median engagement score was statistically significantly lower in undergraduates than in postgraduates U z p indicating that undergraduates find the information provided less sufficient in this case compared to postgraduates Furthermore we examined whether there is a difference in the agreement with the decision between undergraduates and postgraduates in our sample In Case A and Case B we did not find any statistically significant differences between the two groups For Case C median engagement score was statistically significantly lower in undergraduates than in postgraduates U z p indicating that undergraduates agreed

0.8914 the AIs confidence level was high above participants trust was significantly enhanced by seeing the confidence scores This calibration of trust was confirmed by a statistically significant interaction between showing confidence and the AIs confidence level p Further when the AI confidence score was not shown participants trust was generally maintained around the same level across trials of all confidence levels This was confirmed by an ANOVA on the without-confidence conditions main effect of confidence level was not significant p To answer the trust calibration effect by showing confidence score held regardless of whether the model prediction was shown In other words high confidence scores encouraged participants to delegate the decision task to the AI even without seeing its predictions This was confirmed by the insignificant three-way interaction between confidence prediction and confidence level p A similar pattern was observed in the other trust measure agreement percentage as shown in Figure When the confidence score was shown the difference in the agreement percentage between high-confidence levels and low-confidence levels became more pronounced The calibration effect of confidence score on the agreement percentage as indicated by the interaction between confidence and confidence levels was significant p Similarly this calibration effect held

0.8911 statistically significant in three setups Second we find that females generally outperform males This observation holds in out of experimental setups but none of the differences is statistically significant Our results contribute to mixed observations regarding gender differences in deception detection The low number of statistically significant differences is expected because human performance is low unless we show predicted labels Accuracy Predicted label w/o accuracy Predicted label w/ accuracy frequency explanation Predicted label w/ accuracy frequency explanation Predicted label w/ accuracy Predicted label w/ accuracy Predicted label w/ accuracy Predicted label w/ accuracy p p p p p a Human accuracy Trust Predicted label w/o accuracy Predicted label w/ accuracy frequency explanation Predicted label w/ accuracy frequency explanation Predicted label w/ accuracy Predicted label w/ accuracy Predicted label w/ accuracy Predicted label w/ accuracy p p p p p b Trust Figure Human accuracy and trust given varying statements of machine accuracy Figure a and Figure b show that human accuracy and trust generally decline with statements of decreasing machine accuracy despite the fact that machine predictions remain unchanged Note that the decline of human trust with statements of decreasing accuracy is small Only by adding frequency explanations human accuracy

0.8851 the Pearson correlations are available in Table Although we were expecting that all constructs will correlate similar to we were surprised to see that understanding of the process followed correlates with appropriateness of the factors and understanding of the process with deserved outcome see Table Perception and Interplay of Constructs To examine a number of hypotheses regarding participants perception of the Fairness constructs in Scenarios we run a series of Wilcoxon signed ranked tests Table Pearson Correlations for the six constructs of justice Agreement Understanding Appropriateness Fair Deserved Trust Agreement Pearson Correlation -tailed Understanding Pearson Correlation -tailed Appropriateness Pearson Correlation -tailed Fair Pearson Correlation -tailed Deserved Pearson Correlation -tailed Trust Pearson Correlation -tailed People who agreed with the decision also believe that the person in the scenario deserved the outcome We were expecting that the people who indicated agreement with the systems decision would also believe that the person in the scenario deserved the outcome Surprisingly we found significant statistical differences in their opinions Scenario z p Scenario z p In Scenario there was a considerable number of participants who selected options and on the Agreement scale while selected options and on the Deserved scale indicating that they agreed with

0.8465 in scenarios of showing and not showing AI prediction p was thus fully supported Accuracy During the experiment we collected three types of predictions a participants own predictions before they saw any information from the AI b the AIs predictions and c the participants final prediction after seeing AI information which we call AI-assisted prediction We measured the accuracy for each type of prediction On average the participants own accuracy was with only of participants under while the AI accuracy was note this number is lower than model accuracy on test data because of stratified sampling for experiment trials These accuracy p re d n p re d n AI confidence level S it c P e rc e n ta g e w/o confidence w/ confidence Figure Switch percentage across ive confidence levels and various conditions p re d n p re d n AI confidence level A g re e m e n t P e rc e n ta g e w/o confidence w/ confidence Figure Agreement percentage measured as how often participants agree with the models prediction across confidence levels and various conditions numbers did not show statistically significant variations across experimental conditions Thus in our

0.8336 the spectrum between full human agency and full automation in Figure we develop varying levels of assistance from machine learning models Section For example the following three levels of machine assistance gradually increase the influence of machine predictions showing only explanations of machine predictions without revealing predicted labels showing predicted labels without revealing high machine accuracy showing predicted labels with an explicit statement of strong machine accuracy In Section we investigate human performance under different experimental setups along the spectrum We show that explanations alone slightly improve human performance while showing predicted labels achieves great improvement relative improvement in human accuracy However this improvement is still moderate compared to full priming with an explicit statement of machine accuracy relative improvement in human accuracy Our findings suggest that there exists a tradeoff between human performance and human agency Interestingly when predicted labels are shown explanations of machine predictions can achieve a similar effect as an explicit statement of machine accuracy We also find that humans tend to trust correct machine predictions more than incorrect ones indicating that they can somewhat identify when machines are correct We further examine the effect of statements of machine accuracy by varying the accuracy numbers Section

0.8243 examine the level of trust that humans place on machine predictions Our results suggest that humans can somewhat differentiate correct machine predictions from incorrect ones Finally we present individual differences among our participants based on information collected in the exit survey Our dataset and demonstration are available at Human Accuracy We first present human accuracy measured by the percentage of correctly predicted instances by humans Our results suggest that Trust Predicted label w/ accuracy Predicted label w/ heatmap Predicted label examples Predicted label heatmap random Predicted label w/o accuracy p p p p a Trust in machine predictions Trust Predicted label w/ accuracy Predicted label w/ heatmap Predicted label examples Predicted label heatmap random Predicted label w/o accuracy Correct machine predictions Incorrect machine predictions b Trust in correct and incorrect machine predictions Figure The trust that humans place on machine predictions Figure a shows that adding feature-based explanations heatmap can effectively increase the trust level compared to predicted label w/o accuracy p-value in Figure a is computed by conducting t-test between the corresponding setup and predicted label w/o accuracy Figure b breaks down the trust based on whether machine predictions are correct or incorrect and shows that humans trust correct

0.8241 the decision but the person in the scenario did not deserve the outcome In scenario fewer participants but still a considerable number selected options and on the Agreement Scale indicating they agree with the decision while selected options and on the Deserved scale People who found the factors used in the decision making process appropriate will also think that the decision making process is fair The results show significant differences between the responses of the participants in Scenario p with participants in their majority selected and on the scale for S reporting that the factors used in the decision-making were appropriate however they do not believe that the decisionmaking process was fair selected and on the scale for S In Scenario we do not have a statistical significant difference between the two scales where participants in their majority agree that the factors used in the decision making processes were not appropriate and believe that the decision making process was not fair Qualitative results see below show that in Scenario participants felt that the use of gender and age as factors to determine the decision were not appropriate which explains this result People who indicated the the decision making process was

0.8206 increased peoples willingness to rely on AIs prediction in high-confidence cases This trust calibration effect held in AI-assisted decision scenarios where the AIs recommendation was shown and in scenarios where people had to make blind delegation without AI confidence level D if fe re n c e b e tw e e n u m a n a n d A I A c c u ra c y Figure difference between human and AI accuracy across confidence levels seeing the AIs recommendation However in this case study trust calibration did not translate into improvement in AI-assisted decision outcome rejected potentially because there was not enough complementary knowledge for people to draw on While we explored a scenario where participants knew they had additional knowledge that the AI did not have access to it did not make significant difference in the AI-assisted prediction task EXPERIMENT EFFECT OF LOCAL EXPLANATION The second experiment examined the effect of local explanations It had the same setup as Experiment but instead of showing confidence scores we showed local explanations for each AI prediction The main hypothesis we wanted to test was that because local explanation is suggested to help people judge whether to trust

0.8159 machine predictions Figure aWe further introduce random heatmap by randomly highlighting an equal number of words as in heatmap to examine whether humans are influenced by any explanations including random ones Our results are consistent with Hypothesis both feature-based and example-based explanations increase the trust of humans in machine predictions In fact predicted label heatmap leads to a similar level of trust as predicted label w/ accuracy although the latter explicitly tells humans that the machine learning model has an accuracy of approximately In other words when predicted labels are shown heatmap can nudge humans in decision making without making strong statements of machine accuracy Interestingly random heatmap also increases the trust level significantly suggesting that even irrelevant details can increase the trust of humans in machine predictions The fact that heatmap is significantly more effective than random heatmap vs p indicates that humans can interpret valuable information in weight coefficients beyond the placebo effect Humans tend to trust machine predictions more when machine predictions are correct Figure b We next examine whether humans trust machine predictions more when machine predictions are correct than when they are incorrect Figure b shows that in all the five experimental setups with predicted

0.8114 statements of machine accuracy we add frequency explanations to the statement with accuracy and Specifically we show participants The machine predicts that the below review is deceptive It has an accuracy of approximately which means that it is correct out of times instead of The machine predicts that the below review is deceptive It has an accuracy of approximately The results are shown with the red bars filled with stars in Figure a and Figure b We find that frequency explanations reduce the trust that humans place on machine predictions For instance human accuracy in predicted label w/ accuracy frequency explanation is lower p than in predicted label w/ accuracy Similarly human trust in predicted label w/ accuracy frequency explanation is lower p than in predicted label w/ accuracy Furthermore the differences in human accuracy and trust are not statistically significant between predicted label w/ accuracy frequency explanation and predicted label w/o accuracy These observations suggest that frequency explanations can help humans interpret statements of machine accuracy in which case a statement of accuracy with frequency explanation is almost the same as not showing machine accuracy Our frequency explanations are also known as frequent format and have been shown to

0.8107 a petition against the admission system and I would support a protest against the admission system We calculated a mean index for protest behavior Cronbachs M SD Reputation of the university We employed the RepTrakTM Pulse to assess the reputation of a university using an AI-based admission system It consists of the four items It is a university I have a good feeling about It is a university that I trust It is a university that I admire and The university has a good overall reputation Participants rated these statements on a five-point Likert scale do not agree at all to totally agree We used these four items to compute a reliable mean index Cronbachs M SD RESULTS To answer Ha we performed a dependent t-test comparing perceptions of distributive fairness between ADM and HDM Our results show that participants perceived the ADM system M SE as significantly fairer than the HDM system M SE t p r That is regarding the output of the admission process they considered ADM to be less biased in one direction or the other compared to human committee members making the decision Thus our data support Ha As for Hb we carried out a

0.8063 not fair would not trust this systems decision more than a humans decision For both Scenario and Scenario we did not get any significant differences between the two scales Specifically of the participants in Scenario and of the participants in Scenario believe the decision making process was not fair and of the participants in Scenario and of the participants in Scenario would not trust the systems decision more than a humans Next we wanted to examine whether the different decisions in Scenario Case A Case B and Case C affected participants perception of the above constructs Does the participants perception of Agreement Understanding Appropriateness Fair Process Deserved Outcome and Trust change according to the decision of the system given the same scenario To compare the responses in Scenario we followed a within-subject analysis using ANOVA repeated measures followed by a Bonferroni post-hoc test There were significant differences for Agreement p Appropriateness p Fairness p Deserved Outcome p and Trust p in responses provided by the participants Bonferroni posthoc tests showed that participants perceived the decision in Case B proportional outcome as the most just while the decision on Case C as the least Similarly comparing their responses in question Q

0.7971 factor no info vs confidence vs explanation had a significant effect on the switch percentage p and its interaction with model confidence level was also significant p A Tukeys honestly significant difference HSD post-hoc test showed that the switch percentage in the confidence condition was significantly higher than those in the baseline condition p and the explanation condition p but the explanation condition was not significantly different from the baseline p The agreement percentage showed a similar effect albeit less pronounced As shown in Figure the baseline condition green and the explanation condition blue had similar agreement percentages while the with-confidence condition orange had higher percentage when the confidence level was above Nonetheless this effect was not significant on this measure p Taken together was rejected as we found no evidence that showing explanation was more effective in trust calibration than the baseline Accuracy In Experiment the average Human accuracy was while the AIs accuracy was again due to stratified sampling Figure examines the effect of explanation on the accuracy of AI-assisted predictions Similar to Experiment we did not find any significant difference in AI-assisted accuracy across model AI confidence level S it c P e rc e n ta

0.7954 was shown it was the percentage of trials using the AIs prediction among trials where participants and the AI disagreed In conditions where the AIs prediction was not shown it was the percentage of trials in which participants chose to delegate the prediction to the AI among all trials Agreement percentage the percentage of trials in which the participants final prediction agreed with the AIs prediction The main difference between the two measures was that in the with-prediction conditions the agreement percentage would count the trials in which the participants and the AIs predictions agreed and automatically counted as the final decision whereas the switch percentage would only consider cases where they disagreed and had to make an intentional act of switching Therefore we consider switch percentage to be a stricter measure of trust even though agreement percentage was used in prior research Figure shows the switch percentage across the prediction and confidence factors The result that the orange error bars w/ confidence conditions are higher than the green error bars w/o confidence conditions indicates that the participants switched to the AIs predictions or decided to use AI in the without-prediction conditions more often when the AIs confidence scores were

0.7903 of each model individually Across most models participants tended to lean toward rating the model as not at all biased or only somewhat biased For only one model did participants significantly lean toward rating it as biased Nonetheless participants tended not to rate these models as fair In particular for only one model did participants significantly lean toward rating it as fair In contrast for six others they leaned towards rating it as not at all fair or only somewhat fair In spite of these trends there were no models where participants opinions were unanimous and many where opinions were strongly divided Bias Participants rated the bias of each of the two models they saw on a five-point scale from completely biased to not at all biased There were six trade-off pairs two possible disadvantaged groups and two models per pair yielding twenty-four individual models For sixteen of these twenty-four participants significantly tended towards rating the model as not biased as shown in Figure For these sixteen models the percentage of participants who rated the model as either not at all biased or only somewhat biased ranged from to In contrast for only one model FP Outcome Min were responses

0.7902 Cards No Persona Cards No Checklist Cards Checklist Only Model Cards and Checklists Card No Persona Cards Persona Only Model Cards and Persona Card No Checklist Cards Checklist and Persona Model Cards Persona Cards and Checklist Cards Table This table displays our four study conditions names details and the corresponding room assignments Outcome Evaluations We designed and administered four outcome evaluations to assess student learning This includes a matched pre- and post-quiz a post-survey and an open-ended written group proposal In addition we anticipate that student learning will be observed in the deliberation process facilitated by the toolkit We further analyze the student teams deliberation process to evaluate student learning along with all three learning objectives In this section we introduce how these four outcome evaluations are designed and implemented The alignment between the learning objectives and the outcome evaluations is shown in Table Quiz on Conceptual Understanding We administered a pre-quiz and a delayed post-quiz to assess Learning Objective We gave students a toy dataset and a prediction task and ask students to apply the performance metrics on this dataset We used the criminal recidivism prediction task in the pre-quiz and the loan application prediction task in the post-quiz

0.7847 scripts We learned our process involved some ambiguities around whether a subsequent value needed to be filled in For example if a paper was not using crowdworkers then the instructions for our schema were that the question about crowdworker compensation was to remain blank However we found we had cases where reported crowdworker compensation was no for papers that did not use crowdworkers This would be concerning had we had a yes for such a variable but found no such cases We recoded questions about pre-screening for crowdwork platforms implied by using crowdworkers in original human annotation source and the number of human annotators We measured interrater reliability metrics using mean percent total agreement or the proportion of cases where all labelers initially gave the same label This is a more stringent metric than Fleisss kappa and Krippendorfs alpha and our data does not fit the assumptions for those widely-used metrics IRR rates for round one were relatively low across all questions the mean percent total agreement was with the lowest question having a rate of IRR rates for round two were quite higher the mean percent total agreement across all questions was and the lowest agreement score was for

0.7806 assigned to Then the participants were asked to choose their own or the models prediction as their final prediction Finally a feedback message was shown about whether the participant and the model were correct In the with-prediction conditions if the participants own prediction agreed with the AIs prediction we automatically took that prediction as the final prediction A -second count down was imposed on each trial before the prediction submission button was enabled encouraging participants to pay more attention in each decision After the task trials participants completed a demographic survey As discussed participants received a base pay of in addition to the performance-based bonus payment plus cents if correct and minus cents if wrong On average each participant received bonus and a total of compensation for completing the half-hour long experiment Results Trust Prior work suggests that subjective self-reported trust may not be a reliable indicator for trusting behaviors which are what ultimately matter in AI-assisted decision tasks Therefore following recent studies we measured participants trust in the AI by two behavioral indicators Switch percentage the percentage of trials in which the participant decided to use the AIs prediction as their final prediction In conditions where the AIs prediction

0.7738 g e no info confidence explanation Figure Switch percentage across confidence levels and model information conditions AI confidence level A g re e m e n t P e rc e n ta g e no info confidence explanation Figure Agreement percentage across confidence levels and model information conditions information conditions p If anything there was a reverse trend of decreasing the AI-assisted accuracy by showing explanation was thus also rejected Taken together the results suggest a lack of effect of local explanations on improving trust calibration and AI-assisted prediction Our results appeared to contradict conclusions in Lai and Tans study which showed that explanations could improve peoples trust and the joint decision outcome But a closer look at Lai and Tans results revealed a trend of indiscriminatory increase in trust willingness to accept whether the AI made correct or incorrect predictions suggesting similar conclusion that explanations are ineffective for trust calibration However since in their study the AI outperformed human by a large margin this indiscriminatory increase in trust improved the overall decision outcome It is no info confidence explanation A I A s s is te d P re d ti o n A c c u ra

0.7594 labels our participants trust correct machine predictions more than incorrect ones However the difference is statistically significant only in predicted label w/ accuracy p and predicted label w/ heatmap random p These results suggest that humans can somewhat differentiate correct machine predictions from incorrect ones Further evidence is required to fully understanding the reasons why humans dont trust incorrect machine predictions Such understandings can improve both machine learning models and their presentations to support human decision making Heterogeneity in Human Perception and Performance We finally discuss the heterogeneity between participants in our study Here we focus on the participants estimation of their own performance and gender differences Refer to the appendix for additional comparisons Human estimation of their own performance Figure aWe ask participants to estimate their own performance in our exit survey Our results are not exactly aligned with the previous finding that humans tend to overestimate their capacity of detecting lying In fact of the participants correctly predicted their performance Among the remaining overestimated their performance while underestimated their performance Figure a shows the breakdown for three experimental setups In general it seems difficult for humans to estimate their performance One participant who overestimated his performance estimated but

0.7591 machine predictions more than the incorrect ones in all the five experimental setups although the differences are only statistically significant in two setups showing predicted labels is crucial for improving human performance Featured-based explanations coupled with predicted labels are able to induce similar human performance as an explicit statement of strong machine accuracy As such adding feature-based explanations to predicted labels may be more ideal than suggesting strong machine performance as the priming is weaker and may facilitate a higher level of human agency in decision making Explanations alone slightly improve human performance Figure a As Figure a shows human performance in control is no better than chance This finding is consistent with Ott et al and decades of research on deception detection Explanations alone slightly improve human performance over control and the differences are statistically significant for highlight and heatmap not for examples However the best explanations heatmap is not statistically significantly different from highlight p or examples p As a result our findings partially supports Hypothesis a and rejects Hypothesis b These findings suggest that it is difficult for humans to understand explanations on their own This is plausible for example-based explanations since it requires extra cognitive burden

0.7535 this case We did not record what kind of inter-annotator metric was used such as Cohens kappa or Krippendorffs alpha but many different metrics were used We also did not record what the exact statistic was although we did notice a wide variation in what was considered an acceptable or unacceptable score for inter-annotator agreement Table Multiple annotator overlap Count Proportion No information Yes for all items Yes for some items No Table Reported inter-annotator agreement Count Proportion Yes No For multiple annotator overlap table shows that just under half of all papers that involved an original human annotation task did not provide explicit information one way or the other about whether multiple annotators reviewed each item This includes the one paper that reported inter-annotator agreement metrics but did not specify whether overlap was for all items or some items Only three papers explicitly stated that there was no overlap among annotators and so it is quite likely that the papers that did not specify such information did not engage in such a practice For the papers that did involve some kind of multiple annotator overlap the overwhelming majority of this subsample involved multiple annotation of all items rather than

0.7438 task AI had an advantage over the humans but not by much This is in contrast to where the humans performed substantially worse than the AI by After confirming that displaying confidence both improved overall trust and helped calibrate trust with confidence levels we investigated whether this translated to improvement in the accuracy of the AI-assisted predictions Figure shows this AI-assisted accuracy w/o prediction w/ prediction A I A s s is te d P re d ti o n A c c u ra c y model with all variables model w/o marital status w/o confidence w/ confidence Figure Accuracy of the human and AI-assisted predictions across conditions across conditions It suggests that there was no significant difference in AI-assisted accuracy across the prediction and confidence conditions Indeed an ANOVA showed that only the AI confidence level p and its interaction with model completeness p had significant effect Furthermore we also analyzed the difference between AI-assisted accuracy and AI accuracy and none of the factors showed significant effects We originally expected that the AI-assisted prediction ie human-AI joint decision would be more accurate than the AI alone when the AI confidence was low but that did not turn out

0.7438 under number STUDY_ to perform the study and also obtained students consent on using their anonymized data Study Design and Implementation During COVID- class meetings were held remotely via Zoom Students were asked to complete a -step activity using the Value Cards Toolkit during one class meeting All students were assigned to teams and each team was assigned to one of the four conditions The detailed instructions and the Values Cards toolkit were given to the students in digital version through Canvas The study has the following five steps Students complete an individual quiz Students read Model Cards and make individual model selections Students are assigned to teams of and have team discussions via Zoom breakout rooms around The students in a team collaboratively write a short proposal Students complete a post-survey individually and the instructors hold a subsequent reflection session to collect feedback Students complete a post-quiz days after the initial in-class activity Students were given the Model Cards at Step the Persona Cards at Step if applicable and the Checklist Cards at Step if applicable See Table for details on conditions and random assignments Figure Five Steps of Study Design Condition Names Condition Details Room Assignments Baseline Model

0.7432 FP-Outcome-Maj condition includes the model Outcome FP Maj and the model Outcome FP Maj When we quote participants we identify them by participant number and condition trade-off Quantitative Analysis We mapped answers on five-point Likert scales to For example participants could rate whether they would definitely or probably prefer a model that they were unsure or that they would probably or definitely prefer a human judge For such answers we tested whether participants answers tended toward one answer model the other human judge or neither We did so using the unpaired Wilcoxon Signed-Rank test which tests whether a distribution is skewed around zero Significance indicates answers tended toward one answer or the other For questions where participants individually rated each model eg on fairness we tested whether they tended to rate one model higher than the other As each participant rated both models the data was not independent We thus used the paired Wilcoxon Signed-Rank test which measures whether the distribution of differences between pairs of ratings is symmetric Significance indicates one model was seen as more fair biased or useful than the other For each trade-off pair eg FP-Outcomes some participants saw a version where White defendants were disadvantaged

0.7427 when White defendants were favored see Figure Differences based on which group was disadvantaged are discussed further in Section Like bias ratings fairness ratings were polarized of participants rated at least one of the two models they saw as either completely fair or not at all fair We also tested for differences in perceived fairness between models in a pair The model equalizing false positives was rated as more fair than the model equalizing accuracy when Whites were Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Z p Perceptions of Bias Perceptions of Fairness Maj Outcome Race not used Min Maj Outcome Race used Min Maj Acc Race not used Min Maj Acc Outcome Min Maj Acc Race used Min Maj Acc Outcome Min Maj FP Race not used Min Maj FP Outcome Min Maj FP Acc Min Maj FP Race used Min Maj FP Outcome Min Maj FP Acc Min Completely biased Mostly biased Dont know Somewhat biased Not at all biased Not at all fair Somewhat fair Dont know Mostly fair Completely fair Z

0.7354 in all three cases in Scenario we observed significant statistical differences p with the post-hoc test revealing that participants felt that the information provided in Case B was perceived as sufficient indicated sufficient information provided in Case B compared to in Case A and in Case C In all scenarios we did not find any differences in the participants responses between self-reported gender groups Previous training and self reported knowledge on topics related to algorithmic decision making did not have an impact on the responses of participants in our sample Differences between Undergraduate and Postgraduate Participants Since this study ran with undergraduate and postgraduate students in fields adjacent to algorithmic development it is natural to examine whether the participants level of education made a difference in their responses A series of Mann-Whitney U tests were run to determine if there were differences between the two groups Distributions of the engagement scores for undergraduates and postgraduates were similar as assessed by visual inspection in all cases Firstly we wanted to examine whether there is a difference between undergraduates and postgraduates in understanding the process by which the decision was made Scenario was the only scenario where statistical significant difference in understanding

0.728 use of emojis As table shows we found a diversity of approaches to the recruitment of human annotators The plurality of papers involved the papers authors doing the annotation work themselves The next highest category was no information which was found in almost a quarter of the papers using original human annotation Experts professionals was far higher than we expected although we took any claim of expertise for granted Crowdworkers constituted a far smaller proportion than we expected with Amazon Mechanical Turk and other platforms collectively comprising about of papers Almost all of the other crowdworking platforms specified were CrowdFlower/FigureEight with one paper using oDesk Table Original human annotation source Count Proportion Papers authors No information Experts professionals Amazon Mechanical Turk Other crowdwork Other Number of human annotators Our instructions for the question about the number of human annotators was not precise and had one of the lower levels of inter-rater reliability If the paper included information about the number of human annotators the instructions were to put such a number leaving the field blank for no information Most of the disagreement was from differences around how papers report the number of annotators used For example some papers specified the

0.7252 improvement over control These results are consistent with Hypothesis The big performance gap between showing predicted labels and showing feature example-based explanations alone suggests that when humans interact with machine learning models it makes a significant difference whether predicted labels are shown However this observation also echoes with concerns about humans overly relying on machines To further understand human performance with predicted labels we examine all experimental setups with predicted labels in Figure b Although showing predicted labels seems necessary for achieving sizable human performance improvement the effect of presenting machine accuracy can be moderated by showing feature example-based explanations We find that predicted label examples and predicted label heatmap outperform predicted label w/o accuracy and vs without presenting the machine accuracy In this case we observe that heatmap is more effective than examples and leads to comparable human performance with predicted label w/ accuracy There is still a gap between the best human performance predicted label w/ accuracy and machine performance vs These observations suggest that humans do not necessarily trust machine predictions Trust We further examine the levels of trust that humans place on machine predictions when predicted labels are available Since machine performance surpasses human performance in

0.7205 outcome In fact observing the results reported by correct versus incorrect model decisions all these features increased participants willingness to accept the AIs prediction regardless of its correctness which is evidence that they are ineffective in calibrating trust EXPERIMENT EFFECT OF SHOWING AI CONFIDENCE SCORE In the first experiment we tested the following hypotheses with a case study of AI-assisted prediction task Hypothesis Showing AI confidence score improves trust calibration in AI such that people trust the AI more in cases where the AI has higher confidence Hypothesis Showing AI confidence score improves accuracy of AI-assisted predictions is based on the assumption that if holds then humans may be able to adopt the AIs recommendation at the right time and avoid following wrong recommendations In addition we also explored the following research questions Research Question How does showing AIs prediction versus not showing affect trust accuracy of AI-assisted predictions and the effect of confidence score on trust calibration While the former is a common AI-assisted decision-making scenario where the AI gives direct recommendations the latter represents a scenario where the human has to make blind delegation to the AI without seeing its output Blind delegation can happen in realworld scenarios

0.7198 to be true The fact that showing confidence improved trust and trust calibration but failed to improve the AI-assisted accuracy is puzzling and it rejects our This phenomenon could be explained by the correlation between model decision uncertainty and human decision uncertainty because trials where the model prediction had low confidence were also more challenging for humans This can be seen in Figure that the humans were less accurate than AI across all confidence levels although the difference is smaller in the low confidence trials Therefore even though showing confidence encouraged participants to trust the AI more in high-confidence zone the number of trials in which the human and the AI disagreed in these cases were low to begin with while in the low-confidence zone humans predictions were not better substitutes for AIs A caveat to interpret the results here is that if the correlation between human and model uncertainty decreases for example if the human expert and the model each has a unique set of knowledge it is possible that better calibration of trust with the model certainty could lead to improved AI-assisted decisions In summary results of Experiment showed that displaying confidence score improved trust calibration supported and

0.7195 displayed A four factor ANOVA confidence prediction model completeness model confidence level confirmed that the main effect of showing confidence scores was significant p The other two factors prediction and model completeness did not have any significant main effect or interaction partially answering and As can be seen in Figure showing prediction did not affect switch percentages significantly p The insignificant effect of model completeness p suggests that participants did not distrust the partial model Given that the two models had similar accuracy participants acted rationally Figure further examines how showing confidence calibrated trust for cases of different confidence levels The figure shows that when the AIs confidence level was between and there was not much difference between with- and without-confidence conditions In fact participants seemed to trust the model less w/o prediction w/ prediction S it c P e rc e n ta g e w/o confidence w/ confidence Figure Switch percentage measured as how often participants chose the AIs predictions as their final predictions across confidence and prediction conditions The dots indicate the mean percentages All error bars in this and subsequent graphs show one standard error when AI confidence was shown and was less than But when

0.7189 used external human annotation which we discuss later We are confident about our labeling process especially because these individual ratings were followed by an expert-adjudicated discussion-based reconciliation process rather than simply counting majority votes We detail more information and reflection about interrater reliability in appendix section Raw and normalized information scores We quantified the information about training data in papers developing a raw and normalized information score as different studies demanded different levels of information For example our question about whether inter-annotator agreement metrics were reported is only applicable for papers involving multiple annotators Our questions about whether prescreening was used for crowdwork platforms or whether crowdworker compensation was reported is only relevant for projects using crowdworkers However some kinds of information are relevant to all papers that involve original human annotation who the annotators are annotation source annotator training formal instructions or definitions were given the number of annotators involved whether multiple annotators examined the same items or a link to a publicly-available dataset For raw scores papers involving original human annotation received one point each for reporting the six items mentioned above In addition they received one point per question if they included information for each of the

0.7147 -Maj while others saw a version where African-American defendants were disadvantaged -Min We compared the -Maj and the -Min versions of each pair using the Mann-Whitney U test a non-parametric analog of the ANOVA test for comparing two groups For each of the above families of tests we corrected for multiple testing with the Benjamini-Hochberg method As both fairness and bias ratings were ordinal Likert-scale data we calculated the correlation of these ratings with Kendalls Qualitative Analysis We thematically coded free-response explanations participants gave for their choice of one model over another their preference for a human judge over either model and their ratings of fairness and bias Two coders collaboratively developed a codebook from a sample of answers Two coders then independently used that codebook to code the remaining answers We allowed multiple codes per answer to capture the compositionality of responses We created three distinct codebooks The first was for explanations of bias It contained ten high-level codes six of which had sub-codes For this codebook Cohens The second was for why a participant chose one model over the other It had seven codes no sub-codes and The last codebook assessed explanations of why humans or models were

0.7146 to assign a better reputation to universities that adopt an AI-driven admission system In addition distributive ADM fairness perceptions strongly predict a higher reputation of a university deploying AI-driven admission systems Thus if one considers admission through ADM as fair the overall reputation of the university increases Interestingly the perception of procedural ADM fairness has no significant effect on reputation For the universitys reputation whether the admission process is biased or unbiased seems irrelevant as long as the output of the decision is considered just The findings therefore offer partial support for Table OLS regression with organizational reputation Organizational Reputation b SE Step Intercept Gender male Age Faculty Graduation status Semester count HDM DF HDM PF Step Intercept Gender male Age Faculty Graduation status Semester HDM DF HDM PF ADM DF ADM PF Note adj R p p p DISCUSSION AND CONCLUSION This study shows that the use of ADM systems at universities has ambivalent consequences for the institution concerned The expected consequences depend precisely on whether those who are affected perceive such systems as fair or biased Our results suggest that perceptions of distributive fairness and procedural fairness negatively affect the intention to protest ADM usage for admission to

0.7124 no significant effect on intention to exit Thus it is not the experience of an unfair result that deters applicants Instead the mere fact that an ADM system makes the decision is sufficient to persuade potential applicants to move to another university Therefore our data partially support owing to the significant effect of the procedural ADM fairness dimension but not the distributive ADM fairness dimension Overall the second model explains of the variance of the dependent variable whereas the lions share is explained by the ADM-specific fairness variables Table depicts the regression coefficients for the predictors of the dependent variable voice Age and faculty membership both have a significant effect on voice Thus older students are somewhat more sympathetic to protesting in the case of an ADM system declining their university application We further find that studying in the mathematics or natural sciences department decreases the intention to protest the ADM system Table OLS regression with voice Voice B SE Step Intercept Gender male Age Faculty Graduation status Semester count HDM DF HDM PF Step Intercept Gender male Age Faculty Graduation status Semester HDM DF HDM PF ADM DF ADM PF Note adj R p p Including the ADM-specific variables

0.706 for Outcome-RaceUsage-Min Furthermore these same conditions also had the highest proportion of people who strongly favored the model over a judge and for -Maj and -Min respectively That Definitely model Probably model Unsure Probably human Definitely human Maj Race usage Outcomes Min Maj Acc Race usage Min Maj Acc Outcomes Min Maj FP Race usage Min Maj FP Outcomes Min Maj FP Acc Min Z p Z p Z p Z p Z p Z p Z p Z p Figure Participants preferences for a human judge versus either model seen human judges were fallible and models were more objective appeared in of participants free-text justifications in these cases where the participant preferred a model over a judge For example P Outcome-RaceUsage-Maj wrote Human beings are biased and easily manipulated Similarly P Outcome-RaceUsage-Min wrote Models lack bias and personal experiences that they will not be able to use to make decisions However being able to use individualized judgment was also sometimes expressed positively In contrast participants expressed a number of reasons for preferring a human judge of justifications mentioned a judges ability to make individualized case-by-case decisions For example P FP-Acc-Maj wrote I would use a human judge because I

0.7047 to undercover evidence and insights of student learning along with the three objectives FINDINGS In general our class activity suggests that the use of the Value Cards toolkit is promising in achieving the three learning objectives outlined in Table However our results also suggest that there are a number of pros and cons that need to be considered when using the different variants of the Value Cards approach Below we report our findings through the quantitative analysis comparing the two quizzes and the qualitative analysis aggregating the deliberation transcripts group proposal in-class reflection and open-ended questions in the quizzes and post-survey We also present students critiques towards the different variants of the toolkit especially the Persona Cards and the Checklist Cards We use R to represent discussion room and P to identify specific participants We dont include IDs for anonymous surveys and in-class reflection Achieving Learning Objectives Students improved their understandings of the technical definitions and trade-offs of performance metrics in machine learning and were able to apply them in real-world contexts Objective We used pre- and post-quizzes to measure the learning effects on conceptual knowledge As a reminder to our readers the pre-quiz Figure Pre- and Post-Activity Quiz Scores

0.7028 model see Figure We found no statistically significant difference test in initial attitudes towards the model which allows us to postulate that any difference discovered between the two groups is a result of the treatment they were given ie MC-BRP explanation vs no explanation Figure shows the distributions of answers to the four subjective questions in the treatment and control groups The difference in distributions is significant for users in the treatment group agree with the statement more than users in the control group However we find no statistically significant difference between the two groups for the remaining questions test That is MC-BRP explanations help users understand why the model makes large errors in predictions but do not have an impact on users trust or confidence in the model or on their willingness to support its deployment DISCUSSION Since our original motivation was to provide an explanation system that can be used by analysts at Ahold Delhaize we conducted a more in-depth analysis of the results to determine if there was a difference in attitudes between users depending on their background eg Practitioners from Ahold Delhaize or Researchers from the University of Amsterdam Comparing attitudes conditioned on background Table shows

0.7017 or deceptive In other words humans are asked to perform the same task as the machine on the test set We follow a between-subject design each turker is assigned a level of machine assistance along the spectrum Figure and labels reviews after going through three training examples and correctly answering an attention-check question To incentivize turkers to perform at their best we provide bonus for each correct prediction in addition to the cent base rate for a review We also solicit our participants estimation of their own performance and basic demographic information such as gender and education background through an exit survey We only allow a turker to participate in the study once to guarantee sample independence across experimental a Heatmap without showing predicted labels an instance of feature-based explanations b Predicted label with accuracy c Predicted label heatmap without accuracy Figure Example interfaces with varying levels of machine assistance Figure a only presents feature-based explanations of machine predictions in the form of heatmap Figure b shows both the predicted label and an explicit statement about machine accuracy Figure c shows the predicted label with heatmap but does not present machine accuracy We crop the Genuine and Deceptive buttons in

0.6993 control by a wide margin in this task higher levels of trust are correlated with higher levels of accuracy in our experiments However these two metrics capture different dimensions of human predictions because trust is tied to machine predictions This becomes clear when we break down human trust by whether machine predictions are correct or not We find that humans tend to trust correct machine predictions more than incorrect ones which suggests that humans can somewhat effectively identify cases where machines are wrong It is important to emphasize that our focus is on understanding how Percentage of participants Predicted label heatmap Predicted label w/o accuracy Heatmap Under estimate Correct estimate Over estimate a Human estimation of their own performance Accuracy Female Male Useful Not useful p p b Gender and hint usefulness in predicted label heatmap Figure Heterogeneity findings among participants in our study Figure a shows performance estimation by participants in three different experimental setups Figure b presents the performance of participants in predicted label heatmap group by two variables hint usefulness and gender human trust varies along the spectrum rather than manipulating the trust of humans in machines Feature example-based explanations increase the trust that humans place on

0.6963 dependent t-test to determine differences between ADM and HDM with respect to procedural justice On average participants viewed the AI-driven process as fairer M SE than the HDM process M SE t p r This result supports Hb Addressing hypotheses we ran three multiple ordinary least squares OLS regression models We entered the predictors in a two-step process First we tested the effects of the demo graphic and study-specific variables and the distributive and procedural fairness perceptions of HDM In the second step we entered the perceptions of distributive and procedural fairness concerning ADM Thus we account for the proportion of explained variance in the models Tables depict the results The first regression model explains of the variance in the dependent variable exit meaning that the explanatory power of the model is rather low Table depicts the predictors influence on the dependent variable We see in block that faculty membership exhibits a significant negative effect The perception of procedural fairness of HDM also exerts a significant negative effect Table OLS regression with exit Exit b SE Step Intercept Gender male Age Faculty Graduation status Semester count HDM DF HDM PF Step Intercept Gender male Age Faculty Graduation status Semester HDM

0.6934 external or both Tables and show the breakdown for both questions We only answered the subsequent questions about the human annotation process for the papers producing an original human annotated dataset Table Used original human annotation Count Proportion Yes No Unsure Table Used external human annotation data Count Proportion No Yes Unsure Original human annotation source Our next question asked who the annotators were for the papers that used original human annotation The possible options were the papers authors Amazon Mechanical Turk other crowdworking platforms experts/professionals other and no information We took phrases like we labeled with no other details to be an implicit declaration that the papers authors did the labeling If the paper discussed labelers qualifications for the task beyond an average person we labeled it as experts professionals For example some of our boundary cases involved recruiting students to label sentiment One study involved labeling tweets with both English and Hindi text and noted that the students were fluent in both languages which we considered to be in the experts professionals category Another paper we included in this category recruited students to label tweets with emojis noting that the recruited students are knowledgeable with the context of

0.6901 the tutorial and included a question about the reward structure in the comprehension test to ensure that they understood RESULTS We conducted trials on Mechanical Turk over the course of a week in June in batches over weekdays and weekend days at times ranging from morning to evening to account for variations in the population of Turk workers workers completed the experiment we excluded all data from participants who failed at least one of the attention check questions or who required more than three attempts to pass the comprehension test This process yielded a population of participants Table A The participants were male and white and the majority have completed at least a college degree We asked participants to selfreport their familiarity with machine learning and the US criminal justice system on a scale from Not at all to Extremely During the exit surveys participants reported that the experiment paid well was clear and was enjoyable Participants earned an average bonus of median making the average total payment Participants completed the task in an average of minutes median and earned an average wage of per hour median Out of participants who responded to a free text question in the exit

0.6891 learned the coding schema and the reconciliation process which were further refined Second round verification and reconciliation After papers were labeled by five annotators we conducted a second round of verification This was necessary both because there were some disagreements in labeling and changes made to the coding schema discussed in appendix All labels for all papers were independently re-examined by at least two of the six team members Annotators were given a summary of the original labels in the first round and were instructed to review all papers being mindful of how the schema and instructions had changed We then aggregated reconciled and verified labels in the same way as in the first round For papers where there was no substantive disagreement on any question between those who re-examined it in the second round the papers labels were considered to be final For papers where there was any substantive disagreement on any question the paper was either discussed to consensus in the same manner as in the first round or decided by the team leader The final schema and instructions are in the appendix section Finally we cleaned up issues with labels around implicit or blank values using rule-based

0.6887 to feature example-based explanations alone Assuming that humans trust the machine and follow its prediction showing predicted labels can likely improve human performance because the machine accuracy is However showing predicted labels reduces human agency so it is important to understand the size of the performance gap and make informed design choices Hypothesis By combining predicted labels and feature example-based explanations the trust that humans place on machine predictions increases as it has been shown that concrete details can influence the level of trust in general automation We evaluate the above hypotheses using two metrics accuracy and trust Accuracy is defined as the percentage of correctly predicted instances by humans trust is defined as the percentage of instances for which humans follow the machine prediction Note that we can only compute trust when predicted labels are available RESULTS In this section we investigate how varying levels of assistance from machine learning models along the spectrum in Figure affect human predictions We first discuss aggregate human performance using human accuracy and trust Our results show that in this challenging task explanations alone slightly improve human performance while showing predicted labels can significantly improve human performance When predicted labels are shown we

0.6863 be more effective for conveying uncertainty than stating the probability CONCLUDING DISCUSSION In this paper we conduct the first empirical study to investigate whether machine predictions and their explanations can improve human performance in challenging tasks such as deception detection We propose a spectrum between full human agency and full automation and design machine assistance with varying levels of priming along the spectrum We find that explanations alone improve human performance while showing predicted labels significantly improves human performance Adding an explicit statement of strong machine performance can further improve human performance Our results demonstrate a tradeoff between human performance and human agency and explaining machine predictions may moderate this tradeoff We find interesting results regarding the trust that humans place on machine predictions On the one hand humans tend to trust correct machine predictions more than incorrect ones which indicates that it is possible to improve human decision making while retaining human agency On the other hand we show that human trust can be easily enhanced by adding random heatmap as explanations or statements of low accuracies that do not justify trusting machine predictions In other words additional details including irrelevant ones can improve the trust that humans place

0.6852 students submitted the individual model selection students submitted their post-survey Data Analysis We adopted a mixed-methods approach for data analysis First we performed a quantitative analysis on students learning gains from pre- to post- quizzes their change on metrics consideration after the activity and their understanding of diverse perspectives as shown by the multiple-choice questions in the post-survey Second we performed a qualitative analysis on the group deliberation openended responses in post-survey and in-class reflection and the group proposals to give a richer and more comprehensive view of student learning and their demonstration of understanding Quantitative Analysis The main outcome variables are the quiz scores and metrics selections ie their responses to the question of For any algorithmic decision-making system what metrics below do you think are the most important ones to consider in tuning the algorithm Note that for each participant we collected their quiz scores and the numbers of the metrics they chose twice before and after the Value Card activity This allows us to evaluate whether the Value Card activity helped improve students understanding of the concepts and reduce the reliance on single performance metrics in the development of machine learning algorithms learning objective and We conducted

---------------------------------------------------------------------------
TOPIC 1: accountability

---------------------------------------------------------------------------

0.871 is likely to be owed ie how to present information so as to be understandable It is thus necessary to understand transparency contextually and holistically across the ADM process to facilitate different forms of accountability as appropriate To achieve that a more systematic approach to providing useful transparency that facilitates meaningful accountability is needed as we now describe REVIEWABILITY To support meaningful accountability we argue that ADM processes should be designed and developed to be reviewable Reviewability as a general concept involves technical and organisational record-keeping and logging mechanisms that expose the contextually appropriate information needed to assess algorithmic systems their context and their outputs for legal compliance whether they are functioning within expected or desired parameters or any other form of assessment relevant to various accountability relationships In the context of ADM reviewability seeks to provide a holistic view of the technical and organisational elements involved in producing an automated decision considering factors both at design time and at runtime The commission design deployment and use of ADM processes as well as the consequences of use and auditing and investigation of those processes are all within scope of reviewability see Fig This approach is derived from English administrative law

0.8257 across the whole process is important given that accountability relationships and thus what it means to be accountable will differ depending on from and to whom an account is owed Various actors at different stages across the same process may need to account to multiple forums developers regulators courts subjects of decisions and so on Implementing reviewability with a systematic evaluation of what is contextually appropriate for various aspects of the decision-making process helps ensure that relevant accurate proportionate and comprehensible information is available to provide an account By systematically implementing targeted technical and organisational record-keeping and logging mechanisms to enable the provision of contextually appropriate information about the process as a whole reviewability thus supports meaningful accountability relationships between the multiple actors involved in the ADM process and various relevant forums A FRAMEWORK FOR REVIEWABLE ADM Reviewability offers a systematic approach to useful transparency by breaking down the ADM process into stages from the systems conception through to consequences and scrutiny each consisting of several steps see Table These steps and stages can be considered discretely and together underpinning a framework for developing and assessing reviewable ADM processes At each stage there are opportunities to i place limits on

0.7538 sustains an accountability relationship and have been widely established in domains analogous to algorithmic systems Following a definition offered by Mark Bovens accountability in general requires an actor who submits an often technical account of the impact of a proposed/implemented system a forum that evaluates this account and can propose changes in the systems implementation based on the evaluation of its impact the structured relationship between the two the content and criteria for accounting impact and the consequences arising from the account Drawing on Bovens Maranke Wieringa has developed a definition of algorithmic accountability Algorithmic accountability concerns a networked account for a socio-technical algorithmic system following the various stages of the systems lifecycle In this accountability relationship multiple actors eg decision makers developers users have the obligation to explain and justify their use design and/or decisions of/concerning the system and the subsequent effects of that conduct As different kinds of actors are in play during the life of the system they may be held to account by various types of fora eg internal/external to the organization formal/informal either for particular aspects of the system ie a modular account or for the entirety of the system ie an integral account Such

0.7362 ethical questions can be raised by what I have learned Why Design What have I designed for you Which of your goals have I designed the system to support In what situations/contexts do I intend/accept you will use the system to achieve each goal Why How should you use the system to achieve each goal according to my design For what purposes do I not want you to use the system What ethical principles influenced my design decisions How is the system I designed for you aligned with those ethical considerations Prototyping implementation and formative evaluation How have I built the system to support my design vision What have I built into the system to prevent undesirable uses and consequences What have I built into the system to help identify and remedy unanticipated negative effects What ethical scenarios have I used to evaluate the system Continuous post-deployment evaluation and monitoring How much of my vision is reflected in the systems actual use What unanticipated uses have been made By whom Why What anticipated and unanticipated effects have resulted from its use Whom do they affect Why What ethical issues need to be handled through system redesign redevelopment policy or even

0.7218 accountability to elected representatives and so on legal accountability to courts administrative accountability to auditors and regulators professional accountability to internal and external peers and social accountability to civil society and individuals Opacity in algorithmic systems Achieving meaningful accountability of ADM is difficult Commercial considerations and complex decision-making process involving ML often entail considerable opacity Burrell identifies three forms of algorithmic opacity Type of opacity Cause Intentional opacity Details of processes concealed for commercial reasons Illiterate opacity Processes incomprehensible without technological literacy Intrinsic opacity Incompatibility between machine and human reasoning Unwitting opacity Unawareness of relevance of broader processes for accountability Strategic opacity Process information deliberately presented in an inaccessible way Inadvertent opacity Information unintentionally presented in an inaccessible way Table Some types of opacity in algorithmic systems while Burrell refers primarily to model opacity these also relate to broader ADM processes That is the details of proprietary datasets models systems and processes can be deliberately concealed to protect commercial interests what Danaher calls intentional opacity Details of ADM processes may be incomprehensible without relevant technical knowledge illiterate opacity And the mismatch between the complex mathematical nature of ML and human forms of reasoning makes models themselves difficult for even the

0.7174 base to understand trust in general and we augment this further by integrating Siaus perspective on trust in technology which identifies that trust is impacted by Human Environmental and Technological qualities referred to as the technologies HET qualities in what follows We will discuss these steps to go from the ABI model to HET qualities of trustworthy technologies in Section To summarise the contributions of this paper are as follows We draw on social science literature particularly from organisational science to apply established principles of trust to examine the qualities for technologies to support trust in AI-based systems primarily based on the ABI and ABI framework and the HET qualities We identify how trust can be enhanced in the various stages of an AI-based systems life-cycle specifically the design development and deployment stages We therefore introduce the concept of an AI Chain of Trust to discuss the various stages and their interrelations We introduce a FEAS Fairness Explainability Auditability Safety classification of machine learning technologies that support and enable trust and establish the relation between these trust-enhancing technologies and the HET qualities We discuss how our technology classification and trustworthy machine learning techniques relate to various Principled AI framework considered

0.715 a property of a tool in a context of use We can model this by considering traceability as a property of a system a set of elements which interact to produce aggregated behavior by virtue of those interactions Although a tool such as a piece of software can support traceability because traceability relates the specifics of the tools design to the effects of its use the tool cannot on its own be said to be traceable Traceability is a functional property of the tool in use and is poorly defined without this context ADOPTION OF TRACEABILITY Traceability has been adopted as an explicit principle either by organizations as a principle for the responsible use of technology or as policy guidance in official documents For example the United States Presidents Executive Order gives nine principles for the development of trustworthy AI requires that agencies of the US Federal Government must make AI systems Responsible and Traceable when designing developing acquiring and using AI Specifically that principle states Responsible and traceable Agencies shall ensure that human roles and responsibilities are clearly defined understood and appropriately assigned for the design development acquisition and use of AI Agencies shall ensure that AI is used

0.7137 Formalizing Trust in Artificial Intelligence Prerequisites Causes and Goals of Human Trust in AI Trust is a central component of the interaction between people and AI in that incorrect levels of trust may cause misuse abuse or disuse of the technology But what precisely is the nature of trust in AI What are the prerequisites and goals of the cognitive mechanism of trust and how can we promote them or assess whether they are being satisfied in a given interaction This work aims to answer these questions We discuss a model of trust inspired by but not identical to interpersonal trust ie trust between people as defined by sociologists This model rests on two key properties the vulnerability of the user and the ability to anticipate the impact of the AI models decisions We incorporate a formalization of contractual trust such that trust between a user and an AI model is trust that some implicit or explicit contract will hold and a formalization of trustworthiness that detaches from the notion of trustworthiness in sociology and with it concepts of warranted and unwarranted trust We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior and discuss how

0.7077 user trusts the AI model anticipation depends on whether the model is able to carry out its contract This perspective distinguishes trust an attitude of the trustor from being trustworthy a property of the trustee and we say that an AI model is trustworthy to some contract if it is capable of maintaining this contract Trust and trustworthiness are entirely disentangled pursuing one does not entail pursuing the other and trustworthiness is not a prerequisite for trust in that trust can exist in a model that is not trustworthy and a trustworthy model does not necessarily gain trust We say that the trust is warranted if it is the result of trustworthiness and otherwise it is unwarranted Warranted trust is sometimes referred to as trust that is calibrated with trustworthiness In other words trust is the cognitive mechanism to give the illusion of anticipating intended behavior which becomes reality when the trust is warranted and the trustor feels betrayed when the illusion is broken For example consider a user interacting with an AI model via some visual interface GUI and the user trusts the AI model to make a correct prediction on some task There is a correlative but not

0.7024 presence of risk is to anticipate when desired behavior will not happen This is the inverse of therefore XAI is a method of enabling distrust all corresponding to a stated contract The above is only relevant with respect to specific contracts that dictate what precisely is anticipated Therefore the contract must be explicitly recognized by the XAI methodology so that it reveals information that is relevant to create or reveal the capability of the AI to maintain the contract so that the user develops warranted trust or distrust in that contract For the user to achieve their goal of anticipation their trust should be warranted and XAI verifies this by revealing the intrinsic or extrinsic sources causes of the trust EVALUATING TRUST The last question that remains for us to discuss is on the evaluation of trust What should evaluation of trust satisfy Do current methods of evaluating trust fulfill these requirements Vulnerability in Trust and Trustworthiness The distinction of evaluation between trust and trustworthiness stems from their distinction on vulnerability trustworthiness does not require the trustor to accept risk to manifest For instance the evaluation of the accuracy of AI models may be used to increase external trust in

0.6944 or assessments of those systems fidelity to normative governance goals Traceability serves as a foundation for other goals in aligning the behavior of automated systems to human values and in governing those systems to make that alignment plain to anyone potentially affected or harmed by the operation of those systems or the social outcomes they drive Only by making systems traceable can we hold them accountable and ensure they comport with applicable contextually appropriate social political and legal norms

0.687 be avenues of further research within FAT yet they require a different kind of interdisciplinarity INVESTIGATING ALGORITHMIC ACCOUNTABILITY What this systematic literature review demonstrates is that we need to move to an accountability relationship not just of the use the design the implementation or the consequences of algorithmic systems but to consider the entirety of that socio-technical process While the term algorithmic accountability is inherently vague as it leaves a lot room for specification about the accountability relationship it can be specified as follows Algorithmic accountability concerns a networked account for a socio-technical algorithmic system following the various stages of the systems lifecycle In this accountability relationship multiple actors eg decision makers developers users have the obligation to explain and justify their use design and/or decisions of/concerning the system and the subsequent effects of that conduct As different kinds of actors are in play during the life of the system they may be held to account by various types of fora eg internal/external to the organization formal/informal either for particular aspects of the system ie a modular account or for the entirety of the system ie an integral account Such fora must be able to pose questions and pass judgement

0.6855 model-driven ADM process We also indicate some factors that might warrant consideration Commissioning Commissioning involves anything relevant to bringing the algorithmic system and its constituent parts into existence At this stage managers are the actors likely to be particularly relevant Problem definition ADM exists for a purpose Records relating to the aims of and rationale for the algorithmic system giving insight into the values and norms behind its commissioning development and operation are therefore relevant to various accountability relationships Documentation and other records of the systems aim scope and justification what it will do why it is required and the role it will play are worth considering Various impact assessment and procurement guidance documents reflect the need for clear specifications of these aspects see below Also likely relevant is information regarding the decision-making processes or systems that ADM will subsume or replace Information from business analysis or requirements engineering activities common for many organisational technology undertakings will often be pertinent as will any documents such as minutes from board meetings consultancy reports and so on that involve discussions or decisions about the nature of the proposed system Impact assessment These involve assessing the potential implications and risks of an ADM

0.6799 case that such audits can be leveraged to anticipate potential negative consequences before they occur in addition to providing decision support to design mitigations more clearly defining and monitoring potentially adverse outcomes and anticipating harmful feedback loops and system-level risks Executed by a dedicated team of organization employees internal audits operate within the product development context and can inform the ultimate decision to abandon the development of AI technology when the risks outweigh the benefits see Figure Inspired from the practices and artifacts of several disciplines we go further to develop SMACTR a defined internal audit framework meant to guide practical implementations Our framework strives to establish interdisciplinarity as a default in audit and engineering processes while providing the much needed structure to support the conscious development of AI systems GOVERNANCE ACCOUNTABILITY AND AUDITS We use accountability to mean the state of being responsible or answerable for a system its behavior and its potential impacts Although algorithms themselves cannot be held accountable as they are not moral or legal agents the organizations designing and deploying algorithms can through governance structures Proposed standard ISO defines this structure as the system by which the whole organization is directed controlled and held accountable

0.6744 The relationship between trust in AI and trustworthy machine learning technologies To design and develop AI-based systems that users and the larger public can justifiably trust one needs to understand how machine learning technologies impact trust To guide the design and implementation of trusted AI-based systems this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products We start from the ABI Ability Benevolence Integrity Predictability framework augmented with a recently proposed mapping of ABI on qualities of technologies that support trust We consider four categories of trustworthiness technologies for machine learning namely these for Fairness Explainability Auditability and Safety FEAS and discuss if and how these support the required qualities Moreover trust can be impacted throughout the life cycle of AI-based systems and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle In so doing we establish the ways in which machine learning technologies support trusted AI-based systems Finally FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international principled AI policy and technology frameworks that have emerged

0.6743 paper established the connection between trust as a notion within the social sciences and the set of technologies that are available for trustworthy machine learning More specifically we related the ABI framework and HET technology qualities for trust with categories of machine learning technologies that enhance trustworthiness We identified four categories of technologies that need to be considered Fair Explainable Auditable and Safe FEAS technologies These need to be considered in various interrelated stages of a system life cycle each stage forming part of a Chain of Trust The paper shows a close relationship between technologies to improve the trustworthiness of AI-based systems and those that are being pursued in ethical AI and related endeavours We illustrated this by mapping of FEAS technologies on concerns in a large set of international Principled AI policy and technology frameworks

0.6714 a post-deployment audit to pre-deployment audit applied throughout the development process enables proactive ethical intervention methods rather than simply informing reactive measures only implementable after deployment as is the case with a purely external approach Because there is an increased level of system access in an internal audit identified gaps in performance or processes can be mapped to sociotechnical considerations that should be addressed through joint efforts with product teams As the audit results can lead to ambiguous conclusions it is critical to identify key stakeholders and decision makers who can drive appropriate responses to audit outcomes Additionally with an internal audit because auditors are employees of the organization and communicate their findings primarily to an internal audience there is opportunity to leverage these audit outcomes for recommendations of structural organizational changes needed to make the entire engineering development process auditable and aligned with ethical standards Ultimately internal audits complement external accountability generating artifacts or transparent information that third parties can use for external auditing or even end-user communication Internal audits can thus enable review and scrutiny from additional stakeholders by enforcing transparency through stricter reporting requirements LESSONS FROM AUDITING PRACTICES IN OTHER INDUSTRIES Improving the governance of artificial intelligence

0.6662 tackling hindrances to a successful software implementation Together these two stages form the ex ante considerations of an algorithmic system for it is only after these stages that that which we tend to understand as software development ie coding implementation testing comes into the picture as part of the in medias res considerations Here we touch upon the SDLC stages of design implementation and testing/integration Design is about creating the architecture for the application Implementation is where the programming of the software happens generally this happens in a modular way that is programmers/teams each work on separate aspects of the system Testing and integration is where the separately produced aspects of the product are connected that is integrated The integrated whole is subsequently tested in vitro for errors bugs and other unforeseen issues Finally there is the maintenance stage where the product is deployed the software needs to maintained and the in vivo bugs needs to be resolved This stage also requires ongoing evaluations of the products quality and relevance It would be tempting to locate the ex post considerations solely with this last maintenance stage but there is in fact much more to it Many important decisions for instance

0.6634 to have been generated from the product development cycle It ensures that the full scope of expected product processes and that the corresponding documentation required to be completed before the audit review can begin are finished This is also a procedural evaluation of the development process for the system to ensure that appropriate actions were pursued throughout system development ahead of the evaluation of the final system outcome Artifacts Datasheets and Model Cards Two recent standards can be leveraged to create auditable documentation model cards and datasheets Both model cards and datasheets are important tools toward making algorithmic development and the algorithms themselves more auditable with the aim of anticipating risks and harms with using artificial intelligence systems Ideally these artifacts should be developed and/or collected by product stakeholders during the course of system development To clarify the intended use cases of artificial intelligence models and minimize their usage in contexts for which they are not well suited Mitchell et al recommend that released models be accompanied by documentation detailing their performance characteristics called a model card This should include information about how the model was built what assumptions were made during development and what type of model behavior might

0.6606 Algorithmic Design History File Inspired by the concept of the design history file from the medical device industry we propose an algorithmic design history file ADHF which would collect all the documentation from the activities outlined above related to the development of the algorithm It should point to the documents necessary to demonstrate that the product or model was developed in accordance with an organizations ethical values and that the benefits of the product outweigh any risks identified in the risk analysis process This design history file would form the basis of the final audit report which is a written evaluation by the organizations audit team The ADHF should assist with an audit trail enabling the reconstruction of key decisions and events during the development of the product The algorithmic report would then be a distillation and summary of the ADHF Artifact Algorithmic Audit Summary Report The report aggregates all key audit artifacts technical analyses and documentation putting this in one accessible location for review This audit report should be compared qualitatively and quantitatively to the expectations outlined in the given ethical objectives and any corresponding engineering requirements LIMITATIONS OF INTERNAL AUDITS Internal auditors necessarily share an organizational interest with

0.6601 how much humans understand technology remains an important research domain necessary to enable robust human-machine collaboration Traceability thus requires that systems be transparent not just about their function but about whether that function is appropriately communicated to operators and other affected humans This is also important in the context of failure analysis as many accidents result from inappropriate modeling of machines by human operators of humans by machines and their designers or at the point of handoff between human and machine Auditability Another component of the traceability principles as stated is that they support the auditability of systems both before they are fielded and during operation This has several meanings First systems must maintain sufficient records during development and operation that their creation can be reliably established and reproduced This requirement is largely encapsulated in the reproducibility and operational/developmental recordkeeping requirements listed above Beyond requiring that evidence of how a system operated be established auditability requires that this evidence be amenable to review and critique of the systems operation as well as comparison of the fidelity of that evidence to reality Such assessments can be qualitative or quantitative in nature and could happen during development or once a system is

0.659 trustworthiness of the AI increase the trust of the user in a trustworthy AI or increase the distrust of the user in a non-trustworthy AI all corresponding to a stated contract so that the user develops warranted trust or distrust in that contract Let us clarify this claim by unraveling it A key motivation of XAI and interpretability is to increase the trustworthiness of the AI AI is said to be trustworthy to a contract if it is capable of maintaining the contract Then XAI is a method of creating a capability by revealing the relevant signals in the AI reasoning process as in the example of the random baseline-like model An AI model that hides these signals would be less trustworthy as they fail to uphold some contracts eg Table increase the trust of the user in a trustworthy AI The goal of developing trust from the users perspective is to enable the ability to anticipate behavior in the presence of risk Then XAI is a method of allowing the user easier access to the signals that enable this anticipation or increase the distrust of the user in a non-trustworthy AI Similarly the users goal in distrust in the

0.6581 and reassessment of the process are also likely to be relevant to various accountability relationships Procurement In practice ADM will often involve some kind of procurement whether to obtain models or other technical components core to decision-making data for training technical components to support development or deployment service arrangements for outsourcing business workflows external consultancies for risk assessments and so on The nature of any procured product or service can influence the overall algorithmic system The importance of a robust procurement processes for algorithmic systems is well-recognised with a range of guidance regarding this eg Records relating to procurement will likely serve various accountability relationships This will often include details of contractual arrangements tender documents design specifications quality assurance measures and so on Details of suppliers including any due-diligence performed may also be relevant Any salient characteristics of what is being procured eg test or acceptance criteria are often best defined by the managers or documented as part of arrangements with the supplier There should also be suitable mechanisms in place for suppliers to be audited or compelled to provide information as required cf Where procurement entails service engagements eg outsourcing of business processes or engaging cloud or other infrastructure

0.6579 forward in bringing ethics and moral responsibility reasoning into the culture of software development EXTENDED METACOMMUNICATION TEMPLATE Most HCI design and development lifecycles present similar versions of the following stages analysis understanding user needs and defining requirements conceptual design prototyping and implementation and evaluation All models recognize the iterative nature of the design and development process so that every stage can be revisited as necessary The Semiotic Engineering metacommunication template can be segmented according to those stages In Figure we show the correspondence between the metacommunication template segments and a general lifecycle model The numbers above the stages match the numbers used in the next sections We have extended the Semiotic Engineerings metacommunication template by adding a list of explicit guiding questions designers should ask at each step of the design and development lifecycle The next subsection presents the guiding questions and how we propose to use them Guiding Questions The guiding questions can be classified into base questions and ethical questions The base questions are variations of questions considered in usual design lifecycles with the distinction that within semiotic engineering the designer adopts a st-person I my perspective when asking and answering them addressing the user as the nd

0.6579 and policy-level tools Many such tools exist but many more do not Rarely are these tools brought together in an assemblage that resembles anything like the traceability principles espoused by many organizations and summarized in Section or the requirements unpacked from these principles in Section And yet building robustly accountable computing systems requires embodying this principle in its full power In this section we summarize known tools and relate their capabilities and limitations to the requirements laid out in Section Our focus is primarily on technical tools here though as noted many nontechnical tools are also necessary It is likely this cross-functional nature of operationalizing traceability and indeed any ethical principle in technology that makes doing so such a challenge Development Methodology The history of system development especially software system development is littered with failures failures of the system to meet its stated requirements failures of the system to function once delivered or failures of the system to be developed to the point of operability at all These risks have long been recognized Brooks famously pointed out in based on experience developing IBMs System mainframe operating system that adding manpower to a late software project makes it later Early development

0.657 an organisation responds to such accidents can be equally or more important for determining the impact on trust Dietz and Gillespie have shown that scandals or crises which risk damaging the reputation of an organisation can also act as catalysts for culture change bringing about and reinforcing new ethical/trustworthy practice They are opportunities to forge new relationships with stakeholders positive or negative Technology solutions that continuously monitor and possibly transparently share data about service bias are trustworthy technologies that may assist in avoiding or mitigating the impact of trust failures The Chain of Trust denotes the stages in which one can and should consider the trust qualities of machine learning technologies both for initial trust and continuous trust The chain of Trust also identifies the opportunities to maintain the trustworthiness of the system given the evolving nature of the relationship between the system and its users Finally the Chain of Trust provides guidance for experts and the public how and when to evaluate the trustworthiness of the system in relation to the ABI framework not only in the final outcome of the system but also in all the inner stages that leads to such outcome TRUST IN AI-BASED SYSTEMS We

0.6564 not in opposition to real rather it is a description of how a phenomenon becomes real and scientifically legible Impacts are co-constructed objects that emerge through the negotiation of accountability relationships As actors and fora enter into contestations over what an undertaking is and what it does in the world ways of describing and evaluating its effects must be rendered mutually legible between differently positioned stakeholders By agreeing upon categories of impacts these stakeholders stabilize impacts as evaluative objects upon which they can act The what of an algorithmic impact is co-produced with the who when where and why of algorithmic accountability However these impacts exist at a different level of abstraction from the harms that an undertaking may produce in the world They are proxies for harms that are convenient to use within relationships of accountability but must constantly be scrutinized to ensure that they are adequate and appropriate proxies for real-world harms We propose re-purposing Wieringas definition of algorithmic accountability by asserting that in the context of impact assessment actors and fora also have an obligation to explain and justify to each other the ways by which they construct their accounts of the socio-technical algorithmic system itself in

0.6559 actually making decisions using the system as well as to operational support maintaining the system Also potentially relevant are records regarding provisioning and support of technical components such as details of the data and system pipelines including model integrations storage compute and networking scalability and security plans logging mechanisms technical audit procedures etc The management of the records and other data supporting useful transparency is another aspect bearing consideration Details of the operating procedures access management and integrity controls encryption and any other regimes in place to ensure that records and logs are appropriately managed may be relevant Use This step concerns using the ADM system to actually make decisions whether the system decides or where a human makes the final decision As such it involves information on all aspects relevant to decisions actually being arrived at Records of model inputs and outputs are important as are details of what and how information and feedback is presented to users Parameters and metadata associated with a models use also warrant consideration as do operational records at technical systems log level Mechanisms for describing how a model operates eg interpretability or explanation see may also be relevant However this step also involves

0.6541 Having considered which actors and forums are relevant at each stage and step those responsible for ADM processes should then consider which technical and organisational record-keeping and logging mechanisms could provide contextually appropriate information for those accountability relationships We emphasise again that reviewability does not simply mean indiscriminate record-keeping at each step Instead it is about targeted useful transparency providing information that is i relevant to the accountability relationships involved ii accurate complete and representative iii proportionate to the level of transparency likely to be required and iv comprehensible by the relevant forums By considering each stage systematically in this way the ADM process as a whole can be made reviewable facilitating meaningful accounts of its operation to those to whom they may be owed We now discuss what each stage involves indicate which actors are likely to be relevant at each stage and what kind of transparency mechanisms may be available or relevant at each step Note we do not prescribe how to build reviewable systems or to propose new technical or organisational solutions Instead we present a framework for systematically considering the information necessary for providing useful transparency and thereby enabling meaningful accountability in the context of a

0.6521 security assessment and management capabilities while describing transparency accountability and traceability all in aspirational terms rather than as requirements Across a variety of global policy documents then we see that traceability has emerged as a key requirement for the responsible use of software systems This property entails systems where the design methodology underlying data sources and problem definitions are clearly documented and released to stakeholders a kind of structured transparency of the systems structure and development Additionally traceability requires connecting this transparency to outcomes and behaviors of the system encompassing auditability of the system-in-operation as well as the testability of the system during both development and operation Further traceability seeks to relate disclosed information to the problem of whom to hold responsible for these behaviors in cases both of failure and success providing a link between transparency and disclosure of system provenance and determinations of accountability An expansive requirement traceability lies at the core of system hazard mitigation and risk management decisions by system controllers Values Served by Traceability To understand the requirements demanded by this conception of traceability we must explore the goals articulated by the documents which espouse it Traceability is an expansive concept serving many values both

0.6515 for ADM processes to consider accountability systematically First they should assess from which actor and to which forum accounts are likely to be owed at each Forums highly contextual should be considered on a case-by-case Stage Step Commissioning Procurement Problem definition Impact assessment Model building Data collection Pre-processing Model training Model testing Decision-making Deployment Use Consequences Investigation Audit Disclosure Table Reviewability overview of a model-driven ADM process Each step entails record-keeping considerations basis when moving through the framework Accountability of actors is likely to be dispersed with obligations on multiple actors to explain or justify actions or decisions relating to aspects of the process for which they are responsible For identifying relevant actors Wieringas work on applying Bovenss model to algorithmic accountability can assist In particular Wieringa proposes three generic roles for actors in algorithmic systems decision-makers those responsible for deciding to use an algorithmic system and defining its specifications and other fundamental features developers those responsible for specifics of developing the technical components to the required specification and users those who use the system to produce a decision We use managers instead of decision-makers to avoid confusion with actors in the decision-making stage using the system to make decisions

0.6514 problem prevents testing external to development from being formally sound minimizing this gap necessarily requires disclosing information about the design as well as information about the systems performance under test Thus traceable systems must have and release information about robust test and evaluation plans Further such systems must be designed to be testable during development and operation and ideally to be testable by outsiders as well as developers This is driven by the close relationship between traceability and auditability Reproducibility Related to the requirement to minimize the gap between the view of developers and other insiders and stakeholders outside the process of creating or operating the system is the issue of reproducing a systems behavior so it can be evaluated for correctness under the stated set of requirements If a systems behavior cannot be reproduced by a developer it cannot be made plain to an outside stakeholder A practical issue is that even disclosures of source code and data or the use of fully open-source code and data cannot be related to compiled software or trained models unless that compilation or training can be precisely reproduced which is possible using an emerging set of tools and software practices More careful

0.6484 AI will work as intended is a key aspect of interpersonal trust that we will use to define Human-AI trust Indeed anticipation occurs when a user believes that an AI model is capable of maintaining the contract ie a user believes that the model is trustworthy to the contract which is a prerequisite of Human-AI trust Warranted and unwarranted Human-AI trust trust in M to C is warranted if it is caused by trustworthiness in M This holds if it is theoretically possible to manipulate Ms capability to maintain C such that trust in M will change Otherwise trust in M is unwarranted Human-AI distrust If human perceives that M AI is not trustworthy to contract C and therefore does not accept vulnerability to Ms actions then distrusts M contractually to C We say that it is warranted distrust if the distrust is caused by the non-trustworthiness of M CAUSES OF TRUST The next natural question to ask is on the cause behind trust in an AI model As established earlier we say that trustworthiness is a prerequisite to warranted trust What causes a model to be trustworthy And what enables trustworthy models to incur trust We divide causes of

0.6431 Suresh and Guttang split the model development lifecycle into six phases data collection preparation model development evaluation post-processing and deployment The Partnership on AI also considers ML in stages from system design and setup through maintenance and feedback once operational Guidance from the UK Information Commissioners Office and the Alan Turing Institute group propose several tasks for producing explanations that relate to various aspects of the process These understandings of ML as a multi-step process are useful but incomplete First while they acknowledge ML as sociotechnical they primarily focus on model-related issues thereby not fully accounting for human and organisational aspects Second they miss some important aspects of ML processes prior to data collection and following deployment procurement impact assessments consequences of decisions and audit and assessment of that process whether by those responsible or by an external overseer Our framework grounded in administrative laws approach to accountable human decision-making provides a more holistic view by adding two stages commissioning involving procurement problem definition and impact assessment and investigation involving audit and disclosure thereby encompassing more of the human decisions that are crucial to understanding the system Reviewability in practice a systematic approach These stages and steps allow those responsible

0.6392 human elements of ADM actions and decisions of people generally take place in the context of are informed by and constitute organisational systems and processes We note that ADM itself even understood as a process operates not on its own but as part of a wider socio-technical system Breaking down this ADM process into its technical and organisational elements allows us to systematically consider how contextually appropriate record-keeping logging and other documentary mechanisms at each stage of the process can allow for the process as a whole to be reviewed This assists with understanding how and why systems and processes are functioning and how particular decisions are made offering opportunities to identify points of intervention and control across the ADM process such as interventions by organisations at the point of decision-making to manage various risks or external interventions by or at the behest of regulators and others We focus on ADM involving ML but the approach can potentially apply across algorithmic systems As such a reviewability framework potentially offers a useful and holistic form of transparency to support meaningful accountability for ADM And while producing reviewable processes is already achievable there are also numerous research opportunities in this space We

0.639 be made available to system stakeholders affected by the systems operation This could be accomplished via transparency such as making system source documentation code or data available or through more abstracted disclosures such as impact assessments Many proposed approaches would provide standardized disclosures in support of transparency or traceability These include data sheets fact sheets data statements data nutrition labels model cards and other standardized disclosure formats But disclosure alone does not provide traceability and while traceability requires disclosure they must not be equated Indeed if traceability is more akin to an operationalization of accountability than of transparency it may be said that while such tools improve traceability they do so only to the extent that they reflect and enable a broader process of assessment test and evaluation Design proceeds from requirements Because traceability asks that stakeholders be able to understand why design decisions were taken traceability also requires that requirements be disclosed as part of transparency about design Requirements reflect the way a particular systems goal was articulated and approached by designers the critical aspect of problem formulation not otherwise subject to investigation by persons affected by a computing system and often invisible in even required disclosures of artifacts

0.6368 trust in this case the trust will not be betrayed even though it is unwarranted When XAI literature refers to trust we assume that it is referring to trust that is warranted Therefore we contend that when pursuing Human-AI trust unwarranted trust should be explicitly evaluated against and avoided or otherwise minimized See for analysis on the dangers of human usage or avoidance of automation when trust and trustworthiness are misaligned on axes of disuse misuse and abuse Specifically trust exceeding trustworthiness leads to misuse while trustworthiness exceeding trust leads to disuse Finally the notion of warranted distrust is similar to that of warranted trust and easily derived from it we say that the distrust is warranted if it is sourced in the non-trustworthiness of the AI ie the lack of capability of the AI to maintain the contract and otherwise it is unwarranted It stands to ethics that if an AI model is incapable of maintaining some relevant contract it is a desired outcome desired by the developer that the user develop warranted distrust in that contract that will be beneficial in applying the AI model to some scenario despite its flaws DEFINING HUMAN-AI TRUST This section serves as

0.6368 failures during the adversarial testing of the system and the severity of the risk is identified in earlier stages from informative processes such as the social impact assessment and ethnographic interviews The Reflection Stage This phase of the audit is the more reflective stage when the results of the tests at the execution stage are analyzed in juxtaposition with the ethical expectations clarified in the audit scoping Auditors update and formalize the final risk analysis in the context of test results outlining specific principles that may be jeopardized by the AI system upon deployment This phase will reflect on product decisions and design recommendations that could be made following the audit results Additionally key artifacts at this stage may include a mitigation plan or action plan jointly developed by the audit and engineering teams that outlines prioritized risks and test failures that the engineering team is in a position to mitigate for future deployments or for a future version of the audited system For the smile detection algorithm the decision could be to train a new version of the model on more diverse data before considering deployment and add more samples of underrepresented populations in CelebA to the training data

0.636 system its development its context and outcomes While reviewability as a high-level concept has applications in various areas and is relevant for algorithmic systems in general its approach to transparency and accountability of ADM thus goes beyond explanations or other mechanisms more narrowly focused on technical components Drawing from administrative laws approach to identifying factors to assess at points in the human decision-making process reviewabilitys systematic view of ADM focuses on points of review and intervention As elaborates these exist where people designing developing deploying and using ADM take some action or decision relating to the process or where technical components process data in some way At these points contextually appropriate information can be recorded about actions decisions or processing undertaken This offers useful transparency of ADM processes both at design-time and crucially at run-time providing information not just on how the algorithmic system was designed developed or intended to operate but also on how it functions and what kinds of decisions its produces in practice By providing information at key points of the ADM process reviewability assists in assessing individual decisions and in determining whether the algorithmic system as a whole is functioning as intended or required This view

0.6302 Consequences Finally the accountability relationship requires that the forum can impose consequences Consequences can come in many different forms which is closely connected to the kind of obligation the actor has to the forum Bovens p notes three different kinds of accountability based on the nature of the power relation which exists between the actor and the forum vertical accountability horizontal accountability and diagonal accountability Consequences are made most tangible when there is a vertical accountability relationship between actor and forum Here the forum formally wields power over the actor p As one may suspect this is the case in many instances of political and legal accountability but also in disciplinary hearings for instance which are a form of professional accountability Cath et al note that governments have key role in creating new policies specifically with regards to artificial intelligence However as Metzinger warns such policy initiatives may end up being co-opted by the industry as has happened to the High Level Expert Group for AI Ethics he argues He warns that ultimately such initiatives may end up as a toothless version of the thing they set out to be It is precisely those teeth which allow the neccessary enforceability

0.6258 rather than on broader technical and organisational processes of which models are only one part Accountability in ADM Our understanding of accountability in ADM aligns with Bovenss work on accountability as interpreted for algorithmic contexts by Wieringa In this accountability involves an actor from whom an account is owed a forum to whom it is given and a relationship of accountability between them Also considered is the nature of the account and any consequences flowing from it For accountability to be meaningful information given by the actor should support effective deliberation and discussion by the forum and the imposition of any consequences by the forum on the actor such as legal remedies or interventions to correct malfunction Although accountability can be thought of abstractly in general terms in practice it is highly contextual different actors will likely be accountable for different aspects of the ADM process depending on what is to be accounted for and the kinds levels and formats of information needed for a relevant and appropriate account will depend heavily on the forum to whom it is owed Bovens suggests that there are at least five kinds of accountability relationship depending on the actor and the forum political

0.6247 ADM and define undesirable behaviour or functionality ii implement contextually appropriate transparency mechanisms iii review compliance with those limits and general functioning and iv revise the process or take other action as required Reviewability thus supports a non-linear iterative and cyclical process of review feedback and revision in line with the understanding of accountability discussed above and with our view of ADM as a process involving human and technical elements Practitioners may move between steps non-linearly depending on their role and the situation as systems are developed deployed used and revised Not all steps will occur with each algorithmic system and some will be more relevant than others but at a high level these steps and stages will be common to many ADM processes Others have also discussed ML as a process involving multiple steps Lehr and Ohm for instance propose two workflows with eight steps for helping the law understand ML playing with the data involving problem definition data collection data cleaning summary statistics review data partitioning model selection model training and running model involving model deployment Wieringa applies the Software Development Life Cycle model to divide the process into ex ante in media res and ex post stages

0.6215 not be included in an AIA Regulatory agencies do not have adequate access to the types of grounded research technical expertise or entrée to affected communities to ensure that any list of impacts it stipulates is adequate to the potential harms people may experience Neither can a private company that develops algorithmic systems evaluate the comprehensiveness of its own efforts to evaluate impacts for three reasons the possibility of legal endogeneity to reproduce the companys own definitions of compliance as adequate to regulatory requirements limitations in the types of impacts made measureable by the instrumentation built into algorithmic products and the absence of an external forum that can mandate changes to the system Nor can critical academic journalistic or other third party audits stand in for impact assessments despite the fact that such audits can reveal important harms unanticipated by algorithmic systems designers Independent critical audits lack access to the design specifications and internal technical description of algorithmic systems that are often protected by intellectual property laws They also do not in themselves have direct influence over the design and operation of algorithmic systems even if audits and journalistic investigations have occasionally prompted public outcry industry responses and legislative action

0.6198 the assumptions decisions and priorities of those responsible for designing deploying and using ADM and their practical effects As a mechanism for producing useful transparency across the ADM process reviewability can we argue help support other technical legal regulatory accountability mechanisms that can make a significant difference CONCLUSION Given ADMs rising prevalence and risks of potential harm methods for facilitating review and oversight are needed Reviewability offers a legally-grounded holistic systematic and practical framework for making algorithmic systems meaningfully accountable Through targeted technical and organisational record-keeping and logging mechanisms reviewable ADM processes provide contextually appropriate information to support review and assessment both of individual decisions and of the process as a whole Reviewability cannot on its own address all potential problems with ADM for that a wider examination of its political economy and socio-economic context is also needed as are legal protections for individuals and groups However reviewability provides a way to gain a better understanding of ADM processes not only for those employing ADM but also for oversight bodies and those affected by decisions Reviewable ADM could therefore potentially be better assessed for legal compliance and for decision quality and can support those addressing more structural factors Research is

0.617 and on to the effects of those decisions and any subsequent investigations For Ananny and Crawford as for us accountability of these processes requires not just seeing inside any one component of an assemblage but understanding how it works as a system Understanding ADM as a process allows us to identify points across that process for review of relevant human and technical factors by the appropriate forum and if necessary to correct mitigate or otherwise address potential problems This depends on targeted mechanisms to provide useful transparency by capturing technical and human organisational elements across the whole process not necessarily for those subject to decisions but to facilitate understanding of the functioning of the algorithmic system as a whole and to enable meaningful accounts to and oversight more generally by designers developers deployers and overseers That accountable ML/ADM needs a view beyond technical to organisational elements has seen growing acceptance in recent years eg However initiatives aimed at working towards this often lack systematic understandings of ML as a broad process or of how to produce useful transparency to support meaningful accountability across that process Instead proposed approaches have often been piecemeal identifying particular aspects of that process thought to

0.6137 systems or provide remedy for harms These relationships between algorithmic systems and a forum empowered to demand changes to such systems are fundamental to establishing accountability in governing the impacts of algorithmic systems Furthermore algorithmic impact assessments do not produce accountability unless they attend to the constructedness of impacts themselves Indeed not only must there be a relationship between actors and fora in which the forum can pass judgement on or mandate changes for an actor there must also be a relationship in which impacts as evaluative measures are brought into as close an alignment as possible to potential harms Well-established examples from FAccT scholarship show that many cases of harmful algorithmic systems are enabled by the proxy gap between formal metric and actual harm and that such abstractions can obscure the complex relationship between social and technical systems Adding to this mismatch is the challenge of the wide typology of possible algorithmic harms that must be rendered in a commensurable fashion Harms are inherently context-dependent as they affect individuals and communities because of the particularities of their own circumstances whereas impacts are context-independent and measured as standardized metrics As governments companies and institutions begin to solidify AIA practices as

0.6123 fielded As with other transparency tools impact assessments are valuable insofar as they enable traceability and considering the requirements of traceability can help establish the scope of appropriate impact assessment Finally the extent to which a system can be audited effectively is a question both of design and governance The system must have interfaces that enable audit and also policies that allow effective uses of those interfaces In bureaucracies this is driven by freedom-of-information laws in addition to standards that define the sorts of engagements with auditors the system expects To assess whether something like an online advertising system admits a sufficient level of auditing we need the system to report out information auditors want for the systems controller to allow or to be compelled that information to be given to auditors and for there to be standards around how that auditing will take place This is to say nothing of the problem of finding a sufficient number of skilled auditors who can apply such standards a problem which exists even in domains where standards are clear SUPPORTING TRACEABILITY In order to meet the requirements laid out in Section and to support traceability in built systems we need technical organizational

0.6121 Bovens and Zouridis in connection to the discretionary power of civil servants Bovens and Zouridis note that civil servants discretion can be heavily curtailed by ICT systems within the government Building on Lipskys conception of the street-level bureaucrat they make a distinction between street-level bureaucracy screen-level bureaucracy and system-level bureaucracy Each of these types of bureaucracy allow for different measures of discretionary power of civil servants Whereas street-level bureaucrats have a great measure of discretion screen-level bureaucrats discretionary power is much more restricted System-level bureaucracy allows for little to no discretionary power as the system has replaced the civil servant entirely The different forms of bureaucracy are coupled to the way in which systems are playing a role within work processes As Bovens and Zouridis note the more decisive the systems outcome is the less discretion the user has Delegation to systems is thus not a neutral process but one that has great consequences for the way in which cases are dealt with and border cases especially Delegation to systems is important to consider for two other reasons as well First following Bovens logic of delegation and/or accountability it would make sense to start to hold the algorithmic system accountable

0.6101 both trustworthy and trusted The assessment of risk is necessary prior to the assessment of trust When deciding whether an AI requires trust or when evaluating trust verify the existence of vulnerability of the user in the actions of the AI by verifying that the user considers some of the actions of the AI as unfavorable to them and verifying that the user considers both the favorable and unfavorable outcomes as realistic AI developers should be explicit about the contracts that their models maintain AI developers should use the right affordances tomake the relevant contracts explicit This can help to avoid situations where a contract is implicitly assumed by a user despite the developer not considering the contract to be upheld unwarranted trust Successful anticipation while the goal of trust is not indicative of warranted trust The trust can be unwarranted if it is not sourced in trustworthiness in which case the anticipation may depend on a different variable that does not exist in other situations such as the quality of the AI user interface Therefore though simulatability methods are useful and valuable as methods of assessing this property it is dangerous to rely on them solely Trust is only

0.6062 technically literate to understand intrinsic opacity Multiple forms of opacity can combine it may be that data models systems and processes are concealed for intellectual property reasons eg and that even if they were not the models would be incomprehensible Table presents some of these types of opacity ML systems have sometimes been described as a black box which might be a choice made to deliberately obscure a form of intentional opacity However understanding ADM as a sociotechnical process we argue that there is another form of opacity unwitting opacity where those responsible for designing developing deploying and using systems simply dont think to record relevant organisational aspects of ADM processes perhaps unaware of their relevance for meaningful accountability By implementing technical and organisational record-keeping and logging mechanisms that allow processes to be holistically interrogated intentional and unwitting opacity could be substantially addressed However there are also risks in providing too much information As well as the forms of opacity discussed Stohl et al identify two further forms stemming from the transparency paradox where too much visibility actually reduces transparency That is opacity can result from providing too much or the wrong kind of information or information presented inaccessibly whether

---------------------------------------------------------------------------
TOPIC 2: image-classification

---------------------------------------------------------------------------

0.8622 half light at whereas the proportion of lighterskinned unique subjects in IJB-A and Adience is and respectively PPB provides substantially more darker-skinned unique subjects than IJB-A and Adience Even though Adience has labeled unique subjects which is nearly twice that of the subjects in PPB it has darker subjects nearly half the darker subjects in PPB Overall PPB has a more balanced representation of lighter and darker subjects as compared to the IJB-A and Adience datasets Commercial Gender Classification Audit We evaluated commercial gender classifiers Overall male subjects were more accurately classified than female subjects replicating previous findings Ngan et al and lighter subjects were more accurately classified than darker individuals An intersectional breakdown reveals that all classifiers performed worst on darker female subjects Key Findings on Evaluated Classifiers All classifiers perform better on male faces than female faces difference in error rate All classifiers perform better on lighter faces than darker faces difference in error rate All classifiers perform worst on darker female faces error rate Microsoft and IBM classifiers perform best on lighter male faces error rates of and respectively Face classifiers perform best on darker male faces error rate The maximum difference in error rate between the

0.8192 new parliamentarian benchmark annotators including the authors provided gender and Fitzpatrick labels A board-certified surgical dermatologist provided the definitive labels for the Fitzpatrick skin type Gender labels were determined based on the name of the parliamentarian gendered title prefixes such as Mr or Ms and the appearance of the photo Set n M Darker Lighter DF LF LM All Subjects Africa South Africa Senegal Rwanda Europe Sweden Finland Iceland Table Pilot Parliaments Benchmark decomposition by the total number of female subjects denoted as total number of male subjects M total number of darker and lighter subjects as well as female darkerlighter and male darkerlighter subjects The group compositions are shown for all unique subjects Africa Europe and the countries in our dataset located in each of these continents Dataset Lighter Darker IV V VI Total PPB IJB-A Adience Table The distributions of lighter and darker-skinned subjects according to the Fitzpatrick classification system in PPB IJB-A and Adience datasets Adience has the most skewed distribution with of the subjects consisting of lighter-skinned individuals whereas PPB is more evenly distributed between lighter and darker subjects Fitzpatrick Skin Type Comparison For the purposes of our analysis lighter subjects will refer to faces with

0.8084 a Fitzpatrick skin type of III or III Darker subjects will refer to faces labeled with a Fitzpatrick skin type of IVV or VI We intentionally choose countries with majority populations at opposite ends of the skin type scale to make the lighterdarker dichotomy more distinct The skin types are aggregated to account for potential off-by-one errors since the skin type is estimated using images instead of employing a standard spectrophotometer and Fitzpatrick questionnaire Table presents the gender skin type and intersectional gender by skin type composition of PPB And Figure compares the percentage of images from darker female darker male lighter female and lighter male subjects from Adience IJB-A and PBB PPB provides the most balanced representation of all four groups whereas IJB-A has the least balanced distribution Darker females are the least represented in IJB-A and darker males are the least represented in Adience Lighter males are the most represented unique subjects in all datasets IJB-A is composed of unique lighter males whereas this percentage is reduced to in Adience and in PPB As seen in Table Adience has the most skewed distribution by skin type While all the datasets have more lighterskinned unique individuals PPB is around

0.7794 exotic and supernatural elements The center image in Figure is an Realism portrait by Mary Cassatt Expressionists on the other hand used gestural brushstrokes and distorted subjects to portray intense emotions through their works The right image in Figure is an expressionism portrait by Ernst Ludwig Kirchner As can be observed the facial features in the right image have been distorted eg sharp chin resulting in an almost triangular facial contour pointed nose and ears to intensify emotions The left image in Figure is the style translated version of the center image Aesthetic innovations typical of an avante-garde movement like Expressionism are not evident in the style translated version As Kirchner himself said in Expressionism objective correctness of things is not emphasized rather a new appearance is created through radical distortions of subjects to evoke intense emotional experiences The style translated version is exactly as the original but for some color changes Figure i Black Matriarch a Folkart by Clementine Hunter ii Expressionism version of i by iii Giovinetto a Renaissance sculpture by Desiderio da Settignano iv Expressionism version of iii by Face color of Black Matriarch is changed in the translation unlike in Giovinetto a white sculpture Image source

0.7768 At the time of assessment in April and May of the dataset consisted of unique subjects who are public figures One image of each unique subject was manually labeled with one of six Fitzpatrick skin types TB Adience is a gender classification benchmark released in and was selected due to its recency and unconstrained nature The Adience benchmark contains unique individual subjects of those subjects had reference images that were discernible enough to be labeled by skin type and gender Like the IJB-A dataset only one image of each subject was labeled for skin type Creation of Pilot Parliaments Benchmark Preliminary analysis of the IJB-A and Adience benchmarks revealed overrepresentation of lighter males underrepresentation of darker females and underrepresentation of darker individuals in general We developed the Pilot Parliaments Benchmark PPB to achieve better intersectional representation on the basis of gender and skin type PPB consists of individuals from three African countries Rwanda Senegal South Africa and three European countries Iceland Finland Sweden selected for gender parity in the national parliaments Property PPB IJB-A Adience Release Year Subjects Avg IPD pixels Size avg IM Width IM Height Table Various image characteristics of the Pilot Parliaments Benchmark compared with prior datasets

0.7695 what a proper categorization of gender is and whether gender can be categorized at all Rather than addressing the full complexity of this question we follow prior work and use a set of discrete categories Male Female and Unsure in which Unsure is used to both ancient black belt conversational partner groom Imageability C la s s if a ti o n a c c u ra c y Figure Left The computer vision models classification accuracy vs synset imageability for safe synsets which contain at least images More imageable synsets are not necessarily easier for models to recognize with Pearson correlation coefficient r Right Example images from synsets that are non-imageable and hard to classify conversational partner non-imageable but easy to classify ancient imageable but hard to classify groom imageable and easy to classify black belt Synsets P e rc e n ta g e Unsure Female Male Gender Synsets P e rc e n ta g e Dark Medium Light Skin Color Synsets P e rc e n ta g e Child or Minor Adult Over Adult Adult Age Figure The distribution of demographic categories across the safe and imageable synsets which contain at least images The size

0.7492 art and sparks the design of novel methods to address these issues Case study selection We surveyed academic papers online platforms and apps that generate art using AI In order to uncover potential biases from an art historical perspective from the surveyed list we selected papers and platforms that focused on simulating established art movements and/or artists styles Thus papers such as or platforms such as which focus on deviating from established styles to create imaginary patterns are not included in our study To demonstrate various biases we have considered state-of-the-art generative art AI models and platforms/apps such as that focus on simulating established art movements and artists styles The art movements considered as part of case studies have been determined Model Bias type Art Movements Artists Genres Confounding bias Post-Impressionism Vincent Van Gogh Landscape Selection bias Romanticism Gustave Dore Illustration Selection bias racial bias Renaissance Various artists Portraits Transportability bias Post-Impressionism Paul Cezanne Photo Landscape Transportability bias Cubism Futurism Fernand Leger Gino Severini Genre art battle painting Transportability bias Realism Expressionism Mary Cassatt Ernst Kirchner Portraits Transportability bias Renaissance Expressionism Clementine Hunter Portrait Sculpture racial bias Folkart Desiderio da Settignano Transportability gender bias Renaissance Raphael Piero di Cosimo Portraits

0.7474 and followed by a group of artists during a specific period of time Examples of art movements include Ancient Egyptian art Ancient Greek art Medieval Art Renaissance art Modern art etc Within each of these art movements there are sub-categories based on various factors For example modern art includes many subcategories such as Symbolism Impressionism Post-impressionism Cubism Futurism Pop-art and so on As an illustration consider Impressionism and Post-impressionism Both movements originated in France however Post-impressionism originated in reaction to Impressionism While Impressionism was characterized by vibrant colors spontaneous brush strokes and urban life styles Post-Impressionism artists had their own individual styles to symbolically display real subjects and their emotions Similarly each art movement is characterized by unique features that reflects certain trends Thus art movement is a dominant factor influencing artists and artworks Interested readers may refer to where over hundred sub-categories are listed across a dozen art movements Art material Artworks can also be grouped based on the material and techniques used in creating the art Charcoal enamel mosaics tapestry paint and lithography are some examples of art materials Artists use different techniques to create artworks from different materials For example mosaic is a coherent pattern or image

0.7406 a kinetic state Futurism emphasized on objects and events that involved movement such as wars energy of nightclub and so on As such we consider the following artworks for the case study Propellers center image in Figure was a Cubist art by Fernand Leger Leger was fascinated with technology in particular with propellers and viewed them as objects of beauty holding them close to sculptures Right image in Figure is a Futurism art by Gino Servini called Armored Train in Action Severini was inspired by Cubism but was a member of Futurism Futurism used Figure Center Miss Mary Ellison a Realism artwork by Mary Cassatt Right Erna an Expressionism artwork by Ernst Ludwig Kirchner Left translation of the center image according to the style of right image by Distorted subjects a key aspect of Expressionism is absent in the translation Image source art as a symbol for expressing political and social views Severini depicted aspects of war movement and modernity in this work Figure ii can be a potential DAG for this case study note there can be other DAGs based on assumptions The differences in Cubism and Futurism art movements combined with the differences in genre influences the artists

0.7188 Ligher Male Figure The percentage of darker female lighter female darker male and lighter male subjects in PPB IJB-A and Adience Only of subjects in Adience are darker-skinned and female in comparison to in PPB the world the categorizations are fairly coarse Nonetheless the scale provides a scientifically based starting point for auditing algorithms and datasets by skin type Gender Labels All evaluated companies provided a gender classification feature that uses the binary sex labels of female and male This reductionist view of gender does not adequately capture the complexities of gender or address transgender identities The companies provide no documentation to clarify if their gender classification systems which provide sex labels are classifying gender identity or biological sex To label the PPB data we use female and male labels to indicate subjects perceived as women or men respectively Labeling Process For existing benchmarks one author labeled each image with one of six Fitzpatrick skin types and provided gender annotations for the IJB-A dataset The Adience benchmark was already annotated for gender These preliminary skin type annotations on existing datasets were used to determine if a new benchmark was needed More annotation resources were used to label PPB For the

0.7081 parity and prevalence of lighter skin in the region Iceland Finland and Sweden were chosen To balance for darker skin the next two highest-ranking African nations Senegal and South Africa were also added Table compares image characteristics of PPB with IJB-A and Adience PPB is highly constrained since it is composed of official profile photos of parliamentarians These profile photos are taken under conditions with cooperative subjects where pose is relatively fixed illumination is constant and expressions are neutral or smiling Conversely the images in the IJB-A and Adience benchmarks are unconstrained and subject pose illumination and expression by construction have more variation Intersectional Labeling Methodology Skin Type Labels We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer TB The six-point Fitzpatrick classification system which labels skin as Type I to Type VI is skewed towards lighter skin and has three categories that can be applied to people perceived as White Figure Yet when it comes to fully representing the sepia spectrum that characterizes the rest of PPB IJB-A Adience Darker Female Darker Male Lighter Female

0.7064 IBM achieves the worst classification accuracy with an error rate of This rate is nearly times higher than the IBM error rate on lighter faces Intersectional Error Rates To conduct an intersectional demographic and phenotypic analysis the error rates for four intersectional groups darker females darker males lighter females and lighter males are compared in aggregate and then for South Africa Across the board darker females account for the largest proportion of misclassified subjects Even though darker females make up of the PPB benchmark they constitute between to of the classification error Lighter males who make up of the benchmark contribute only to of the total errors from these classifiers See Table in supplementary materials We present a deeper look at images from South Africa to see if differences in algorithmic performance are mainly due to image quality from each parliament In PPB the European parliamentary images tend to be of higher resolution with less pose variation when compared to images from African parliaments The South African parliament however has comparable image resolution and has the largest skin type spread of all the parliaments Lighter subjects makeup n of the images and darker subjects make up the remaining n of

0.6972 Representational bias Renaissance Various artists Portraits Label bias Ukiyo-e Various artists Various genres Table Summary of Case Studies described in the paper Note there could be more than one type of bias associated with each For illustrative purposes only one bias type is discussed in each Model denotes the algorithm/platform listed under corresponding references based on the experimental set-ups reported in these state-of-the-art AI models and platforms These include Renaissance art Modern art Cubism Futurism Impressionism Expressionism Post Impressionism and Romanticism and Ukiyo-e art It is to be noted that due to the paucity of existing AI applications that study non-western art forms Ukiyo-e is the only non-western art form studied The genres span landscapes portraits battle paintings genre art sketches and illustrations Art material considered includes woodblock prints engravings paint etc The study includes artists across cultures such as Black folk artist Clementine Hunter American painter Mary Cassatt Dutch artist Van Gogh French illustrator and sculptor Gustave Dore Italian artist Gino Severini amongst others In the next section we discuss work related to computer generated art AI FOR ART GENERATION Computer generated art has a long history In s painter Harold Cohen began exhibiting paintings generated by a program

0.6913 The NIST Evaluation of Automated Gender Classification Algorithms report revealed that gender classification performance on female faces was to lower than performance on male faces for the nine evaluated algorithms Ngan et al The gender misclassification rates on the Pilot Parliaments Benchmark replicate this trend across all classifiers The differences between female and male classification error rates range from to The relatively high true positive rates for females indicate that when a face is predicted to be female the estimation is more likely to be correct than when a face is predicted to be male For the Microsoft and IBM classifiers the false positive rates FPR for females are double or more than the FPR for males The FPR for females is more than times that of males with the Face classifier Darker and Lighter Error Rates To conduct a phenotypic performance analysis the differences in darker and lighter skin type error rates for each gender classifier are compared first in aggregate Table and then for South Africa Table All classifiers perform better on lighter subjects than darker subjects in PPB Microsoft achieves the best result with error rates of on darker subjects and on lighter individuals On darker subjects

0.6871 enough for images Results of the demographic analysis We annotated demographics on the synsets that are considered both safe Sec and imageable Sec and that contain at least images We annotated randomly sampled images from each synset summing up to images Due to the presence of multiple people in an image each image may have more than one category for each attribute We ended up with attribute categories annotated annotations for gender annotations for skin and annotations for age This was the result of obtaining and consolidating worker judgments Fig shows the distribution of categories for different synsets which mirrors real-world biases For gender there are both male-dominated synsets and female-dominated synsets but the overall pattern across all synsets reveals underrepresentation of female as the blue area in Fig Left is significantly larger than the green area Relatively few images are annotated with the Unsure category except a few interesting outliers birth images labeled Unsure and scuba diver The gender cues in these synsets obscured because birth contain images of newborn babies and scuba diver contains people wearing diving suits and helmets The figure for skin color Fig Middle also presents a biased distribution highlighting the underrepresentation of people with

0.685 genre and these usually depicted royals aristocrats and other important people in society Portraiture had to convey aesthetic aspects of the subject depicted such as their power beauty etc In contrast genre painting depicted scenes from every day lives of ordinary people Landscapes animal painting and still life painting are some other prominent genres Abstract or figurative art are the most common genres for contemporary art An artist usually can work across different genre types however art historians mark certain artists as representatives of a particular genre For example Anthony van Dyck is recognized as a portraitist Alfred Sisley as a landscape painter and Piet Mondrian as an abstract artist though each one of them worked in a number of different genres Wikiart dataset provides an extensive list of genres based on artists and artworks Artists There are many aspects that characterize an artists style In addition to factors such as art movement art material and genre an artists style can be characterized by factors such as their cultural backgrounds their art lineage or schools from whom they learned or who influenced them and other subjective aspects such as their cognitive skills beliefs prejudices and so on Consider for example

0.6734 balanced on the basis of skin type than existing benchmarks To our knowledge this is the first gender classification benchmark labeled by the Fitzpatrick TB six-point skin type scale allowing us to benchmark the performance of gender classification algorithms by skin type Second this work introduces the first intersectional demographic and phenotypic evaluation of face-based gender classification accuracy Instead of evaluating accuracy by gender or skin type alone accuracy is also examined on intersectional subgroups darker females darker males lighter females and lighter males The evaluated commercial gender classifiers have the lowest accuracy on darker females Since computer vision technology is being utilized in high-stakes sectors such as healthcare and law enforcement more work needs to be done in benchmarking vision algorithms for various demographic and phenotypic groups RELATED WORK Automated Facial Analysis Automated facial image analysis describes a range of face perception tasks including but not limited to face detection Zafeiriou et al Mathias et al Bai and Ghanem face classification Reid et al Levi and Hassner a Rothe et al and face recognition Parkhi et al Wen et al Ranjan et al Face recognition software is now built into most smart phones and companies such as Google IBM

0.6572 are inclined to answer yes when asked the question there a concept in the image As a result some images with weak visual evidence of the corresponding synset may be successfully verified The final and perhaps most compelling reason for non-imageable synsets to have been annotated in ImageNet is that search engines will surface the most distinctive images for a concept even if the concept itself is not imageable For example identifying whether someone is Bahamian from a photograph is not always be possible but there will be some distinctive images eg pictures of a people wearing traditional Bahamian costumes and those will be the ones Table Examples of synsets in the person subtree annotated as unsafe offensive unsafe sensitive safe but non-imageable and simultaneously safe and imageable For unsafe offensive synsets we only show their synset IDs The annotation procedure for distinguishing between unsafe and safe synsets is described in Sec the procedure for non-imageable vs imageable is in Sec We recommend removing the synsets of the first two columns from ImageNet entirely and refrain from using synsets from the third column when training visual recognition models Unsafe offensive Unsafe sensitive Safe non-imageable Safe imageable n sexual slur n

0.6496 biases as discussed next Figure i Confounding bias due to genre material and art movements ii Confounding bias due to artists unobserved emotions Figure i Example of selection bias ii Illustration of see Section iii Illustration of selection bias in datasets see Section Causal effect of on is not identifiable across i ii and iii Sample Selection bias Sample selection bias or selection bias for short is the bias that is induced by preferential selection of units for data analysis In a DAG a special variable is used to denote the selection of the variable in the analysis indicating selection and indicating otherwise Consider for example Figure i Here and indicates that both inputs and outputs are selection dependent ie with respect to both and The case study discussed below will further clarify these concepts Case Study To illustrate selection bias let us consider an example described in the ArtGAN model The authors state that their model is able to recognize artist Gustave Dores preferences as the generated images resonate with the dull color found in Dores artworks Mostly engravings were selected for analysis Graph ii in Figure depicts a possible DAG for this case Let denote Dores style depicts

0.6481 Subjects denotes the number of unique subjects the average bounding box size is given in pixels and IM stands for image Figure shows example images from PPB as well as average faces of males and females in each country represented in the datasets We decided to use images of parliamentarians since they are public figures with known identities and photos available under non-restrictive licenses posted on government websites To add skin type diversity to the dataset we chose parliamentarians from African and European countries Fig shows an approximated distribution of average skin types around the world As seen in the map African countries typically have darker-skinned individuals whereas Nordic countries tend to have lighter-skinned citizens Colonization and migration patterns nonetheless influence the phenotypic distribution of skin type and not all Africans are darker-skinned Similarly not all citizens of Nordic countries can be classified as lighter-skinned The specific African and European countries were selected based on their ranking for gender parity as assessed by the Inter Parliamentary Union Inter Parliamentary Union Ranking Of all the countries in the world Rwanda has the highest proportion of women in parliament Nordic countries were also well represented in the top nations Given the gender

0.6457 based on the color of the face indicating potential racial biases Case Study We consider AbacusAIs online tool that converts a user uploaded photo into a different gender male to female and vice versa One of their demos shows translation of the Renaissance painting of Mona Lisa into a masculine face Thus we experimented with other Renaissance paintings Figure i is a portrait of a man holding an apple by Raphael and Figure iii is a portrait of a young man by Italian artist Piero di Cosimo Figure ii and iv are the gender translated versions of i and iii by both of which fail to identify the original paintings as those of men Young men with long hair were mistaken to be women and thus the gender-translated versions of these images depicted masculine faces with beards In the Renaissance era it was common for young men to have long hair often extending from ears to shoulders Thus by not understanding the differences due to culture across genders and age men are being stereotyped as having short hair and thus exhibits transportability bias BIASES IN DATASETS In this section we discuss biases due to unrepresentative datasets and due to inconsistencies

0.6424 of the ratings For quality control we manually select synsets as gold standard questions in the appendix half of them are obviously imageable should be rated and the other half are obviously non-imageable should be rated They are used to evaluate the quality of workers If a worker has a high error on the gold standard questions we remove all the ratings from this worker We also devise a el io ni st ne ut ra lis t ne co m er th st la a ge nt na na lis t al er ex pe rim en te r tr es m an ho st er g m us te ac he r nu rs e nn er tr c tr ai ne r ai tr es s synset im a g e a b ty Figure The distribution of raw imageability ratings for selected synsets irreligionist and nurse have more well-accepted imageability than host and waltzer Imageability S y n s e Figure The distribution of the final imageability scores for all of the safe synsets The median is heuristic algorithm to determine the number of ratings to collect for each synset Please refer to the appendix for

0.642 their culture beliefs and introspective capabilities Consider the Behance Artistic Media dataset which provides labels based on emotions such as happy scary peaceful etc Such labels could be based on annotators beliefs and can therefore be noisy Case Study To illustrate how annotation inconsistencies can induce bias consider the ArtGAN model This model uses the label information to train the GANs discriminator The authors claim that by using labels pertaining to genre cityscapes portraits etc art media eg sketch and study engraving and style eg Ukiyo-e they are able to generate images of those categories However the annotations are not necessarily reliable indicators of genres or styles For example sketch and study category includes several other categories such as portraits religious paintings and allegorical works to name a few Figure iii illustrates this bias Let and denote the environments corresponding to two different annotators Suppose denotes art movements style eg Ukiyo-e and is the artwork Annotation inconsistencies across annotators affects the artworks labels this is indicated by the selection variable pointing to When there are differences in the target variable ie label of artworks the causal effect of on is not identifiable using the annotations provided Thus a generative model

0.6371 in annotation Figure i Portrait of a man by Raphael iii Portrait of a young man by Cosimo ii iv Gender translations of i and iii respectively Young men with long hair were mistaken as women by Representational bias The bias that arises because of having a dataset that is not representative of the real world is referred to as representational bias This is a particular type of selection bias Specifically in the context of art datasets there may be imbalances with respect to art genres eg large number of photographs vs few sculptures artists eg mostly European artists vs few native artists art movements large number of works concerning Renaissance and modern art movements as opposed to others and so on The availability of artworks is one of the main constraints in collecting a dataset that is representative of the bygone times but preferences of the dataset curators can also play a role in contributing to bias Case Study Consider that was trained using about Renaissance portraits of mostly white people Quite naturally the system performs poorly on dark skinned people Faces depicting different races appearances etc have not been pooled into the dataset thus contributing to representational bias

0.6339 Anglo-Saxon n demographer n Queen of England n profanity n taxi dancer n epidemiologist n basketball player n sexual slur n orphan n piano maker n bridegroom n gendered slur n camp follower n folk dancer n beekeeper n criminative n separatist n mover n gymnast n criminative n crossover voter n policyholder n ropewalker n obscene n theist n great-niece n rider n pejorative n Zen Buddhist n vegetarian n trumpeter returned by the search engine This issue is amplified by the presence of stock photography on the web which contributes to and perpetuates stereotypes as discussed at length in eg Overall this results in an undoubtedly biased visual representation of the categories and while the issue affects all synsets it becomes particularly blatant for categories that are inherently non-imageable Thus in an effort to reduce the visual bias we explicitly determine the imageability of the synsets in the person subtree and recommend that the community refrain from using those with low imageability when training visual recognition models Annotating imageability Extensive research in psycholinguistics has studied the imageability aka imagery of words which is defined as the ease with which the word arouses imagery For annotating imageability most prior

0.6284 be recovered based on the engravings considered in the analysis In fact in addition to engravings Dores worked on paintings For example landscapes such as The Lost Cow paintings such as Little Red Riding Hood religious painting such as The Wrestle of Jacob are not greyish and exhibit colors reflective of the genre Thus by merely selecting engravings for analysis sample selection bias is induced and style of Dore cannot be identified Note the failure to capture Dores style in can be additionally attributed to other types of biases such as confounding bias label bias and even framing effect bias For the purpose of illustrating selection bias we have focused on describing selection bias only In general there can be more than one type of bias in a case study Case Study As an other example of selection bias consider the case of As illustrated in Figure racial bias was evident in this application Figure ii can also be used to describe this scenario Let denote the set of input images selected for analysis and let represent the generated images Selection of Renaissance portraits of mostly white people was used in the analysis thus As evident the generated images were

0.6264 annotate explicitly Quality control For quality control we have pre-annotated a set of gold-standard questions in the appendix for measuring the quality of workers The workers accuracy on a gold standard question i is measured by intersection-over-union IOU Ai Gi Ai Gi where Ai is the set of categories annotated by the worker and Gi is the set of ground truth categories For example for an image containing a black female adult and a white female child Gi dark light Female Adult Child If a worker mistakenly take the child to be an adult and annotates Ai dark light Female Adult the annotation quality is computed as We exclude all responses from workers whose average IOU is less than After removing high-error workers we aggregate the annotated categories of the same image from independent workers Each image is annotated by at least two workers For any specific category eg Adult we require consensus from max workers where ni is the number of workers for this image For any image we keep collecting annotations from independent workers until the consensus is reached In the annotation results the consensus is reached with only two workers for of the images and workers are

0.6198 background have received substantial attention in computer vision research Kakadiaris et al Ganguly et al Ahmad Radzi et al Illumination is of particular importance when doing an evaluation based on skin type Default camera settings are often optimized to expose lighter skin better than darker skin Roth Underexposed or overexposed images that present significant information loss can make accurate classification challenging With full awareness of the challenges that arise due to pose and illumination we intentionally chose an optimistic sample of constrained images that were taken from the parliamentarian websites Each country had its peculiarities Images from Rwanda and Senegal had more pose and illumination variation than images from other countries Figure The Swedish parliamentarians all had photos that were taken with a shadow on the face The South African images had the most consistent pose and illumination The South African subset was also composed of a substantial number of lighter and darker subjects Given the diversity of the subset the high image resolution and the consistency of illumination and pose our finding that classification accuracy varied by gender skin type and the intersection of gender with skin type do not appear to be confounded by the quality of sensor

0.6194 of the evaluated classifiers Analysis of Results The overall gender classification accuracy results show the obfuscating nature of single Darker Female Darker Male Lighter Female Lighter Male Group C on fid en ce S co re s Gender Female Male Figure Gender classification confidence scores from IBM IBM Scores are near for lighter male and female subjects while they range from for darker females metrics Taken at face value gender classification accuracies ranging from to on the PPB dataset suggest that these classifiers can be used for all populations represented by the benchmark A company might justify the market readiness of a classifier by presenting performance results in aggregate Yet a gender and phenotypic breakdown of the results shows that performance differs substantially for distinct subgroups Classification is worse on female than male subjects and worse on darker than lighter subjects Though helpful in seeing systematic error gender and skin type analysis by themselves do not present the whole story Is misclassification distributed evenly amongst all females Are there other factors at play Likewise is the misclassification of darker skin uniform across gender The intersectional error analysis that targets gender classification performance on darker female lighter female darker male and

0.6146 The image neither exhibits any distorted features nor has gestural brushstrokes that portray intense emotions Thus the style translation does not capture the subtleties of the Expressionism art movement Case Study As another case study to demonstrate biases in style transfer we consider GoART This app allows a user to convert an uploaded photo into various styles spanning art movements such as Byzantine Expressionism Cubism Ukiyo-e and artists such as Van Gogh We consider a folk art by Clementine Hunter titled Black Matriarch shown in Figure i Figure ii shows the Expressionism version of the Black Matriarch from As can be noticed the face is tinted in red However this kind of effect is not pronounced in light colored faces Consider Figure iii This is an early Renaissance sculpture by Desiderio da Settignano The face in the Expressionism version of this sculpture Figure iv does not seem to have shades of red as significant as in Figure ii Similar results were observed with other styles such as Byzantine wherein the face of Black Matriarch was tinted with shades of blue while the fairer faces were not heavily tinted There is noticeable difference in the way faces are converted across styles

0.6126 This is a particular instance of selection bias that has to do with dataset curation Algorithms trained using datasets with severe class imbalances are bound to be biased Figure iii illustrates this Suppose denotes artist and denotes artworks then class imbalances corresponds to ie type of artworks influences selection into the dataset in this case mostly white Renaissance portraits were selected into the dataset As condition iii fails in selection backdoor criteria there is representational bias Label bias This type of bias is associated with the inconsistencies in the labeling process Different annotators may have different preferences which can get reflected in the labels created A common instance of label bias arises when different annotations could be used to represent an artwork For example a scene with clouds may be annotated as a cloudscape by some annotators and more generally as a landscape by others Yet another type of label bias arises when subjective biases of evaluators can affect labeling Confirmation bias a type of human bias is closely related to this type of label bias For example in a task of annotating emotions associated with artworks the labels could be biased by the subjective preferences of annotators such as

0.6108 differently and the artwork differently Also there is the effect of unobserved factors such as the artists emotions that influence the artist and the artwork The left most image in Figure corresponds to the Futurism version of Propellers Kinetic patterns which is a distinct feature of Futurism is absent in the translated image Given that the original image is that of a mechanical object propellers a Futurism version of it should have depicted the movement of the propellers much like in Armoured train in Action that shows the fractured landscape which accentuates the trains force and momentum as it cuts through the countryside Thus there is bias in transferring styles Case Study The previous case study encompassed style transfer across art movements which were similar in many ways We now consider a case study involving style transfer between Realism and Expressionism two art movements that have marked differences from one another We consider common genre namely portraits across the two art movements Thus this case study serves as a good test to see if style transfer from is effective given that the difference between two styles is significant Realism focuses on representing subject matter truthfully without artificiality and avoiding implausible

0.6046 too diverse and the visual cues are too weak to be learned by the model accuracy only Imageable hard to classify Bridegroom synset ID n is an example of a category with a mismatch between imageability and accuracy It is annotated as imageable perfect score of because it easily arouses imagery albeit highly culturally-biased imagery The retrieved search result images are as expected culturally biased but correctly verified for inclusion in ImageNet However the accuracy of the classifier in this case is low only partially because of the visual diversity of the composition of images but primarily because of confusion with a closely related synset n which also corresponds to the term bridegroom but with a slightly different definitions n a man who has recently been married versus n a man participant in his own marriage ceremony This highlights the fact that classification accuracy is not a perfect proxy for visual distinctiveness as it depends not only on the intra-synset visual cues but also on the inter-synset variability Non-imageable easy to classify Finally Ancient person who lived in ancient times is deemed non-imageable score of because the imageability annotators have never seen such a person so it is di cult

0.6026 Viaduct of the Arc River Valley and The Gulf of Marseille Seen from LEstaque In some of his works such as Gardanne Cezanne painted the landscape with intense volumetric patterns of geometric rhythms most pronounced in the houses reflective of Cubism The generated images in do not exhibit such geometric rhythms Case Study In the previous case study we analyzed bias in the context of style transfer from one genre to another from photograph to landscape As another illustration let us consider the problem of style transfer across art movements and genres In order to demonstrate transportability bias in this setting we consider DeepArtio an online platform that maps the style of one image to the content of the other using a neural network architecture We consider a case study that involves Cubism and Futurism two art movements in the modern art era Both these movements had many common aspects yet they diverged in subtle ways Therefore we find it to be an interesting case study for analysis Both Cubism and Futurism focus on representation of objects from multiple perspectives/viewpoints and emphasize on geometrical shapes Cubism is concerned with forms in static relationships while Futurism is concerned with them in

0.6024 images Table shows that all algorithms perform worse on female and darker subjects when compared to their counterpart male and lighter subjects The Microsoft gender classifier performs the best with zero errors on classifying all males and lighter females On the South African subset of the PPB benchmark all the error for Microsoft arises from misclassifying images of darker females Table also shows that all classifiers perform worse on darker females Face is flawless on lighter males and lighter females IBM performs best on lighter females with error rate Examining classification performance on the South African subset of PPB reveals trends that closely match the algorithmic performance on the entire dataset Thus we conclude that variation in performance due to the image characteristics of each country does not fully account for the differences in misclassification rates between intersectional subgroups In other words the presence of more darker individuals is a better explanation for error rates than a deviation in how images of parliamentarians are composed and produced However darker skin alone may not be fully responsible for misclassification Instead darker skin may be highly correlated with facial geometries or gender display norms that were less represented in the training data

0.5957 of We are more interested in the breakdown accuracies for each synset and how they correlate with the imageability The networks testing accuracy on the easily imageable synsets score is which is higher than the accuracy of on the synsets deemed non-imageable score Overall there is a positive correlation between imageability and accuracy Pearson correlation coefficient r with a p-value of as depicted in Fig left To better understand this we analyze four representative examples also depicted in Fig right which highlight the different aspects at play here Imageable easy to classify A category such as black belt is both deemed imageable score of and is easy to classify accuracy of The retrieved images contain visually similar results that are easy to learn by the model and easy to distinguish from other people categories Non-imageable hard to classify On the other end of the spectrum conversational partner is deemed non-imageable score of only as it doesnt evoke a prototypical visual example The images retrieved from search engines contain groups of people engaged in conversations so the annotators verifying these images in the ImageNet pipeline correctly labeled these images as containing a conversation partner However the resulting set of images is

0.5951 Gender Shades Intersectional Accuracy Disparities in Commercial Gender Classification Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender In this work we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups Using the dermatologist approved Fitzpatrick Skin Type classification system we characterize the gender and skin type distribution of two facial analysis benchmarks IJB-A and Adience We find that these datasets are overwhelmingly composed of lighter-skinned subjects for IJB-A and for Adience and introduce a new facial analysis dataset which is balanced by gender and skin type We evaluate commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group with error rates of up to The maximum error rate for lighter-skinned males is The substantial disparities in the accuracy of classifying darker females lighter females darker males and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair transparent and accountable facial analysis algorithms Keywords Computer Vision Algorithmic Audit Gender Classification INTRODUCTION Artificial Intelligence AI is rapidly infiltrating every aspect of society From helping determine Download our gender and

0.5924 There are at least two different notions of styles when it comes to artworks one which is related to the art movement and another which is related to the artist As described in Section there are many aspects to each of the above notions of style Yet researchers conveniently define style in a manner that suits their models performance This is a consistent problem across several models For example in the authors claim that their model learns Ukiyo-e style since the generated images are yellowish like Ukiyo-e artworks In a single model is used to learn styles of artists and art movements Like in the justification to have learned Ukiyo-e style seems to be based on color features Thus style has been defined based on the color of the generated art Given that most Ukiyo-e works were woodblock prints it is thus natural for the generated art to be yellowish Ukiyo-e is a form of Japanese art The works usually depicted landscapes tales from history scenes from the Kabuki theatre and other aspects of everyday city life Some unique characteristics of Ukiyo-e included exaggerated foreshortening asymmetry of design areas of flat unshaded colour and imaginative cropping of figures Foreshortening refers

0.5898 people who are not well represented in the LFW benchmark In response to these limitations Intelligence Advanced Research Projects Activity IARPA released the IJB-A dataset as the most geographically diverse set of collected faces Klare et al In order to limit bias no face detector was used to select images containing faces In comparison to face recognition less work has been done to benchmark performance on gender classification In the Adience gender and age classification benchmark was released Levi and Hassner b As of The National Institute of Standards and Technology is starting another challenge to spur improvement in face gender classification by expanding on the study Intersectional Benchmark An evaluation of gender classification performance currently requires reducing the construct of gender into defined classes In this work we use the sex labels of male and female to define gender classes since the evaluated benchmarks and classification systems use these binary labels An intersectional evaluation further requires a dataset representing the defined genders with a range of phenotypes that enable subgroup accuracy analysis To assess the suitability of existing datasets for intersectional benchmarking we provided skin type annotations for unique subjects within two selected datasets and compared the distribution of

0.5898 of the different color areas reveal the underrepresentation of certain groups handle ambiguous visual cues as well as to include people with diverse gender expression Skin color We annotate skin color according to an established dermatological metric individual typology angle ITA It divides the spectrum of skin color into groups which is too fine-grained for our purpose Instead we combine the groups into Light Medium and Dark Melanin index is another metric for skin color which is used by the Fitzpatrick skin type classification system However we opt to use the more modern ITA system Similar to prior work skin color is used as a surrogate for race membership because it is more visually salient Age We annotate perceived age groups according to discrimination laws which led to the categories of Child or Minor under years old Adult Over Adult and Over Adult Annotation instructions We use crowdsourcing to annotate the attributes on Amazon Mechanical Turk We downloaded all ImageNet images from safe synsets in the person subtree whose imageability score is or higher An image and the corresponding synset form a task for the workers and consists of two parts First the worker sees the image the synset including

0.5839 lighter male subgroups provides more answers Darker females have the highest error rates for all gender classifiers ranging from For Microsoft and IBM classifiers lighter males are the best classified group with and error rates respectively Face classifies darker males best with an error rate of When examining the gap in lighter and darker skin classification we see that even though darker females are most impacted darker males are still more misclassified than lighter males for IBM and Microsoft The most improvement is needed on darker females specifically More broadly the error gaps between male and female classification along with lighter and darker classification should be closed Accuracy Metrics Microsoft and Face APIs solely output single labels indicating whether the face was classified as female or male IBMs API outputs an additional number which indicates the confidence with which the classification was made Figure plots the distribution of confidence values for each of the subgroups we evaluate ie darker females darker males lighter females and lighter males Numbers near indicate low confidence whereas those close to denote high confidence in classifying gender As shown in the box plots the API is most confident in classifying lighter males and least confident

0.5783 darker females darker males lighter females and lighter males Due to phenotypic imbalances in existing benchmarks we Figure Example images and average faces from the new Pilot Parliaments Benchmark PPB As the examples show the images are constrained with relatively little variation in pose The subjects are composed of male and female parliamentarians from countries On average Senegalese subjects are the darkest skinned while those from Finland and Iceland are the lightest skinned created a new dataset with more balanced skin type and gender representations Rationale for Phenotypic Labeling Though demographic labels for protected classes like race and ethnicity have been used for performing algorithmic audits Friedler et al Angwin et al and assessing dataset diversity Han and Jain phenotypic labels are seldom used for these purposes While race labels are suitable for assessing potential algorithmic discrimination in some forms of data eg those used to predict criminal recidivism rates they face two key limitations when used on visual images First subjects phenotypic features can vary widely within a racial or ethnic category For example the skin types of individuals identifying as Black in the US can represent many hues Thus facial analysis benchmarks consisting of lighter-skinned Black individuals would

0.5731 app called AIportraits It was pointed out that skin color of people of color was lightened in the apps portrait rendition Figure provides an illustration of the same Furthermore AI algorithms are best thought of as data-fitting procedures As the author in beautifully describes these algorithms are like tourists in a foreign country that can repeat and combine phrases from the phrasebook but not truly understand the foreign language or culture Many types of latent biases in generative art especially those concerning art history have not been analyzed in any of the past studies Further socio-cultural impacts of biases in generative art have not been investigated We aim to address these issues in this work We motivate the problem with an illustration Consider an image-to-image translation model such as CycleGAN which has been used to create images across different artists styles Figure b shows an example of Van Gogh version of a photograph Figure a as rendered by the CycleGAN model As can be seen the affect conveyed by the original image is lost in the Van Gogh version the red flowers perhaps indicative of Spring are no longer evident instead a dry season is reflected The generated image seems

0.573 in which each component element is built up from small regular or irregular pieces of substances such as stone glass or ceramic held in place by plaster/mortar entirely or predominantly covering a plane or curved surface even a three dimensional shape and normally integrated with its architectural context Mosaics were traditionally used as decoration for floors and walls becoming very popular across the Ancient Roman World Different art movements saw the prevalence of different materials For example during the Renaissance period sculptures were made out of various materials like marble white stone gold etc Thus material and technique employed to create art can influence the artist and the resulting artwork An elaborate list of various materials can be found in the WikiArt dataset Genre Genre of an artwork is based on the depicted themes and objects A hierarchy of genres was developed in the century According to this hierarchy history paintings namely paintings depicting scenes of important historical mythological and religious events were considered to be the top ranked genre and this was so until the mid century History paintings were usually large and typically narrated a story such as a battle allegory or the like Portraiture was another prominent

0.5649 dark skin The average percentage of the Dark category across all synsets is only and the synsets with significant portion of Dark align with stereotypes rapper images labeled Dark and basketball player An exception is first lady as most images in this synset are photos of Michelle Obama the First Lady of the United States when ImageNet was being constructed Limitations of demographic annotation Given the demographic analysis it is desired to have a constructive solution to improve the diversity in ImageNet images Publicly releasing the collected attribute annotations would be a natural next step This would allow the research community to train and benchmark machine learning algorithms on different demographic subsets of ImageNet furthering the work on machine fairness However we have to consider that the potential mistakes in demographics annotations are harmful not just for the downstream visual recognition models as all annotation mistakes are but to the people depicted in the photos Mis-annotating gender skin color or age can all cause significant distress to the photographed subject Gender identity and gender expression may not be aligned similarly for skin color or age and thus some annotations may be incorrect despite our best quality control efforts So releasing

0.5645 examining a subset of the synsets we find the imageability results to be reasonable overall but we also observe a few interesting exceptions Some synsets with high imageability are actually hard to characterize visually eg daughter and sister they should not have any additional visual cues besides being female Their high imageability scores could be a result of the mismatch between ease to arouse imagery and ease to characterize using images Daughter and sister are hard to characterize visually but they easily arouse imagery if the annotator has a daughter or a sister The definitions based on ease of characterization with visual cues is more relevant to computer vision datasets but we adopt the former definitions as a surrogate since it is well-accepted in the literature and there are mature procedures for annotating it using human subjects Another interesting observation is that workers tend to assign low imageability to unfamiliar words For example cotter a peasant in the Scottish Highlands is scored while the generic peasant is scored Prior works have demonstrated a strong correlation between familiarity and imageability which explains the low imageability of the less frequent cotter However low familiarity with a concept is anyway an important factor

0.5634 details Results and impact on ImageNet after removing the non-imageable synsets We annotate the imageability of synsets in the person subtree which have been marked as safe synsets in the previous task Fig shows the imageability ratings for a selected set of synsets Synsets such as irreligionist and nurse have well-accepted imageability irreligionist is deemed to be decidedly non-imageable nurse is deemed to be clearly imageable In contrast it is much harder to reach a consensus on the imageability of host and waltzer Fig shows the distribution of the final imageability scores for all of the safe synsets The median is only synsets have imageability greater than or equal to Table shows some examples of non-imageable synsets The complete list is in the appendix After manually examining the results we suggest that all synsets in the person subtree with imageability less than be considered non-imageable and not be used for training models There would be images and synsets lagged including hobbyist job candidate and bookworm there would be images and synsets remaining including rock star skier and cashier More examples are in Table Future researchers are free to adjust this threshold as needed Limitations of the imageability annotation By manually

0.5591 not adequately represent darker-skinned ones Second racial and ethnic categories are not consistent across geographies even within countries these categories change over time Since race and ethnic labels are unstable we decided to use skin type as a more visually precise label to measure dataset diversity Skin type is one phenotypic attribute that can be used to more objectively characterize datasets along with eye and nose shapes Furthermore skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals Roth Poorly exposed images that result from sensor optimizations for lighter-skinned subjects or poor illumination can prove challenging for automated facial analysis By labeling faces with skin type we can increase our understanding of performance on this important phenotypic attribute Existing Benchmark Selection Rationale IJB-A is a US government benchmark released by the National Institute of Standards and Tech LightestDarkest Figure The global distribution of skin color Most Africans have darker skin while those from Nordic countries are lighter-skinned Image from Encyclopedia Britannica Encyclopedia Britannica NIST in We chose to evaluate this dataset given the governments involvement and the explicit development of the benchmark to be geographically diverse as mentioned in Sec

0.5539 person subtree as others are inappropriate categories for visual recognition and should be filtered out Of the remaining synsets have sufficient data at least images to warrant further exploration On those we provide a detailed analysis of the gender skin color and age distribution of the corresponding images and recommend procedures for better balancing this distribution While categories may seem small in comparison to the current set it is nevertheless sufficiently large-scale to remain interesting to the computer vision community eg the PASCAL dataset has only classes CelebA has attributes COCO has object categories the fine-grained CUB- dataset has bird species Further note that the most commonly used subset of ImageNet is the set of categories in the ImageNet Large Scale Visual Recognition Challenge ILSVRC which remains unaffected by our filtering the three ILSVRC synsets of the person subtree are bridegroom n safe imageability ballplayer n safe imageability and scuba diver n safe imageability There is still much remaining to be done outside the person subtree as incidental people occur in photographs in other ImageNet synsets as well eg in synsets of pets household objects or sports It is likely that the density and scope of the problem is smaller

0.553 Microsoft and Face have released commercial software that perform automated facial analysis IBM Microsoft Face Google A number of works have gone further than solely performing tasks like face detection recognition and classification that are easy for humans to perform For example companies such as Affectiva Affectiva and researchers in academia attempt to identify emotions from images of peoples faces Dehghan et al Srinivasan et al Fabian Benitez-Quiroz et al Some works have also used automated facial analysis to understand and help those with autism Leo et al Palestra et al Controversial papers such as Kosinski and Wang claim to determine the sexuality of Caucasian males whose profile pictures are on Facebook or dating sites And others such as Wu and Zhang and Israeli based company Faception Faception have developed software that purports to determine an individuals characteristics eg propensity towards crime IQ terrorism solely from their faces The clients of such software include governments An article by Aguera Y Arcas et al details the dangers and errors propagated by some of these aforementioned works Face detection and classification algorithms are also used by US-based law enforcement for surveillance and crime prevention purposes In The Perpetual Lineup Garvie and colleagues

---------------------------------------------------------------------------
TOPIC 3: advertising

---------------------------------------------------------------------------

0.8179 showed that advertisers can circumvent existing limitations on targeting users based on their interests in sensitive topics like religion and sexual orientation Furthermore we show that malicious advertisers can exploit Facebooks suggestions to discover new facially neutral free-form attributes that allow extremely biased targeting Look-Alike Audience Targeting In this section we show how the recently introduced look-alike audience targeting mechanism can be exploited by advertisers to covertly implement discriminatory advertising We first briefly describe the look-alike audience targeting feature of Facebook we then explain how this feature can be exploited to implement discriminatory advertising Look-alike audience selection Recently Facebook introduced the look-alike audience targeting feature to help advertisers reach people that are similar to ie look like their existing set of customers Look-alike audiences are a particularly useful feature for advertisers who have limited data about their customers and want to grow their customer base Advertisers can use it to outsource the job of marketing ie identifying the attributes of their potential customers and finding them to Facebook To select look-alike audiences advertisers need to first provide Facebook with information about their existing initial set of customers called the source audience An advertiser can choose source audience users in a

0.811 is a gay actor of the audience for Matt Dallas are men interested in men which is times more than the US distribution Thus a malicious advertiser may use Matt Dallas as a facially neutral proxy for targeting or excluding gay users Second an advertiser can use the suggestion mechanism to search for extremely biased free-form attributes For example suppose an advertiser is interested in conservative leaning audiences and the most biased free-form attribute they know is Fox with of conservative audience The advertiser can start with Fox and keep choosing more and more conservative attribute suggestions until she reaches attributes with extreme conservative audience bias Below we show a sequence of suggested attributes starting from Fox that leads to The Sean Hannity Show a free-form attribute with conservative audience Fox Fox News Channel Sean Hannity Mark Levin Rush Limbaugh The Rush Limbaugh Show The Sean Hannity Show Summary In this section we demonstrated that many curated attributes beyond ethnic affinity exhibit correlations with sensitive attributes like race which makes them potential vectors for discrimination We also investigated whether the freeform attribute targeting mechanism allows advertisers to target or exclude sensitive groups of users in a discriminatory manner Specifically we

0.81 when aiming to exclude sensitive group members can also be gauged from the table In particular there are a number of curated attributes with low representation ratio ie high disparity some of which achieve high recall for members not belonging to the sensitive group Free-form attributes We begin our investigation by gathering an extensive though not exhaustive list of free-form attributes that are supported by the Facebook marketing API The API provides two useful calls that we exploit i given a piece of text the API provides a list of free-form attributes that match the given text and ii given an attribute the API provides a marketing-api Table Free-form attributes that may be used for discriminatory targeting We show the percentage of the attribute audience that are members of the sensitive group as well as the fraction of the US Facebook population that are members of the sensitive group as a reference Free-form Attribute Potential Target PT PT Audience US Audience Marie Claire Female myGayTripcom Man interested in Man BlackNewscom African American affinity hoc Magazine Asian American affinity Nuestro Diario Hispanic affinity Table Examples of free-form attributes that can be targeted by advertisers In the parenthesis we show the number

0.7745 the divisiveness of an ad by analyzing the differences in reactions of people with different ideological persuasions to the ad Specifically using US census-representative surveys we look at how conservative and liberal-minded people differ in a how likely they are to report the ad b how strongly they approve or disapprove the ads content and c how they perceive truthood or falsehood in ads claims Our analysis shows that IRA ads elicit starkly different and polarizing responses from people with different ideological pursuasions How effectively done was the targeting of the socially divisive ads We find that the Click Through Rate CTR a traditional measure of effectiveness of targeting of the IRA ads are an order of magnitude times higher than that of typical Facebook ads The high CTR suggests that the ads have been targeted very efficiently A deeper analysis of the demographic biases in the targeted audience reveals that the ads have been targeted at people who are more likely to approve the content and perceive fewer false claims and are less likely to report What features of Facebooks ad API were leveraged in targeting the also analyze the construction or specification of targeting formulae for the ads

0.7711 to prevent addicted people to receive ads about alcoholic beverages a discriminatory advertiser could explicitly exclude them Using Facebooks attribute suggestions We first investigate the free-form attributes suggested by Facebook to better understand the criteria used to select these suggestions Table shows the suggestions returned by the Facebook Marketing API right column given a freeform attribute left column We selected attributes associated with news outlets biased towards conservative audience to check whether their respective suggestions are also similarly biased For instance almost of the audience of Townhallcom are very conservative Facebook users whereas the average amount of very conservative US users of Facebook is about We note that the audiences corresponding to most of the suggested attributes also exhibit a strong bias towards conservative audience From all suggestions presented only Harpers Magazine and RedEye have a less conservative audience in comparison with the US distribution A malicious advertiser could exploit free-form attribute suggestions from Facebook in two different ways First a malicious advertiser could exploit the Facebooks attribute suggestions to discover attributes that are facially neutral but are similarly biased as a given free-form attribute For example one suggestion from Facebook for myGayTripcom is the free-form attribute Matt Dallas who

0.768 hours in We observe that both the campaigns receive several thousands of impressions with the truck driver campaign receiving over impressions and the secretary campaign receiving over impressions The campaigns collectively cost less than The demographics of the users receiving the impressions exactly matched the targeting criteria All the truck driver ad impressions were to men or consumers who Google believes are men and those for the secretary ad were all to women This finding demonstrates that an advertiser with discriminatory intentions can use the AdWords platform to serve employment related ads disparately based on gender We also had ads for housing approved targeted and served disparately Figure c The ad was suggestive of attending a open house for buying or renting a house The final destination however had text indicating that the ad was created and served as part of a study This ad was targeted to both male and female demographics who were American Football Fans or Baseball Fans These interests were chosen intentionally to target the male demographic more With possibilitycylabcmuedu/housing these interests Googles AdWords estimated that the ad would be targeted to between B and B women and between B and B men We kept the

0.7638 of the population are not considered Race Most inclusive Most exclusive Asian US Politics Liberal US Politics Very Conservative Frequent travelers African American affinity Interest Vegetarianism Interest Country music Black African American affinity US Politics Very Conservative US Politics Very Liberal US Politics Conservative Interest Online games Interest Mountain biking Indian Interest Motorcycles US Politics Very Conservative Interest Online games Away from hometown Interest Ecotourism Primary OS Mac OS X White US Politics Very Conservative African American affinity US Politics Conservative US Politics Very Liberal Interest Hiking Interest Online games find facially neutral free-form attributes that disproportionately target or exclude users of a sensitive group For example an advertiser seeking to create an audience excluding certain ethnic groups may choose to select her target audience from users interested in particular news media sites or magazines To answer these questions and understand how vulnerable the Facebook ad platform is to these kinds of indirect discrimination we investigate how strongly curated attributes other than ethnic affinity correlate with ethnicity and whether free-form attributes that are facially unrelated to sensitive attributes can be used as proxies for sensitive attributes We executed these experiments by automatically querying the Facebook ad interface for the number

0.7634 we first investigate whether biases in source audience selection propagate to look-alike audience selection Later we show how an advertiser seeking to selectively target people of a particular race could simply create a small in the order of a few thousands but highly biased source audience consisting primarily of people of a particular race as described in Section and use it to effectively target a large in the order of tens of millions yet similarly or worse exaggeratedly-biased lookalike audience Bias in look-alike audience selection In this section we construct several highly biased source audiences and check if and how the selection biases in source audience propagate to look-alike audiences Similar to what we did in Sections and we use the North Carolina voter database to Table Top most over-represented and under-represented attributes in a source audience of African Americans and its two closest look-alike audiences In parentheses we show the value of the representation bias of each attribute Over-represented Attributes Under-represented Attributes Source Audience African American affinity US politics very liberal Liberal content engagement Interest Gospel music Interest Dancehalls Asian American affinity Hispanic Spanish dominant affinity Expats Mexico Hispanic all affinity Expats all countries Look-Alike Audience African American affinity

0.7456 narrow the audience to people who are interested in a specific religion Finally we note that it is possible to target or exclude gay and LGBT users or people sympathetic to their causes via attributes like LGBT community M Gay Table Suggestions for the most conservative news outlets The left column shows a set of freeform attributes for conservative news outlets and the right column shows the corresponding free-form attributes suggested by Facebook The percentage of very conservative users in the audience of each of these free-form attributes is shown in parentheses Very Conservative US Facebook Population Input Attribute Attribute Suggestions Townhallcom The Daily Caller RedState TheBlaze Hot Air news site The American Spectator The Daily Caller Townhallcom The American Conservative National Review Weekly Standard Human Events Commentary RedState Harpers Magazine US News World Report The Patriot Post American Patriot Patriot Nation Patriot Update NewsBustersorg Guns Patriots RedEye Americas Conservative Voice American Thinker National Review Fox Nation The Cullman Times Montgomery Advertiser The Huntsville Times The Tuscaloosa News alcom pride M Same-sex marriage M as well as groups of vulnerable people including Addicted REHAB AA While this last set of free-form attributes might be useful for example for an advertiser

0.7449 used or more interests and behaviors in their formula creating very complex formulas with up to distinct attributes Figure shows the top attributes that appear in the ads target formula based on the number of times they appeared in different ads There were distinct attributes and the most present attributes interest were African-American history and African-American Civil Rights Movement appearing in ads We can note a prevalence of attributes related to African-American and Hispanic Population with interests like Mexico Hispanidad and Latin hip hop Next we investigate aspects of the Facebook ads platform design that might have favored the IRA ads to massively explore this particular targeting strategy Figure Top attributes based on the number of advertisements they appeared Figure Cumulative Distribution Function CDF for the number of suggestions The Role of Attribute Suggestions Facebook provides a tool for advertisers that given a target attribute it presents a list of other attributes that target people with similar demographic aspects For example in the list of suggested targeting interests for Townhallcom a page with an audience in which of the users are very conservative users according to Facebook there are other pages with similar bias towards very conservative users ie The

0.7433 bodies Action taken By users Action taken Complaints action Reports per Million Users Table Comparative complaint handling by online platform providers according to NetzDG total numbers NetzDG Reports As percentage of million users Facebook avg Million Million Twitter avg Million Figure Facebook and Twitter NetzDG Transparency Report Original and Weighted by Number of Users The results of this analysis are stark user complaints per million users listed in NetzDG transparency reports are between and times higher on Twitter than they are on Facebook There are several possible explanations for this massive discrepancy of reporting NetzDG complaints between Facebook and Twitter Facebook is first checking NetzDG reports under its Community Standards reporting mechanisms and if these reports fall under their own Community Standards not reporting them as part of their transparency reports Twitter is not The NetzDG reporting mechanism is much harder for users to find on Facebook than on Twitter As a result Facebook users are less likely to make reports under NetzDG than Twitter users The Facebook NetzDG reporting mechanism is designed in a way that users are unlikely to complete it This could lead to a considerable number of users not completing reports made under this form Twitter

0.741 variety of ways including by ing a custom audience or by specifying them to be the fans of their Facebook page After specifying a source audience Facebook allows advertisers to specify a geographical region either countries or groups of countries from which the look-alike audience should be chosen Facebook orders ranks all users in the geographical region based on their similarity to ie how closely they look like the source audience and allows advertisers to select look-alike audiences by specifying a percentile range eg or over these ordered users from the geographical regions population Thus an advertiser can select X to Y percentile of closest matching users from any countrys population to target In practice Facebook limits Y to Potential for discrimination Our concern is that a malicious advertiser seeking to place discriminatory advertisements could exploit look-alike audiences as follows they could start by creating a highly biased highly discriminatory source audience and use the look-alike audience feature to find a larger set of users that is similarly biased effectively scaling the bias to much larger populations Put differently our concern is that when the source audience is discriminatory its look-alike audience would also be discriminatory In the following sections

0.7335 discrimination The potential for discrimination on the Facebook ad platform was first publicly highlighted when researchers discovered the ability to exclude people based on their ethnic affinity a curated attribute when targeting ads related to housing Angwin and Parris Jr Facebook responded by banning the use of ethnic affinity attribute for certain types of ads Facebook More recently researchers discovered the ability to target people interested in or holding anti-semitic viewpoints via free-form attributes like jew haters Angwin et al b These findings raise several questions about the potential for discriminatory targeting using Facebooks curated and free-form attributes First given that ethnic affinity-based targeting was disallowed for its potential correlation with ethnicity race of users are there other demographic behavioral or interest attributes that are similarly correlated if not more Second given that there exist hundreds of thousands of free-form attributes can malicious advertisers productad_preferences Table Most inclusive and exclusive curated attributes for each race In parentheses are the recall and representation ratio for a population from North Carolina These were obtained by uploading voter records filtered to contain only a single race and then measuring the size of the subaudience targeted by each attribute Attributes present in less than

0.7329 men and women in occupation y represent themselves in their biographies other than explicit gender indicators then scrubbing these indicators should be sufficient to remove all information about gender from the biographies ie P G y P G y where is a random variable representing a biography without explicit gender indicators G is a random variable representing the binary gender of the biographys subject is a random variable FEMALE R GE ND ER G Representation BOW FEMALE Representation WE FEMALE Representation DNN Gender find Figure Gap female y versus female y for each occupation y for all three semantic representations with and without explicit gender indicators Correlation coefficients Table Pairs of occupations with the largest values of male the percentage of mens biographies that are only correctly predicted as y when their indicators are swapped for which the predicted label changes from y y y male attorney paralegal architect interior designer professor dietitian photographer interior designer teacher yoga teacher Table Pairs of occupations with the largest values of female the percentage of womens biographies that are only correctly predicted as y when their indicators are swapped for which the predicted label changes from y y y female model rapper teacher

0.7204 the attribute myGayTripcom has an audience of men interested in men while only of the US population in Facebook consists of men interested in men We also News/Newspapers a number of attributes with very biased audiences in terms of racial affinities For example BlackNewscom has an audience with of the users with African American affinity in contrast with of African American affinity in the reference population the audience of hoc Magazine is composed of users with Asian American affinity which corresponds to times more in comparison with the reference population Similarly Nuestro Diario has an audience with of Hispanic affinity on the reference population These results suggest that a malicious advertiser could easily find free-form attributes to launch discriminatory ads based on gender race and sexual orientation More worryingly some free-form attributes allow a malicious advertiser to target people based on their beliefs Table presents a few sensitive free-form attributes from our dataset along with their potential audience in the US These attributes correspond to a large audience with specific religious beliefs including islam M catholic church M and evangelicalism M Thus although it is not possible to target religion using curated attributes one can use free-form attribute targeting to

0.7007 design Recent studies show that the resulting ad delivery may be problematically skewed users who differ on sensitive attributes such as gender age race may receive very different types of ads For example search queries with Black-sounding names are highly likely to be shown ads suggestive of arrest records In another study women were shown relatively fewer advertisements for high-paying jobs than men with similar profiles When ads are about housing credit or employment such disparities can harm equality of opportunity One cause of problematically skewed ad delivery is explicit targeting of users based on sensitive attributes which can be tackled by disallowing ad targeting based on sensitive attributes especially for housing credit and employment ads Although major ad platforms like Google and Facebook had disallowed targeting of opportunities ad based on sensitive attributes the advertisers could still exploit other personally identifiable information such as area code or using a biased selection of the source audience in the Lookalike audience tool by Facebook Following a lawsuit Facebook removed targeting options for housing credit and employment ads Other studies again reveal that ad delivery mechanisms could still result in skewed audience distribution based on sensitive attributes even in the absence of

0.6966 men through the AdWords interface that could steer their ads toward or away from women The simplest way gender-skewed advertising could have emerged is if the advertiser directly targeted on gender ie Factor II AdWords offers the ability to set demographic parameters to explicitly target ads toward or away from a single sex While such explicit intentional gender targeting is supported by the AdWords interface we wanted to explore whether the Barrett Group could actually use this feature to target their advertisement To do so we performed another study in three phases First in we constructed several ad campaigns that targeted job-related ads on the basis of gender using Googles advertising platform AdWords Figure shows two of the ads that were approved by Google Ad a is for a secretary job targeted towards women while ad b is for a truck-driving job targeted towards men The other pairs of differentially targeted ads varied by pay seniority level and educational requirements We show them in the Supplementary Materials Our ads all had the same display and destination URLs This page has the words Test ad No jobs here We also verified that Google rejects some advertisements at this stage by intentionally

0.6958 Liberal content engagement US politics very liberal Interest Gospel music Interest Soul music Hispanic Spanish dominant affinity Expats Mexico Asian American affinity Hispanic all affinity Expats all countries Look-Alike Audience African American affinity Liberal content engagement US politics very liberal Interest Gospel music Interest Dancehalls Asian American affinity Hispanic Spanish dominant affinity Expats Mexico Hispanic all affinity Expats all countries construct several groups of randomly selected people based on their ethnicity Asian Black White Hispanic gender political affiliation registered Democrat or Republican and age We construct a source audience corresponding to each group and for each source audience we ask Facebook to construct look-alike audiences from the US in five percentile ranges closest matching and of the US population Note that the audiences in the different percentile ranges do not overlap with one another and each subsequent percentile range becomes less similar ie less closely matching to the source audience Each of the five look-alike audiences we create for every source audience of people consist of approximately million people thus totaling to an approximate of million unique people in the US Thus look-alike audiences allow expansion of the source audience by over three orders of magnitude The key remaining question

0.6913 the ads directed to biased audiences could leverage the already existing societal divisiveness to further amplify it among the masses To understand these nuances of targeted advertising in this section we focus on the relationship between the targeted population and the ideological divisiveness in reporting approval and false claim identifying behaviors for the ads Table reports the correlation values between the targeted population and the tendency of the population to report approve and identify false claims Reporting We observe a negative correlation in the case of reporting for both Liberals and Conservatives also see Figure a Liberals C on se rv at iv es Reported Reported a Reporting Targeted Ad P ro po io n R or te d Non-Targeted Targeted b Reporting Bias Liberals C on se rv at iv es Approved Approved c Approval Targeted Ad A ro va co re Non-Targeted Targeted d Approval Bias Liberals C on se rv at iv es Find Find e False Claims Targeted Ad P ro po io n FC s Non-Targeted Targeted False Claims Bias Figure Relationship between targeting and the responses by ideological groups ace show the proportion of population targeted and their tendency of response Each circle represents

0.6817 and Hotelscom Hannák et al to discriminatory prioritization of service requests and offerings from certain users over others in crowdsourcing and social networking sites like TaskRabbit Hannák et al In this paper we focus on the potential for discrimination in online advertising which underpins much of the Internets economy Specifically we focus on targeted advertising where ads are shown only to a subset of users that have attributes features selected c T Speicher et al Potential for Discrimination in Online Targeted Advertising by the advertiser Targeted ads stand in contrast to non-targeted ads such as banner ads on websites that are shown to all users of the sites independent of their attributes The targeted advertising ecosystem comprises of i advertisers who decide which users an ad should not be shown to ii ad platforms such as Google and Facebook that aggregate data about their users and make it available to advertisers for targeting and iii users of ad platforms that are consumers of the ads The potential for discrimination in targeted advertising arises from the ability of an advertiser to use the extensive personal demographic behavioral and interests data that ad platforms gather about their users to target their ads

0.6768 An intentionally malicious or unintentionally ignorant advertiser could leverage such data to preferentially target ie include or exclude from targeting users belonging to certain sensitive social groups eg minority race religion or sexual orientation Recently the Facebook ad platform was the target of intense media scrutiny Angwin and Parris Jr and a civil rights lawsuit for allowing advertisers to target ads with an attribute named ethnic affinity After clarifying that a users ethnic affinity does not represent the users ethnicity but rather represents how interested the user is in content related to different ethnic communities Facebook agreed to not allow ads related to housing employment and financial services be targeted using the attribute Facebook and renamed it to multicultural affinity In this paper we conduct a systematic study of the potential for discriminatory advertising on the Facebook advertisement platform We focus on Facebook because it is one of the largest online advertising platforms in terms of number of users reached by ads the number of advertisers and the amount of personal data gathered about the users that is made available to advertisers Furthermore Facebook is an innovator in introducing new methods for targeting users such as cus Unfortunately Facebook was

0.6736 respectively These results strengthen our inference that the look-alike audience feature in Facebook is able to both capture the biases in a source audience and propagate the biases to the larger audiences it helps construct Discriminatory audience creation Having observed that the look-alike audience selection mimics the biases of the source audience selection we now check whether the bias propagation is sufficiently strong to lead to discriminatory audience creation To answer this question we compute the disparity of the sensitive attribute on which the source audience was biased and observe how disparate that attribute remains in the look-alike audiences made by Facebook Note that since we are observing look-alike audiences built from source audiences where the sensitive attribute was severely exaggerated we expect the disparity measure to reflect the disparity in favor of the attribute Source Look-alike audience er ag e ra nk Politics Republicans Ethnicity White Age Gender Men Figure Comparison of the average ranks of top over-represented and under-represented attributes in look-alike audiences built from different types of biased source audiences Average ranks for over-represented attributes are indicated by upward triangles downward triangles are used for the average ranks of underrepresented attributes Figure shows how source audiences that

0.6719 the ith biography is the target label ie occupation for that biography is the predicted label for that biography with its original gender indicators and y i is the predicted label for that biography when its gender indicators are swapped For example S female nurse surgeon is the set of biographies for female surgeons who are incorrectly predicted as nurses but correctly predicted as surgeons when their biographies use male indicators We also identify the total set of biographies that are only correctly predicted as having occupation y when their gender indicators are swapped and then calculate the percentage of these biographies for which the predicted label changes from y to y Tables and list for the BOW representation the five pairs of occupations with the largest values of For example of male paralegals whose occupations are only correctly predicted when their gender indicators are swapped are incorrectly predicted as attorneys when their biographies use male indicators Similarly of female rappers whose occupations are only correctly predicted when their gender indicators are swapped are incorrectly predicted as models when their biographies use female indicators Without Explicit Gender Indicators Remaining gender information If there are no differences between the ways that

0.6719 gender indicators BOW WE DNN Ac ra Figure Occupation classifier accuracy for each semantic representation with and without explicit gender indicators With Explicit Gender Indicators True positive rate gender gap For each semantic representation we quantify gender bias by using our datasets testing split to calculate the occupation classifiers TPR gender gap ie the difference in TPRs between binary genders and each occupation y P Y y G y where Y and Y are random variables representing the predicted and target labels ie occupations for a biography and G is a random variable representing the binary gender of the biographys subject Defining the percentage of people with gender in occupation y as P G Y y Figure shows Gap female y versus female y for each for the BOW representation with explicit gender indicators Figure depicts the same information for all three representations with and without explicit gender indicators Compounding imbalance We define the gender imbalance of occupation y as gender is underrepresented if or equivalently if The gender imbalance is compounded if the underrepresented gender has a lower TPR than the overrepresented if and is underrepresented Theorem If and then P G Y Y y Proof Via Bayes theorem

0.6659 were disparate in favor of an ethnic group tend to produce look-alike audiences also disparate in favor of that ethnicity although as the audiences become less similar the disparity tapers off Only one of these audiences the look-alike audience for White has a disparity below the threshold obtained from the disparate impact rule Biddle These results show that Source Look-alike audience D is pa ri ty Asian Black Hispanic White Figure Disparity in favor for each ethnicity when the look-alike audiences are created from an audience biased on that ethnicity look-alike audiences selected using highly biased source audiences can be highly discriminatory Summary In this section we investigated whether it is possible to start with a small discriminatory source audience and then leverage Facebooks look-alike audience feature to construct a considerably larger discriminatory audience We show that in order to select a look-alike audience Facebook tries to infer the attributes that distinguish the audience from the general population and propagates these biases in the selection of look-alike audiences Such bias propagation can amplify the explicit intentionally created or implicit unintentionally overlooked biases in a source audience of a few thousand to a lookalike audience of tens of millions As Facebook

0.6658 Bidding Strategies with Gender Nondiscrimination Constraints for Online Ad Auctions Interactions between bids to show ads online can lead to an advertisers ad being shown to more men than women even when the advertiser does not target towards men We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination proving our strategies to be optimal in some settings We use simulations to understand other settings CONCEPTS Information systems Online advertising KEYWORDS targeted advertising online auctions fairness constraints MDPs INTRODUCTION Prior work found Google showing an ad for the Barrett Group a career coaching service promoting the seeking of high paying jobs more often to simulated men than to simulated women Later work enumerates possible causes of this disparity One possibility raised by Google itself is that the Barrett Group targeted both men and women equally but other advertisers on average focused more on women which would be in line with subsequent findings In this possibility the Barrett Group found itself outbid for just women by the other advertisers who were willing to pay more

0.6635 terms of cost impressions clicks and CTR We obtained high impact ads corresponding to of the entire dataset These ads account together to of the total number of impressions of clicks of the cost and of the CTR For the purposes of our study where we require manual inspection of the ads to identify their targets and to run surveys our ensuing analyses concern those high impact ads run before the US elections ads wordstreamcom/blog/ws//// facebook-advertising-benchmarks Impressions Clicks Cost USD fbcom/brownunitedfront/ fbcom/brownunitedfront/ fbcom/patriototus/ fbcom/blacktivists/ fbcom/Blacktivist-/ fbcom/blacktivists/ fbcom/Blacktivist-/ fbcom/blacktivists/ fbcom/blackmattersus/ fbcom/blackmattersusmvmnt/ fbcom/blackmattersusmvmnt/ fbcom/timetosecede/ fbcom/Woke-Blacks-/ fbcom/Dont-Shoot-/ fbcom/Igbtun/ fbcom/copsareheroes/ fbcom/blackmattersus/ fbcom/BlackJourneyJustice/ fbcom/blackmattersus/ fbcom/patriototus/ fbcom/MuslimAmerica/ fbcom/South-United-/ fbcom/Memopolis-/ fbcom/South-United-/ fbcom/Dont-Shoot-/ fbcom/Woke-Blacks-/ fbcom/blackmattersusmvmnt/ fbcom/patriototus/ fbcom/South-United-/ fbcom/savethea/ Table Most popular landing pages per impressions clicks and cost a b Figure Cumulative Distribution Function CDF of the ads on their a clicks impressions and costs b click-through-rates Summary This section describes and characterizes the ads in the IRA dataset Our analysis highlights the landing pages that paid for the ads and identifies the most successful ads in terms of impressions and clicks We find that the ad campaigns were intensified near to the US election period Among our main findings we show that the typical CTR for these

0.6605 just after the newly elected US President Donald Trump assumed office Landing Pages We first explore the ad landing pages the urls to which users who clicked on the ads were redirected There are unique landing pages corresponding to all the ads Figure shows the top landing pages per number of ads posted The most popular landing page fbcom/Black-Matters-/ posted advertisements Interestingly one of the top landing pages the musicfbinfo invites users to install a browser extension which was reported to send spam to the Facebook friends of those who installed it This landing page received impressions clicks and spent around US The domain musicfbinfo was also promoted by other pages accounting for of all ads We also find that the most popular landing pages are Facebook pages accounting for of all ads followed by blackmattersuscom and Instagram For ads we were not able to identify their landing pages because these pages were already blocked webarchiveorg/web//musicfbinfo/ wiredcom/story/ russia-facebook-ads-sketchy-chrome-extension/ Cost Impressions Clicks and CTR Figure a shows the cumulative distribution functions CDFs of all the ads in the dataset on their number of impressions clicks and amount spent to advertise The most expensive ad cost USD The highest number of impressions

0.6596 a result if an advertiser like The Barrett Group sets equal bids for men and women it could end up only reaching men if it is outbid by other ads for female users The realtime auction makes it difficult for advertisers to figure out how to treat protected classes similarly In their study of Facebook ads Lambrecht and Tucker suspect that the higher competition for reaching younger women was the reason behind lower impressions of job-related ads for women than men in spite of gender-neutral targeting criteria and bids Behavior of Other Consumers User behavior could also play a role in the disparate ad results found by Datta et al Googles understanding of which users are likely to respond to an ad is built off observations of millions of users behavior Factor I For example Google could have found that a males are more likely to click this ad than females are b females are more likely to click other ads than this ad or c there exist other contemporary ads that females are more likely to click than males are Googles computers may have targeted The Barrett Groups ads in response to one of the above findings to increase

0.6594 Daily Caller RedState and TheBlaze In order to investigate if the IRA ads have used suggestions to elaborate complex targeting formulas we crawled the attribute suggestions for each attribute that appear in the dataset of highly impact ads Figure shows the cumulative distribution function for the number of suggested attributes that appear in the same formula We can see that around of the ads that potentially used this feature because they have at least three target attributes suggested by Facebook as part of the same formula There are of ads with more than suggested attributes in the same formula As an example all the interests including Islam Ramadan Islamism used in the target formula of the ad ID appear as suggestions for at least one of the others in the formula For ad ID we were able to find out of of the interests using the interest suggestion feature This provides evidence that this feature may have been a key element used by the IRA campaign to choose the target audience Summary In this Section we show that the vast majority of the IRA ads use attribute-based targeting containing complex target formula that includes interest and behavioral attributes that are

0.6581 audience and the percentage of Targetable users who match the sensitive attribute as per Facebooks estimates Voter Records Facebook Users Validation of Custom Audience Attribute Number Percent Targetable Targetable matching sensitive attribute Male Female White Black Asian Hispanic Age Age Age We downloaded the public voter records from North Carolina giving us M records Using data from the voter records we then created custom audiences on Facebook for each sensitive attribute selecting a random subset of users from the voter file with each attribute For example we created a custom audience of women by uploading a list of voters listed as female we created a custom audience of white users by uploading a list of voters listed as white We created these custom audiences by uploading records containing the following fields last name first name city state zip code phone number and country We then examine how many of these records match to Facebook accounts that can be targeted with advertisements and then evaluate whether the created audiences are indeed discriminatory Whenever we target an audience either based on attributes or by specifying a custom audience Facebook provides an estimate of the number of users in the audience who can

0.656 consequences of the design choices taken by Facebook and Twitter in order to comply with NetzDG Because public transparency reports are required by NetzDG it is also possible to see how these design choices influenced users decisions to submit complaints via the respective platforms under NetzDG Based on the NetzDG transparency reports provided by Facebook and Twitter we compiled a summary Fig of the number of reports made in which quarter by which entities complaints bodies or individual users and how the platforms responded As Facebook has a far larger number of active users in Germany than Twitter we also provided an additional metric of Reports per million users Based on aggregating data from several different sources Facebook avg Million Million Twitter avg Million and we believe it is possible to provide a more accurate estimate of how many reports are provided by individual users Facebook By complaints bodies Action taken By users Action taken By complaints bodies Action taken By users Action taken By complaints bodies Action taken By users Action taken Complaints action Reports per Million Users Twitter By complaints bodies Action taken By users Action taken By complaints bodies Action taken By users Action taken By complaints

0.6509 that the ads were well-targeted in a way towards that population which was more likely to believe and approve and subsequently less likely to report or identify false claims in them Summary Our findings show that the IRA ads reached audiences that are very biased towards African-Americans and Liberals More important we show that ads were overall targeted towards a population that is more likely to believe and approve and subsequently less likely to report or identify false claims in them CONCLUDING DISCUSSION In this paper we provide an in-depth quantitative and qualitative characterization of the Russia-linked ad campaigns on Facebook Our findings suggest that the Facebook ads platform can be abused by a new form of attack that is the use of targeted advertising to create social discord These ads showed to be divisive were times more effective than a typical Facebook ad were biased especially in terms of race and political leaning and tended to be targeting more the users who are less likely to identify their inappropriateness We also provide strong evidence that these advertisers have explored the Facebook suggestions tool to engineer the targeted populations While this tool may be helpful in many ways it needs

0.65 the correlates The conditions of Datta et als study however allow us to probe this issue further The simulated users lacked other attributes which could be correlated with gender They all engaged in the same behavior As a result we can rule out that the difference in received advertisements resulted from differences in age affluence or prior work experience If Google inferred these attributes from user behavior all thousand users should have resulted in identical inferences The only differentiating data in these experimental conditions was gender If Google did use correlations with gender it used correlations found in other populations of real consumers and applied them to Datta et als synthetic population Decisions of Other Advertisers Other advertisers can influence the targeting of an advertiser such as The Barrett Group through the selection of their ad auction bids This possibility was raised by Google itself Todd If advertisers other than The Barrett Group were willing to pay more to reach women users The Barrett Groups advertisement may have ended up predominantly being served to men Factor I If advertisers in general consider female consumers to be a more valuable demographic they would set higher bids to advertise to them As

0.6483 example suppose that two similarly qualified users Alice and Bob both select jobs as their preferred ad category Not only should those advertisers in the subsection on compositional fairness could obtain a large allocation at the expense of advertisers that dont want to target such a user Note that advertisers can bid on users that are not targeted and thereby pay nothing for those users Furthermore it is rare for a user to be wholly unqualified for a category from the perspective of advertising For example an individual who is unqualified for a job is likely a good candidate for a job training ad categories on a user profile the two users then see job ads with the same total likelihood but they should also see a similar mix of high-paying and low-paying job ads This composition of definitions becomes subtle when users select multiple preferred categories Suppose that Alice continues to choose jobs as her preferred category but Bob chooses both jobs and household product ads Suppose further that Alice is allocated a job ad with probability and Bob sees an ad in each of the two categories with probability each This allocation satisfies inter-category envy-freeness However within the

0.6479 a Cost of K-parity for Equal price Female valuable scenario p T ot al U ti lit y R at io r r r b Cost of ratio for Equal price Female valuable scenario p T ot al U ti lit y R at io c Cost of K-ratio for Equal price Female valuable scenario p T ot al U ti lit y R at io K-parity Optimal K-parity Simple d Costs for parity for the optimal strategy and for simple immediate value bidding for Expensive female Equal value scenario p T ot al U ti lit y R at io r K-ratio Optimal r K-ratio Simple e Costs for ratio for the optimal strategy and for simple immediate value bidding for Expensive female Equal value scenario Probability of male viewer p O ve rb id r fe m al e vi er Ratio Ratio Ratio Average amount of overbidding for the female viewers for values of p Ratio of avg men value to avg men price O ve rb id r fe m al e vi er g Average overbidding for female viewers for ratios of average male viewer value to price of male viewers with p Expected lifespan

0.6452 spam Something else b Reasons of Inappropriateness Figure Distribution of the high impact ads on the a proportion of reported ads in our dataset b reasons of inappropriateness Likelihood of reporting the ads The first axis of divisiveness that we explored was reporting We surveyed respondents regarding Whether they would report the ad shown and If they would why do they find the ad inappropriate Answer choices given drawn directly from Facebooks reporting interface were sexually inappropriate violent offensive misleading disagree false news spam and something else Figure shows the reporting responses for the high impact IRA ads For over of these ads at least of the respondents responded that they would have reported the ads We observe that the majority of the ads were reported on the grounds of being offensive violent and misleading Additionally a substantial proportion of the reported responses belonged to the something else category In such cases the respondents entered free-text to explain their reason for inappropriateness Out of the responses that we received in the free-text box the pre-dominant reasons were that the ad incites racism and that the ad creates divide in the society Next to examine ideological divisiveness we find that the

0.6395 finally sees an Goddard v Google Inc d ND Jurin v Google Inc d ED Rosetta Stone Ltd v Google Inc d ED Va But see Cigar Inc v GoTocom Inc d holding that a keyword tool was not entitled to Section immunity because the alleged fraud is the use of the trademark name in the bidding process and not solely the information from third parties that appears on the search results page Goddard v Google Inc d ND That neutral tool concept was used extensively in Fair Council of San Fernando Valley v RoommatesCom LLC Fd th to determine whether a website has engaged in development that would negate protection providing neutral tools to carry out what may be unlawful or illicit searches does not amount to development for purposes of the immunity exception To be sure the website provided neutral tools which the anonymous dastard used to publish the libel but the website did absolutely nothing to encourage the posting of defamatory content indeed the defamatory posting was contrary to the websites express policies ad This is true even if an advertiser tried to target its employment ads toward women Google is therefore still in the position of

0.6356 ad was kept active for about hours and observed that the ad received over impressions of which were to men This again demonstrates that an advertiser can intentionally use proxies for gender to target housing ads disparately based on gender We look at this scenario in detail to explore whether an employer or employment agency using Googles ad network can engage in explicit intentional discrimination Using AdWords the career coaching service Barrett Group could have intentionally targeted their ad toward males or limited targeting to females Our small study also suggests that Googles review process does not weed out employmentrelated advertisements explicitly targeted by gender While our study shows that such direct targeting as one possible explanation for the advertising outcome it cannot tell us whether whether the Barrett Group actually used the demographic choices to target their advertisement Other Possibilities for the Advertiser In fact the Barrett Group denied targeting on gender and claimed to have sought those older than receiving high pay and of executive-level experience Todd pointing to another possibility the Barrett Group could have made other choices that led indirectly to targeting on gender For purposes of this analysis we set aside the issue of targeting

0.6321 of audience that can be targeted or excluded with the attribute Topic Free-form attributes Religion Islam M Catholic Church M Evangelicalism M LGBT LGBT Gay pride M Same-sex Vulnerable people Addicted REHAB AA Support group list of other related attribute suggestions For instance the list of related attributes for The New York Times includes The Washington Post The Wall Street Journal and The Economist We start with a seed set of names of news outlets extracted from three different sources Google News Leskovec et al List of Newspapers and the top newspapers from Alexa We first identify around free-form attributes that exactly match with the names of the news outlets We then execute a snowball sampling on these attributes using Facebooks related attribute suggestions recursively starting from them This process resulted in retrieving nearly free-form attributes We begin by trying to find attributes from the above set of attributes that can be used to primarily target or exclude people belonging to sensitive groups Table shows example free-form attributes that could be exploited for discriminatory targeting For example the attribute Marie Claire has an audience with of women a much larger fraction than the proportion of US women in Facebook Similarly

0.6317 of people belonging or not belonging to sensitive groups that have a certain curated or free-form attribute Discriminatory audience creation We now explore how both curated and free-form attributes are correlated with ethnicity Curated attributes We conduct our analysis in the way described in Section We use the custom audience mechanism to create groups of people from the North Carolina voter records that only contain particular ethnicities White African-American Asian and Hispanic We then create sub-audiences by choosing to only target users matching each curated attribute and observe the size estimates of these sub-audiences The percentage of users from each audience for whom Facebook inferred a curated attribute reveals how prevalent the attribute is within the audiences of different ethnicities The top three inclusive and exclusive attributes per ethnicity are shown in Table The results point out that ethnic affinity is by far not the only and in many cases not even the most disparate feature with respect to ethnicity For example when targeting Asians on Facebook it is more effective to do so based on political leaning or eating habits The tradeoffs between representation ratio and recall for members outside the sensitive group which an advertiser has to consider

0.6273 such as name phone number and email address so that Facebook can directly place the ads to them This kind of targeting does not appear in the IRA dataset Look-alike audience target For this targeting option advertisers provide to Facebook a list of users similar to that one in the PII or a list of people who liked the advertiser Facebook page Then Facebook attempt to target a similar audience to the group in this specific list Only of the high impact ads used this option Attribute-based targeting allows the advertiser to create a target formula based on a wide range of elements that include user basic demographics ie gender age location language advanced demographics ie political leaning income level Parents with children preschoolers interests ie newspapers religion politics and behaviors ie Business Travelers or New Vehicle buyers Recent work showed that the number of possible interests provide by Facebook is greater than Facebook allows one to include or exclude users with each of those attributes and combine multiple attributes as part of a target formula The vast majority of the high impact ads out of the used this option to elaborate a formula We found that of the ads

0.622 Ltd v Google Inc d ED Va claim against Google for unjust enrichment based on its practice of allowing trademarks to appear on its AdWords advertising platform was barred because in that context Google is no more than an interactive computer service provider Goddard v Google Inc d ND allegations based on keywords in Googles AdWords advertisements were barred because Keyword Tool was a neutral tool permitted within the scope of CDA immunity scenarios which reveal how potential legal liability shifts depending upon how targeting occurs Possible Causes of Ad Targeting We now go through the possible causes of disparate ad targeting outlined in Section and explore the legal ramifications of each of them Targeting was fully a product of the advertiser selecting gender segmentation In this scenario Google is probably not creating or developing content Instead by allowing but not requiring advertisers to choose to target their ads to men or women Google is providing a neutral tool that is protected by Section This tool allows third parties to determine who receives their ads which is likely protected as a publisher function Policies that allow advertisers to control who sees their ads are precisely the sort of website policies

0.6218 pastor professor software engineer professor surgeon physician surgeon representing the biographys target label ie occupation In turn this would mean that the TPRs for genders and are identical P Y y G y P Y y G y where Y is a random variable representing the predicted label ie occupation for Moreover it would also mean that P G Y y PG Y y making it impossible to predict the gender of a scrubbed biographys subject belonging to occupation y better than random In order to determine whether scrubbing explicit gender indicators is sufficient to remove all information about gender we used a balanced subsample of our dataset to predict peoples gender We created a subsampled training split by first discarding from our datasets training split all occupations for which there were not at least biographies for each gender For each of the remaining twenty-one occupations we then subsampled biographies for each gender to yield biographies balanced by occupation and gender To create a subsampled validation split we first identified the occupation and gender from those represented in the subsampled training split with the smallest number of biographies in our datasets validation split Then we subsampled that number of biographies

0.6122 Potential for Discrimination in Online Targeted Advertising Recently online targeted advertising platforms like Facebook have been criticized for allowing advertisers to discriminate against users belonging to sensitive groups ie to exclude users belonging to a certain race or gender from receiving their ads Such criticisms have led for instance Facebook to disallow the use of attributes such as ethnic affinity from being used by advertisers when targeting ads related to housing or employment or financial services In this paper we show that such measures are far from sufficient and that the problem of discrimination in targeted advertising is much more pernicious We argue that discrimination measures should be based on the targeted population and not on the attributes used for targeting We systematically investigate the different targeting methods offered by Facebook for their ability to enable discriminatory advertising We show that a malicious advertiser can create highly discriminatory ads without using sensitive attributes Our findings call for exploring fundamentally new methods for mitigating discrimination in online targeted advertising Keywords Discrimination advertising Facebook INTRODUCTION Much recent work has focused on detecting instances of discrimination in online services ranging from discriminatory pricing on e-commerce and travel sites like Staples Mikians et al

0.6104 likely suggested by Facebook Next we investigate the extent to which these formulas allowed advertisers to reach demographic biased audiences ANALYZING THE TARGET AUDIENCE We start by describing our methodology to reproduce the IRA queries without running the ad and gather the demographics of the of the targeted users Assessing the Audience Demographics Before launching an advertisement in Facebook the advertiser can get the estimated audience ie the number of monthly active users likely to match the target formula Our methodology consists of using the Facebook Marketing API to reproduce the targeting formula of all high impact IRA ads and get the demographics of the population that matches each targeting formula without running any ad This methodology has been extensively used recently for different purposes including inferring news outlets political leaning study migration and gender bias across countries and for public health awareness and lifestyle disease surveillance For our analysis we considered seven demographic categories political leaning race gender education level income location in terms of states and age As a baseline for comparison we also gathered the demographic distribution of the United States Facebook population Only of the used attributes that appear in the IRA ads targeting formulas are

0.6094 be targeted with advertisements this estimate is called the potential reach We first target only the custom audiences created without any additional targeting attributes specified and use the potential reach estimate to measure how many records in the audience are Targetable Facebook previously defined the potential reach as the number of daily active people on Facebook that match the audience you defined through your audience targeting selections Finally in order to validate that advertisements targeted to these custom audiences would indeed be discriminatory we take each custom audience and then target users with the corresponding sensitive attribute eg for the male voter records audience we target the Male attribute we then measure the potential reach and use the potential reach to measure what percentage of the Targetable users in the audience actually have that sensitive attribute according to Facebook Ideally the percentage of Targetable users with the sensitive attribute would be however Facebook may not know the attributes of some users may have errors in their matching algorithm or there may be errors in the user-provided data making this percentage smaller It is important to note that definitions in our data sources the voter file and census data do not

0.6087 P G Y Y y P Y y Y y If and then P G Y Y y P G Y Y y so the gender imbalance for the true positives in occupation y is larger than the initial gender imbalance in that occupation As explained in Section if the initial gender imbalance is due to prior injustices an occupation classifier will compound these injustices which may correspond to indirect discrimination FEMALE R GE ND ER G professor physician attorney photographer journalist nurse psychologist teacher dentist surgeon architect painter model poet filmmaker software_engineer accountant composer dietitian comedian chiropractor pastor paralegal yoga_teacher interior_designer personal_trainer rapper Figure Gap female y versus female y for each for the BOW representation with explicit gender indicators It is clear from Figure that there are few occupations with an equal percentage of men and women ie almost all occupations have a gender imbalance and that for that for occupations in which women conversely men are underrepresented Gap female y conversely Gap male y In other words there is a positive correlation between the TPR gender gap for an occupation y and the gender imbalance in that occupation Figure illustrates that this is also the case for

0.6043 line of the violin of the data is present between the two thick lines around the center Group Report Approval False Claims Liberals Conservatives Table Pearsons r correlation between targeting and the ideological divisiveness for the high impact ads p no statistical significance in the case of false claims We note that most of the ads target audiences that are more biased towards the African-Americans population and the Liberals More specifically about of the IRA ads target an audience with a higher proportion of African-Americans than in the US Facebook distribution This difference is even accentuated for Liberals with more biased in comparison with the reference distribution The percentage of ads with bias score superior to is for African-American and for Liberals Our dataset suggests the presence of those ads that target extremely biased populations of conservatives Liberals Hispanic and especially African-Americans The target audiences for the IRA ads are slightly biased towards women and young adults years which are omitted from Figure due to space constraints Targeting audience and Divisiveness Next we investigate if the advertisers target the ads towards audiences that are less likely to identify their inappropriateness due to their ideological perception bias Additionally we examine if

0.6032 obtained by all ads but cost only of the total cost of all ads in the dataset Finally we compute the click-through rate CTR of these ads which is a typical metric to measure the effectiveness of an ad It is computed as a ratio between the number of clicks and the number of impressions received by an ad Figure right shows the cumulative distribution function of the CTR of the ads excluding those with values for clicks impressions and cost The median CTR is and of the ads have a CTR higher than The average CTR is These are incredibly high values for CTR As a comparison WordStream released a report as of April which shows the average CTR for Facebook ads across all industries is As an example Retail is Fitness is Health care and Finance is This means that these political ads have a CTR that is about an order of magnitude higher than a typical Facebook ad High Impact Ads Our analysis reveals that only a few ads are responsible for most of the cost impressions and clicks Considering this we defined a set of high impact ads as the union of the top ads in

---------------------------------------------------------------------------
TOPIC 4: ML

---------------------------------------------------------------------------

0.97 contains the celebrity faces labeled as plain We choose the celebA data set to test our approach on high-dimension images MNIST-USPS This dataset consists of MNIST and USPS images which include different styles hand-written digits We set the sample source as a binary protected attribute The normal group contains digits from class and the abnormal group includes digits from class MNIST-Invert We take the images from MNIST and create a duplicate which is inverted to build this dataset The binary protected attribute is then original or inverted The normal group contains digits from class and the abnormal group contains digits from class Implementation Due to the different characteristics of our selected data sets we have implemented different networks for them For the SVDD based encoder network we use a convolutional neural network with two modules filters followed by filters and a final fully connected layer of units for MNIST-USPS and MNIST-Invert data sets we use a convolutional neural network with three modules filters filters and filters followed by a fully connected layer of units for celebA data set we use a fully connected neural network with two hidden layers with and units for the COMPAS Recidivism data set We use

0.9301 to membership queries On CIFAR the convolutional model is reconstructed in gradient queries compared to membership queries Unknown model class In the scenario where we do not know the true model class beforehand we experiment with MNIST Reconstructing the -layer model with the -layer network and vice versa MNIST Reconstructing the -layer model with the convolutional network and vice versa CIFAR Reconstructing the VGG model with the ResNet network and vice versa We refer the reader to Section for details on the models Figure displays our results R The model are equal to in one gradient and membership query Figure When querying with Gaussian generated inputs we seem to see a larger gap between the performance of gradient queries and the performance of membership queries We find that gradient queries seem to help more when the the model class of is more complex than the true classifier For example we see a x decrease in the number of queries needed to reconstruct MNIST -layer with a -layer network But we only get an initial x decrease in the number of queries needed to reconstruct MNIST -layer with a -layer network Similarly reconstructing the -layer network with the convolutional network works

0.9186 classifier is from a model class that is more complex than the model class of the true classifier and gradients processed with SmoothGrad a saliency map denoising technique Instead of returning the raw gradient x SmoothGrad returns an average of gradients around the input x x N i Second we manipulate the complexity of the task to test whether gradients help more or less on more complex tasks We experiment on both MNIST and CIFAR Finally we manipulate the complexity of the model class to test whether gradients help more when the model is simpler We train three models on each of the two tasks that are chosen to display a range of complexity Dependent measure We measure the accuracy of our reconstructed classifier on a test set of images from the task MNIST or CIFAR Experimental procedure We split our datasets into three parts A training set of images and ground-truth labels for the true classifier The training set for MNIST has examples and for CIFAR has examples A training set of images for the reconstructed classifier Note that does not have access to ground-truth labels so it must query for labels A test set of images and ground-truth

0.9184 Confidence FMNIST Confidence LFW Points Flipped N u m b e r o Points Flipped Points Flipped Points Flipped Points Flipped Figure Top row Prediction confidence on the horizontal axis percentage of stable points experiencing LUF on the vertical axis For FMNIST confidence is calculated as the absolute difference between the two most confidently predicted classes for other datasets confidence is Note the differences in scale between the graphs adversarial German Credit and Adult models display especially high leave-one-out unfairness as well as LFW Bottom Row A bar chart displaying what percentage of points in the dataset are affected by each one of the points taken out Each bar shows the number of points left-out points whose absence changed the prediction of the percentage of points shown on the axis Notably every single point that was taken out of the dataset affected at least one other individuals prediction Note the difference in scale on the axis of hidden layers of sized and For the Adult dataset the additional models were a narrower -hidden layer of size and a deep model with the same architecture as the main German Credit models For FMNIST we trained a shallower model with one set

0.9062 Equal Employment Opportunity Commission The distribution distance results are shown in Figure b We can see that deep fair SVDD achieves better overall fairness performance especially for the celebA data set Lastly we show the test set AUC scores for four data sets in Figure c we notice that in COMPAS MNIST-Invert and MNIST-USPS data sets the deep SVDD performs slightly better than the other two approaches while in the celebA data set the deep fair SVDD performs slightly better than other two approaches a Fairness by p -rule b Fairness by distribution distance c AUC Scores Figure Comparison of deep fair SVDD with deep anomaly detection baseline methods on all four selected data sets We evaluate the fairness performance for all the models trained on original data sets and plot the fairness by p -rule and distribution distances in Figure a b We also evaluate the anomaly detection performance and show the AUC scores in Figure c Note deep fair SVDD achieves better fairness results with a slight loss in terms of the AUC score a Deep SVDD MNIST-Invert b Deep SVDD MNIST-Invert c Deep SVDD celebA d Deep SVDD celebA e Deep Fair SVDD MNIST-Invert Deep Fair SVDD

0.9056 Training In all cases we choose a base model appropriate to the data For the tabular data we use a simple three-layer multi-layer perceptron For images we use multi-layer convolutional neural networks For text datasets we use a shallow convolutional model with attention In all cases we compare DWAC and softmax models of equivalent size but also explore varying the dimensionality of in the DWAC model We use the standard train/test split where available and Accuracy Dataset Softmax DWAC Adult Income Covertype Lending Club Fashion MNIST CIFAR- Subjectivity Stack Overflow IMDB Amazon Calibration MAE Dataset Softmax DWAC Adult Income Covertype Lending Club FashionMNIST CIFAR- Subjectivity Stack Overflow IMDB Amazon Table Accuracy higher is better and calibration lower is better on various datasets using the single best-scoring predicted label from softmax and DWAC models of equivalent size with standard deviations in parentheses otherwise sample a random of the data for a test set and always use a random of the training data as a validation/calibration set For measuring accuracy and calibration on test data we average over trials with different splits of the training data into a proper training set and a validation/calibration set with the same split being given to

0.9032 of neurons The fmnist model is a modified LeNet architecture This model is trained with dropout The LFW face-matching model consists of a concatenation layer composing the two input images a -layer convolutional stack followed by a dense layer and a Sigmoid output German Credit Adult and Seizure models are trained for epochs FMNIST and lfw models are trained for German Credit models are trained with a batch size of FMNIST and Adult Seizure and LFW used batch sizes of German Credit Adult Seizure and lfw models were trained with Adam lr and FMNIST with SGD lr The experiments outlined above were also performed on models with two other architectures per dataset in order to compare results across architecture presented in Figure For German Credit and Seizure datasets one additional architecture was a shallower model of a -hidden layer model of size and the other a narrower model Deep PGD Trades Smoothed Linear dataset base acc gen err base acc gen err base acc gen err base acc gen err base acc gen err German Credit Adult Seizure FMNIST LFW Table Test accuracy and generalization error for models Confidence German Credit Confidence Adult Smooth Trades Deep Linear PGD Confidence Seizure

0.8979 example we do not observe any information about the second feature z The core model assigns weight to the unseen directions weight for the second feature in Table c On the other hand in the presence of a spurious feature the full model can fit the training data perfectly with a smaller norm by assigning a non-zero weight for the spurious feature weight for the feature s in Table c This non-zero weight for the spurious feature leads to different assumptions for the unseen directions In particular the full model does not assign weight to the unseen directions In Table d the full model True Parameters target y z spurious s a Training Example z s y b Model estimated parameters Core model Full model c Model prediction Core model y z Full model y z s d Table A simple linear regression example that shows when removing a spurious feature increases the error a There are core features z z z one spurious feature s and the target is y b There is only one training example with no information about z c Core models estimate for the parameters is however the full model interpolates data with a smaller

0.8963 we rst look at the signedness of and For a given partition captures the disparity in robustness between points in relative to points not in see Considering all possible partitions based on class labels and sensitive attributes where available for all ve datasets both carlini-wagner and DeepFool agree with the signedness of the direct computation times ie sign sign sign sign Further the mean difference between and or ie is for DeepFool and for carlini-wagner with variances of and respectively There is agreement between the direct computation and the DeepFool and carlini-wagner estimates of b This behavior provides evidence that adversarial attacks provide meaningful upper bounds on G in terms of the behavior of identifying instances of robustness bias Audit of Commonly Used Models We now evaluate ve commonly-used convolutional neural networks CNNs Alexnet VGG ResNet DenseNet and Squeezenet We trained these networks using PyTorch with standard stochastic gradient descent We achieve comparable performance to documented state of the art for these models on these datasets A full table of performance on the test data are described in Table Appendix After training each model on each dataset we generated adversarial examples using both methods and computed for each possible partition

0.8959 plots show LUF for models of slightly different architecture as described in the experimental setup and the bars on the blue line show the minimum and maximum LUF values over random seeds on the main architecture shown in main results Notice the difference in scale across the graphs the same between PGD and TRADES training We determined the radius for adversarial training by finding the minimum distance with respect to the adversarial norm between any two points of different classes over a large sample of the dataset If this was impossible because this distance was zero we chose a distance smaller than that between over of cross-class pairs of points in the sample For TRADES training we used all of the same hyperparameters as PGD training with the addition of the TRADES parameter which was for Adult and German Credit and for Seizure and FMNIST Notice that for face-matching problems the threat model for finding adversarial examples is less clear eg it is not obvious if the attacker has access to individual images or pairs of images As we are unaware of an established threat model for face-matching we do not evaluate LFW in this section For randomized smoothing we

0.8929 we talk about ways to approximate G for more complicated models EVALUATION OF ROBUSTNESS BIAS USING ADVERSARIAL ATTACKS As described in Section we argued that adversarial attacks can be used to obtain upper bounds on G which can then be used to measure robustness bias In this section we audit some popularly a UTK classifier DeepFool b UTK classifier carlini-wagner c UTK classifier Rand Smoothing d ResNet DeepFool e ResNet carlini-wagner ResNet Rand Smoothing g Alexnet DeepFool Alexnet carlini-wagner i Alexnet Rand Smoothing vgg DeepFool vgg carlini-wagner l vgg Rand Smoothing m Densenet DeepFool n Densenet carlini-wagner o Densenet Rand Smoothing p Squeezenet DeepFool q Squeezenet carlini-wagner r Squeezenet Rand Smoothing Figure UTKFace partitioned by race We can see that across models that different populations are at different levels of robustness as calculated by different proxies DeepFool on the left carlini-wagner in the middle and Randomized Smoothing on the right This suggests that robustness bias is an important criterion to consider when auditing models for fairness used models on datasets mentioned in Section for robustness bias as measured using the approximation given by adversarial attacks Evaluation of c and d To compare the estimate of G by DeepFool and carlini-wagner

0.8811 The difference between the ratio of TPR/TNR of the core and full model can be small straight young or large male Chinese independent of positive/negative ratio for the identity term has a better average accuracy than the core model The CoreRST does not use the spurious feature therefore its robust accuracy unlike the full model is exactly equal to its accuracy In Section we prove that the full model and the coreRST should have the same predictions however unlike our theory we observe that the large gap among the accuracy of different groups in the full model is all no lipstick or earrings only earrings only lipstick both lipstick and earrings With hair color and necklace accuracy robust accuracy With only necklace accuracy robust accuracy Table Two models trained on the CelebA dataset Top model uses hair color and lipstick a binary feature indicating if a person is wearing lipstick or not bottom model only uses hair color the robust accuracy against hair color drop more rapidly for the bottom model The group that has lowest accuracy for the bottom model only earrings have a better accuracy when hair color is also used as an extra spurious feature number of

0.8791 labels for and We first train models to serve as the true classifier We train three types of models on MNIST a -layer network multinomial logistic regression a -layer neural network with ReLu activations and a network with two convolutional layers each followed by a max-pool layer followed by two dense layers We also train three types of models on CIFAR the same convolutional network used for MNIST with the input dimension changed appropriately a VGG network and a ResNet- network Next we train a new classifier from the same model class as the true classifier The inputs x given to are randomly sampled from the training set for Depending on the condition of the experiment the classifier also receives either x x and x or x and x is the output of the SmoothGrad algorithm After training we compute the accuracy of our reconstructed classifier on the test set Follow-up experiments unknown model class and data distribution An adversary trying to reconstruct the classifier may not know the model class of or the data distribution So in followup experiments we reconstruct the classifier with a classifier from a different model class and reconstruct the classifier using Gaussian generated queries

0.8747 labels should be given to this instance T es t in st an ce s Credibility T es t in st an ce s Figure Empirical distribution of credibility scores from the softmax top and DWAC bottom models when trained on Fashion MNIST and tested on MNIST digits which have the same input format but different content with the latter using our proposed measure of nonconformity probability as a measure of nonconformity and the DWAC model using our proposed measure of nonconformity give low credibility to the vast majority of out-of-domain examples as shown in Figure The credibility scores from DWAC however are noticeably shifted closer to zero indicating that the sum of the weights of the corresponding class is a better measure when we are concerned about the possibility of out-of-domain data For in-domain data the credibility values will be approximately uniformly distributed We see a similar result when training on CIFAR- images and predicting on the Tiny Images dataset and an even more extreme difference in the case of the Covertype dataset where we treat one out of seven classes as out-of-domain data and train a model on only the six remaining classes For that setup the mean

0.8725 much better than reconstructing the convolutional network with the -layer network We have been fairly loose when referring to the relative complexities of different models and it is unclear to us how to compare VGG and ResNet- in terms of complexity Interestingly however we find that although gradient queries still lead to a x decrease when reconstructing ResNet- with VGG they help very little when reconstructing a VGG model with a ResNet- network Unknown data distribution We now analyze the setting where we do not know the data distribution Instead we query using randomly generated Gaussian queries ie x N Id Figure displays our results On MNIST we find that Gaussian queries lead to a greater gap in performance between gradient and membership queries compared to when using images from the data distribution -layer network we see at least a x decrease compared to the x decrease we saw in Section when using queries from the data distribution On the MNIST convolutional network we see that in with a single gradient membership query or membership queries independent of the distribution the queries are generated from gradient queries we get to accuracy On the other hand it takes membership queries to

0.8722 spurious feature decreases the overall accuracy in all three datasets see the first two rows of Table Note that the change in accuracy varies widely among different groups in the CelebA dataset removing the spurious wearing-earrings feature increases the accuracy for celebrities who only wear either lipstick or earrings and in the Double-MNIST dataset removing the second image label increases the accuracy for data points where the concatenated images have the different labels name core features z target y spurious feature s Example z y s Double-MNIST two MNIST images label of the left image label of the right image CelebA celebrities photo wearing lipstick wearing earrings True True Toxic-Comment-Detection comment w/o identity terms toxic or not identity terms cuz i shouldnt be blocked just for being non-toxic black Table A summary of the three datasets that we used in this work Double-MNIST is a synthetically generated dataset where each data point is a concatenation of two images from the MNIST dataset Double-MNIST CelebA Toxic-Comments all same labels different labels all no lipstick or earrings only earrings only lipstick both lipstick and earrings all group size in percentage full model accuracy core model accuracy full model robust accuracy coreRST accuracy

0.8708 predictions We can view these instances as moving from deep SVDDs predicted normal group to Table Anomaly prediction results for deep SVDD and deep fair SVDD Z and Z represent the number of predicted anomalies with protected status variable value as and respectively There is a large overlap between these two models anomaly predictions COMPAS celebA MNIST-Invert MNIST-USPS SVDD Z Z Ours Z Z Overlap ratio a Instances moved from normal to abnormal group b Instances moved from abnormal to normal group Figure Illustration of how deep fair SVDD makes the anomaly detection results fairer We visualize the sampled non-overlapping predictions between deep SVDD and deep fair SVDD The instances in a can be seen as moved from deep SVDDs predicted normal group to deep fair SVDDs predicted abnormal group and vice versa for b deep fair SVDDs predicted abnormal group to make the results fairer Observing the digits from Figure a we can see that deep fair SVDD is improving the fairness by moving instances that are prone to be anomalies to the abnormal group One common feature of those instances is that they are dissimilar to a regular style of digit and many of them are digits It

0.8703 MNIST-Invert g Deep Fair SVDD celebA Deep Fair SVDD celebA Figure The visualization of the random selected normal and abnormal examples determined by deep SVDD top row and deep fair SVDD bottom row for MNIST-Invert data set and celebA data set Compared to the deep SVDDs prediction results the size of instances with different protected status variable values are more balanced in fair SVDDs predictions Overall speaking deep fair SVDD achieves much better fairness with a minimal loss in anomaly detection performance Further we analyze the interesting result on the celebA data set In the celebA test set both the normal and abnormal groups have a balanced number of males and females Thus optimizing fairness in the celebA data set may also improve the anomaly detection performance We have observed similar results in the following experiments on the trade-off analysis of deep fair SVDD Figures shows examples of the randomly selected normal and anomalous examples according to deep SVDD and deep fair SVDDs predictions For the MNIST-Invert data set we can see that both the MNIST instances and Inverted MNIST instances are distributed evenly in the normal/abnormal groups determined by deep fair SVDD On the contrary there are more MNIST

0.8684 in Table For each model we predict labels for the entire training plus test subsets datasets on which the model was not trained We calculate weighted average f-score across harmful and non-harmful labels Because each model can be evaluated on each of the three datasets on which it was not trained twelve dataset permutations exist in total As before the reported f-score for each dataset permutation represents an average over five models Figure shows these results full tabular data appear in the Appendix A See for technical specifications Founta et al Davidson et al Waseem Hovy Golbeck et al Testing Dataset gh te d A ve ra ge S co re Trained on Founta et al Trained on Davidson et al Trained on Waseem Hovy Trained on Golbeck et al Founta et al B Pr p Pr e Adv p Adv e p e Founta et al Davidson et al Waseem Hovy Golbeck et al Testing Dataset gh te d A ve ra ge S co re Trained on Founta et al Trained on Davidson et al Trained on Waseem Hovy Trained on Golbeck et al Figure Weighted average f-score organised by testing dataset Marker shape indicates training dataset and

0.8634 our proposed K-shot fairness experiments because training on fine-tuning tasks with a minimal number of epochs and training points is not expected However we find that it is the most relevant fair transfer learning method to use as comparison We used the same transfer methodology and hyperparameters as described in Madras et al and used a neural network with a hidden layer of nodes as the encoder We used another neural network with a hidden layer of nodes as the multilayer perception MLP to be trained on the fairly encoded representation We used the demographic parity and equal opportunity adversarial objectives for the first and second LAFTR model respectively We trained each encoder for epochs and swept over a range of s We trained with all the data not held out as one of the testing tasks When training a MLP from the encoder on each of the transfer tasks we found that LAFTR struggled to produce useful results with only training points from the new task over any number of training epochs We found that we were able to get reasonable results from LAFTR using fine-tuning points and epochs of optimization using a minimal number of epochs was unsuccessful

0.8605 PyramidNet U depth no bottleneck for CIFAR- We re-implemented Deep CNN in pytorch and used the publicly available repo to train PyramidNet We use another deep convolutional neural network which we refer to as Deep CNN CIFAR and PyramidNet U depth with bottleneck for CIFAR and CIFAR-Super For Adience and UTKFace we additionally take simple deep convolutional neural networks with multiple convolutional layers each of which is followed by a ReLu activation dropout and maxpooling As opposed to architectures from torchvision which are pre-trained on ImageNet these architectures are trained from scratch on the respective datasets We refer to them as UTK classifier and Adience classifier respectively These simple models serve two purposes they form reasonable baselines for comparison with pre-trained ImageNet models fine-tuned on the respective datasets and they allow us to analyze robustness bias when models are trained from scratch In sections and we audit these datasets and the listed models for robustness bias In section we train logistic regression on all the mentioned datasets and evaluate robustness bias using an exact computation We then show in section and that robustness bias can be efficiently approximated using the techniques mentioned in and respectively for much more complicated models

0.8601 of leave-one-out unfairness with two robust classification methods adversarial training and randomized smoothing We find that models trained adversarially using projected gradient descent PGD aswell as models trained with the TRADES algorithm have significantly higher rates of LUF in most cases approximately doubling the number of unstable points over standard training On the other hand models that are made robust by post-hoc smoothing with Gaussian noise almost always have similar rates of expected LUF Taken together these results suggest that LUF and robustness are not inherently tied to one another but that certain classes of models may provide beneficial properties for both warranting further study Setup We use the same experimental setup as in Section for measuring leave-one-out unfairness In these experiments we only train deep models For adversarial training we use PGD with an radius and PGD steps on FMNIST and Seizure datasets For the Adult and German Credit datasets we use radius On the German Credit dataset we use the norm The radius remained Confidence German Credit Confidence Adult Arch Deep Linear Arch Confidence Seizure Confidence FMNIST Confidence LFW Figure Effect of random seed and architecture on LUF results in deep models from Figure The red and green

0.8535 MNIST dataset which we call Double-MNIST We concatenate each image in MNIST with another random image from the same class with probability of and a random image from other classes with probability The original images label is the target y and the concatenated images label is the spurious feature s Note that the features that determine the target the first image are completely disjoint from the feature that determine the spurious feature the second image We train a two-layer neural network with hidden units on this dataset Using for the training data the model achieves accuracy However for our experiments where we need unlabeled data we used labeled examples unlabeled examples and for test data CelebA The CelebA dataset contains photos of celebrities along with different attributes We choose wearing lipstick which indicates if a celebrity is wearing lipstick as the target and wearing earrings as the spurious feature We train a two-layer neural network with hidden units on this dataset For our purposes in this work we use labeled examples unlabeled examples and for test data Toxic-Comment-Detection The Toxic comment dataset is a public Kaggle dataset containing Wikipedia comments Each comment is labeled by human raters as toxic or

0.8494 epoch of training Thus we conclude that pre-trained models bring in biases due to the distributions of the data on which they were pre-trained and the resulting learned decision boundary after pre-training We additionally see that these biases can persist even after fine-tuning Comparison of Randomized Smoothing and Upper Bounds We have now presented two ways of measuring robustness bias via upper bounds and via randomized smoothing While there are important distinctions between the two methods it is worth comparing them To do this we compare the sign of the randomized smoothing method and the upper bounds as sign sign i and sign sign i We see that there is some evidence that the two methods agree The Adience UTKFace and CIFAR- dataset have strong agreement at or above between the randomized smoothing for both types of upper bounds DeepFool and Carlini-Wagner while the CIFAR dataset has a much weaker agreement above but closer to and CIFAR-Super has an approximately agreement It is important to point out that it is not entirely appropriate to perform a comparison in this way Recall that the upper bounds provide estimates of using a trained model However the randomized smoothing method estimates not directly

0.8445 of the dataset An example of the results for the UTKFace dataset can be see in Figure With evidence from Section that DeepFool and carlini-wagner can approximate the robustness bias behavior of direct computations of we rst ask if there are any major differences between the two methods If DeepFool exhibits adversarial robustness bias for a dataset and a model and a class does carlini-wagner exhibit the same and vice versa Since there are different convolutional models we have different comparisons to make Again we rst look at the signedness of and and we see that sign sign This means there is agreement between DeepFool and carlini-wagner about the direction of the adversarial robustness bias To investigate if this behavior is exhibited earlier in the training cycle than at the final fully-trained model we compute and for the various models and datasets for trained models after epoch and the middle epoch For the rst epoch of the partitions were internally consistent ie the signedness of was the same in the rst and last epoch and were internally consistent We see that at the middle epoch of the partitions were internally consistent for DeepFool and were internally consistent for carlini-wagner Unsurprisingly

0.8415 batch normalization and ReLU activations in these networks Note for the fair deep SVDD model we have another classification branch We employ a fully connected neural network with three hidden layers as the sensitive attribute discriminator for all the data sets We set the trade-off hyper-parameter default to and the center c as the mean of all the instance embeddings We set the learning rate as e for Adam optimizer and conduct mini-batch training with batch size as The weight decay hyper-parameter is set to Evaluation Metrics and Baselines In our experiments we evaluate two aspects of the proposed approaches and the baseline methods The first aspect is the ability to detect anomalies We evaluate the anomaly detection performance using the common Area Under the ROC Curve AUC The AUC measure can be thought of as the probability that an anomalous example is given a higher anomaly score than a normal example In this way the higher the AUC score is better The benefit of using AUC is because it represents the anomaly detection performance across various anomaly score thresholds t The second aspect is the Table Characteristics of original training set and balanced training set used in experiments We

0.8402 embeddings for both deep SVDD and deep fair SVDD to show why deep fair SVDD make fairer anomaly predictions This analysis is important as deep fair SVDDs objective is to learn a fair representation which is independent on the protected status variable z pf z pf z As shown in Figure the red and blue points represent the test instances with the sensitive attribute value as z and z respectively We first analyze the visualization results from deep SVDD in each plot we can find some regions dominated by one particular color which indicates the correlation between feature embeddings and the protected status variable On the contrary observing from the deep fair SVDDs result we can see that the red and blue points are almost uniformly distributed in the feature space especially in the celebA data set Deep fair SVDD is demonstrated to learn a fair representation that is independent of sensitive attributes Running Time Analysis We have also reported the training time for deep fair SVDD and compared it against the deep SVDD approach in Table Training deep fair SVDD takes longer time because we have a new fairness objective and it is learned through adversarial training We leave

0.8388 of layers removed as well as a model with no dropout Finally for LFW we compare with a ResNet model pre-trained on ImageNet and modified to take in two inputs and have a Sigmoid output aswell as a model whose filters are twice the size of the original model For experiments comparing the extent of expected LUF across models seeded differently we perform the main experiments outlined in the paragraphs above over different random seeds for all tabular and time series datasets and three different random seeds for image datasets Further details on model construction can be found in the appendix LUF in Deep Models Figure shows the prevalence of leave-one-out unfairness on all five datasets The first row plots the percentage of individuals experiencing luf ranging over the confidence of the baseline models prediction On every dataset examined deep models display nontrivial expected LUF ranging from to The second row shows the number of points in out of that lead to a given percentage of individuals having their predictions changed when only is removed from the dataset The percentage per point on axis and the number of points that change this percentage of outcomes is on axis Notably the

0.8372 In these follow-up experiments we analyze the same factors but with a subset of conditions For example since in the main experiment we found virtually no difference between SmoothGrad queries and gradient queries we omit SmoothGrad queries from our followup experiments EXPERIMENTAL RESULTS AND DISCUSSION Main experiments gradient queries versus membership queries Figure shows the results of our main experiments described in Section Type of query Across all experiments training with gradient queries leads to orders of magnitude fewer queries required to learn the model For example for the MNIST convolutional model we get to accuracy in gradient queries compared to membership queries We find practically no difference between gradient queries and SmoothGrad queries despite picking the hyperparameters for SmoothGrad that produced the best saliency maps See Appendix D Complexity of model class We find that the gap in performance between gradient queries and membership queries is larger for models of lower complexity As an extreme case consider the -layer network on MNIST We find a x decrease in the number of queries required With gradient queries it takes only one query to reconstruct the model get the same performance as the original classifier This makes sense because with gradient

0.8332 signedness of and When has positive sign higher magnitude indicates a higher robustness of members of partition as compared to members not included in that partition similarly when is negatively signed higher magnitude corresponds to lesser robustness for those members of partition see We may interpret shared signedness of both where G is deterministic and where G is measured by randomized smoothing as described in Section as positive support for the c measure Similar to Section we consider all possible partitions across CIFAR- CIFAR- CIFAR-Super UTKFace and Adience For each of these partitions we compare to the corresponding We nd that their sign agrees times ie sign sign thus giving a agreement Furthermore the mean difference between and ie is with a variance of This provides evidence that randomized smoothing can also provide a meaningful estimate on G in terms of measuring robustness bias Audit of Commonly Used Models We now evaluate the same models and all the datasets for robustness bias as measured by randomized smoothing Our comparison is analogous to the one performed in Section using adversarial attacks Figure shows results for all models on the UTKFace dataset Here we plot for each partition of the dataset on

0.8289 training examples ac ra full model minority full model majority core model minority core model majority Figure In Double-MNIST dataset as we increase the number of training data the gap between the performance of core model and full model shrinks The majority groups contains data points where the labels of two concatenated images are the same of data and minority group contains data points where the labels of the concatenated images are different of data mitigated by the coreRST We have not observed a large accuracy boost using unlabeled data in the Toxic-Comment-Detection dataset After some error analysis we observed that as mentioned in Garg et al there are some examples that can be toxic with respect to some identity terms but not the others Furthermore we observed some biases in annotations in this dataset eg username is gay has labeled as toxic As a result some accuracy drop of the core model is inherent and cannot be mitigated by unlabeled data Effect of training data size Figure shows that the gap between the accuracy of the core model and the full model decreases as the training data size increases In particular it shows the accuracy on two different groups

0.8277 is important to show that these non-overlapping instances are not randomly distributed but are all prone to be anomalies This interesting finding demonstrates that our proposed model is optimized to make fair and accurate anomaly predictions instead of random altering predictions to satisfy group-level fairness We can observe similar results from Figure b that instances moved from deep SVDDs abnormal group to deep fair SVDDs normal group are prone to be normal points a COMPAS Deep SVDD b celebA Deep SVDD c MNIST-Invert Deep SVDD d MNIST-USPS Deep SVDD e COMPAS Deep Fair SVDD celebA Deep Fair SVDD g MNIST-Invert Deep Fair SVDD MNIST-USPS Deep Fair SVDD Figure The t-SNE visualization of the feature embeddings for test instances Red and blue points represent test instances with different sensitive attribute values Comparing to deep SVDDs results top row the deep fair SVDDs learned embeddings bottom row are more fair as blue and red points are always blended together which are hard to separate Table Training time results measured by seconds Training deep fair SVDD takes longer time due to the min-max optimization of the adversarial learning COMPAS celebA MNIST-Invert MNIST-USPS Deep SVDD Ours Embedding Visualization We visualize and compare the learned

0.8272 reduce the number of overrepresented group in original training set to generate balanced training set COMPAS Recidivism celebA MNIST-Invert MNIST-USPS Original z Original z Balanced z Balanced z a Deep SVDD p -rule b DCAE p -rule c Deep SVDD distribution distance d DCAE distribution distance Figure Two methods of evaluating the unfairness for existing deep anomaly detection methods on both the original training sets blue bars and balanced training sets orange bars Note the larger fairness by p -rule and smaller distribution distances means the model is fairer Observed from these figures we can see that training deep anomaly detection models with a balanced training set can slightly improve the fairness in most cases However in most cases the fairness by p -rule does not satisfy the rule black horizontal line advocated by the US Equal Employment Opportunity Commission ability to be fair in terms of protected status variables We use aforementioned p -rule equation and distribution distance equation measures as our evaluation metrics We compare deep fair SVDD with two popular deep anomaly detection methods deep SVDD and deep convolutional autoencoders DCAE We duplicate the deep fair SVDDs encoder network architecture for those two deep anomaly detection baselines

0.8259 but not in the case Similar trends were observed for all the M L ea r standard mean-m mi-normal standard mean-m mi-normal M L en d er standard mean-m mi-normal standard mean-m mi-normal li te r standard mean-m mi-normal standard mean-m mi-normal S u sh g e standard mean-m mi-normal standard mean-m mi-normal S u sh en d er standard mean-m mi-normal standard mean-m mi-normal S u sh iS ea o d standard mean-m mi-normal standard mean-m mi-normal Figure Changes in the and measures NOTE The subfigure rows sequentially show the results for the MLM-Year MLM-Gender Flixster Sushi-Age Sushi-Gender and MLM-Seafood datasets respectively The X-axes of these subfigures represent the independence parameter in a logarithmic scale The Y-axes of subfigures in the first and the second columns represent in a linear scale and in a linear scale respectively Note that the ranges are changed to emphasize the differences The results for the and mi-normal terms are overlapped in some subfigures a mean-m b c mi-normal Figure Changes of means and standard deviations of predicted ratings according to the parameter NOTE The X-axes of these subfigures represent the independence parameter in a logarithmic scale These subfigures sequentially show the means and

0.8229 s on error Consequently conditions only on the true parameters or only on the distribution of the spurious feature and the target cannot guarantee that error does not increase after removal of the spurious feature Therefore removing the spurious feature may aggravate the error even under favorable conditions such as balanced datasets y s in the train and test data or if disjoint features determine the target and the spurious feature i for all i We then study multiple spurious features and show that removing one spurious feature inadvertently makes the model more susceptible to the rest of the spurious features in line with the recent empirical results Finally we show how to leverage unlabeled data and the recently introduced Robust Self-Training RST to remove the spurious features but retain the same performance as the full model The new model CoreRST model is robust to changes in s and it can perform when s is not available Empirically we analyze the effect of removing spurious features by training a convolutional neural network on three datasets the CelebA dataset for predicting if a celebrity is wearing lipstick where we use wearing earrings as the spurious feature the Comment-Toxicity-Detection dataset for predicting

0.8181 Performance and intrinsic bias evaluations were performed using a single University of Cambridge Computer Lab GPU machine and the extrinsic using the University of Cambridge Research Computing Services Wilkes GPU cluster In-Domain Classification Table shows the weighted average f-score of each model evaluated in-domain on the held-out test subset of the dataset on which it was trained Bias-mitigated models tend to perform nearly as well in-domain as baseline for Founta et al and Davidson et als datasets On these datasets we see that adversarial debiasing yields slightly higher classification performance than preferential sampling or a combination of approaches Bias-mitigated f-score is roughly equal to baseline for Waseem Hovy and Golbeck et als less biased datasets D WH G B Pr p Pr e Adv p Adv e p e Table Weighted average f-score for baseline and bias mitigated models evaluated in-domain The highest score for each dataset appears in bold Cross-Domain Classification One measure of a systems generalisability is its cross-domain classification performance In this case it is impossible to perform multi-class classification because each dataset uses different labels Therefore in keeping with previous research we restrict cross-domain classification to a binary task between harmful and non-harmful tweets as defined

0.815 in the Double-MNIST dataset the majority different labels and minority same labels When we train a model on more training examples we observe more data variation directions therefore the gap between the full and the core models decreases Multiple spurious features Finally we study multiple spurious features In the celebA dataset we first use hair color and wearing earrings as a spurious feature We then remove the hair color and only use wearing earrings as the spurious feature In line with our theory in Section Table shows that The gap between robust accuracy against wearing earrings and standard accuracy increases when we remove hair color Celebrities who wear lipstick but not earrings have the lowest accuracy due to the use of earrings as a spurious feature This group accuracy drops even lower after removing the hair color attribute RELATEDWORK AND DISCUSSION This work is motivated by work in fairness in machine learning that aims to construct a model that is robust against changes in sensitive features The techniques used in this work is similar to work in robust machine learning that tries to understand the tradeoff between the robust accuracy against perturbed inputs and the standard accuracy In the following

0.814 be explained to the labeler by identifying the subset of features most useful to the labeler for interpretability and displaying the range or variation in those features within the batch Experiments On the Dark Reactions data set we evaluated batch selection strategies within positive uncertainty regions Q-best where the highest uncertainty instances were chosen a random selection and k-means both where the instance closest to the center was chosen and where the most uncertain instance in the cluster was included in the batch Evaluating the resulting quality of the model using the Matthews Correlation Coefficient a measure that takes into account true and false positives and negatives and a batch size of k-means with when the most uncertain instance per cluster was the best batch selection strategy for this data see Figure M C C a ve ra g e o Batch Selection Strategies batch size Random Figure A comparison of batch selection strategies for choosing queries within uncertainty regions on the Dark Reactions data set Using this batch selection strategy and the interpretability method above to describe the uncertainty region we were able to generate explanations of uncertainty associated with each chosen batch with a batch size of A

0.813 the full models reliance on the spurious feature Furthermore this reliance helps some groups eg celebrities who wear earrings and lipstick while it does not have any effect on other groups eg celebrities who wear earrings but not lipstick Recall that the robust accuracy of the core model is exactly equal to its accuracy since it does not use the spurious feature Finally in line with our theory in Proposition the robust accuracy of the full model is always lower than the robust accuracy of the core model Robust Self-Training The forth row of the Table shows the accuracy coreRST model as explained in Section The coreRST ga y om os ex u al b lin d b la it e m u sl im m al e st ra t d je is fe m al e ch in es e yo u n g am er an in d n ch ri st n eu ro p ea n positive negative TPR TNR full model TPR TNR core model Figure The identity terms used as spurious features The Red line indicates the ratio of positive comments containing the identity terms over the negative comments containing the identity term

0.8098 trade-off between fairness by p -rule and anomaly detection performance in all the data sets Note the ranges from to and it is visualized in each plot with the order from left to right respectively In all four datasets the fairness by p -rule value increases as increases The AUC scores decrease in most data sets as increases The Trade-off between Fairness and Anomaly Detection Performance This section analyzes the trade-off between fairness performance and anomaly detection performance of deep fair SVDD We retrain and test the deep fair SVDD under different values of hyperparameter range from to within equation The hyper-parameter controls the weight of the discriminators loss term within the adversarial loss function and directly determines the trade-off between the fairness performance and anomaly detection performance Figure shows the results in all four selected data sets the fairness by p -rule increases as increases The AUC score drops as the fairness by p -rule value goes up for COMPAS MNIST-Invert and MNIST-USPS data sets We have also noticed one different result in the celebA data set both fairness by p -rule and AUC score increase as the increases We have analyzed this case before when comparing deep fair

0.8094 this implies that as the training progresses so does the behavior of the adversarial Our full slate of approximation results are available in Appendix A robustness bias However it is surprising that much more than of the final behavior is determined after the rst epoch and there is a slight increase in agreement by the middle epoch We note that of course adversarial robustness bias is not necessarily an intrinsic value of a dataset it may be exhibited by some models and not by others However in our studies we see that the UTKFace dataset partition on Race/Ethnicity does appear to be significantly prone to adversarial attacks given its comparatively low and values across all models EVALUATION OF ROBUSTNESS BIAS USING RANDOMIZED SMOOTHING In Section we argued that randomized smoothing can be used to obtain lower bounds on G which can then be used to measure robustness bias In this section we audit popular models on a variety of datasets described in detail in Section for robustness bias as measured using the approximation given by randomized smoothing Evaluation of c To assess whether the estimate of G by randomized smoothing is an appropriate measure of robustness bias we compare the

0.805 DCAE achieves higher fairness by p -rule with a balanced training set However the improvements are not ideal because both approaches only satisfied the rule on one data set celebA Moreover for the MNIST-USPS data set both deep SVDD and DCAE become more unfair with a balanced training set Figure c and d shows the distribution distance which reflects the overall fairness of each model The smaller distances indicate the models predictions are more likely to be independent with the sensitive attribute We can observe a similar trend as we have seen in Figure a and b that learning on a balanced training set can only provide marginal improvements We learn from these results that a fair anomaly detection approach is needed to mitigate deep anomaly detection algorithms unfairness Evaluating Deep Fair SVDD We now evaluate our proposed deep fair SVDD networks performance and make a comparison with deep SVDD and DCAE Figure a shows the fairness by p -rule on abnormal groups We can see that deep fair SVDD outperforms both deep SVDD and DCAE in all four data sets Moreover deep fair SVDDs fairness by p -rule are greater than which satisfies the rule advocated by the US

0.8032 the given image Performing this attack on both training and test images allows us to estimate training set membership since models with memorization will be able to better reconstruct the training images than unseen images We use training images and unseen test images images total The model takes the images with the lowest reconstruction loss to be members of the training set The AUC of the white-box attacks represents the success level of the attack at predicting training set membership Higher AUC means that the attacker can more accurately infer if an individuals data was used to train the GAN Result DP Protects GANs From Membership Inference Prior work demonstrates that membership inference is unsuccessful in GANs that are trained on large enough datasets In Figure a we observe this to be true across all privacy levels when the training and test sets are from the same distribution When the training and test set are from different datasets within the same domain GANs trained without privacy are vulnerable to membership inference In this setting of dataset shift DP protects GANs from membership inference We qualitatively analyze the effectiveness of this white-box attack in Figure c Models trained without privacy can

0.8031 It makes sense that a minimal number of training epochs for the new task is unsuccessful because the MLP trained on the fairly encoded data is trained from scratch The results are presented in figure We were able to generate similar results with LAFTR to Fair-MAML using training points from the new task after epochs of optimization These results are given in the appendix We observe that Fair-MAML achieves the best trade off between fairness and accuracy both in terms of demographic parity and equal opportunity In our proposed problem setting LAFTR was not successful at learning with minimal data and a small number of fine-tuning epochs for the new task The pre-trained neural network shows some ability to learn the new task using little data and fine-tuning epochs At low s Fair-MAML is able to achieve higher accuracy than the pre-trained neural network and LAFTR Crucially Fair-MAML is able to learn more accurate representations that are also fairer for a range of s than both of the baselines In order to generalize to new states only communities are needed in order to achieve strong predictive accuracy and fairness using Fair-MAML Fair-MAML with Fairness Warnings Motivation We next consider Fairness

0.8018 norm by putting weight for the spurious feature s d For the prediction we can replace s by which results in the full model implicitly assigning weight of for the second feature while the core model assigns wight of to the second feature As a result if then removing s increases the error implicitly assigns weight to the second feature unseen direction at training while the core model assigns weight As a result if is small then the full model which uses s has lower error and if is large then core model which does not use s has lower error Intuitively by using the spurious feature the full model incorporates into its estimate The true target parameter and the true spurious feature parameters agree on some of the unseen directions and do not agree on the others Thus depending on which unseen directions are weighted heavily in the test time removing s can increase or decrease the error We formalize the conditions under which removing the spurious feature s increases the error We state that in addition to the true parameters the distribution of core-features in train and test data are also critical for identifying the impact of removing

0.8017 the toxicity of a comment where we use identity terms as the spurious features and finally a synthetically-generated dataset where we concatenate each MNIST image with another image and use the label of the new image to be the spurious feature Our empirical results are four folds Removal of the spurious feature lowers the average accuracy and disproportionately affects different groups increase for some groups and decrease for others The full model is not robust against the spurious feature and changing the spurious feature at the test time lowers its accuracy substantially The CoreRST achieves similar average accuracy as the full model while being robust against the spurious feature In the CelebA dataset we show that removing the spurious hair color makes the model less robust against wearing earrings SETUP Let z Rd denote the core features which determine the prediction target y z Let s denote a spurious feature We study the over-parameterized regime where we observe n d triples si as training data For an arbitrary loss function the standard error for a model M Rd R R is error s where the expectation is over test points z We are also interested in robustness of a model

0.7999 u ra Figure Accuracy of DWAC in comparison to a softmax model on the -class Fashion MNIST dataset for varying dimensionality of the output layer Performance is indistinguishable for a DWAC model of the same size but accuracy drops if we decrease the size of the output layer too much T-shirt Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Boot Figure Learned embeddings of the Fashion MNIST training data when using a DWAC model with a two-dimensional output layer the same accuracy can often be obtained using a lower-dimensional representation ie few total parameters In some cases however reducing the dimensionality too much eg two-dimensional output for Fashion MNIST results in a slight degradation of performance On the other hand using a two-dimensional output layer means that we are able to more easily visualize the learned embeddings without requiring an additional dimensionality reduction step eg using PCA or t-SNE In this way we could look directly at where a test instance is being embedded relative to training instances with no loss of fidelity Figure shows the embeddings learned by our model for the Fashion MNIST training data using a two-dimensional output layer Pleasingly there is a natural semantics to this

0.7999 percentage of changed outcomes resulting from varying the random seed prior to initializing and training models as well as from the choice of model architecture Figure shows these results for all of the datasets studied in Section alongside the corresponding measurements for LUF The experimental setup largely follows that described in Section We isolate the effect of each potential variable causing instability unfairness architecture Confidence I n d i v i d u a l s l i p p e d German Credit Confidence Adult Arch Deep LUF Linear LUF Seed Confidence Seizure Confidence FMNIST Confidence LFW Figure Arbitrariness in decision outcome as a result of changes in random seed and small changes in architecture are presented alongside expected LUF ie arbitrariness from small changes in the training set Calculation methods are described in We present these results to motivate a wider connection between learning algorithm stability and fairness beyond LUF Notice the difference in scale across graphs random seed and leave-one-out unfairness in its own experiment keeping other sources of instability controlled For the random seed experiments we train the same model with different random seeds and calculated the effects of instability in the same manner as calculating

0.7999 queries the -layer network is identifiable in one query compared to membership queries x decrease in the number of queries needed to reconstruct the model On CIFAR we find that the convolutional network which is the same as the convolutional network used for MNIST also has at least a x decrease in the number of queries needed On the other hand VGG and Resnet- show only a x decrease in the number of queries needed to reach accuracy Complexity of task We find that the relative reduction in queries needed seems to depend on the complexity of the model class rather than the complexity of the task But not surprisingly the absolute number of queries needed increases with the complexity of the task On both MNIST and CIFAR gradient queries lead to a x decrease for reconstructing the convolutional network suggesting that for the relative decrease in query complexity depends more on the complexity of the model class than the complexity of the task However as might be expected for both gradient and membership queries the absolute number of queries needed increases as the complexity of the task increases On MNIST the convolutional model is reconstructed in gradient queries compared

0.7999 available and for naive Naive estimation Cor Cor Cor Cor Naive e st b s tr b s Figure Experiment on the UCI Adult dataset independence assumption is violated Absolute difference between the true bias and the estimated bias for direct and corrected Cor estimation when to points of common data over are available and for naive Naive estimation In the general setting the estimated bias and the true bias are related as follows Theorem The true bias can be derived from as sr r s r s s r where Pa x a y Pa x a y and are as defined in Proof To see notice that from the definition of we have Pf x y a Pf x y a Py a Pa x y a Pa x y a Py a a Py a a Sample size e st b s tr b s Uniform Positive sampling Active sampling Sample size e st b s tr b s Direct estimation Uniform Direct estimation Positive Figure The absolute difference between estimated bias and the true bias based on different active sampling strategies left and based on direct estimation over selected samples using uniform/positive sampling right Both are

---------------------------------------------------------------------------
TOPIC 5: risk-assessment

---------------------------------------------------------------------------

0.6497 calculated in this model and will also demonstrate that there always exist pricing schemes where both low and high risk policyholders strictly benefit from being pooled together regardless of their risk levels Finally Table also shows an example of insolvency-based premium calculation but when high risk players are even riskier they each have a chance of a loss As expected the total amount of money that needs to be collected in premiums is higher Here even-split pricing still lowers the high-risk policyholders price but it increases the low risk policyholders price compared to being in a separate insurance pool Surprisingly under proportional pricing the low risk policyholders pays less than when did when the low risk policyholders risk was lower at Later results will show that this kind of anti-social incentive for other policyholders to have higher risks is a necessary feature of pricing policies like this MODEL AND ASSUMPTIONS Model and terminology Insurance model We consider the case where there are two types of people low risk or high risk There are a total of low risk players and high risk players Each person is considering purchasing insurance that would cover them completely in case of a specific loss

0.6321 next section will analyze how such discretion hinders risk assessments as a tool for achieving substantive criminal justice reform De ning Risk Risk assessments aim to predict risk defined as the likelihood of crime Pretrial risk assessments estimate the risk that a defendant will be rearrested before trial or will not appear for trial sentencing and parole risk assessments estimate the risk that a defendant or inmate will recidivate Such predictions typically consider a period of time ranging from six months to two years Forecasting crime while ignoring the impacts of incarceration causes risk assessments to overvalue incarceration Releasing someone by definition increases that persons likelihood to commit a crime in the near future If crime risk is the primary criterion then release will always appear to be adverse Yet there are many harms associated with incarceration Pretrial detention significantly increases a defendants likelihood to plead guilty be convicted and receive long prison sentences Time spent in prison is associated with negative outcomes including sexual abuse disease and severe declines in mental and physical well-being After being released former inmates face significant challenges in ending work a barrier that is stronger for blacks than whites and su er disproportionately from

0.6232 risk players The proof is given in Appendix A The corollary shows that this implies that the grand coalition is core-stable Lemma For max-subsidy pricing is decreasing in and constant in is decreasing in both and Corollary Assume a submodular cost function with max-subsidy pricing The grand coalition where all players are in the same insurance pool is core-stable P Showing that the grand coalition is core-stable means that there does not exist a group of players whose members all strictly prefer being together to being in the grand coalition Any low risk player is indifferent between any arrangement that has low risk players and pays higher cost in any coalition with so there is no set where the low risk players get strictly lower price High risk players most prefer being with more low risk players and high risk players so being in the grand coalition is their optimal arrangement Next we investigate the max part of max-subsidy pricing we show that any pricing scheme where the high risk players pay less than max-subsidy is one where the low risk players have an incentive to defect Lemma Assume a submodular cost function Then any pricing scheme where the high

0.6195 we take a solidaristic perspective in how we might divide this cost The rst section implements even split pricing where both players pay the same amount This approach most closely matches what advocates of solidarity might suggest Interestingly we show that sometimes even-split pricing can strictly benefit both the low risk and high risk players financially However there are also situations where even-split pricing is too aggressive and ends up hurting both the low risk and high risk players Next the second section explores a more flexible notion of fairness one where we minimize the cost paid by the more expensive high risk participants while maintaining stability This pricing scheme might be useful in cases where we wish to financially support high risk players as much as we can without causing low risk players to wish to defect Finally the last section applies these results specifically to the insurance premium case and discusses the implications for the fairness debate Even-split price Consider the even-split pricing scheme defined below definition With even-split pricing both the high and low risk participants pay the same amount AA As mentioned before this pricing scheme follows a natural philosophy of solidarity all participants should pay

0.6133 information asymmetry because it is impossible to distinguish between the low risk and high risk participants The grand coalition pool falls apart even though it is possible to produce a pricing scheme where both low risk and high risk players benefit from being combined Similarly this result matches the analysis in works like Kasy and Abebe which demonstrated that in certain situations enforcing fairness can reduce welfare for both groups Max-subsidy In this section we explore the following pricing scheme that aims to help subsidize high risk players price as much as possible while still ensuring low risk players have an incentive to participate in the grand coalition As mentioned before this is a more flexible notion of solidarity than even-split pricing we will explore it as a complement to the results we derived there First we define the pricing scheme definition Max-subsidy pricing follows this pricing policy A A A A A A A A First we will show that with this pricing scheme high risk players most prefer being in the grand coalition while low risk players most prefer being with as many other low risk players as possible and do not care about the presence of high

0.613 Case Study Predictive Fairness to Reduce Misdemeanor Recidivism Through Social Service Interventions The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses Often due to constraints of caseload and poor record linkage prior interactions with an individual may not be considered when an individual comes back into the system let alone in a proactive manner through the application of diversion programs The Los Angeles City Attorneys Office recently created a new Recidivism Reduction and Drug Diversion unit RD tasked with reducing recidivism in this population Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting The program seeks to improve outcomes by developing individually-tailored social service interventions ie diversions conditional plea agreements stayed sentencing or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods for individuals likely to experience subsequent interactions with the criminal justice system a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case

0.6087 the RAI accurately predict the likelihood of an adverse outcome under the observed historical decisions This evaluation approach can be misleading since Y Y For instance it will conclude that a valid counterfactual model of risk under baseline performs poorly because its predictions will be systematically inaccurate for cases that are responsive to treatment Evaluation on the Control Population The standard counterfactual approach to evaluation computes error metrics on the control population The PR curve evaluated on the control population plots Y T against Y T and the ROC and calibration curves are similarly defined by conditioning on T When the control population is not representative of the full population ie T X as is the case in nonexperimental settings this evaluation may be misleading since T T A method that performs well on the control population may perform poorly on the treated population or vice-versa In child welfare cases where the perpetrator has a history of abuse are more likely to be screened in Since there is more information associated with these cases a model may be able to discriminate risk better for these cases than on cases in the control population with little history Doubly-robust DR Counterfactual Evaluation

0.6011 risk assessment earned the lowest The other two groups both earned average rewards of and were statistically indistinguishable Analyzing behavior through the lens of the two most common strategies yields complementary evidence for disparate interactions ie interactions with the risk assessment that lead participants to disproportionately make higher risk predictions about black defendants and lower risk predictions about white defendants Influence of risk scores Because we presented the same population of defendants to the control and treatment groups we could directly measure how presenting the risk score to participants affected the predictions made about each defendant For each defendant we measured the influence of the risk assessment on the treatment groups predictions as described in Equation excluding the defendants for whom r c The risk assessment exhibited an average influence of as this number is greater than it suggests that treatment group participants placed more weight on the risk assessment than on their own judgment A twosided t-test found no statistically significant difference between the risk assessments influence when its prediction was less or greater than the control groups prediction r c or r c respectively Splitting the defendants by race tells a more complex story Figure When the risk

0.5977 No Safety Threat Impending danger or Present danger An overall risk-safety score is obtained by multiplying the risk and safety scores together Figure shows the risk-safety score breakdown across the range of AFST ventile scores and the Right panel of Figure shows a breakdown of the screening decisions across the range of risk-safety scores The two panels in Figure show a breakdown of the risk and safety scores by AFST score ventile We find that there is a much stronger correspondence between screening decisions and the call worker assessed risk-safety score than with the AFST score This suggests that call workers are largely continuing to rely on their own assessments rather than those of the AFST tool Furthermore there is no clear association between the risk and safety assessments provided by the call workers and the AFST ventile score An analysis is underway to better understand what factors influence override decisions and whether the overrides and weak reliance on the AFST tool is negatively affecting the overall quality of the screening process Given that decisions are being made with only weak adherence to the AFST tool it is far from clear that the predictive fairness properties of the models would

0.5976 possible policy-holders of them have a chance of suffering the loss low risk and have a chance of suffering the loss high risk Table describes this scenario with expected-value premiums as assumed in much of the insurance literature The rst row describes the premiums collected if low risk and high risk individuals are in separate insurance pools potentially at different companies The second and third rows consider two different ways of pricing premiums when all of the individuals are in the same insurance pool One clarifying point it is important to distinguish between the reduction in variance a policyholder purchases when she buys insurance and the reduction in variance that occurs in an insurance pool when more people are added The rst case is a motivating reason why people purchase insurance but the second case is a reason why larger insurance pools are helpful and is a major focus of this paper Note that the total amount of premiums stays the same in all three situations which is a feature of the expected value pricing scheme The second row has even-split pricing which might be consider solidaristic Note that the low risk policy-holders pay strictly more and the high risk

0.5961 Total Low risk High risk Separate pools Pooled even-split pricing Pooled proportional pricing Table Example with solvency-based premium calculation with same risk levels as above Total Low risk High risk Separate pools Pooled even-split pricing Pooled proportional pricing Table Example with insolvency-based premium calculation where risk level is the same for low risk group but for the high risk group purchasing a reduction in the variability of their costs For example consider an individual purchasing insurance in the model above Without insurance her loss in each time period would have expectation A and standard deviation p A A In the case that is large this standard deviation could be quite large she might need to establish a costly financial cushion to handle this uncertainty in her losses If she purchases insurance her loss each time period is equal to her premium with standard deviation The ability to have consistency in her losses might be very valuable to her which explains why she would be willing to pay a premium that is strictly greater than her expected loss MOTIVATING EXAMPLE Consider a scenario with a single insurable loss with value for example consider insuring a car against total loss There are

0.5946 likely to reduce crime risk and improve well-being across the population Because risk assessments focus on individuals they can entrench historical injustice by failing to recognize changing social circumstances Risk assessments as with all machine learning assume that population characteristics are constant such that factors producing certain outcomes in the past will produce those outcomes at the same rates in the future Even if jurisdictions enacted reforms that reduce crime risk assessments would be blind to these new circumstances In turn risk assessments would overestimate crime and recommend incarceration for individuals whose crime risk has decreased Following interventions such as text messages that remind defendants to appear in court risk assessments have produced zombie predictions that overestimate risk because they fail to account for the risk-reducing benefits of these reforms And because incarceration increases the likelihood of crime after someone is released these false positive predictions will exacerbate the cycle of recidivism and incarceration that risk assessments are meant to remedy Risk assessments su er from a similarly individualistic approach to bias they diagnose bias as a behavior exhibited by individuals typically due to implicit bias Risk assessments are therefore designed to replace the discretion of biased judges with objective

0.5944 readily implemented use of PRM within the Allegheny County child protection context was one in which a model would be deployed at the time a referral call was was received by the County The objective was to help call workers determine whether a maltreatment referral is of sufficient concern to warrant an inperson investigation referred to as screening-in the call Calls that are thought to be innocuous and are not further investigated are said to be screened-out In this section we describe the development and implementation of the Allegheny Family Screening Tool AFST Then and now Allegheny Countys Department of Human Services is fairly unique in the United States it has an integrated client service record and data management system This means that the Countys child protection hotline staff are in principle already able to access and use historical and crosssector administrative data eg child protective services mental health services drug and alcohol services homeless services related to individuals associated with a report of child abuse or neglect Although this information is critical to assessing child risk and safety concerns it is challenging for County staff to efficiently access review and make meaning of all available records Beyond the time

0.5939 the same amount For a cost function that is linear as is assumed in much of the literature on insurance such a pricing scheme must strictly hurt the low risk players and strictly help the high risk players For a submodular cost function the result is more complicated The lemma below describes a situation where even-split pricing makes the grand coalition core-stable that is no subgroup of players wishes to deviate and form their own pool Lemma With even-split pricing if the below inequality is satisfied then the grand coalition c is core-stable A A A The proof is presented in Appendix A It is worth pausing to realize why Lemma is useful The inequality states the grand coalition is stable against the deviation where all the low risk players form their own group in c The lemma tells us that the inequality implies that something stronger that the grand coalition is stable against deviations from every other possible combination of low risk and high risk players c Overall this result suggests that there exist situations where both the low risk players and the high risk players benefit financially from solidarity as implemented in even-split pricing To understand why this

0.5872 behind risk assessments is to eliminate the subjective biases of judges risk assessment implementations allow judges to decide how to respond to the information and recommendations provided Many judges use this discretion to ignore risk assessments or to use them in selective ways In both Kentucky and Virginia risk assessments failed to produce significant and lasting reductions in pretrial detention because judges tended to override recommendations suggesting release Judges in Cook County Illinois diverged from the pretrial risk assessment of the time releasing defendants at drastically lower numbers than recommended A juvenile risk assessment faced similar issues judges frequently overrode the risk assessment when it recommended release but rarely when it recommended incarceration leading to a dramatic and chronic increase in detention Similar patterns have been observed in Santa Cruz and Alameda County California When they do use risk assessments judicial decisions are rife with biases Two experimental studies found that people are more strongly swayed by a risk assessments suggestion to increase estimates of crime risk when evaluating black defendants compared to white defendants Judges in Broward County Florida have penalized black defendants more harshly than white defendants for being just above the thresholds for medium and high risk

0.5834 reconfigured to help in the effort of reducing the risk of recidivism and lowering incarceration rates Risk assessments were reframed as a way of effectively identifying and allotting scarce resources to programs and interventions that were proven to work Andrews and Bonta Andrews et al Cullen and Genderau Etienne To this end a third generation of actuarial tools integrated factors that were conceived as criminogenic needs or intervenable factors that are believed to impact ones risk of recidivating These included a more diverse set of inputs such as employment status and history of substance abuse which effectively reconstituted risk as a dynamic phenomenon that could be intervened upon Andrews et al While many of these dynamic factors held less predictive power than the static attributes included on second generation tools these needs factors were desirable because they enabled criminal justice practitioners to consider a broader set of treatment options aimed at lowering risk Thus predictive risk and intervenable needs factors were combined into a hybrid risk needs model one that used regression to identify statistically significant risk needs factors The key difference between second and third generation assessments then was the incorporation of these less predictive but dynamic risk needs

0.5828 method reduced exclusion errors for each population subgroup studied in Costa Rica urban/rural gender_head with_children family_size region segmentation attribute di ar A status quo AI-based a Colombia urban/rural gender_head with_children family_size region segmentation attribute di ar A status quo AI-based b Costa Rica Figure Subgroup disparities for the status quo versus AI-based methods according to five segmentation attributes The AI-based method substantially reduces disparities across most segmentation attributes in both countries Similarly agencies like USAID establish a per-country poverty assessment tool These central indices are then consumed in a distributed manner by a wide diversity of social programs and NGOs serving as core and unified criterion for prioritizing and selecting beneficiaries in the country The use of the central methodology is most often mandated by law in an effort to focus public funds on the population segments most in need as well as avoiding manipulation of public funds for political or private interests However the centralization of prioritization and selection criteria creates strong tensions across the diversity of institutions mandated to consume the central poverty index These institutions include everything from cash transfer programs to subsidized healthcare systems subsidized energy housing and child care services scholarship programs pensions to non-formal

0.581 screener and call screening supervisor review when deciding if the referral should be investigated Beyond this point the risk scores do not impact the referral progression process Figure Referral Progression Process Provide family with information for other services or agencies they may find helpful New Child Welfare Case Opens Investigation Findings/Service Decision Provide family with information for other services or agencies they may find helpful Call information received and processed Assigned Call Screener collects additional information from sources including but not limited to the individual who reported the maltreatment and the Client View application that displays individual-level prior service involvement Call Screener assigns risk and safety ratings based on information collected NEW STEP Call screener runs the Allegheny Screening Tool Consultation with the Call Screening Supervisor In limited cases a field screen is conducted Call Screening Process Child Welfare Call Screening Decision Walden Figure Referral progression process The decision point that relies on the AFST is highlighted assessment of the risk and safety of the alleged victims The risk score denotes whether the call worker believes the case to be Low Moderate or High risk The safety score denotes whether the call worker believes the alleged victims to be at

0.5731 of offender risk While earlier tools were designed to facilitate effective treatment new statistical assessments were based on a static notion of risk one that was not easily changed through intervention These assessments gave rise to what some scholars have dubbed a new penology whereby penal policies shifted away from rehabilitative interventions to more administrative approaches to population management that rely almost exclusively on incapacitation to mitigate risk Feeley and Simon As Maurutto and Hannah-Moffat argue when assessments pivot towards prediction rather than intervention risk is conceived of as a negative strategy that incapacitates and manages but never produces productive transformations Maurutto and Hannah-Moffat Since the s the shift towards standardized decisionmaking and away from individualized treatment accompanied a sharp increase in incarceration rates Harcourt These prediction-oriented risk assessments were a part of a suite of tools and policies introduced in the s and s that relied heavily on prior criminal history to predict and systematize interventions to prevent future crime Other policies included state and federal sentencing guidelines which were ostensibly designed to standardize decision making across jurisdictions and minimize bias in sentencing A growing body of scholarship has linked these colorblind policies to trends in mass incarceration and

0.5708 growing racial disparities in the criminal justice system Van Cleve and Mayes Schlesinger Harcourt Hamilton Scholars have argued that prediction-oriented risk assessments produce a ratchet effect on profiled populations because they fail to take into consideration the criminogenic nature of the criminal justice system itself Harcourt Hamilton Studies have demonstrated diminishing returns on the use of incarceration as a means of lowering crime Zimring In fact the ripple effects of incarceration such as the weakening of family ties and diminishing employment opportunities can exacerbate the likelihood of future recidivism at both the individual and collective level Cullen et al DeFina and Hannon Mitchell et al As Hamilton has argued assessments that rely on a static prediction-oriented notion of risk can act as a self-fulfilling prophecy that justifies and widens the net of social control over marginalized populations Hamilton When risk is constituted as a static phenomenon it can only suggest what should not be done ie release and gives little insight into what can be done to improve outcomes Beck By the s critiques of this prediction oriented approach started to emerge heralding in the next iteration of assessment Proponents of evidence-based justice reform argued that risk assessment should be

0.5703 revenue as well as increase access and improving targeting abilities This is in general a biased direct method and we take care to avoid extrapolation from interest rates However for the sake of comparing against constant treatment assignment using the direct method reduces variance price ex pe ed se en t r en e maj min a Revenue curves for segments price no term al d en Ab Aa b Distribution of constant interest rate Es tim at ed re ve nu e E Y p personalized price c Estimated revenue comparison interest rate E D p A Low I non-Low I d Comparing take-up Figure Willingness to pay for elective vaccine Figures a and b and Microcredit Figures c and d

0.5681 DISTRIBUTED GOVERNANCE VIA INTERACTIVE DECISION SUPPORT Sections and demonstrate that the AI-based targeting system provides superior accuracy both at the global and subgroups levels and that it reduces disparities among population subgroups when compared to status quo methods Unfortunately these results do not entail that the AI targeting system is fair in absolute terms On the contrary Figure shows that substantial performance disparities exist demanding careful reflection on algorithmic fairness and urban rural urban ex cl us io n er ro r st at us q average unbalance a COL status quo urban rural urban ex cl us io n er ro r A as ed average unbalance b COL AI-based urban rural urban Im pr ov em en t c COL improvement family_size ex cl us io n er ro r st at us q average unbalance d CR status quo family_size ex cl us io n er ro r A as ed average unbalance e CR AI-based family_size Im pr ov em en t CR improvement Figure Subgroup-level performance for two examples of country-attribute pairs All subgroups substantially benefited from switching to the AI-based method Error bars denote confidence intervals Urban Gender of Head Children Family Size Subgroup

0.5647 factors that could inform the selection of treatment interventions beyond incapacitation By the s these efforts were further supplemented by fourth generation tools that added responsivity factors such as levels of intelligence self-esteem and psychological disorders in order to improve response outcomes to treatment Andrews and Bonta The risk-needs-responsivity RNR framework has reasserted the authority of a correctional treatment approach one that envisions offenders as individuals capable of reform through intervention Today risk assessments are used for two primary purposes which Monahan and Skeem deem prediction-oriented and reduction-oriented approaches to assessment Monahan and Skeem Prediction-oriented assessments are used to facilitate accurate and efficient prediction of future recidivism while reduction-oriented tools are intended to inform treatment and supervision plans In recent years weve seen a resurgence of public interest in prediction-oriented assessments particularly at the pretrial stage where release decisions are viewed as essentially a forecasting task Yet we argue that even pretrial decisions should be characterized as a moment of intervention rather than mere prediction The push to adopt pretrial assessments like the Public Safety Assessment is tightly coupled with efforts to eliminate cash bail which empirical analysis has demonstrated to be ineffective at lowering near-term risks failure to appear

0.5646 are consistently above across the entire range of ventile scores Prior to seeing the AFST score call workers are asked to enter in two scores reflecting their When a call scored or above the calls were labeled as mandatory screen-ins and only supervisors were allowed to screen them out ventile score Sc re en in d is io n ea kd ow n Decision Unknown/Pending Already active Screen out Screen in AFST Call worker assessed score S cr ni ni g de si on b re do n Decision Unknown/Pending Already active Screen out Screen in Figure Left Breakdown of screening decisions by AFST ventile score Ventile scores of indicate mandatory overrides Right Breakdown of screening decisions for call worker assessed risk-safety score Data consists of referrals that have been screened since the AFST was first deployed Using the Model in Practice The intent of the model is to inform and improve the decisions made by the child protection staff As stated in the background it was never intended that the algorithm would replace human decision-making To implement the model a supplemental step in the call screening process was added to generate re-referral and placement risk scores that the call

0.5608 evaluating black defendants participants were more likely to deviate positively from the risk assessment and less likely to deviate negatively participant predictions matched the risk assessment at an equal rate for both races Participant deviations from risk scores For each prediction made by participants in the treatment group we measured how far and in which direction that prediction deviated from the risk assessments recommendation That is we measured p r The average deviation among the treatment group predictions was with a median deviation of Participants deviated to a higher risk prediction of the time matched the risk assessment of the time and deviated to a lower risk prediction of the time The results from Section suggest that these deviations tend to make participant predictions less accurate than the risk assessment As in the previous section these statistics differ by defendant race While the average deviation for white defendants was the average deviation for black defendants was p difference in means This difference emerged because participants were more likely to deviate positively from the risk assessment when evaluating black defendants and to deviate negatively when evaluating white defendants the average deviation magnitude was the same across race for both positive and

0.5604 characteristics This section contains some additional notes on possible objections to the insolvency-based premium model First this model as written assumes that the insurance company doesnt have any stockpile of money it could use as a cushion in case costs are unexpectedly high In reality insurance companies would almost surely have such a financial cushion but it also seems certain that would want to be compensated financially for the opportunity cost of not using this money in other ways A reasonable solution would be to have the policyholders pay enough in premiums to at least compensate the insurance company for lost interest on the insurance stockpile such a scheme would produce a premium of the same form but with a constant in front of the Secondly the function as written produces an average premium that is strictly higher than the expected value of losses Some might object to this why would someone pay more than their expected loss The key is that an individual purchasing insurance is Total Low risk High risk Separate pools Pooled even-split pricing Pooled proportional pricing Table Example of pricing with expected-value premiums for low risk group chance of suffering loss and high risk groups chance

0.559 referrals Implementation challenges As discussed in Section data logs from the first year of implementation indicate that supervisors ventile score C al l or ke r r is sc or e ea kd ow n Risk_Score High Risk Moderate Risk M Low Risk L A ventile score C al l or ke r s et y sc or e ea kd ow n Safety_Score Present Danger Impending Danger No Safety Threat Figure Call worker assessed Risk left and Safety right scores across the AFST score ventiles are overriding in every mandatory screen-ins Furthermore these rates differ from one supervisor to the next The data indicates that the tool is resulting in lower screen-in rates for lowest risk cases but the override decisions for the highest risk cases have been difficult to explain A challenge in the implementation of the AFST is that it coincided with the major reform in the child welfare laws in the State of Pennsylvania that we previously discussed One effect of these changes is that they established a State wide hotline These State calls are handled slightly differently by the County staff and the algorithms deployment was not well integrated into this new business process This

0.5585 the true counterfactual evaluation Notably the observational evaluation suggests the observational model outperforms the counterfactual model The true counterfactual evaluation shows the counterfactual model performs better Child Welfare We also apply counterfactual learning and evaluation to the problem of child welfare screening The baseline intervention is screen-out which means no investigation occurs The data consists of over calls to the hotline in Allegheny County Pennsylvania each containing more than features describing the call information as well as county records for all individuals associated with the call The call features are categorical variables describing the allegation types and worker-assessed risk and danger ratings The county records include demographic information such as age race and gender as well as criminal justice child welfare and behavioral health history The outcome is re-referral within a six month period Our approach contrasts to prior work which used placement out-of-home as the outcome This outcome is only observed for cases under investigation therefore it cannot be used to identify Y the risk under no investigation We use random forests to train the observational and counterfactual risk assessments as well as the propensity score model We used reweighing to correct for covariate shift but did not observe

0.5579 last equation is saying that the cost associated with low risk individuals plus the cost associated with high risk individuals is equal to the cost associated with low risk individuals combined with high risk individuals which violates the fact that is strictly submodular Next we will show that there must exist some anti-social incentives for example any pricing scheme where is increasing with A on some interval A is also one where players have an incentive to defect Lemma Assume that lim A A A for some constant Then it is not possible to have a pricing scheme that satisfies all three of the following properties efficiency Aligned incentives low risk players prefer that high risk players have lower risks on some interval including A A A A A Stability for every level of risk A A both low risk players and high risk players benefit by being pooled together Property is stated from the perspective of the low risk player but the same logic in the statement and proof would work if it were stated from the perspective of the high risk player Note that Lemma does not exclude the case where the low risk players cost might stay

0.5499 justice and how fair algorithms can reinforce discrimination CONCEPTS Social and professional topics Computing technology policy Applied computing Law KEYWORDS risk assessment criminal justice system fairness social justice INTRODUCTION Across the United States many oft-opposed groups have united around risk assessments as a promising path forward for the criminal justice system Democrats and Republicans conservative states and liberal states criminal defense organizations and prosecutors In turn risk assessments have proliferated in recent years in of people in the US lived in a jurisdiction using a pretrial risk assessment compared to just four years prior A scan of US jurisdictions found that more than two-thirds used a pretrial risk assessment Risk assessments are mechanisms for identifying potential risks the likelihood of those risks manifesting and the consequences of those events Within the criminal justice system risk assessments are most widely used in the contexts of pretrial detention to predict the likelihood that a criminal defendant will fail to appear in court for trial and in some jurisdictions will commit a crime before trial and sentencing and parole to predict the likelihood that a defendant or inmate will commit a crime in the future Although actuarial risk assessments have existed within the

0.5498 determines the risk assessments influence on participants We split defendants into two categories those for whom r c Group and those for whom r c Group For each group we regressed the algorithms influence on predictions about each defendant Equation on that defendants demographic attributes and criminal background along with the value of r c For Group the risk assessment exerted more influence as r c increased but less influence for defendants with a previous failure to appear on their records For Group the risk assessment similarly was more influential as r c increased Three other attributes were also statistically significant the risk assessment exerted more influence on participants making predictions about black defendants defendants who were arrested for a violent crime and defendants with more prior convictions Thus when r c participants were more strongly influenced to increase their risk predictions for black defendants in two ways they responded both directly to race and to a feature that is correlated with race prior convictions Table A Percent of predictions White Black Increase risk risk Figure The rate at which participants deviated from the risk assessments prediction toward higher and lower levels of risk broken down by defendant race When

0.5379 a pricing scheme that maximally subsidizes the high risk group while maintaining an incentive for lower risk people to stay in the insurance pool Next we demonstrate that with this new model the price charged to each individual has to depend on the risk of other participants making naive actuarial fairness inefficient Furthermore we prove that stable pricing schemes must be ones where players have the antisocial incentive desiring riskier partners contradicting motivations for using actuarial fairness Finally we describe how these results relate to debates about fairness in machine learning and potential avenues for future research CONCEPTS Theory of computation Algorithmic game theory Applied computing Law Economics KEYWORDS cooperative game theory submodular cost function fair cost sharing insurance solidarity actuarial fairness INTRODUCTION Imagine the following situation an individual wishes to purchase car insurance and goes to her local insurance company The insurance company combines her with other people insuring cars of the same value Within the insurance pool some people are at low risk of being in an accident and some people at a high risk The insurance company calculates a total amount of premiums that it needs to collect from the entire set of people higher risk individuals

0.5374 such that price affects outcome such as anchoring to reference prices which attenuates future take-up or sunk-cost fallacies when high prices encourage usage/non-wastage this is explored in Alternatively interest rates might affect default probability if individuals are liquidity-constrained Evidence is mixed in lending assumes this finds no effect and finds some effect of rates on default Alternatively interest rates might have an effect on the extensive demand margin amount borrowed In this setting we might instead consider price as a continuous treatment with a composite outcome of take-up and observed outcome conditional on take-up such as amount borrowed or default outcome From the perspective of optimization we generally view price as a treatment and optimize for corresponding downstream outcomes eg conduct an intention to treat analysis CASE STUDIES Willingness to pay for elective vaccine We build a case study from a willingness-to-pay study for vaccination against tick-borne encephalitis in Sweden The vaccine for tick-borne encephalitis TBE is elective and the study is interested in assessing determinants of willingness to pay to inform health policy Demand is associated with price and income as well as individual contextual factors such as age geographic risk factors trust perceptions and knowledge about tick-borne disease

0.5345 whereas the DR evaluation suggests the counterfactual model performs better Since we do not have access to the true counterfactual to validate these results we further consider how well the models align with expert assessment of risk Expert Evaluation At various stages in the child welfare process social workers assign treatment based on their assessment of risk Social workers decide whether to screen in a case for investigation whether to oer services for a case under investigation whether to place a child out-of-home after an investigation Assuming that social workers are competent at assessing risk we expect the group placed out-of-home to have the highest risk distribution followed by those offered services and we expect those screened out to have the lowest risk Figure shows that the counterfactual model exhibits this expected behavior whereas the observational model does not The observational model assesses the screened out population to have more high risk cases than any other treatment group The observational model underestimates risk on the treated groups These cases should be assigned treatment but the observational model would suggest they should be screened out Such a mistake can have cascading effects We are particularly concerned about screening out cases that

0.5333 Beer Together How Externalities of Size Complicate Notions of Solidarity and Actuarial Fairness Consider a cost-sharing game with players of different costs an example might be an insurance company calculating premiums for a population of mixed-risk individuals Two natural and competing notions of fairness might be to a charge each individual the same or b charge each individual according to the cost that they bring to the pool In the insurance literature these approaches are referred to as solidarity and actuarial fairness and are commonly viewed as opposites However in insurance and many other natural settings the cost-sharing game also exhibits externalities of size all else being equal larger groups have lower average cost In the insurance case we analyze model where costs strictly decreases with pooling due to a reduction in the variability of losses In this paper we explore how this complicates traditional understandings of fairness drawing on literature in cooperative game theory First we explore solidarity we show that it is possible for both groups high risk and low risk to strictly benefit by joining an insurance pool where costs are evenly split as opposed to being in separate risk pools We build on this by producing

0.5333 Judicial decisions made with a risk assessment in Kentucky similarly increased racial disparities in pretrial outcomes It is clear that the rst assumption behind risk assessments that they replace biased discretion with neutral objectivity does not hold up to scrutiny Despite being hailed as objective risk assessments shift discretion to different people and places rather than eliminate discretion altogether Yet the presence of subjective judgment is not itself dispositive as an argument against risk assessments For if the objectivity sought in risk assessment discourse is impossible then any reform will rest to some degree on discretion It is therefore necessary to turn to the second assumption motivating risk assessments and evaluate with these subjectivities in mind whether risk assessments can spur criminal justice reform CRIMINAL JUSTICE REFORM Although advocates tend to assume that risk assessments will promote criminal justice reform altering decisionmaking procedures to promote fairness and objectivity does not necessarily reduce incarceration and racial discrimination Sentencing reform offers a striking case of how the well-intentioned pursuit of administrative perfection characteristic of twentieth century civil rights reforms ultimately accelerated carceral state development In concerned about the racial disparities produced by the judicial discretion to set criminal sentences Congress passed the

0.5294 of value where such a loss can happen either zero times or once during the insurance time period The low risk policyholder has probability A of suffering such a loss while the high risk policyholders has probability A such probabilities are perfectly known to all participants as well as the insurance companies We will assume that A A that all losses occur independently of each other and that the insurance pool is using insolvency-based premium calculation General model beyond insurance We again assume there are two types of people each with a low or high cost associated A R A R The total cost generated by a set of low risk players and high risk players is A A The cost is monotone and high risk players are more costly for any A A the increase in from adding a single low risk player is strictly lower than the increase in from adding a single high risk player We will assume the function is strictly submodular in as in Lemma and also that it is continuous in A A Additionally we will assume that a player with cost contributes nothing to the total cost implying that a pool whose members

0.5279 policy-holders pay strictly less than if they were in separate insurance pools this is a necessary property of any solidaristic pricing scheme with this model The third row describes proportional pricing which might be viewed as an actuarially-fair pricing scheme Note that both types of policy-holders pay amounts proportional to their risk in this case they neither benefit nor are hurt from being in an insurance pool together Next Table analyzes the same scenario but under the insolvency-based premium calculation with externalities of size as discussed previously This example uses to give a chance of insolvency Note that the total cost of insuring all of the individuals is lower when they are pooled together as opposed to being in separate homogeneous pools The second row reflects equal pricing both low and high risk policy-holders see their costs decrease From the perspective of the low risk policyholders the decrease in overall costs due to externalities of size outweighs the costs of pooling with a higher risk group The last row contains the values for proportional pricing which shows that both types of policy-holders see strictly lower costs than they would get alone Later sections will describe exactly how proportional pricing is

0.5266 is a chimera rather than removing discretion to create neutral and objective decisions risk assessments shift discretion toward other people and decision points Second drawing on legal critiques of rights as tools for achieving just outcomes I describe how risk assessments are an ill-advised tool for reducing the centrality and legitimacy of incarceration risk assessments are indeterminate tools that provide no guarantee of reducing incarceration are made intellectual by their individualistic conceptions of risk and bias and are likely to legitimize the structure and logic of criminal punishment Rather than presenting a viable approach to decarceral criminal justice reform risk assessments present a superficial solution that reinforces and perpetuates the exact carceral practices that require dismantling Risk assessments can however be reinterpreted to point toward more substantive criminal justice reform A proper challenge to risk assessments requires not technical or procedural reforms but an epistemic reform that provides a new interpretation of both risk assessments and the criminal justice system Thus having analyzed the impacts of risk assessments within the criminal justice system I turn to questioning what risk assessments tell us about the criminal justice system Returning to the fairness of risk assessments I reinterpret recent results regarding the

0.5245 the observational model is not accounting for that fact that treatment reduced risk for the screened-in cases We see further evidence that the observational model performs poorly on the treated population in the drop in ROC curves between the control evaluation and DR evaluation in Figure Deploying such a model would mean failing to identify the people who need and would benefit from treatment The observational and control evaluations do not show this significant limitation DR evaluation is the only evaluation that illustrates the poor performance of the observational model on the treated population We also evaluate the different models according to whether they are equally predictive in the sense of being equally well calibrated across racial groups Research suggests child welfare processes may disproportionately involve black families Here we ask whether the observational or counterfactual model is more equitable We compare calibration rates by race in Figure The observational evaluation suggests that the counterfactual model of risk is poorly calibrated by race The DR evaluation shows that the counterfactual model is well-calibrated by race and indicates that the observational model underestimates risk on both black and white cases Overall the observational evaluation suggests that the observational model performs better

0.5228 for all the people named on a call could take hours The increasing pressure on ensuring that investigative resources are focused on the highest risk children means that there is some potential for predictive analytics to assist call screeners to more quickly and accurately assess each referral Predictive Risk Modelling PRM uses routinely collected administrative data to predict the likelihood of future adverse outcomes By strategically targeting services to the riskiest cases it is hoped that many of the adverse events can be prevented Additionally unnecessary investigations which are burdensome for families and costly for the system could be avoided PRM has been used previously in health and hospital settings Panattoni et al Billings et al and has been suggested as a potentially useful tool that could be translated into child protection settings Vaithianathan et al However the use of predictive analytics in the area of child welfare is contentious There is the possibility that some communities such as those in poverty or from particular racial or ethnic groups will be disadvantaged by the reliance on government administrative data because they will typically have more data kept about them simply by dint of being poor and on welfare Such families

0.5198 low probabilities of rearrest to be labeled high risk and for recommendations to be altered to reduce how many people are released Judicial responses to risk assessments exacerbate racial disparities and diminish release rates These forms of discretion make the impacts of risk assessments brittle and prone to political capture Achieving decarceral outcomes through risk assessments requires particular behaviors and circumstances which the criminal justice system is generally not amenable to As a result of this indeterminacy risk assessments provide no guarantee of reducing incarceration and in fact are often wielded in ways that resist decarceral outcomes Yet because of the discourse that positions risk assessments as a tool for reform even ineffective implementations may enhance perceptions of fairness and reduce the political will for more systemic changes Individualistic Risk assessments are based on individualistic conceptions of both risk and bias that lead to individualistic and intellectual remedies for racial discrimination and mass incarceration Risk assessments treat risk at the level of individuals de ning risk in terms of someones likelihood to be arrested in the future This approach treats risk as a measure of difference across individuals an objective and static fact of identity rather than as a social

0.5186 pricing scheme For conciseness we will define A A and A A Applying the results of Lemma tells us that an equal split pricing scheme is possible whenever A A p A p Applying the results of Lemma tells us that the max-subsidy pricing scheme in the insurance case is A A A p p A A A p p Next we consider the potential implications of these results on debates around fairness in insurance As mentioned before it is important to note that there are two countervailing forces at work one is the additional costs that each person potentially brings to the pool the other is the cost-savings that the collective enjoys by enlarging the pool and thereby reducing variance This creates interesting dynamics essentially the submodularity of the cost function produces extra wiggle room In some cases the benefits of increasing the size of the pool can swamp out the cost of including riskier participants Under these circumstances low-risk people are willing to let high-risk people join the pool or are willing to remain in the pool if high-risk people join if doing so results in lower prices for them even if everyone is charged the same price

0.5149 issues The theory of change regarding how risk assessments will improve the criminal justice system is grounded in two key assumptions The rst assumption is that risk assessments will mitigate judicial biases by providing objective decisions about defendants With this goal in mind and following growing evidence that risk assessments and other machine learning models can be biased recent work has focused on developing technical procedures to measure and promote algorithmic fairness Of particular concern is ensuring that risk assessments do not discriminate against blacks relative to whites The satisfaction of statistical metrics for fairness has become a central component of evaluating the objectivity of risk assessments The second assumption is that risk assessments will promote criminal justice reform This is expected to occur through objective risk assessments replacing discriminatory policies and reducing incarceration For example Senators Kamala Harris and Rand Paul introduced the Pretrial Integrity and Safety Act of proposing to replace money bail with risk assessments so that pretrial release would be based on risk rather than wealth and so that pretrial release rates would increase Several states have implemented pretrial risk assessments with these same goals Many endorsements of evidence-based sentencing are similarly grounded in the goal

0.5145 score was lower than the control groups average prediction r c the risk assessment exerted a similar influence on participants regardless of the defendants race vs p Yet when the risk assessment predicted a higher risk than the control group r c it exerted a stronger average influence on predictions about black defendants than on predictions about white defendants vs a two-sided t-test p and of the difference in means This outcome cannot be explained by differences in the raw disparities between the risk assessments and the control groups predictions ie the value of r c since the values of r c do not differ significantly across defendant race the average disparity for both races is when r c and when r c Breaking out Figure based on the value of r c indicates that the risk assessment exerts an equal influence on predictions about both races at all values of r c except for when r c Figure A Thus the risk assessment leads to larger increases in risk for black defendants as measured by the shift in participant predictions precipitated by the risk assessment is identical when r c the risk assessment generates an average reduction of for

0.5115 the globe Contributions In summary we show that AI-based targeting reduces the errors local to every population subgroup studied in both countries rendering the transition to AI methods uncontroversial we find substantial disparities in exclusion errors across population subgroups in both methods and the error disparities across subgroups are substantially narrowed by the shift from the status quo methods to the AI-based Methodology We extend the accuracy analysis in Section to the subgroup level For every subgroup and each method ie status quo and AI-based exclusion errors were computed by setting acceptance thresholds equal to the poverty rate base rate of each subgroup This choice of thresholds is analogous to the analysis in Section and emulates the most prevalent practice where social institutions set acceptance thresholds that admit a number of beneficiaries equal to the poverty Reduction in Area Under the Curve AUEIC Reduction in Exclusion Error constant budget ta tb ta Reduction in Inclusion Error constant budget inc ta tb inc ta Increase in Poor Population Covered constant budget Colombia Costa Rica Table Comparative Accuracy Results and their Impact on Coverage of the Poor rate For example Figure a shows the exclusion error rates of urban/rural subgroups in Colombia

0.509 have cost produces total cost Terminology We will often refer to the participants in the pooled activity as players agents or policyholders Sometimes we will refer to the groups they form as pools or coalitions A collection of coalitions is a coalition structure A coalition structure is core-stable if there does not exist a group of player so that each player would strictly prefer to be in as opposed to being in their present pool We will sometimes use the notation c to refer to a coalition with low risk players and high risk players The pool containing all of the players will be called the grand coalition and can also be written c We will use to refer to the prices charged to low and high risk players respectively Technical assumptions One common assumption in arguments about insurance is moral hazard which relates to the incentives people have to change their risks or costs in the general model By this argument charging a high-risk individual more will incentivize them to reduce their risks failing to charge them more would incentivize riskier behavior This increased cost is then passed on to the entire pool In this paper we do not

0.5084 is well-calibrated by race according to the control and DR evaluations but shows inequities according to the observational evaluation because black cases were more likely to get treatment which mitigates risk see for more details The observational model bottom row is poorly calibrated for both black and white cases according to the DR evaluation C m O m Risk de Treatment Screened out Investigated Services Placed Figure Child welfare risk distributions by treatment type for counterfactual and observational models We expect risk to increase with the severity of treatment assigned with Placed out-of-home having the highest risk distribution and Screened out of investigation having the lowest see The counterfactual model displays this trend The observational model does not underestimating risk on cases where child welfare effectively mitigated the risk fail without a tutor has a higher observational FPR for men but as before it is wrong to conclude that the model is unfair We distinguish our notion of counterfactual fairness from prior work which considered counterfactuals of the protected attribute an approach which is counterproductive in our settings of interest Consider a female student who is at high risk of failing because of gender discrimination at home or in the

0.5072 and collateral restrictions such as restrictions on welfare benefits and employment opportunities for drug offenders tends to reinforce the cycle of incarceration for people facing substance abuse issues Faced with the high costs of incarceration large jail populations booked with low-level misdemeanor offenses and poor outcomes for these individuals with complex needs some communities are turning to restorative justice and pre-trial diversionary programs as an alternative to incarceration in an effort to break this cycle The design and implementation of these programs is as variable as the needs of the populations they serve including for example mental health services community service or restitution substance abuse treatment and facilitated meetings between victims and offenders Use of these programs has expanded rapidly over the last two decades and recent examinations of opportunities to improve outcomes in the criminal justice system have identified wide support for their continued expansion Evaluations of diversionary programs have generally shown success in reducing the time spent in jail without posing an increased risk to public safety as well as increasing utilization of social services by individuals with mental health and substance abuse issues Although evidence around the relative short-term costs and savings has been considerably mixed depending

0.5043 often the decision to not intervene For instance in education one might assess the likelihood of poor outcomes if a student is not offered support in child welfare it is natural to assess risk if the call is not investigated We refer to the baseline treatment as control and the non-baseline treatment as treatment X X Rd denotes the covariates or features which may include a protected or sensitive attribute A X PT X denotes the propensity score whose estimate we denote by X In the child welfare setting X contains call details and historical information on all associated parties T is whether the case is screened-in for investigation and Y is whether the case is re-referred to the hotline in a six-month period We use subscripts i to index our data eg Xi are the features for case i We use Y X to denote our predicted label and s X to denote the predicted score which is the models estimate of the target outcome our RAI Learning models of risk In this section we introduce observational standard practice and counterfactual forms of model training Observational The observational RAI produces risk estimates by regressing Y on X for the

---------------------------------------------------------------------------
TOPIC 6: policing

---------------------------------------------------------------------------

0.869 true expected crimes per cell is increased in areas with low crime reporting eg to a rate of true crimes on average in Rafael Uribe Uribe and in Usaquén At the higher Id District Antonio Nariño Barrios Unidos Bosa Candelaria Chapinero Ciudad Bolívar Engativá Fontibón Kennedy Los Mártires Puente Aranda Rafael Uribe Uribe San Cristóbal Santa Fe Suba Teusaquillo Tunjuelito Usaquén Usme Figure Normalized average crime in each cell The left side depicts the average over true intensity integrals while the right side uses predictions from the SEPP model trained on only reported crime data In both cases we normalize by dividing by the respective maximum average prediction value end of victim crime reporting rates grid cells in Puente Aranda on average only require a rate of true crimes and cells in Kennedy only to be selected as a crime hot spots More concretely this means that on average the minimum true crime rate that leads to a predicted hot spot in Rafael Uribe Uribe is times the minimum crime rate required in Kennedy In order to rule out the possibility that Kennedys threshold is artificially high because all of the cells in the district are regularly selected as hot

0.8246 according to each districts victimization rate The background intensity of the SEPP is a sum over bivariate Gaussian distributions centered at locations spread out evenly on the Bogotá map Each background crime triggers offspring according to a triggering function that is Gaussian in space and exponential in time coinciding with the model we are fitting see Equation Since the data will be used to predict hot spots on a fixed grid we impose a grid of cells on the Bogotá map as depicted in Figure District membership of each cell is decided based on its center and each Id District Pop rate Rep rate Antonio Nariño Barrios Unidos Bosa Candelaria Chapinero Ciudad Bolívar Engativá Fontibón Kennedy Los Mártires Puente Aranda Rafael Uribe Uribe San Cristóbal Santa Fe Suba Teusaquillo Tunjuelito Usaquén Usme Figure Bogotá district map with division into grid cells for hot spot prediction and survey-based victimization and victim crime reporting rates Districts differ notably in size population numbers and rates point is attributed to the district of the cell it falls into We discretize the time component into daily units and simulate crime data for timesteps years as follows Sample a set of candidate points C G C

0.8081 True Allocation Ab so te M is al lo ca n pe r m ill io n Misallocation per million dollars in Michigan with Laplace b Misallocation Districts sorted by true allocation Al lo ca n True allocation Allocations true and noise for Michigan With Laplace c Allocations True Allocation M ul tip at iv e Al lo ca n Er ro r Multiplicative Allocation Error in Michigan with DAWA d Multiplicative Allocation Error True Allocation Ab so te M is al lo ca n pe r m ill io n Misallocation per million dollars in Michigan with DAWA e Misallocation Districts sorted by true allocation Al lo ca n True allocation Allocations true and noise for Michigan With Dawa Allocations Figure Fairness in allocation for Michigan using the Laplace Mechanism top and DAWA bottom Assessing Fairness To assess fairness we consider the difference between the allocation vector based on the noisy statistics o and the allocation vector based on true counts o assessing disparities across assignees in this case districts An allocation mechanism is fair if the distance measures do not vary much across districts We can measure fairness ex ante ie before running the randomized allocation mechanism as

0.7712 Aranda or Kennedy while underestimating the number of hot spots in districts such as Usaquén Rafael Uribe Uribe or Engativá The direction of the introduced error aligns with the victim crime reporting rates of the respective districts as compared to a Bogotá-wide average with fewer of the true hot spots detected in low reporting areas and instead overly many hot spots predicted in high reporting areas In Usaquén which with has the lowest victim crime reporting rate among all districts only of the number of true hot spots are predicted on average Meanwhile in Kennedy which has a comparatively high reporting rate of the model on average predicts the number of true hot spots Thus far we have disregarded cases in which none of the true hot spots fall into a given district but the prediction model selects one or more cells Figure gives a summary of the fraction of cases with no true hot spots further confirming the observed displacement effect of hot spot predictions In Usaquén the number of times crime hot spots are predicted when none of the true top crime hot spots lie in the district is over twice as high in the full data SEPP

0.734 policing and ground truth what assumptions we make about underlying crime rates We assume the simplest form of context Assumption Context The only information retained about a crime is a count Assumptions about ground truth are both critical and complicated For some neighborhood A let be the underlying ground truth crime rate for the neighborhood We will assume that this is observed via discovered and reported incidents Let dA be the rate at which police that visit neighborhood A discover incidents Let wd be the weight of the discovered incidents within all Why should this be the goal Suppose there are exactly enough police officers to stop all the crime and no more then a deployment according to the true crime rates will perfectly police all regions incidents Similarly let rA be the rate at which incidents are reported from neighborhood A and let be the weight of reported incidents among all incidents We will assume that wd The total rate of incident data from neighborhood A is then wd dA rA We note here that discovered incidents are directly implicated in the feedback loop since police are deployed in areas based on the results of the predictive model Reported

0.7242 generator p house needs generator p house gets generator p house needs generator We assume that there is a total number of houses in the village and that all houses within the same village are interchangable The total number of houses in the village that need generators is given by C C Given that c houses need generators there is a c probability that a randomly selected house will be in need Given that a house is in need the probability it obtains a generator equals min c p house needs generator equals c p house needs generator C c PC c c c PC c c c PC c Similarly we can use term to expand p house gets generator Pk gets generator needs generator Pk needs generator Pk gets generator does NOT need generator Pk does NOT need generator p house gets generator house does NOT need generator We expand the remaining term by conditioning on C The sum AB CB CB U AB Table Probability of receiving resources across two groups in motivating example Utilization included for reference starts from c because if c there are houses in need of generators c Pk gets generator needs generator

0.7133 we set the relative count to for cases in which the district has zero true hot spots and the model correctly predicts zero hot spots and exclude cases with zero true but non-zero predicted hot spots We see that the SEPP model that was trained on all crime data ie reported and unreported performs well at selecting the correct number of hot spots uniformly over all districts S This observation is unsurprising given that the fitted model closely resembles the data generating model Figure Relative number of predicted crime hot spots for a selection of Bogotá districts Each data point represents a district-specific fraction at a given evaluation time step days in a given simulation run runs A total of hot spots are selected at each time step See Figure for relative predicted hot spot counts for all districts In contrast the SEPP model that was trained on only reported crime data S is found to have differential performance across districts Although in some districts eg in Tunjuelito the relative hot spot counts of the two models appear to be similar the model with under-reporting on average overestimates the number of hot spots in districts such as Antonio Nariño Puente

0.7045 patrols where areas with low victim crime reporting rates are met with artificially decreased police presence while areas with higher reporting rates are chronically over-policed Crime threshold for hot spot selection Calculating relative counts of predicted hot spots gives insights into how much under or over-policing we can expect per district A natural way of comparing between districts is to look at the true crime rates required for a cell to be selected as a hot spot If this threshold is much lower for some districts than for others the consequence could be more average police presence in these districts despite similar or even higher crime levels in other areas Figure Fraction of prediction time steps with no true hot spots in districts We separate instances into cases with predicted and no predicted hot spots See Figure for a version with all districts Figure shows that the predicted crime rates implied by the reporting data SEPP model present a differentially well-adjusted approximation of true crime rates Comparing the normalized maps in the Figure the reported crime SEPP appears to overestimate the relative concentration of crimes in the high-reporting regions Kennedy and Antonio Nariño and underestimate the relative concentration of crimes

0.7006 assumption is often justified as the criminology literature tends to describe crime hot spots as micro areas of only a few blocks or street segments with high concentration of crime Indeed in their randomized controlled eld trials the researchers affiliated to PredPol omit the spatial component of the SEPP altogether and discretize crimes into cells before modeling DISCUSSION Our analysis demonstrates how predictive policing systems exclusively trained on victim crime reporting data can lead to spatially biased outcomes due to geographic heterogeneity in crime reporting rates This in turn can result in over-policing of certain communities while others remain under-served by police Our endings are based on synthetic crime data simulated according to district-level victimization and victim crime reporting rates published by Bogotás chamber of commerce Cámara de Comercio de Bogotá We empirically evaluate the equity of predictions across districts of a hot spot prediction algorithm similar to the models used by PredPol Our endings suggest that districts with low crime reporting rates have fewer of their crime hot spots detected by the algorithm Conversely districts with high crime reporting rates are found to have a higher concentration of predicted hot spots than the true crime levels would justify Moreover

0.699 educational assistance to disadvantaged children In fiscal year Title I funding amounted to a total of billion of which roughly billion was given out through basic grants which are our focus Problem Definition The federal allocation is divided among qualifying school districts in proportion to a count of children in the district aged to who live in families who fall below the poverty level or receive a form of federal financial aid This proportion is then weighted by a factor that reflects the average per student educational expenditures in the districts state The allocation formula is described formally in Table where the outcome represents the fraction of the total allocation which changes annually the district will receive Table Title I Funding Allocation Assignees are all US school districts outcomes are the fraction of allocated funds for each school district Assignees School Districts Outcome Q eli where average per student expenditures for state containing district a number of eligible students in district a MaX bA Fair Decision Making Using Privacy-Protected Data FAT January Barcelona Spain True Allocation M ul tip at iv e Al lo ca n Er ro r Multiplicative Allocation Error in Michigan with Laplace a Multiplicative Allocation Error

0.6895 the estimated intensity function for future predictions On each prediction day we compute the models intensity integrals in each of the Bogotá grid cells These integrals correspond to the absolute predicted crimes per cell and are subsequently used for hot spot selection Since police are generally only able to patrol small fractions of a city effectively we select the top cells with highest predicted crime as hot spots which corresponds to approximately of the citys area Results are aggregated over simulation runs where each simulation samples a new crime data set Equity between districts Relative number of predicted hot spots We now discuss the equity of hot spot selection at a district-level We start by examining how the number of predicted hot spots compares to the number of true hot spots per prediction day in each district In the case where police are deployed in accordance with the models predictions this directly corresponds to the degree of police presence per district relative to a best-case hot spot policing program in which the true crime distribution is known Figures and depict the relative hot spot counts for a subset of districts over all evaluation time steps and simulation runs For Figure

0.6845 the Laplace mechanism the mean allocation for small districts is typically much higher than the true allocation while the mean allocation of larger districts is typically lower This is shown in Figure a which plots the multiplicative allocation error of a district versus its true allocation The districts are shown sorted by true allocation The smallest districts see a increase for a increase for and a increase for The largest districts see their allocations decrease by for for and for The Laplace mechanism adds -mean noise to the data and in expectation the noisy counts should be the same as the true counts However these counts could be negative and since negative counts are rounded to this adds an upward bias to the noisy counts Moreover this bias increases the total number of students thus bringing down the weight of larger districts True Allocation Multiplicative Allocation Error in Florida with Laplace a Multiplicative Allocation Error True Allocation Misallocation per million dollars in Florida with Laplace b Misallocation Algorithm Total Min Max Laplace DAWA Inflationary Laplace DAWA Inflationary c Misallocation per million dollars for Michigan Figure Fairness in allocation for Florida with the Laplace mechanism and misallocation statistics for Michigan Figure

0.6785 incentivized to be quick more than they are incentivized to be accurate For example once parents of a woman called to report the death of their daughter due to medical negligence which had occurred at a hospital in Saket They called from their home which led to the call taker mark this as a crime at the callers residence instead of the hospital Dispatch and PCR Van Once the form is filled with details of the crime it is automatically sent to the dispatch section on the fourth floor of PHQ through the PA software The Dispatch floor is divided into zones mirroring districts of Delhi though they have been revised to the zones still remain The call is transferred to its appropriate zone according to location from where it is transferred to its respective Police Control Room PCR van The officer on the Dispatch console manually provides the call details to the PCR van officers over microphone The dispatch function was planned such that as soon as the call takers close the PA form a message with all call details is sent to the handheld devices of the PCR van officers They would then send the investigation report called

0.6734 in low-reporting districts such as Rafael Uribe Uribe and Usaquén Moreover crime rate prediction seems to perform poorly in areas with little true crime While the ground truth shows clear differences between crime intensities in areas such as Ciudad Bolívar and Usme the model predictions in these districts appear to be almost indistinguishable In order to measure equity of model predictions between districts we consider the minimum true crime rate that leads to a predicted hot spot at each prediction step and summarize the results in Figure Since exact crime counts vary over time and this metric omits steps with no predicted hot spots falling into the respective district the average thresholds have some variability even for full data models However for districts that are regularly predicted to have hot spots the full data SEPP model S exhibits very similar hot spot prediction threshold of around expected crimes per cell and time step where the low threshold is explained by the population scaling we conducted while simulating Bogotá crime data In contrast the model trained on only reported crime data results in varying thresholds even across districts which are regularly predicted to have hot spots The district-wide average threshold of

0.6713 to be larger than the difference between the full and reporting data models The conflict between the equity measures is observed because the relative predicted hot spot counts are an aggregate metric over all cells and not sensitive to which cells are selected in contrast to the minimum true crime threshold for hot spot selection Rescaling of the reporting data SEPP predictions increases predictions in all cells of a district by the same factor without accounting for how much crime was unobserved in each of the cells As a consequence the rescaled model selects an approximately correct number of hot spots in many of the districts while the exact cells might not coincide with the true hot spots In order to recover the cell-wise true crime distribution a cell-by-cell rate of crime reporting would be required which presupposes separate representative surveys in hundreds of micro-areas While incorporating victimization survey information does help to reduce disparities to some extent it evidently does not suffice in order to fully debias the prediction system Comparison to a moving average model In this section we study the behavior of a simple moving average MAVG prediction model to assess whether our endings hold more generally

0.6685 yes whether they have reported the crime to the police Results of the surveys are available on the CCB website and are used to inform the definition and adjustment of the citys public policies Not all of the published reports stratify results by districts For our experiments we use victimization and victim crime reporting rates stratified by district based on the survey that covers the rst half of Districts population sizes and rates are depicted in Figure Both the crime victimization rates and the victim crime reporting rates vary significantly between different districts with victimization rates between and and victim crime reporting rate from to Although the range of both rates can be expected to vary significantly between different cities and countries this data allows us to analyze the impact of differential crime reporting on predictive policing in a realistic scenario Synthetic Data Generation We simulate location and time of reported and unreported crime incidents in Bogotá districts according to the victimization and victim crime reporting rates displayed in Figure In order to minimize possible errors due to model misspecication and instead concentrate on the effect of differential reporting rates we sample data directly from a high-intensity SEPP and subsample

0.6656 from and discard all points that fall outside of the city bounds or time horizon For each district and data within its bounds C C we subsample Bin C of the points to form the true crime data set D where population victimization rate C To get a data set of only reported crime we subsample Bin D crimes for each district where D D is the set of crimes falling into the given district and victim crime reporting rate We implicitly assume that each person is victim of at most one crime which leads to the time scaling factor in step as the CCB survey provides rates of victimization for a half-year period In addition district population counts are scaled by to speed up the run time of the whole simulation The described sampling procedure for the true data D ensures that crime is sampled according to population size and victimization rates but remains distributed according to a thinned SEPP that can be accessed for evaluation of the ground truth conditional intensity Since D the true expected number of crimes in a subarea of district in time C C can be computed as E C C HC C C

0.6628 on predictive policing an important exemplar problem demonstrating c D Ensign SA Friedler S Neville C Scheidegger S Venkatasubramanian Runaway Feedback Loops in Predictive Policing these feedback issues Predictive policing is increasingly employed to determine where to send police who to target for surveillance and even who may be a future crime victim Perry We focus on the most popular of these forms of predictive policing with PredPol HunchLab IBM and other companies entering the market which attempts to determine how to deploy police given historical crime data Definition Predictive Policing Given historical crime incident data for a collection of regions decide how to allocate patrol officers to areas to detect crime Once police are deployed based on these predictions data from observations in the neighborhood is then used to further update the model We will call these observations discovered incidents as opposed to reported incidents that are crime incidents reported to the police eg via calls Since such discovered incidents only occur in neighborhoods that police have been sent to by the predictive policing algorithm itself there is the potential for this sampling bias to be compounded causing a runaway feedback loop Indeed Lum and Isaac have shown that

0.6503 the halaat report from these handheld devices which would also give the exact location of the scene of crime But the plan did not work out for a variety of reasons Dispatch and PCR personnel say that it was too much work charging the devices and also claimed that they were not adequately trained to operate the devices HQ officers those in the DMD argue that carrying a handheld device would be surveillance for the PCR van officers something that they did not want because they sometimes cut corners when it comes to actually visiting the crime locations An investigation in HQ revealed this to be true many PCR van officers negotiate with their Dispatch counterparts into being assigned fewer investigations during their shift They also do not visit all the crime scenes but sometimes merely replicate the report of the investigating officer from the police station into the halaat report Heinous crimes go to green diary Next the dispatch officer sends a copy of four heinous crimes rape robbery eve teasing and snatching to the dispatch command room as soon as they receive them from the call centre If the halaat report confirms the crime it is entered into

0.6448 to serious crime statistics Predictive policing models The literature on predictive policing has considered a range of different modeling approaches for spatio-temporal crime forecasting and hot spot selection To the best of our knowledge only a small subset of models have been deployed and evaluated in practice PredPol one of the largest vendors of predictive policing software in the US has been one of few companies to publish modeling details of their hot spot prediction algorithm The PredPol algorithm relies on a Self-Exciting Spatio-Temporal Point Process SEPP model that uses the location and time of historical incidents to predict the spatio-temporal distribution of future crime within a city Hot spot predictions for subsequent time steps can be obtained by evaluating the predicted crime distribution on a grid of cells overlaying the city This model which has its roots in seismology separates crime occurrences into background crime and offspring crime with the rationale that similar to earthquakes which often trigger close-by aftershock earthquakes crime tends to form clusters in time and space with burglars returning to the same areas or gang conflicts leading to retaliatory violence While the SEPP method models both the space and time distribution explicitly many other common

0.6443 to an evaluation time step days and a simulation run runs See Figure for all districts We explore the rescaling approach as an additional model in our hot spot prediction simulation by taking the integrated intensities in grid cells supplied by the reporting data SEPP and dividing them by the victim crime reporting rate of the respective district After rescaling we select the cells with the top highest predictions as hot spots analogous to the other models The relative predicted hot spot counts of the rescaled model S are displayed along the other models in Figure Across the displayed districts the mean relative number of predicted hot spots is just as close or closer to the number predicted by the full data model S than the reporting data based predictions S This indicates that the rescaling strategy successfully reduces outcome disparities However this conclusion is called into question when examining the implied minimum true crime rate for hot spot selection shown in Figure For example in Usaquén the rescaled model implies a visibly lower average true crime threshold for hot spot selection than the full data model and in Engativá the difference between the full data and rescaled models appears

0.6432 delaying the halaat report they are entered in the Green Diary draft with a pending status The status is updated only if the halaat report is received before the Diary is to be sent for mapping ie by AM However if the report does not come these calls are mapped without being corroborated by haalat reports Spurious calls in such a manner can light up an area as criminal in mapping even though the underlying data could be completely false Which in turn means that we do not know if these crime calls were indeed what they claimed Though Green Diary is slated to be a verified document of all calls there were some discrepancies in certain calls and in their halaat reports We observed many cases where a crime was recorded as no matter of snatching police speak for it was a bogus call in the halaat report but was present in the Green Diary Slippages between the officer managing the consoles where heinous crime details are received from dispatch and the one who prepares the Green Diary ensure that sometimes calls that were found untrue upon investigation were also recorded as true calls ANALYSIS One of the arguments

0.6311 spot we examine the absolute predicted hot spot counts and nd that at no time step more than of Kennedy is selected as hot spot area with a mean of These endings imply that crime hot spot prediction in real-world settings with differently sized regions and differential victimization and crime reporting rates can have noticeably biased outcomes that lead to over-policing of some areas of a city while others have higher levels of crime Scaling by victim crime reporting rates It is not unusual for police and predictive policing algorithms to leverage data sets beyond registered crime incidents In the case presented here one could imagine pairing the reported crime data with the survey data to attempt to correct the bias introduced by differential crime under-reporting Intuitively this entails rescaling the predicted crime rates according to the reporting rates Of course in most cases exact crime reporting rates are unknown to the police However as we discuss in this section even in cases where the reporting rates are known this rescaling does not necessarily recover the true crime distribution at the nest level Figure True crime thresholds for hot spot selection in a set of Bogotá districts Each point corresponds

0.6286 PredPol will be critical to our experimental investigations so we describe it in more detail here PredPol Mohler et al assumes that crimes follow an earthquake aftershock model so that regions that previously experienced crime are likely to experience crime again with some decay Mohler et al model the crime rate in region r at time t as follows where tin represents the time of an event in region r quantifies the time decay of a shock and captures the degree to which aftershocks are generated from an initial event They use an expectation-maximization procedure to determine the parameters of the model Note that this model only uses incident data including both discovered and reported incidents see Section per region to determine the true crime rate and does not use any context in the form of demographics arrest profiles and so on PredPol in essence is predicting where incidents will be reported or discovered since thats all it sees not where crime will happen Each day officers are sent to the areas with highest predicted intensity and the resulting discovered incident data is fed back into the system Predictive Policing with Urns We will model the predictive policing process by

0.628 practices they do point to a need for further investigation in settings that more closely mirror standard practice Our work presents an initial step in this direction In this paper we empirically demonstrate how predictive policing systems trained exclusively on victim crime reporting data rather than arrest data may nevertheless suffer from significant biases due to variation in reporting rates Our analysis is based on a simplified crime simulation patterned after district-level crime statistics for Bogotá Colombia released by Bogotás chamber of commerce Cámara de Comercio de Bogotá CCB We demonstrate that variation in crime reporting rates can lead to significant mis-allocation of police These endings corroborate the effects of differential victim crime reporting on predictive policing models hypothesized in We also discuss the limitations of using reporting rates from existing crime victimization surveys to attempt to correct for such biases BACKGROUND RELATEDWORK Feedback loops and other biases in predictive policing Having already described the work of Lum and Isaac we focus here on Ensign et al theoretically characterize why feedback loops occur by modeling arrest-based predictive policing systems via a generalized Pólya urn model Their analysis also considers a scenario in which both reported and detected crimes ie arrests

0.628 The effect of differential victim crime reporting on predictive policing systems Police departments around the world have been experimenting with forms of place-based data-driven proactive policing for over two decades Modern incarnations of such systems are commonly known as hot spot predictive policing These systems predict where future crime is likely to concentrate such that police can allocate patrols to these areas and deter crime before it occurs Previous research on fairness in predictive policing has concentrated on the feedback loops which occur when models are trained on discovered crime data but has limited implications for models trained on victim crime reporting data We demonstrate how differential victim crime reporting rates across geographical areas can lead to outcome disparities in common crime hot spot prediction models Our analysis is based on a simulation patterned after district-level victimization and crime reporting survey data for Bogotá Colombia Our results suggest that differential crime reporting rates can lead to a displacement of predicted hotspots from high crime but low reporting areas to high or medium crime and high reporting areas This may lead to misallocations both in the form of over-policing and under-policing INTRODUCTION Police departments around the world have been experimenting with

0.6251 individuals The authors of study the help-seeking behavior of women who experience intimate partner violence The study finds that for the lowest income women the severity of violence does not predict whether law enforcement is contacted With increasing income the severity becomes more indicative of victim crime reporting In the Bogotá survey data there appears to be no clear association between reporting rates and socioeconomic advantage at the district level Ciudad Bolívar a district with large urban slums that is home to some of the most socioeconomically disadvantaged residents of Bogotá has a reporting rate of In line with previous research this lies well below the average reporting rate across districts of However Usaquén the district with the lowest reporting rate of is also one of the wealthiest districts in Bogotá We hypothesize that this is in part explained by the spatial clustering of specific crime types In particular Usaquén experiences a greater proportion of residential burglary and theft than other districts Given that victim crime reporting rates vary based on perceived severity this might contribute to a decreased victim crime reporting rate Additionally this may also be influenced by intra-district heterogeneity of wealth as socioeconomically disadvantaged neighborhoods such as

0.6207 surveys to better understand factors that drive differences in crime reporting rates and to assess discrepancies between official crime statistics and victimization-based measures of criminal activity According to the report released by the Bureau of Justice Statistics which oversees the annual US National Crime Victimization Survey NCVS of aggravated assaults of robberies of simple assaults and only rapes/sexual assaults are reported to police In this section we briefly overview different sources of disparities in victim crime reporting in the US context We note that while our data simulation is based on a survey conducted in Bogotá and crime reporting rates are observed to be considerably lower there a number of our conclusions apply to geography-associated disparities in reporting rates in general In particular our analysis indicates that to the extent that these sources of disparity coincide with geography we can expect significant under- or over-targeting to result The likelihood that a crime is reported to police has been found to be greater for older victims and when the victim is a woman It is also greater if a third party is present if a weapon is present or the victim is injured Furthermore reporting rates tend to increase with the

0.6112 Runaway Feedback Loops in Predictive Policing Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime Discovered crime data eg arrest counts are used to help update the model and the process is repeated Such systems have been empirically shown to be susceptible to runaway feedback loops where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate In response we develop a mathematical model of predictive policing that proves why this feedback loop occurs show empirically that this model exhibits such problems and demonstrate how to change the inputs to a predictive policing system in a black-box manner so the runaway feedback loop does not occur allowing the true crime rate to be learned Our results are quantitative we can establish a link in our model between the degree to which runaway feedback causes problems and the disparity in crime rates between areas Moreover we can also demonstrate the way in which reported incidents of crime those reported by residents and discovered incidents of crime ie those directly observed by police officers dispatched as a result of This research was funded in part

0.6103 fraction of crime victims who has reported the offenses to the police can generally not be assessed based on only police data but require large-scale surveys Often these surveys are not conduced or published with a high-enough spatial resolution to give a sense of differences at a local level For instance the US Bureau of Justice Statistics conducts a bi-annual National Crime Victimization Survey with around households and publishes rates of victimization and crime reporting on a national level and aggregated by urban suburban and rural areas In order to study the effect of differential victim crime reporting on predictive policing systems which are generally limited to a single city a higher spatial resolution of victimization and crime reporting rates is required Fine-grained data sets like this are rare and based on availability we draw on district-level data from Bogotá Colombia collected by Bogotás chamber of commerce Cámara de Comercio de Bogotá CCB The bi-annual CCB crime perception and victimization survey includes approximately randomly selected participants from all socio-economic statuses and all urban districts of Bogotá Among other questions participants are asked to indicate whether they have been the victims of a crime in the present calendar year and if

0.6101 converge to only sending police to the neighborhood with the most crime This replicates within our urn model the feedback loop problems with PredPol found by Lum and Isaac Recall from Lemma that skew occurs even if the difference in crime rates between the two neighborhoods is small Note that while we included a notion of decay in our urn model in order to more closely model PredPol we found similar results under the urn model without decay Discovered and Reported Incidents Now considering both reported and discovered incident data we repeat the experiments Again well assume that both discovered and reported incidents are reported at the underlying true rate Assumption and well assume that these incidents are equally weighted ie that wd The results shown in the bottom left of Figure show that while the error in police deployment is not as great as if only discovered incidents are used the urn still does not converge to the correct rate Here its important to note the strength of the assumption that incidents are reported at the true underlying rate and not influenced by police deployment we suspect that this assumption helps this convergence to be closer to though still

0.6085 incidents on the other hand are not To start our examinations we make the following assumptions In the subsections below we explore what happens as we vary these factors Assumption Truth in Crime Data If an officer goes to a location A with an underlying ground truth crime rate of the officer discovers crime at a rate of Ie dA Reported incidents are also reported at a rate that matches the underlying ground truth crime rate ie rA Note that Assumption allows the predictive policing system to operate in a generous context There are many reasons to believe that this assumption does not hold We will show that even in this optimistic setting problems occur Assumption Discovery Only Incident data is only collected by an officers presence in a neighborhood Neighborhoods with no officers will contribute no incidents to the data Ie wd and We will also start with the assumption that all incident data is made up of discovered incidents We will modify this assumption to also account for reported incidents in Section Uniform crime rates Let us start by assuming that the crime rate is uniform between areas Assumption Uniform Crime Rate If an officer goes to a

0.6075 below While CMAPS is the first attempt at automated hotspot mapping it is predicated on previous manual hotspot mapping initiatives within Delhi Police The first instance of crime hotspot mapping occurred in when Delhi Police mapped instances of car-jacking across the city in an effort to detect patterns and curb further instances This effort was soon extended to four other crimes snatching robbery rape and eve teasing These would be manually entered by the Digital Mapping Division DMD housed within Delhi Police According to the Standard Operating Procedure stuck on the wall of the DMD this manual mapping would continue till such time that automatic mapping ie CMAPS was set up In our time at the Delhi Police Headquarters we learnt that currently both manual and automated crime mapping are being carried out simultaneously The manually plotted maps are sent to heads of police everyday on the basis of which resources are allocated and subordinates are briefed on the law and order status in the city Simultaneously the login ID and password from CMAPS is provided to all District Police Commissioners who can use them to brief their Station House Officers who can in turn decide on resource management increasing

0.6064 patrolling/policing in problem areas in their jurisdiction The system uses background data on the geographic boundaries in Delhi railways metro pillars police station jurisdictions problem areas identified in historical data etc It is also equipped with layers that can help law enforcement analyse crimes by optimising for a variety of considerations For instance there are filters for crowded places like railways stations and schools and other filters for areas historically and culturally associated with crime such as bars There are also filters that can point out ghettos migrant colonies and minority settlement areas Data practices within past mapping initiatives thus form baselines for spatial analysis of crime and for application of layers that can be used to analyse crime within CMAPS In this paper we study the implications of using historical data from law enforcement as ground truth for New Delhis predictive policing system with a focus on input data coming in from the Dial helpline Call Dial Call Centre Dispatch Halaat Report if Heinous Police Control Room PCR Command Room Dispatch Floor Green Diary Comprehensive Diary Digital Mapping Division Crime Maps Crime Mapping and Prediction System CMAPS Crime and Criminal Tracking System CCTNS Figure Diagrammatic representation of institutional actors

0.5975 an ex-post measure of fairness which can be evaluated on a single apportionment outcome we consider the expected value of this measure The second isolates a particular state evaluating the difference from quota of the expected apportionment for that state and then aggregates over the states It can be Table Apportionment of seats in parliament seats Assignees are all Indian states outcomes are seats in the Lower House of Parliament Assignees States Outcomes Q tot tot total population in state a Calculate quota bA Round to nearest positive integer max Round a Average-expected-deviation b Per-state expected deviation c Max-multiplicative fairness Figure Allocation of seats to the Lower House of the Indian Parliament using population counts with Laplace noise seen as an ex ante measure of fairness if for example two states had equal expected deviation from quota then prior to any execution of the randomized algorithm they may not prefer the other states future outcome We note that an expected deviation from quota of zero was used as a fairness criterion by Grimmett in the context of a randomized but non-private method for apportionment Empirical Findings Experimental Setup We used the state population totals published by the Indian Parliament in

0.594 dG where HC G C C C C Figure depicts a summary of the sampled number of crimes per district the number of crimes expected according to above integral and the number of crimes as implied by the CCB survey showing that the synthetic data set has the desired rates of victimization for each of the districts RESULTS Hot spot prediction procedure We t SEPP models see Equation on the full and reported crime data by discarding the data from the rst simulated time steps and training on the subsequent days years of sampled incidents Ignoring the rst time steps omits the period in which the data generating SEPP is converging to its equilibrium rate and provides a data set that more closely resembles the crime data over fixed time windows we would expect to see in practice In addition the time range of approximately years is reasonably close to real crime data sets and falls well within the range of years that is suggested by PredPol specifically The fitted models are used to predict crime intensities on a dayto-day basis for evaluation days where after each time step the data for the time step is observed and added to

0.5884 r Top vs Top Top vs Random r r r With improvement policy Without improvement policy D is co v er ed O n ly A ll In d en Top vs Top Top vs Random r r r Top vs Top Top vs Random r r r With improvement policy Without improvement policy Figure PredPols relative deployment to region Top versus Top or Random Along the top row we use the model which only accounts for discovered incidents those based on police having been deployed to an area Along the bottom row we use the model which accounts for both discovered and reported incidents Left PredPol operating as usual Right discovered incident entries modified using our improvement policy Police deployment based on underlying crime rates would send of the force to Top instead of Top and of the force to Top instead of Random These correct crime rates indicated with a dashed red line appear to be what PredPol converges to under the improvement policy Discussion and Limitations In this paper we show that urn models can be used to formally model predictive policing as well as indicate remedies for problems with feedback We demonstrate this both formally and

0.587 the effective true level of crime required for the model to predict a hot spot is found to vary by more than a factor of two across the districts We explore if known victim crime reporting rates can be used to debias hot spot predictions by scaling crime expectations appropriately The results suggest that this is unsuccessful when reporting rates are known at a district level but hot spots are predicted at a smaller individual cell level since noise introduced by individually thinned crimes is propagated to the rescaled predictions which makes singling out of specific cells in comparison to other cells in the same district difficult Prior work has focused on feedback loops and the potential harms of arrest data-based predictive policing systems Yet in practice predictive policing systems are based on data from victim crime reports Our work presents an initial step toward understanding the effect of bias in victim crime reporting data on predictive policing systems Our analysis demonstrates the importance of considering reporting rate variation when assessing predictive policing systems for potential harms and disparate impacts Limitations Crime location vs survey location Victimization surveys generally provide us with information on crime reporting based on where people

0.5865 compared to the reported crime SEPP The same fraction increases more than threefold in the high-reporting district Antonio Nariño and almost twofold in Puente Aranda Notably Figure also shows that the displacement effect both impacts districts that almost always have areas with highly concentrated crime and districts that do not This phenomenon is a function of victimization rates population sizes and the size of districts Finally average absolute numbers of over- or under-predicted hot spots are displayed in Figure Although comparison of relative counts ensures that districts of different sizes are evaluated similarly in some cases we might be interested in the number of grid cells affected by the introduced bias as they roughly relate to the number of impacted individuals For example we see that the displacement of predicted hot spots based on differential victim crime reporting rates leads to on average too many hot spots predicted in Kennedy while only too many cells in Antonio Nariño are selected on average Overall differential reporting rates across districts seem to lead to differentially well-measured aggregate crime levels which distorts the distribution of hot spots If the police follows the models recommendations the consequence would be an unfair allocation of police

0.5832 we should not be surprised that discovered incidents in region A happen at a rate of nine to one even if the crime rate is the same across both regions In such a scenario if we see a crime in region A where police go of the time we should simply drop the incident record of the time analogously in region B where police only go of the time we drop the incident record of the time One way to interpret our fix is as a form of rejection sampling we are dropping sampled values according to some probability scheme to affect the statistic we are collecting The importance-sampling analog to this scheme would be to use weighted balls where the weight of each ball is inversely proportional to be rate at which police are sent Effectively we want a scheme where as more police are sent smaller weights are assigned to discovered incidents But such a scheme is precisely the Thompson-Horvitz estimator used in survey designs with unequal probability distributions Horvitz and Thompson and so we see that our proposal is a rejection-sampling variant of Thompson-Horvitz estimation Reported and Discovered Incidents Now we consider what happens if there are

0.5824 the Green Diary a comprehensive document with verified accounts of all the calls related to four heinous crimes at the Dial call centre Understanding crime events and categorising them correctly also require certain level of interpretative understanding of categorisation rules which are informal in the PHQ and usually taken from on-ground policing experience of the officers For example once a shopkeeper called to report how two men came to his shop asking for two bags of ghee and when he put them on the counter they asked for a bottle of Chyawanprash As soon as he turned to get the Chyawanprash bottle the men ran away with the bags of ghee The call taker had categorised the crime as snatching A debate ensued on the Dispatch floor on whether the crime was indeed snatching or if it was a case of robbery Finally it was decided that because the two men for all intents and purposes snatched the ghee bags from the shopkeeper this was snatching The men then ran away on their bike fulfilling more conditions of snatching The crime was finally recorded in the Green Diary as snatching These choices are arbitrary and based on common understandings of

0.5824 crime rates dB and dA are very similar In other words the only scenarios where feedback does not drive the outcome away from the true result are when we effective ignore feedback by downweighting the importance of discovered crime or when the crime rates are similar enough for the feedback to not matter However it is precisely when crime rates are different that predictive policing is of value because resources are then deployed differently Thus once again the urn model reveals problems via simulation in existing models for predictive policing Modifying the urn model to account for feedback In order to learn the crime rate we want the Polya urn to contain balls in proportion to the relative probability of crime occurrence As we have seen a standard Polya urn with stochastic update rates will converge to a distribution that has no relation to the true crime rates Here we present a simple change to the urn process which does guarantee that the urn proportion will converge to the ratio of replacement ie crime rates Discovered Incidents Only Again we first consider what happens if Assumption is in place Consider the standard Polya urn update rule the probabilities and model

0.579 data and generate crime according to the drug usage rates described above At each simulation day PredPol trains on the previous days of incident data and produces predicted crime rates rA and rB The decision of where to send police is made probabilistically by a Bernoulli trial with p This models the targeting effect of sending more police where more crime is expected simulating a typical use of PredPol Mohler et al To counteract the effects of the feedback we can employ the same strategy as in Section The key insight is that we need only filter the inputs presented to PredPol rather than trying to modify its internal workings Specifically once we obtain crime report data from the system we conduct another Bernoulli trial with p rB where rO is the predicted rate of the district we did not police that day and only add the incidents to the training set if the trial succeeds In other words the more likely it is that police are sent to a given district the less likely it is that we should incorporate those discovered incidents Evaluating the PredPol simulation and its repair Simulating the effects of PredPol on policing as described

0.5718 not the same as the correct rate Evaluating the modified urn Using this improvement policy to determine when to replace balls we can now determine if the urns can learn the true crime rate despite the issue of observational bias Again using the estimated daily drug usage per region as the underlying true crime rate and the historical incident data as the prior for the urn color distribution we simulate the urns ability to find the relative crime rates in two regions the Top and Top incident regions and a Random region As shown in the right of Figure urns under this improvement policy converge to a distribution of colors that represents the true crime rate whether using only discovered incidents or both discovered and reported incidents Fixing PredPol Modifying PredPol in a black-box manner The urn models we explore provide a justification for the observed feedback loop failures of PredPol But can we remedy PredPol itself using the improvements described in Section We first demonstrate how asymmetric feedback affects PredPol by simulating the decisions a precinct might take after running it We run PredPols prediction model using the Lum and Isaac data and implementation trained on Oakland historical crime

0.5655 both charts left converge to the incorrect rate red line while with the improvement policy right the charts appear to converge correctly to the true crime rates and Top and second the Oakland neighborhood with the most incidents as compared to a randomly chosen region with many fewer incidents Random We simulate the effect of the historical incident data on the prior for the system by determining the number of balls for each region in our urn based on the past number of incidents We use the full number of incidents and for regions Top Top and Random respectively as the starting number of balls in the urn from each region The urns are then updated based on the estimated number of daily drug use incidents ie and Discovered Incidents Only First we begin with Assumption in place stating that well only consider discovered incident data ie This allows us to isolate the part of the data that causes the feedback loop in order to examine its effect The results shown in the top left of Figure demonstrate that even if police sent to a neighborhood discover crime incidents according to the true crime rate Assumption the urn model will

0.5651 magnitude and at the true and noisy allocations barely differ Finding T Under some privacy mechanisms districts with a greater entitlement can receive a smaller expected allocation Consider two districts a and b where a has a smaller number of eligible students than b Naturally the true allocation of a will be smaller than the true allocation of b and the inversion of this relationship would violate a commonly held notion of fairness Under the Laplace mechanism in expectation we can show that the allocation for a will be no larger than the allocation for b However this is not true for the DAWA algorithm because of bias in estimated counts In particular for DAWA a smaller district may be grouped with other larger districts while a larger district may be grouped with other smaller districts This results in the smaller district getting a larger expected allocation than the larger district Empirically we find that using DAWA with out of the districts exhibit at least one inversion where a larger district gets a smaller allocation Mitigating unfairness Here we introduce a post-processing step designed to mitigate the inequities present due to the noise introduced for privacy We design the approach

0.5557 b shows the absolute dollars misallocated per million dollars allocated In terms of raw dollar amounts the largest districts see the greatest misallocation and see a drop in funding of about see Figure c On interpretation of this behavior is that larger districts are being taxed to ensure that students in all districts enjoy the same level of privacy protection The results for DAWA Figures d and have more disparity than those of the Laplace mechanism At some districts get about their true allocation while others get only a tenth of their true allocation in expectation whereas under the Laplace mechanism every district gets at least x of their true allocation For districts in Florida see Figure a and Figure b we see almost no difference at At there is very little difference between the true and noisy allocations between districts both additively and multiplicatively At we see the same effect of larger districts being taxed However the effects are less prominent than in Michigan This is because there are fewer small counts in Florida as well as fewer districts overall resulting in a lower variance estimate of the total count used in the denominator of the allocation formula Finding

0.5537 simplest predictive policing setting a precinct has a single police officer and polices two regions A and B Every day the police officer is sent to one neighborhood where they may or may not observe an incident if they do it is logged and we refer to such a log as a discovered incident In addition residents might report incidents that are also logged we call these reported incidents The goal is to build a predictive model for where to send the officer on each day Specifically the goal is to distribute the police officers in proportion to the crime in each area Goal Effective Policing A region with percent of the crime in the precinct should receive percent of the police Achieving this goal requires learning the relative crime rates of the regions To understand the behavior of predictive models we will make some simplifying assumptions We will firstly assume that the predictive model only uses current statistics in some form to make predictions Assumption Predictive Model The officer tosses a coin based on current statistics to decide where to go next To fully specify a predictive model we also need to understand context what information is collected during

0.5445 study A more detailed review of SEPP can be found in SEPPs separate events into two types background events and offspring events Background events are generally assumed to occur independently across space and time according to a Poisson point process Each event can then cause offspring events in its vicinity according to a triggering function decaying in space and time The rate of events at locations G R and times C is characterized by the conditional intensity defined as C HC G CC C C G G where HC G C is the history of events up to time C which we will omit for simplification of notation The background intensity G is often assumed to be time-independent while the triggering function C C G G is generally chosen to be separable in time and space for computational simplicity For each event G C the number of offspring events follows a Poisson distribution with mean C If properly normalized CC GG induces the probability distribution of the locations and times of these events After model fitting the SEPP can be used to predict the locations and times of future events Assume we want to predict the number of events C

0.5423 training window Illustrating runaway feedback in urns To the best of our knowledge there is no theoretical characterization of the asymptotic distributions in this full model once the notion of decay is included We present empirical evidence illustrating the problems with using this model to learn crime rates In our experiments pd Using the Lum and Isaac data we consider a two neighborhood police deployment scenario using first the two regions of Oakland with the most historical incident data Top Without Improvement Policy With Improvement Policy Top vs Top Top vs Random Top vs Top Top vs Random D is co ve re d O n ly A ll In d en Figure The distribution over days versus percentage of balls from region Top in the urn over runs A police force deployed based on the underlying crime rates would send of the force to Top instead of Top and of the force to Top instead of Random the green line shown Top row discovered incidents only both charts left converge to sending of the force to Top while with the improvement policy right the charts appear to converge to the correct crime rates Bottom row all incidents equally weighted

---------------------------------------------------------------------------
TOPIC 7: fairness/sensitive-attributes

---------------------------------------------------------------------------

0.8981 conditional probability and its converse is equivalent to what in ML fairness is variously called sufficiency equalized odds or conditional procedure accuracy sometimes expressed as the conditional independence A D Y Similarly their equal probability and its converse is equivalent to what is called sufficiency or conditional use accuracy equality A Y D Coles fairness definition is identical to equality of opportunity A D Y Linns definition is equivalent to predictive parity A Y D Darlingtons criterion is equivalent to sufficiency in the special case where A R and Y have a multivariate Gaussian distribution This is because for this special case the partial correlation X is equivalent to A Y R In general though we cannot assume even a one way implication since A Y R does not imply X see for a counterexample Similarly Darlingtons criteria and are equivalent to independence and separation only in the special cases of multivariate Gaussian distributions Darlingtons definition is a relaxation of what is called independence or demographic parity in ML fairness ie A R it is equivalent when A and R have a bivariate Gaussian distribution Guions definition people with equal probabilities of success on the job have equal probabilities of

0.8576 to modern day categories Utility-based criteria are omitted but will be discussed below We find that non-comparative criteria discussed by Cleary and Jones do not map onto any of the independence conditions used in ML fairness Similarly Thorndikes and Darlingtons have no Historical criterion ML fairness criterion Relationship Guion individual relaxation Cleary sufficiency when Clearys criterion holds for all subgroups then we we have equivalence when R and Y have bivariate Gaussian distribution Einhorn and Bass sufficiency both involve probability of Y conditioned on R but Einhorn and Bass are only concerned with the conditional likelihood at the decision threshold Thorndike Darlington sufficiency equivalent when variables have a multivariate Gaussian distribution Darlington Darlington separation equivalent when variables have a multivariate Gaussian distribution Darlington independence equivalent when variables have a bivariate Gaussian distribution Cole separation relaxation equivalent to equality of opportunity Linn sufficiency relaxation equivalent to predictive parity Jones mean fair Jones at position n Jones general criterion Peterson and Novick separation equivalent conditional probability and its converse Peterson and Novick sufficiency equivalent equal probability and its converse Table Relationships between testing criteria and MLs independence criteria counterparts that we know of There are conceptual similarities between Jones criteria and the

0.8341 to statistical parity requires that the assigned label is independent of sensitive attributes but only conditional on the true classification of the individual For binary classification tasks a class-balanced classifier results in equal false positive and false negative rates across groups One can also modify a given classifier to be class-balanced while minimizing loss by adding label noise Hardt et al The well-calibrated condition requires that conditional on their label an equal fraction of individuals from each group have the same true classification A well-calibrated classifier labels individuals from different groups with equal accuracy Hebert-Johnson et al extend calibration to multi-calibration which requires the classifier to be well calibrated on a collection of sets of individuals eg all those described by circuits of a given size The class-balanced solution Hardt et al also fails to be well-calibrated Chouldechova and Kleinberg et al independently showed that except in cases of perfect predictions or equal base rates of true classifications across groups there is no class-balanced and well-calibrated classifier A number of recent works explore causal approaches to defining and detecting unfairness Nabi and Shpitser Kusner et al Bareinboim and Pearl Kilbertus et al See the beautiful primer of Pearl et al

0.8244 on important definitions or theorems by compromise Darlington shows that definition is the only one of the four whose errors are uncorrelated with the demographic variable where by errors he means errors in the regression task of estimating Y from R Category Description individual Fairness criterion defined purely in terms of individuals non-comparative Fairness criterion for each subgroup does not reference other subgroups subgroup parity Fairness criterion defined in terms of parity of some value across subgroups correlation Fairness criterion defined in terms of the correlation of the demographic variable with the model output Table Categories of Fairness Criteria In Cole continued exploring ideas of equal outcomes across subgroups defining fairness as all subgroups having the same True Positive Rate TPR recognizable as modern day equality of opportunity That same year Linn introduced but did not advocate for equal Positive Predictive Value PPV as a fairness criterion recognizable as modern day predictive parity is a property of the test itself This is contrary to Thorndike Linn and Cole who take fairness to be a property of the use of a test The latter group tended to assume that a test is static and focused on optimizing its use whereas Clearys

0.798 constraints Equal Opportunity and Equal Odds Equal opportunity requires that each group achieves equal true positive rate TPR or false positive rate FPR while equal odds requires both equal TPR and equal FPR We use the following shorthand to denote different measures of performance including TPR and FPR computed for each group using the true labels and the noisy labels where tpr fpr tpr fpr tpr and fpr are taken with respect to the noisy labels ENFORCING FAIRNESS CONSTRAINTS ON NOISY LABELS CAN BE HARMFUL Recent results have established that enforcing fairness constraints improves classifier accuracy when the labels suffer from label noise that is uniform across different groups However as we shall see adding fairness constraints can lead to harm when group-dependent noise is present in the labels Parity Constraints on Noisy Labels Harms Groups with Clean Labels The first message that we wish to deliver is that naively enforcing parity constraints on the noisy labels may harm the accuracy of the classifier for the groups that are not affected by label noise Without loss of generality we present our results in settings where we wish to train a classifier with equal TPR across groups Similar Table Label noise

0.7951 a learner that is unaware of the labels noisiness ie whenever tpr tpr then Theorem Assume that a classifier is subject to equal odds in the presence of group-dependent label noise Then for any two groups we have tpr tpr tpr fpr fpr fpr tpr fpr Unless the classifier is random on the noisy training data ie tpr fpr it is impossible to satisfy equal odds over the clean data whenever and Proof Noticing that tpr tpr and fpr fpr equalizing fairness metrics on the noisy data and applying Lemma we obtain tpr tpr tpr fpr tpr fpr fpr tpr fpr tpr fpr tpr tpr fpr The argument for FPR is symmetrical fpr fpr tpr fpr tpr fpr tpr fpr tpr fpr tpr fpr tpr fpr Therefore tpr tpr fpr fpr when tpr fpr The proof follows by a direct application of Lemma Theorem implies that the true fairness violation is proportional to the difference in error rates across the different sub-groups We offer two remarks First if the error rates are systematically biased towards a particular group then a perceived fair classifier will lead to unequal odds Second the above bias will be reinforced when the trained model is

0.7846 and their characteristics in Assume we have collected statistical information for binary and for example historical records of college success as well as the binary prediction for admission The prediction can be positive or negative and for either outcome it can match the true label of true positives of true negatives or be mistaken of false positives of false negatives Based on these records we can compile a confusion matrix true label Y positive negative total prediction R positive negative total On this basis we can define some important statistics of confusion matrices Accuracy Positive Predictive Value PPV Negative Predictive Value NPV False Positive Rate FPR False Negative Rate FNR From here on we assume that we have observed sufficiently many cases such that the observations relative frequencies in our tables approximately match the true probabilities Now in order to formulate fairness measures for confusion matrices we need one matrix for each of two groups values of the random variable truth Y R Table Group Ap truth Y R Table Group Based on this the three group fairness measures defined in the previous section can be formulated in terms of statistics of these two confusion matrices Proposition For binary variables

0.7792 statistical parity seeks to equalize the percentage of people receiving a particular outcome across different groups Definition Statistical Parity A predictive model satisfies statistical parity if z z Y y Z z y Z z Equality of odds requires the equality of false positive and false negative rates across different groups Definition of Odds A predictive model satisfies equality of odds if z z y Y Y y Z y Y y Z y Equality of accuracy requires the classifier to make equally accurate predictions across different groups Definition of Accuracy A predictive model satisfies equality of accuracy if z z Z Y Y Z z Y Y Z z Predictive value parity which can be thought of as a weaker version of calibration requires the equality of positive and negative predictive values across different group Definition Predictive Value Parity A predictive model satisfies predictive value parity if z z y Y Y y Z z Y y Y y Z z Y y Statistical Parity Equality of Odds and Accuracy as Rawlsian EOP We begin by translating Rawlsian EOP into the supervised learning setting using the mapping proposed in Figure Recall that we proposed replacing e with effort-based utility

0.7782 predicted to be suitable applicants Properties and Relations In this section we discuss some important properties of the fairness measures introduced above The discussion follows see the appendix for proofs First the accuracy of a predictor is the degree Sufficiency is closely related to calibration Calibration means that the predicted store reflects the true score Calibration and sufficiency are equivalent up to reparametrization cf p to which it agrees with the true label a perfect predictor is a predictor that completely agrees with the true label ie Next we state an important property that is shared by sufficiency and separation but not by independence Proposition If we have a perfect predictor then sufficiency and separation hold Independence is not in general compatible with a perfect predictor independence and perfect predictors are only compatible if the true label is evenly distributed between groups ie if we have which is not the case in general Proposition motivates a definition that will be important in the following It is a distinction between different kinds of group fairness measures Definition A fairness measure is conservative if the measure is necessarily satisfied in the case of a perfect predictor Otherwise a fairness criterion is non-conservative

0.7708 a for all subgroups A a In other words for each subgroup and level of ground truth performance Y y the expected error in Rs prediction of the value y is zero We point out these overlooked concepts not to advocate for their use but to map out the geography of concepts related to fairness more completely Compromises Darlington points out that Thorndikes criterion is a compromise between one criterion related to sufficiency and one related to separation see Section and Tables and In general a space of compromises is possible in terms of correlations this might be modeled using a parameter where values of and imply Darlingtons definitions and respectively This also suggests exploring interpolations between the contrasting sufficiency and separation criteria For example one way of parameterizing their interpolation is in terms of binary confusion matrix outcomes Definition λ-Thorndikian fairness A binary classifier satisfies λ-Thorndikian fairness with respect to demographic variable A if both a TP TP is constant for all values of A and b is constant for all values of A Note that -Thorndikian fairness is equivalent to sufficiency while -Thorndikian fairness is equivalent to separation Petersen and Novick showed that -Thorndikian fairness requires that either

0.7626 where there is no such bias will not hold if the risk distributions under treatment vary by protected attribute since requires Y A also requires T A Y which forbids discrimination in treatment assignment when controlling for risk under treatment If does not hold it is possible that still holds if the conditional and marginal probabilities are such that all terms exactly cancel however there is no semantic reason why this should hold Theorem assumes PT a a positivity-like assumption that holds in all settings that are suitable for algorithmic RAIs Violations of this assumption indicate perfect or imperfect treatment assignment historically for a group Predictive parity Base parity and demographic parity may be ill-suited for settings where base rates differ by protected attribute due to disparate needs Here we may instead desire parity in an error metric such as precision Positive predictive parity requires the precision also known as positive predictive value to be independent of the protected attribute and negative predictive parity requires the negative predictive value to be independent of the protected attribute We define observational Predictive Parity oPP as Y A Y and counterfactual Predictive Parity as Y A Y where corresponds to negative predictive parity

0.7591 with and exhibits disparity amplification As a result in the rest of the paper we focus on equalized odds rather than predictive parity or calibration We leave as future work the identification of a discrimination criterion and a worldview that together motivate the predictive parity or calibration test CONNECTION TO MISCLASSIFICATION Here we show that the definition of disparity amplification is closely related to that given by Zafar et al in their treatment of disparate misclassification rates First we motivate the issue of disparate misclassification rates with an example Let and be independent and uniformly random binary variables If where is the XOR both protected groups are given the positive label exactly half of the time so there is no output disparity However one group always receives the correct classification and the other always receives the incorrect classification so the disparity in the misclassification rates is as large as it can be This shows that a lack of disparity amplification does not imply a lack of disparity in misclassification rates Conversely a lack of disparity in misclassification rates does not imply a lack of disparity amplification To see this modify the above example so that instead Now both groups have

0.7494 the PMF of That is for s s Abusing notation we denote by the collection and call it the of S We denote by the support of the namely the set s x r st r s An accuracy profile is a distribution of scores for a calibrated classifier S Because S is calibrated the conveys information about the performance of S and is constrained by properties of the underlying distribution on X For example the expectation is exactly the base rate for the population Proposition Proof in For any groupwise calibrated soft classifier S for all groups G Accuracy profiles also provide useful geometric intuition for reasoning about the effects of post-processing calibrated scores We elaborate on this in Section see Figure Group Fairness Measures Several well-studied measures of statistical fairness eg look at how the following key performance measures of a classifier differ across groups We define these statistics formally Definition For a hard classifier Y and group we define the false positive rate of Y for PrY Y false negative rate of Y for PrY Y positive predictive value of Y for PrY Y negative predictive value of Y for PrY Y The probability statements in the

0.7493 does not hold may hold but it is difficult to reason why this should hold in any setting Like Theorem Theorem assumes a mild positivity-like assumption Equalized odds In settings where TPR and FPR are more important than predictive value we may desire parity in TPR and FPR a fairness notion known as Equalized Odds Observational Equalized Odds requires that Y A Y and counterfactual Equalized Odds cEO requires that Y A Y T E O Assume PY a and PT a If holds then cEO holds if and only if the following balance condition holds C PY Y PT Y Y PY PY PY Y aPT Y Y a PY a PY Y PT Y Y a PY a PT Y Y PY PY The balance condition is satisfied under the following independence conditions which comprise sufficient conditions for to imply cEO C Y A Y A T A Y Y Y Y T A The rst two conditions of require and so requires to hold In settings where there is discrimination in treatment assignment even when controlling for true risk is unlikely to hold Even if there is no such discrimination will not hold if there are differences

0.7417 partition of the universe X We denote by the set of instances x in group and by the random variable distributed uniformly over Note that for any events E and E E E Definition Base rate The base rate of a group G is PrY When X is finite is simply the fraction of individuals x in the group for whom Y x A classifier is a randomized function with domain X G A hard classifier denoted Y outputs a prediction in interpreted as a guess of the true type Y x A soft classifier denoted S outputs a score s interpreted as a measure of confidence x We restrict our attention to soft classifiers with finite image We call a classifier group blind if its output is independent of the input group For all groups G we call a hard classifier Y non-trivial on if PrY and PrY Hard classifiers are trivial on if they are not non-trivial on A post-processor is a randomized function with domain G As with classifiers a post-processor can be hard or soft A hard post-processor denoted D outputs a prediction in A soft post-processor denoted outputs a score s Observe that for a

0.7335 A second group of measures are sensitive to any conditional or unconditional dependence between sensitive class variables and the decisions or outcomes influenced by the algorithms output eg whether or not a loan or parole is actually granted Among the various measures which take this form are the rule and the Calders-Verwer gap as well as some definitions of statistical parity which measure outcome disparity rather than score disparity Other possible measures in this class might test for a conditional dependence between class and outcome given the score assigned to an individual Measures in this group can be understood as measures of outcome bias they detect the causal influence of class on outcome either through some effect on score in the unconditional case or by some other causal process in the conditional case A third group of measures are sensitive to the stability of the relationship between behavior and score when one conditions on class eg measures of balance disparate mistreatment and predictive equality These measures judge there to be no bias if the probability of receiving a given score given whether or not one engaged the behavior is invariant among classes This is just to require that behavior render

0.724 a group For example Figure shows the expected true positives true negatives false positive and false negatives when using a threshold General impossibility of equalizing PPV It is not always possible to directly post-process a soft groupwise calibrated classifier into a hard one with equalized PPV or NPV for all groups as we demonstrate by counterexample in Proposition Before proceeding we note that our counterexample is somewhat contrived in particular the induced by the soft classifier S in the proof of Proposition takes only one value on each group When the of S is more nicely structured on each group we will see that there are general methods to equalize PPV or NPV Proposition Fix two disjoint groups and with respective base rates and such that Then there exists a soft classifier S that is groupwise calibrated but for which there is no post-processor D G such that D S equalizes PPV unless for i or Proof of Proposition Consider the classifier S such that if x and if x This classifier is trivially groupwise calibrated Since for i and we conclude that is well-defined for and The proof now follows from the characterization of PPV in Proposition This is

0.7218 G for which This corresponds to the case where the two thresholds are the same and therefore individuals with that score must be mapped to with probability and to with probability with the remainder mapped to The corresponding threshold post-processor is defined as follows s s s else s else s else s s Using two thresholds allows the equalization of both PPV and NPV across groups in general whereas without deferrals we could only equalize one or the other In our full paper we first demonstrate the existence of near-trivial classifiers that equalize PPV and NPV by deferring on all but the highest and lowest scores We now claim the existence of meaningful non-trivial threshold deferring post-processors that equalize PPV and NPV across groups Proposition Proof in Let S be a soft classifier with nice that is groupwise calibrated for a set of groups G Suppose that for all G Then there exists a non-trivial threshold post-processor such that the hard classifier Y S equalizes and for all G The following example demonstrates that it is sometimes possible to equalize PPV NPV FPR and FNR using deferrals but without equalizing the themselves Example Eqalizing PPV NPV and with Thresholds

0.7192 which they are sensitive Our critical discussion presupposes that when the dependencies to which a measure is sensitive exist the measure will report bias On that assumption then one broad set of measures diagnose bias when the score returned by an algorithm is probabilistically dependent on a class variable like race or sex either conditionally or unconditionally In the case that the measure is sensitive to an unconditional dependence between class and score these measure effectively test for departures from the Independence condition in the terminology of Barocas and Hardt Among the various measures which take this form are statistical parity and demographic parity Other measures in this class test for a conditional dependence between class and score given the feature variables used by the algorithm eg conditional statistical parity as described by Corbett-Davies et al Whether sensitive to conditional or unconditional dependencies between class and score these measures can be conceptualized as measures of procedural bias in that they are sensitive to a particular feature of the procedure by which scores are produced In particular they detect the direct causal or influence of class in the conditional case or some proxy of class in the unconditional case on score

0.7177 satisfied over the noisy labels while being violated over the clean labels Before proceeding we require extending Proposition of into the situation with group-dependent label noise A similar result appears in Lemma For each group we have that tpr tpr fpr fpr tpr fpr Proof Expanding using law of total probability we have tpr P P P P tpr fpr Note in the above we drop the dependence on when conditioning on This is because is trained purely on the noisy labels and encodes all the information has about A similar derivation holds for fpr We also note that in the special case where all groups suffer from an identical rate of label corruption the learner can be oblivious to the specific error rates Theorem Consider a classification problem with noisy labels where the noise rates are independent of group membership so that and Then it follows that tpr tpr if equal odds equalizing both TPR and FPR on the noisy labels is imposed The proof follows by applying the assumption of equal error rates and equal odds on the noisy labels with Lemma However things break down in the general case If we impose equal odds across groups on

0.7138 independence is equivalent to Proposition For binary variables sufficiency holds both groups have the same positive predictive value PPV ie and the same negative predictive value NPV ie Proposition For binary variables separation holds both groups have the same false positive rate FPR ie and false negative rate FNR ie Other Kinds of Fairness In this section we discuss kinds of fairness that are not group fairness measures but that will play an important role in our discussion below The group fairness measures introduced above are observational measures ie they can be measured based on data that are typically available In may cases we have access to labeled data a predictive model and labels or a different kind of access to the sensitive characteristic of individuals However there are other kinds of fairness considerations that are not measurable in terms of these quantities Kamishima et al propose to distinguish three different kinds of fairness The first kind prejudice subsumes the notions of group fairness discussed above The second kind underestimation is due to the fact that a model may be unfair due to the finiteness of training data The definition of the third kind negative legacy is particularly important Definition

0.7111 of the correlation between culture and test score in Darlingtons notation plotted against the correlation between test score and ground truth according to his definitions The correlation between the demographic and target variables is assumed here to be fixed at shows that for any given non-zero correlation between the demographic and target variables definitions and converge as the correlation between the test score and the target variable approach When the test has only a poor correlation with the target variable there may be no fair solution using definition Figure enables a range of further observations According to definition for a given correlation between demographic and target variables the lower the correlation of the test with the target variable the higher it is allowed to correlate with the demographic variable and still be considered fair Definition on the other hand is the opposite in that the lower the correlation of the test with the target variable the lower too must be the the tests correlation with the demographic variable Darlingtons criterion is the geometric mean of criteria and a compromise position midway between the two however a compromise may end up satisfying nobody psychometricians are not in the habit of agreeing

0.7078 the roles played by the model test score R and the target variable Y That is one criterion can be transformed into another by swapping the symbols R and Y for example separation can be transformed into sufficiency A R Y A Y R In this section we will refer to this type of correspondence as converse ie separation is the converse of sufficiency When viewed in this light some asymmetries stand out Converse Cleary criterion Clearys criterion considers the case of a regression model that predicts a target variable Y given test score R One could also consider the converse regression model mentioned in passing by which predicts model score R from ground truth Y as an instrument for detecting bias say that a test has connotations of unfair for a subgroup if the converse regression line has positive errors ie for each given level of ground truth ability the test score is higher than the converse regression line predicts Converse calibration In a regression scenario the calibration condition PY R r A a r can be rewritten as R r A a r or r R r A a The converse calibration condition is therefore ER y Y

0.7073 the quantity P P as the subgroup fairness or alternately the -unfairness of The notion of subgroup fairness imposes a statistical constraint on combinatorially many groups definable by the protected attributes This is in contrast to more common statistical fairness definitions defined on coarse groups definable by a single protected attribute Given a protected attribute xi and a value for that attribute a define or more generally to any fairness constraint that can be expressed as a linear equality on the conditional moments E t X y D X X y where X y is an event defined with respect to X y and t X Equality of false positive rate is a particular instantiation of this kind of constraint where is the event y and t D X the x xi a denoting the set of individuals who have that particular value of their protected attribute In contrast to subgroup fairness we refer to a classifier D as marginally fair if it satisfies false positive subgroup fairness with respect to the functions for each protected attribute xi and realization a If the algorithm D fails to satisfy the -subgroup fairness condition then we say that D is -unfair with

0.7065 definitions above reflect two sources of randomness the sampling of from the group and any random choices made by the classifier Y Among previous works some focus on equalizing only one or both of the false positive rates and false negative rates across groups called balance for the negative and positive classes respectively Equalizing positive and negative predictive value across groups is often combined into one condition called predictive parity We split the value out to be a separate condition for the positive and negative predictive classes Predictive parity appears to be a hard-classifier analogue of calibration both can be interpreted as saying that the output of the classifier hard or soft contains all the information contained in group membership Our results highlight that the relationship between these notions is more subtle than it first appears see Section for further discussion THE LIMITS OF POST-PROCESSING Suppose throughout this section that S is a groupwise calibrated soft classifier Our goal in this section is to make binary predictions based on and possibly the subject to equalizing PPV among groups That is we wish to make a prediction using a hard post-processor D such that Y D S equalizes PPV among groups

0.6987 policies The second equality holds because of conditional exogeneity of and the law of iterated expectations If is unrestricted then up to arbitrary tie breaking for We assume that individuals are additionally characterized by a binary variable corresponding to protected groups such as gender or race This variable may or may not be part of Fairness Definitions Numerous definitions of fairness have been proposed in the literature We will focus on the following popular definition of fairness corresponding to the notion of predictive parity or calibration This equality is the basis of tests for preferential discrimination in empirical economics See for instance A similar requirement could be imposed for Another related requirement is balance for the positive or negative class which indicates equality of false positive respectively negative rates Predictive parity requires that expected merit conditional on having received treatment or is the same across the groups Balance requires that the probability of being treated conditional on merit is the same across the groups For the binary case balance and predictive parity cannot hold at the same time unless either prediction is perfect or base rates are equal In our subsequent discussion we focus on predictive parity as the leading

0.696 One must then focus attention on fair use of the test scores rather than on the scores themselves Contrary to Cleary Thorndike argued that sharing a common regression line is not important as one can achieve fair selection goals by using different regression lines and different selection thresholds for the two groups As an alternative to Cleary Thorndike proposed that the ratio of predicted positives to ground truth positives be equal for each group Using confusion matrix terminology this is equivalent to requiring that the ratio TP FP/TP be equal for each subgroup According to Thorndike the situation in Figure a is fair for test cutoff x Figure b is unfair using any single threshold but fair if threshold x for group Similar to modern day ML fairness eg Friedler et al in Thorndike also pointed out the tension between individual notions of fairness and group notions of fairness the two definitions of fairness one based on predicted criterion score for individuals and the other on the distribution of criterion scores in the two groups will always be in conflict The conflict was also raised by others in the period including Sawyer et al in a foreshadowing of the compas

0.6926 given by the calibrated soft classifier S into binary predictions A representative example is a judge making a bail decision based on a score provided by a software package Following we consider the following four performance measures for the resulting binary classifier the positive predictive value PPV namely the fraction of individuals that have the property among all individuals that the classifier predicted to have the property The false positive rate FPR namely the fraction of individuals that were predicted to have the property among all individuals that dont have the property The negative predictive value NPV and false negative rate FNR which are defined analogously Ideally we would like to equalize each one of the four measures across the groups ie the measure will have the same value when restricted to samples from each group Unfortunately however we know that this is impossible in general This leads us to a broad question that motivates our work Under what conditions can we post-process a calibrated soft classifiers outputs so that the resulting hard classifier equates a subset of FNR FPR across a set of protected groups How can we balance these conflicting goals Results Post-Processing With Thesholds In a first

0.6889 equal opportunity The techniques in Agarwal et al and Kearns et al also apply equally well to equality of false negative rates and equality of classification rates also known as statistical parity Each fairness constraint is defined with respect to a set of protected groups We define sets of protected groups via a family of indicator functions G for those groups defined over protected attributes Each X G has the semantics that indicates that an individual with protected features x is in group We now formally define false positive subgroup fairness Definition False Positive Subgroup Fairness Fix any classifier D distribution P collection of group indicators G and parameter For each G define P Pr P y P FPD where FPD D X y and D X y denote the overall false-positive rate of D and the false-positive rate of D on group respectively We say D satisfies -False Positive FP Fairness with respect to P and G if for every G P P We will sometimes refer to FPD FP-base rate Since we do not consider other measures in this paper we refer to this notion as simply subgroup fairness Given a fixed subgroup G we will refer to

0.6814 in which theyre defined Darlington helped to shed light on the relationship between Cleary and Thorndikes conceptions of fairness by expressing them in a common formalism He defines four fairness criteria in terms of the correlation between the demographic variable and the test score Following Darlington Clearys criterion can be restated in terms of correlations of the culture variable with test scores If Clearys criterion holds for every subgroup then Similarly Thorndikes criterion is equivalent to requiring that The criterion is motivated by thinking about R as a dependent variable affected by independent variables A and Y If A has no direct effect on R once Y is taken into account then we have a zero partial correlation ie Y An alternative starkly simple criterion of recognizable as modern day demographic parity is introduced but not dwelt on Darlingtons mapping of Clearys and Thorndikes criteria lets him prove that theyre incompatible except in the special cases where the test perfectly predicts the target variable or where the target variable is uncorrelated with the demographic variable Figure reproduced from Darlingtons work criterion only holds if A R and Y have a multivariate normal distribution Figure Darlingtons original graph of fair values

0.6796 relatively simple less confounded way In a DIF analysis the item is evaluated against something designed to Source Criterion Category Proposition Guion people with equal probabilities of success on the job have equal probabilities of being hired for the job individual Is the use of the test fair Cleary a subgroup does not have consistent errors non-comparative Is the test fair to subgroup a Einhorn and Bass y R a is constant for all subgroups a subgroup parity Is the use of the test fair with respect to A Thorndike ra A y A a is constant for all subgroups a subgroup parity Is the use of the test fair with respect to A Darlington equivalent to R correlation Is the test fair with respect to A Darlington Darlington equivalent to Y Darlington Darlington is maximized where is the subjective value placed on subgroup attribute A correlation Does the test produce the culturally optimum optimal outcome A Cole ra Y a is constant for all subgroups a subgroup parity Is the use of the test fair with respect to A Linn y R a is constant for all subgroups a subgroup parity Is the use of the test fair with

0.6789 hard soft post-processor is a randomized function D G G that takes as input the output of a deferring soft and post-processes it into a deferring hard soft classifier We also introduce new versions of the FPR and FNR conditioned on not deferring Definition The conditional false positive rate and conditional false negative rate of a deferring hard classifier Y for a group are respectively PrY Y Y PrY Y Y We additionally consider a version of the accuracy profile conditioned on not deferring which we call the conditional For non-deferring soft classifiers Definitions and coincide Definition The conditional of a classifier S for a group is the PMF of conditioned on not outputting That is for s s Note that the conditional is undefined if Abusing notation we denote by the collection and call it the conditional of S The conditional error rates are applicable generally but they can be difficult to interpret The consequences of using the conditional FPR and FNR are discussed further in Section along with a discussion of different deferral models They are also amenable to the consideration of additional goals which we will briefly address For example one could seek to minimize the total

0.6733 respect to A Jones a a non-comparative Is the test fair to subgroup a mean fair Jones a subgroup a has equal representation in the top-n candidates ranked by model score as it has in the top-n candidates ranked by Y for all n non-comparative Is the test fair to subgroup a general standard Jones a subgroup a has equal representation in the top-n candidates ranked by model score as it has in the top-n candidates ranked by Y non-comparative Is the use of the test fair to subgroup a at position n Peterson Novick ra Y a is constant for all subgroups a and ra Y a is constant for all subgroups a subgroup parity Is the use of the test fair with respect to A conditional probability and its converse Peterson Novick y R a is constant for all subgroups a and y R a is constant for all subgroups a subgroup parity Is the use of the test fair with respect to A equal probability and its converse Table Early technical definitions of fairness in educational and employment testing Variables R is the test score Y is the target variable A is the demographic variable The Proposition

0.6727 will be unassociated with any of them In that circumstance and only in that circumstance will the frequencies of B and S be non-accidentally invariant over classes As noted by Chouldechova and Kleinberg it is then possible in fact trivial to ensure joint minimization of score- and behavior-relative measures of bias C is not d-connected to either S or B on any conditioning set and so is independent of either variable conditional on the other Second if one conditions on a terminal variable in a path that closes the path Hence score-relative measures of bias can be minimized by ensuring that score perfectly predicts class because in that case to condition on S is in effect to condition on C And similarly if it happens that behavior perfectly predicts class or score behavior-relative measures of bias can be minimized because to condition on behavior is in effect to condition on class or respectively score Kleinberg who reach much the same result quantitatively Note that perfect prediction of C by S requires the existence of disparate treatment in the wide sense Thus score-relative error bias and procedural bias cannot be jointly eliminated ie per Barocas and Hardt Independence and Sufficiency cannot

0.6727 coupling Then by the theorem in p such attains the value of for the second term of This means that the construct accuracy which is the average of the two terms is which is what we want Moreover and have the same distribution so satisfies demographic parity Theorems and demonstrate that the WAE worldview combined with the desire to avoid disparity amplification while retaining the utility of models motivates the demographic parity test Equalized Odds and WYSIWYG We now argue that a similar relationship exists between the equalized odds test and the WYSIWYG worldview Theorem If the WYSIWYG worldview holds a model that passes the equalized odds test does not have disparity amplification under Definition Moreover if the WYSIWYG worldview holds every construct optimal model satisfies equalized odds Proof Let Y and be the supports of and respectively Applying the WYSIWYG worldview to the definition of equalized odds we get for all Y and Therefore we have This concludes the proof of the first statement For an optimal model we have by the WYSIWYG worldview Because fully determines the value of Definition implies that every optimal model satisfies equalized odds On the other hand our intuition is that when the

0.6692 the lack of disparity amplification it may not be sufficient as an anti-discrimination measure as it effectively allows certain forms of discrimination On the other hand if the test disallows a construct optimal model the test may be too strict in a way that lowers the utility of the model Therefore to argue that a worldview motivates an empirical test we will prove the following two statements a Every model that passes the empirical test does not have disparity amplification and b every optimal model passes the empirical test We apply this reasoning to demographic parity Definition and equalized odds Definition showing that the WAE and WYSIWYG worldviews respectively motivate these empirical tests More formally we will prove statements a and b for every joint distribution of and that is consistent with the worldview Table summarizes these results Demographic Parity and WAE Theorem A model that passes the demographic parity test does not have disparity amplification under Definition Moreover Table Summary of the results in Section We say that a worldview motivates an empirical test if it precludes disparity amplification Definition but does not preclude a perfectly predictive model The Were All Equal WAE worldview motivates the demographic parity test

0.6689 and if the worldview does not hold the demographic parity test tends to lower the utility of the model The WYSIWYG worldview motivates the equalized odds test and if the worldview does not hold the equalized odds test allows models that have disparity amplification Finally regardless of the worldview the predictive parity and calibration tests do not effectively prevent disparity amplification Here we assume that WAE and WYSIWYG do not hold simultaneously Were All Equal Worldview WYSIWYG Worldview Demo Parity Definition Theorem Always suboptimal Theorem Equal Odds Definition Amplification allowed Theorem Theorem Predictive Parity Definition Amplification allowed Theorem Calibration Definition Not robust to post-processing Theorem if the WAE worldview holds every construct optimal model satisfies demographic parity Proof By the definition of demographic parity the left-hand side of is Since the total variation distance is always nonnegative demographic parity ensures the lack of disparity amplification If the WAE worldview holds we have so every optimal model satisfies This implies demographic parity by Definition The first part of Theorem shows that we can guarantee that a model will not have disparity amplification by training it to pass the demographic parity test However this does not mean that demographic parity is appropriate

0.6664 equal s-TPR and s-TNR values across sensitive groups and equivalently equalized odds is the aim of achieving equal s-TPR and s-TNR across sensitive groups CV comparative-sensitive-TPR accuracy -accuracy -accuracy sensitive-accuracy TNR sensitive-TNR BCR sensitive-calibration comparative-sensitive-accuracy comparative-sensitive-TNR TPR sensitive-TPR CV comparative-sensitive-TPR accuracy -accuracy -accuracy sensitive-accuracy TNR sensitive-TNR BCR sensitive-calibration comparative-sensitive-accuracy comparative-sensitive-TNR TPR sensitive-TPR sensitive-calibration Figure Examining the relationships between different measures of accuracy and fairness when considered across all examined datasets and algorithms including both baseline and fairness-aware algorithms under numerical-binary preprocessing A simple sample correlation statistic is then computed for each set of pairs of measurements Strongly positively correlated metric pairs are shown in blue and strongly negatively correlated pairs are shown in red The top figure shows the results when run on the original datasets and the bottom when the datasets are downsampled to be balanced by class and sensitive attribute Letting any of the above measures be denoted Y Y s the values can then be aggregated for comparison by taking the mean directly s S Y Y or by taking the mean over comparisons analogous to DI and CV Y Y Y Y or Y Y Y Y s In each of these cases as we saw

0.6663 populations it labels a person as positive only if they are a blue man or a green woman This equalizes false positive rates across the four specified groups but of course not over the finer-grained subgroups defined by the intersections of the two protected attributes Kearns et al also proposed an approach to the problem of fairness gerrymandering rather than asking for statistical definitions of fairness that hold over a small number of coarsely defined groups ask for them to hold over a combinatorially or infinitely large collection of subgroups defined by a set of functions G of the protected attributes Hébert-Johnson et al independently made a similar proposal For example we could ask to equalize false positive rates across every subgroup that can be defined as the intersection or conjunction of d protected attributes for which there are d such groups Kearns et al showed that as long as the class of functions defining these subgroups has bounded VC dimension the statistical learning problem of finding the best distribution over classifiers in subject to the constraint of equalizing the positive classification rate the false positive rate or the false negative rate over every subgroup defined over G is solvable

0.6663 soft classifier S D S is a hard classifier and S is x X s y soft post-processor hard Y soft S hard post-processor D Figure We call a classifier that returns results in a soft classifier to differentiate it from those which return results in which we call hard classifiers We refer to classifiers that take as input the output of a soft classifier as post-processors a soft classifier As with classifiers we call a post-processor group blind if its output is independent of the group and we restrict our attention to post-processors with finite image The restriction to finite image is for mathematical convenience and also because digital memory leads to discrete universes our results generalize to infinite images as well In Section we expand the definitions of both classifier and post-processors to allow an additional input or output the special symbol indicating a deferral Calibration Several works concerning algorithmic fairness focus on various notions of calibration The following calibration notions are defined only over soft classifiers Definition Calibration Soft We say a soft classifier S is calibrated if s for which s Pr Y X s E Y X s s The probability above is taken over

0.6634 is identical with respect to the construct space More formally is independent of ie Worldview WYSIWYG Under the What You See Is What You Get WYSIWYG worldview the observed space accurately reflects the construct space More formally CONSTRUCT CRITERIA We introduce two construct criteria for models By using the construct these criteria must be combined with a worldview for application to a model Unlike the more readily applied empirical tests construct criteria depend upon the attribute truly relevant to the classification task Here we consider the case where and are categorical but not necessarily binary and in Section we generalize the definition to numerical Disparity Amplification When is binary the size of a models discriminatory effect is commonly measured by the difference in positive classification rates Output disparity generalizes this measure for the case of non-binary categorical Definition Output Disparity Let the output of a model be categorical The output disparity of the model is the quantity However not all output disparities are bad in every context In particular because we want the model to accurately reflect the construct we allow an output disparity insofar as it can be explained by the inter-group disparity in This happens when Since a

0.66 in the risk distributions under treatment since the last condition of requires Y A requires further conditions such as parity in the TPR/FPR against the outcome under treatment If these conditions are not met could imply cEO if holds but it is difficult to reason about why this would hold when the independence conditions do not Our theoretical analysis suggests that in many settings equalizing the observational fairness metric will not equalize the counterfactual fairness metric We conclude by noting that the theorems hold when conditioning on any features X and in this context these theorems are relevant to individual notions of fairness Original Reweighed C ou nt er fa l O er va na l Ba se R at e Group A A Figure Counterfactual and observational base rates before and after applying a fairness-corrective method that reweighs training data X-axis controls the bias of treatment assignment toward group A Before reweighing Original counterfactual base rates are equal holds but observational base rates are different doesnt hold for since group A is more likely to get treated Reweighing achieves but no longer holds Experiments on synthetic data We empirically demonstrate that equalizing the observational metric can increase disparity in

0.6582 Hardt this condition is called Sufficiency It is worth emphasizing the important difference between the two classes of measures of error bias The former class is sensitive to the independence of score and class conditional on behavior ie whether SCB while the latter are sensitive to the independence of behavior and class conditional on score ie whether BCS We call the first group including balance misclassification predictive equality and false positive and false negative rates behavior-relative measures of error bias and we call the second group including calibration and predictive parity score-relative measures of error bias Intuitively it might be said that behavior-relative measures are appropriate when one is concerned to ensure that correct and incorrect predictions are similarly distributed in distinct classes while score-relative measures are appropriate when one is concerned to insure that scores are as informative as possible no matter what class is being considered That said we do not endorse the intuitions or the measures for reasons given below Causal Structure and Performance Over the last three years a number of investigators have shown that various measures of bias cannot be driven to zero outside special conditions and that particular measures of different kinds of bias

0.6571 its space is denoted A probabilistic classifier is represented by a function X that outputs for each given sample X the probability that belongs to the positive class The deterministic classifier predicts class if and class otherwise where is a classification threshold Note that the function depends only on the feature but not on the sensitive attribute thus predicting using satisfies fairness through unawareness The central goal of this paper is to provide a statistical test to detect if a classifier fails to satisfy a prescribed notion of machine learning fairness A statistical hypothesis test can be cast with the null hypothesis being the classifier is fair against the alternative hypothesis being the classifier is not fair In this paper we focus on statistical notions of group fairness which are usually defined using conditional probabilities A prevalent notion of fairness in machine learning is the criterion of equality of opportunity which requires that the true positive rate are equal between subgroups Definition Equal opportunity A classifier X satisfies the equal opportunity criterion relative to Q if where is the classification threshold Another popular criterion of machine learning fairness is the equalized odds which is more stringent than the equality of

0.6571 First it is essential to keep in mind that this paper focuses on probabilistic notions of fairness in particular we provide the Wasserstein statistical test for probabilistic equality of opportunity and probabilistic equalized odds Probabilistic notions are only approximations of the original definitions and the employment of probabilistic notions are solely for the technical purposes Due to the sensitivity of the test result on the choice of fairness notions a test that is designed for probabilistic notions may not be applicable to test for original notions of fairness due to the interplay with the threshold and the radical difference of both the test statistic and the limiting distribution If a logistic classifier is rejected using our framework for probabilistic equal opportunity it does not necessarily imply that the classifier fails to satisfy the equal opportunity criterion and vice versa The same argument holds when we test for probabilistic equalized odds Second the outcome of the Wasserstein projection test is dependent on the choice of the underlying metric on the feature the sensitive attribute and the label spaces Indeed the test outcome can change if we switch the metric of the feature space for example from the Euclidean norm to a

0.6542 argue that arguments against independence proposed in are flawed in making unwarranted assumptions about conservative fairness measures such as sufficiency and separation We prove that sufficiency and separation are not incrementally conservative which means that these measures are not necessarily preserved if we increase the accuracy of a predictor We then state arguments in favor of independence section finding that independence captures aspects of fairness not covered by sufficiency and separation Finally we discuss how to balance different fairness considerations section FAIRNESS MEASURES DEFINITIONS AND PROPERTIES This section introduces and discusses the most important measures of group fairness formulates these measures for the case of binary variables and discusses other relevant fairness measures setting the stage for the discussion in later sections Definitions of Group Fairness Here we state the most important group fairness measures following the discussion in These measures are formulated using random variables all measures we consider correspond to statistical properties of these variables The variables have the following interpretation is the true label ie the characteristic that we want to predict is the prediction which can be the output of an algorithm is the characteristic indicating group membership ie the property with respect to which we

0.6535 Suppose D refers to the distribution and D to a suitable distribution over X Y concretely in the demographic parity setting D Y while in the equality of opportunity setting D DEO Y Y Existing fairness measures as risks We begin with a simple observation the DI and MD fairness measures are transformations of suitable false positive and negative rates and thus are statistical risks Specifically for any randomised classifier X with predictions Y X Bern X we have FNR PY Y and FPR PY Y Consequently for D DI D FPR FNR D MD D FPR D FNR D Observe that one can equally choose D DEO so as to yield approximate fairness measures for the equality of opportunity setting clearly when eg DI DEO we recover Equation The notion that fairness measures are risks on D is implicit in prior surveys eg Žliobaite By making this notion explicit we may now succinctly state the fairness-aware learning problem Fairness-aware learning general case Informally fairness-aware learning involves finding a randomised classifier X so that Y is well predicted but Y is not Formally suppose that we measure how well can predict Y and Y via two risks a performance measure

0.6506 the protected attribute combinations ii extend the notation to the non-binary case for prediction tasks involving regression analysis for example iii extend the probabilistic notation to non-labeled data

0.6505 and respectively will obey any parity constraint However suppose group has a set of clean labels while group has clean labels when the ground truth is but there is a chance that corrupting noise will cause the observed label to be flipped from the true value when In this case fair trained on both groups achieves perceived equal True Positive Rates TPR between the two groups and is the best one to do so this indeed hurts group prediction performance as opposed to accuracy before but the labels in group are not affected by noise Although also considers this single-group noise setting and shows that fairness interventions could aid in reducing the error caused by label bias our observation demonstrates a special case where potential harm occurs Example A simple classification problem to illustrate the possibility of harming the clean group when training a fair True Positive Rate TPR model over a set of noisy labels Group Group Pooled fair Example A simple classification problem to illustrate the possibility of wrongly perceived fairness due to training on noisy labels Group Group Pooled fair Example A classifier may appear to achieve parity when it does not Furthermore imposing a parity constraint

0.6488 the privileged class and Y is the binary classification label where is the positive outcome and is the negative outcome Let Y be the predicted outcomes of some algorithm We can define accuracy and fairness measures in terms of conditional probabilities of outcome variables Y Y with respect to variables like Y and S Accuracy measures We consider the standard accuracy measures the uniform accuracy PY Y the true positive rate TPR PY Y and the true negative rate TNR PY Y We also consider A recent tutorial puts the number of fairness measures at Figure Examining the results of the Feldman et al algorithm under different preprocessing choices numerical versus numerical-binary Each dot plots the result of a single split of the data in terms of the labeled metric under both preprocessing choices The gray line shows equality between the preprocessing choices The model used within the Feldman algorithm is listed and some variants of the algorithm had the tradeoff parameter optimized for either accuracy or disparate impact value the balanced classification rate BCR a version of accuracy that is unweighted per class Definition BCR PY Y PY Y Fairness measures Fairness measures can be divided into three broad

---------------------------------------------------------------------------
TOPIC 8: fairness/optimization

---------------------------------------------------------------------------

0.9815 Proof For every arm after it was pulled times Thus the regret that stems from this phase is bounded by The optimal algorithm pulls the optimal arm for the remaining rounds Thus any additional regret is a consequence of 𝐴𝐿𝐺 and therefore the regret is bounded by The second case is a generalization of uni which was presented in Section Formally const if there exists such that Algorithm Constant function utility maximization algorithm Input Black-box bandit algorithm 𝐴𝐿𝐺 for do Phase for do pull arm if Invoke 𝐴𝐿𝐺 for the remaining rounds Phase for all and for all Algorithm is a variation of Successive Elimination that achieves a regret of max To illustrate the motivation to use Algorithm instead of Algorithm consider the following MAB instance Fix horizon number of arms and define to be for every and every Let Set and set the expected rewards of the other arms arbitrarily Note that Thus the first phase of Algorithm pulls each arm and incur a regret Algorithm will pull each arm exactly once and then invoke the black-box algorithm Thus in this case the regret is bounded by the regret of 𝐴𝐿𝐺 Algorithm leverages the fact that functions in const

0.9811 at most min log times and therefore the regret from this phase is bounded by min log Lemma proves that the second phase ends after each arm was pulled at most min times Hence the regret from this phase is bounded by min The regret from the third phase depends on the approximation error of The approximation error of on a single entry is bounded by min We divide the analysis into two cases For arm with the regret stems from the approximation error of which is bounded by min For arm with given the clean case if arm is pulled during the third phase it implies that the reward gap is very close to the transfer cost Ie Thus rounds were not sufficient to approximate correctly and If max the regret is bounded by this term Otherwise the regret is bounded by min To summarize the regret resulting from the approximation phases is bounded by min log min The regret from the third step is bounded min min log The regret of the fourth phase depends on the regret of the black-box algorithm and hence the additional term which completes the regret analysis Lemma Fix any MAB instance the

0.9739 all Hyper-cube of values in the clean event while and do Phase Pull all arms once update counters confidence bounds and while st max min and do Phase Pull all arms once update counters confidence bounds and while st and and do Phase Pull arm the minimal number of times so update and counters Invoke 𝐴𝐿𝐺 for the remaining rounds Phase Lemma Fix any arbitrary instance of MAB and let and a hyper-cube Then after pulling each arm at most times for every max min Proof After pulling each arm times with high probability for every That is Since is 𝐿-Lipschitz we obtain max min Special Cases In this section we present two private cases that achieve better regret than the worst-case regret of presented in Section First if has very loose fairness requirements ie for all and In such a case pulling each arm for rounds and invoking a black-box algorithm for the remaining rounds achieves a regret of max Formally let min if for all and for all Proposition Fix any MAB instance with horizon and fairness function min pulling all arms and invoking a black-box bandit algorithm for the remaining rounds achieves a regret bounded by max

0.9727 first phase of Algorithm ends after each arm is pulled at most min log times Proof In order for the first phase to end we need to approximate all the reward gaps sufficiently well If then given the clean event all arms will be pulled log times in order to obtain a confidence interval smaller than which ensures that either for all Otherwise if ie for all Assuming the clean event after pulling each arm log times and We examine two cases If then this implies that Therefore arm will not satisfy the condition in Line If then hence Therefore arm will not satisfy the condition in Line Combining the above together the first phase ends after each arm was pulled at most min log times Lemma Fix any instance The second phase of Algorithm ends after each arm is pulled at most min times Proof Assuming the clean case after pulling each arm times is a ball of radius log centered around such that for every log Thus Therefore max min If as shown in Lemma the condition in Line is satisfied trivially Putting the above observations together after each arm is pulled at most min no arm satisfies

0.9725 has no approximation error and thus can combine phases in Algorithm to one phase Note that if an arm is pulled times we can stop pulling it In the first phase Lines arms are pulled either at most times or until the algorithm is certain that In the second phase Line the black box algorithm is invoked and thus the regret is also bounded by the regret of the black box algorithm Proposition For any MAB instance with fairness function const Algorithm achieves a regret of max Proof We start by analyzing the regret of the first phase For arm with given the clean event arm is pulled exactly times in the first phase as in the optimal algorithm For arm with if log arm is pulled times and the regret will be bounded by log Otherwise log Arm is pulled as long as log Thus the regret of the first phase is bounded by log The regret that stems from the second phase is bounded by the regret of 𝐴𝐿𝐺 Combining the two together we get that R Algorithm 𝐴𝐿𝐺 max Instance Dependent Bounds In this section we analyze the instance dependent regret of Algorithm rather than the general

0.9722 means of the groups induced in the cluster Lemma Let U A B and U U Uk be a partition of U Let C c be a fair set of centers with respect Then is on the line segment connecting and Proof For the sake of contradiction assume that there exists an i such that is not on the line segment connecting and Note that see or Lemma in the Appendix for a proof of the following equation p p A A i p p B B i Let c i be the projection of to the line segment connecting and Then by Pythagorean theorem for convex sets we have c i c i c i c i Therefore since c i we have c i A i and c i B i Thus replacing with c i decreases the fair k-means objective The above lemma implies that in order to find a fair set of centers we only need to search the intervals B i Therefore we can find a fair set of centers by solving a convex program The following definition will be convenient Definition Given U AB and a partition u U Uk for i let A

0.9663 Input Integers m and numbers i and centers for i and m and Mj for m m for m for t T do m i i for i C c Compute fj C for all m C max m fj C C fj C for m t max m Normalize s such that m end return C Certificate of Optimality Next we give a min/max theorem that can be used to find a lower bound for the optimum value Using this theorem we can certify that in practice the multiplicative weights update algorithm finds a solution very close to the optimum Theorem Let S m and C c m i i i m and S Then max C min S fj C min C max S fj C Moreover max C min S fj C min C max m fj C Proof Let CC and let be the corresponding parameters for CC respectively Note that Z Therefore m C Hence because for any S we have S C Hence S C C C Therefore there exists a S such that fj C C C Thus because fj is convex we have fj C fj C fj C C C fj

0.9652 can bound E s by b a Finally in Case it holds that s see Equation Note that each of these results holds for all values of Combining these and setting B we can show that E s n a b a Using same E s E E s upper bound also holds for E s Consider t t Using Equation we have E t t x E t t x t n a b a x t n a b a Similarly for t t we can show that E i s ti n a b a s Pick an n such that ab a t Then we can show E at aE t i ti a t n aa b a Further we have t Eat t Eat a t is concave Eat a t Eat a t b is increasing Now substituting Equation in the above rearranging and choosing C B a a we can complete the proof Remark The only properties of the beta distribution used in the proof are that is concave and increasing and an upper bound on the inverse moment of the beta distribution Abstracting these properties gives an analogous proof for Equation

0.9648 arm anymore If the regret is be bounded by log If the regret is log log For arm if the optimal algorithm does not pull arm at all If Algorithm will not pull arm anymore The regret is bounded by log If Given the clean event it implies that log Algorithm will play arm max times If the regret is bounded by log If the regret is bounded by log Otherwise if and the regret is bounded by log The analysis of the cases above implies that after the loop that starts on Line ends for every sub-optimal arm log Note that log Hence the regret in the end of the exploration phase is bounded by log Algorithm Fairness-Aware-ETC Input number of exploration rounds for do pull arm for rounds for do if then pull arm for max rounds pull an arbitrary arm from argmax until the execution ends The remaining rounds less than are allocated an arbitrary arm with the highest observed expected reward Given the clean event the reward gap between the optimal arm and the sub-optimal arm the algorithm commits to is less than Hence the regret before allocating the remaining rounds is bounded by log Putting

0.9645 the other arms have an expected reward of is an instance where the th arm is the optimal arm and all the other arms expect arm have an expected reward of and arm has an expected reward of Fix an algorithm 𝐴𝐿𝐺 If 𝐴𝐿𝐺 pulls each arm at least times the utility of 𝐴𝐿𝐺 U 𝐴𝐿𝐺 I On the other hand the optimal algorithm will pull only arm and hence the utility of 𝑂𝑃𝑇 is The regret is then RI U 𝑂𝑃𝑇 I U 𝐴𝐿𝐺 I Otherwise there exists arm that 𝐴𝐿𝐺 draws less than times With probability at least in the instance that generated the sequence of rewards That is For example Denote by the number of times arm is pulled by 𝐴𝐿𝐺 u 𝐴𝐿𝐺 The optimal algorithm will pull arm times and arm the remaining rounds The utility of 𝑂𝑃𝑇 is u 𝑂𝑃𝑇 Finally observe that R 𝐴𝐿𝐺 u 𝑂𝑃𝑇 u 𝐴𝐿𝐺 Claim is 𝐿-Lipschitz and sums to at most one Proof is 𝐿-Lipschitz is continuous and piece-wise linear hence it is sup Lipschitz sup and therefore is Lipschitz sums to at most notice that max max if In the example above for large enough we obtain EXPERIMENTS

0.9637 values of Within each such cell the function is linear See Figure for an example visualization of these cells Our algorithm can be separated into two steps First we find the separating hyperplanes of In particular we recover unsigned weighted normal vectors wiAi or wiAi for i The second step then recovers the sign information for these normal vectors More precisely the two steps are the following Recover such wiAi wiAi for some permutation p of Algorithm a Recover a vector s such that x s Algorithm b Together the matrix Z and vector s identify the function We analyze the first step in Section and the second step in Section Algorithm Recovery of Function l Z l s return Z s Algorithm a Recovery of Z Function l Pick N Id and let Z tl tr l l for i do tl tr return Z Function tr while tl tr do tl tr u u u if tr tl then return tr if then tr else if then tl else throw Failure throw Failure Algorithm b Recovery of s Function Pick X such that x and See Appendix B M Solve for s Rh such that Ms x x

0.9593 i xi B i li for all i return C c Proposition Let x be the optimal solution for a fixed partition U U Uk Then there does not exist any other optimal solution with an average cost better than fAx or fB x for groups A and B respectively Proof Let y be another optimal solution Without loss of generality suppose fAx fB x If fAx fB x then by our discussion on the line search algorithm x and it is the only optimal solution Therefore y x Now suppose fAx fB x For the sake of contradiction and without loss of generality assume fAx fAy but fB x fB y Therefore fAy fB y First note that y because if fA fB then for any other x in the feasible region fAx fB x which is a contradiction because fAx fB x Hence we can decrease one of the coordinates of y by a small amount to get a point y in the feasible region If the change is small enough we have fAy fAy fB y fB y but this is a contradiction because it implies x y y which means x was not an optimal solution

0.9591 Convergence Lloyds algorithm for the standard k-means problem converges to a solution in finite time essentially because the number of possible partitions is finite This also holds for the Fair-Lloyd algorithm for the fair k-means problem Note that for any fixed partition of the points our algorithm finds the optimal fair centers Also note that there are only a finite number of partitions of the points Therefore if our algorithm continues until a step where the clustering does not change afterward then we say that the algorithm has converged and indeed the solution is a local optimum However note that in the case where there is more than one way to assign points to the centers ie there exists a point that have more than one closest center then we should exhaust all the cases otherwise the output is not necessarily a local optimal This is not a surprise because the same condition also holds for the Lloyds algorithm for the k-means problem For example see Figure Adjacent points have unit distance from each other The centers are optimum for the illustrated clustering However they do not form a local optimum because moving c and c to the left by

0.9573 t continuous functions differentiable on a c with sa ta or sc tc and x x a c then if s t is increasing decreasing on a c then s t is also increasing decreasing The functions in satisfy this rule s t are both continuous and differentiable and for a sa ta Using this rule s bh bh Rh bh R bh R R bh This term is decreasing in R because the denominator R is increasing faster than the numerator R is increasing This tells us that s t is decreasing in R so is decreasing as well The upper bound of R is an upper bound for all of Next we consider a bound for any i L For i hi where i x i i bh x fh Fh P Using bounds i R hi and R from Lemmas and we substitute and rewrite R hi bi bh bh Rh bh R hi bi bi Rh bh R hi bi bi bh Rh bh At R this is again undefined Using the same process of rewriting the fraction as s t we get s bi i R hi bi t bh bi R bh Using LHôpitals

0.9572 regret given by Theorem We first add several notations The instance-dependent bounds revolve around two main parameters which is the gap between the reward gap of arm and the transfer cost and min which is the minimal gap between the reward gap and the transfer cost The smaller is the more rounds are required in order to identify whether the reward gap is smaller or larger than the transfer cost and what is the optimal sub-algorithm Given an MAB instance let be dimensional ball centered around with radius log After pulling each arm times with high probability the observed vector of expected rewards vector lies in Denote by the dimensional ball centered around with the largest radius such that for all max min Theorem Fix any arbitrary instance of MAB and let Then the regret of Algorithm is bounded by min log Proof In order to analyze the instance dependent bounds of Algorithm we analyze the instance dependent bound that stems from each of its phases The term clean event used in this proof and in the following proofs is similar to the clean event defined in Subsection Lemma argues that the first phase ends after each arm was pulled

0.9537 describes a vector of expected rewards The vector is composed of arm with expected reward subset of arms with expected reward and the remaining arms have expected reward of The size of is The fairness function is piecewise linear with pieces The first piece for values less or equal to is constant and equal to The second piece for values between to is linear that goes from to The third piece for values greater than is constant and equal to In Claim we prove that is 𝐿-Lipschitz and sums to at most one Following the optimal algorithm described in Section for every instance in the optimal algorithm pulls all the sub-optimal arms according to the fairness function This means the optimal algorithm does not pull at all the arms in and pulls times sub-optimal arms not in The algorithm pulls the optimal arm the remaining rounds In order to distinguish between two instances in the decision-maker must pull each arm at least times otherwise with positive probability the decision-maker is unable to distinguish between at least two instances We use the following notation for simplicity I I is an instance where the th arm is the optimal arm and all

0.951 at most a small amount Let x be an optimal selection for Program Target Using Lemma we can show that x is feasible for Program Denoised with high probability It follows any solution x which is optimal for Program Denoised has value at least as large as x ie Using n if not we can set to min n and to min n m i m i xi wi These suffice to show that solving Program Denoised gives a good solution for Program Target which satisfies the claims in the Theorem It remains to solve Program Denoised Unfortunately even checking if Program Denoised is feasible is NP-hard see Theorem a constant-factor approximation in utility to Program Denoised is also NP-hard We overcome this hardness by allowing solutions to violate the constraints of Program Denoised by a small additive amount p Towards this consider the linear-programming relaxation of Program Denoised for s We show that any basic feasible solution BFS of LP-Denoised has a small number of fractional entries Lemma max m m i LP-Denoised for s st p m i m i xi n Lemma An optimal solution with p fractional entries Any basic feasible solution x m of LP-Denoised

0.9494 of centers C c with respect is in the convex hull of m i Then we can generalize the convex program in Equation tom demographic groups as the following min st M U i i i m i m i i where i and M In the same standard trick as for the case of two groups is used to make the objective function linear and M i i i is the average cost of group The set of s found by solving the above convex program will be a fair set of centers with respect We can solve this using standard convex optimization algorithms including gradient descent However similar to the case of two groups we can find a fair set of centers by searching a standard m simplex Namely we only need to search the following set to find a fair set of centers Z C c m i i i m The following notations will be convenient For C c and m let fj C M U i i i and C max m fj C Then the convex program represented in is equivalent to min C i m i i Similar to the case of two

0.9487 C Therefore max S fj C min S fj C Note that this holds for any CC Z and this implies the first part of the theorem Let C be the optimum solution to min max S fj C Note that by Theorem this is an optimum solution to the problem of finding a fair set of centers for the groups in S Moreover for any set of centers c outside the convex hull of the centers of groups in S we have max S fj C max S fj C proof of this is similar to Lemma Hence max S fj C min max S fj C Thus we have max C min S fj C min C max S fj C max S fj C min C max S fj C min C max m fj C Note that we can use Theorem to get a lower bound on the optimum solution of the convex program in For example suppose C is a solution returned by a heuristic Then min m fj C max C min m fj C min C max m fj C Therefore min m fj C is a lower bound for the optimum solution

0.9436 it follows that Z t only depends on random variables from the t-th iteration and that for all t t Z t and Z t are independent random variables This allows us to use the Chernoff bound on the sum t s Z s Pr t s Z s Pr t s Z s E t s Z s Using Lemma and Z s B Es for each s t we get at a t s Z s ab a b Substituting this in Equation we get Pr at a ab a b Let C be the constant C B ab ab and a B a C Recall that a and b are fixed constants strictly larger than Now we can prove the result as follows E t Eat a a d a a d a a d a is increasing a a b a C a b C C a b C Choosing CB C ab and simplifying completes the proof Remark The only properties of the beta distribution used in the proof are that is an increasing function and that the median of t is upper bounded by a suitable value Abstracting these properties will give an analogous

0.9414 tells us that there exists only one max-utilizing allocation Next we examine the fairness constraint and rewrite it as C C i Pi C i C i Pi C i C C C Pj C C Pj C C i For notational convenience we denote mi C C We will prove this theorem by considering subterms of each side independently and proving that they are equal L Fj mi i Pi C C mi Pj C mi C i P mi Pj C mi C mi Pi C C Pi C mi C Pi C C where the rst step comes from the fact that Pj C mi Fj mi Pi C The max-utilizing allocation has the property that i Fj Having the additional property that Fj mi implies that i mi This allows us to rewrite the above lemma in terms of the max-utilizing allocations i Fj i Pi C i C Pj C C Next we consider the second subterm in the equality L Fj mi i C C i Pi C i C C C Pj C C P What is C written out in terms of the CDF of a distribution We will consider C C

0.9406 general setting or be used to curtail polarization ALGORITHMIC RESULTS For each arm a let its mean reward be a In this case the unknown parameters are the expectations of each arm a for a We assume that the reward for the t-th time step is sampled from a Bernoulli distribution with probability of success For a probability distribution q C and a small enough constant we define B q to be the set of all probability distributions that lie inside C such that a probability distribution B q has at least probability mass on each arm More formally B q C is an -ball of radius centered at q Let V C denote the set of vertices of C and argmax V C P T Let be a small enough constant Given the description of C any probability distribution q B q C that lies in the constrained region and the sequence of rewards the CG algorithm Algorithm run forT iterations has the following constrained regret bound E O where t min and d min The algorithm works for any lower bound L on with a L instead of in the regret bound Here is the difference between the

0.9395 re-scale all by Use bipartite matching to round fractional assignment to an integral one denoted return The first step is filtering where we convert a feasible fractional solution to the LP to another fractional solution with the additional constraint that if ie client has a non-zero fractional assignment to facility then This fractional solution satisfies all the constraints of the LP except the capacity constraint and also satisfies up to a slack factor of on the The parameters used here will be chosen later In the process the opening cost term in the objective increases by a factor at most The next step is to consider all the facilities L and set and add all such facilities to the opened set Again this step only increases the opening cost term in the objective by a factor of At every point of time the algorithm maintains a set which is the set of clients such that In other words has a significant fractional assignment to unopened facilities As long as is non-empty the algorithm chooses with the smallest value of It then opens facilities in the vicinity Define for convenience Suppose is the set of fractionally open facilities in Because we

0.9392 in Appendix A establish the necessary anti-concentration and concentration bounds for showing that the change points are spaced sufficiently far apart Lemma but still within some line segment of u and v that is not too big Lemma Lemma Let ab Sd be unit vectors such that ab c for some scalar c Suppose we pick random vectors N Id Let t t R be scalars such that au tv and tv Then Pt t c Lemma Let a Sd be a unit vector Suppose we pick random N Let t R be the value such that Then Pt l Finally the proof of our main theorem for Algorithm a follows by combining the probabilistic guarantees of Lemmas and with the deterministic proof of correctness in Lemma Theorem With probability Algorithm a succeeds in Oh log queries If the Algorithm succeeds it returns a matrix Z such that wiAi or wiAi for some permutation p of If the Algorithm fails then it notifies of the failure Proof By Lemma if ti tj and ti l for all i and i then Algorithm a succeeds to return a matrix Z such that wiAi or wiAi for some permutation p of The

0.9389 of Thus the regret that stems from this phase is bounded by The second phase approximates only for entries with low cost of opportunity ie Lemma shows that after each arm is pulled at most times is approximated up to a factor of in the relevant entries An immediate consequence of Lemma is that after at most rounds max min Thus the regret from this phase is bounded by The regret from the first two phases is bounded by max In the third phase the algorithm pulls each arm with low opportunity cost until the condition in Line is met therefore For arm with the regret stems form the approximation error of which is bounded by For arm if arm is pulled during the third phase This happens when the reward gap is very close to the transfer cost and rounds are not sufficient to approximate correctly max the regret is also bounded by this term Otherwise assume max Note that the regret is bounded The second part of this expression is bounded by the approximation error of ie We bound the first part of the regret by bounding To do so we now look into the two possible cases

0.9383 return vT and Notice that Algorithm chooses an allocation at every round which is fair with respect to its estimates of the parameters of the candidate distributions hence asymptotic convergence of its output to an optimal α-fair allocation follows directly from the convergence of the estimates to true parameters However we seek a stronger finite sample guarantee as stated in Theorem Theorem Let Suppose that the candidate distributions are Poisson distributions with unknown parameters in the vector where lies in the known interval G Suppose we run Algorithm for t O rounds where is some distribution specific function to get an allocation v and estimated parameters for all groups i Then with probability at least For all i in G Let D maxi G C C where denotes the total variation distance between two distributions Then v is D-fair has utility at most smaller than the utility of an optimal D-fair allocation ie v Remark Theorem implies that in the limit the allocation from Algorithm will converge to an optimal α-fair allocation As t p for all i meaning D and more importantly v will be α-fair and optimal To prove Theorem we first show that any sequence of allocations

0.9368 A B B and li B i Also let MA B Since B i is a line segment for the ith cluster we only need to find the distance xi of its center from Then the distance of the center from is li xi where li is the length of the line segment B i By the average cost of group A with respect to a partition u in terms of xi s i p A i A i A A i A A A i Similarly the average cost of group B is MB U B B i li xi Hence our goal is to solve the following optimization problem max A A i MB U B B i li xi subject to xi li i Note that this is a convex program because the maximum of two convex functions is a convex function One can see that the point on the line segment B i that has distance xi from is xi B i li where li is the length of B i Using a standard trick to write the objective function as a linear function we can state the problem as the following Corollary U Uk be

0.9365 by using standard clustering algorithms with minor modification If our data is composed of groups as we discussed then optimizing the cost function on would correspond to the minimization problem min cost For most known cost functions and algorithms it turns out that adding a weight for each point still allows for efficient algorithms Let us consider assigning a weight of to all the points in Then the weighted minimization problem is min cost The observation is now the following Observation be a clustering produced by an 𝛼-approximation algorithm for the problem Then achieves an approximation to the fair clustering problem definition with the same cost function Proof Let be the objective value for the fair clustering problem and be the corresponding clustering Then by definition we have max cost cost Because the solution found by the algorithm is an approximation we have that cost cost This implies that each term is implying the desired approximation bound This observation implies for example that for the fair versions of k-means and k-median we can use known constant factor approximation algorithms to get factor approximation algorithms directly APPROXIMATION ALGORITHMS FOR FAIR k-CLUSTERING For the fair k-median and k-means problem we now develop

0.9323 relaxations In our setting with multiple groups we show that the LP methods can be adapted to obtain approximation algorithms Notation As before the set of clients is composed of groups and the set of valid locations for facilities is L They lie in a metric space whose distance function is denoted by will always denote the capacity bound A solution will consist of a subset L and an assignment function The quantity Opt will denote min max where the minimization is over such that for all We prove that it is possible to achieve a constant factor approximation as long as there is a constant any fixed constant slack in the capacity constraint Theorem Let be defined as above and let Opt denote the optimum objective value Then for every there is an efficient polynomial time algorithm that finds a solution with the following properties the objective value is Opt for every we have Remark It is an interesting open problem to study the case of hard capacity constraints Likewise we are assuming that the capacity bound is independent of the facility Modifying the rounding algorithm to allow different capacities at different locations is also an interesting direction We

0.9313 has at most min mp fractional values ie m i xi min mp The proof follows by specializing well-known properties of BFSs to LP-Denoised We remark that this result is tight see Fact A Proof sketch of Theorem Using Lemma we can show that x is feasible for Program Denoised with probability at least p Assume that this event happens Then x is also feasible for LP-Denoised Consider the basic feasible solution x to LP-Denoised from Step of Algorithm Since x is optimal for LP-Denoised it follows that x has a value at least as large as x i m i xi wi Further since the rounded solution x from Step of Algorithm only increases the utility of x Thus m i x m i xi wi This establishes the first claim in Theorem It follows from Lemma that x picks at most p more elements than x Thus x violates Equation so Equation by at most p By the same argument x violates the fairness constraints of Program Denoised by at most p additive Combining this with Lemma we can show that with probability at least p x violates the fairness constraints of Program Target by at most additive

0.9299 proof of Equation Remark It might be possible to tighten the dependence of Equation on using techniques from In particular the factor may improve to roughly Specifically the inverse CDF of the draw of t Proof sketch of Theorem Fix an iteration t N In the proof we show an upper bound on Eat Then using that is concave and increasing we can prove the desired upper bound on t Eat t Let Dt denote the set of all random variables from the first t iterations Dt B s st X s st Y s st Then we prove the following lemmas Lemma Upper bound on t when t is close to For all iterations s N constant and values of if s then s Proof s is an increasing function of U s Y s X In the worst case we have U s Y s X Substituting this in Equation s s U s X s U s X U s Y s s Lemma Upper bound on E t when t is not close to For all iterations s N constant values of and parameters ab it holds s E s s U t X b a

0.9258 by the Lagrangian principle Moreover can be shown to be an instance-dependent threshold function with the threshold Pr Y X x i p x where x i Pr Gi A i Pr Gi A i X x is the scaling factor of that is determined by the form of Observe that the term Pr Y X x is exactly the threshold for the unconstrained optimal classifier and the remaining term i p x can be regarded as a threshold correction induced by Theorem Solution characterization and computation for q Q lin Given any parameters p there exist optimal Lagrangian parameters such that X is an optimal fair classifier for Group-Fair Moreover can be computed in polynomial time as a solution to the following convex program arg min arg min i p i i p max This theorem asserts that Group-Fair can be solved efficiently up to an arbitrary accuracy first compute the optimal Lagrangian parameters via and then output the fair classifier X The running time depends on how fast we can solve Program Since Program is convex and we can apply standard convex optimization algorithms eg the stochastic subgradient method to compute an ε-approximate such in time The proof

0.9251 in this illustrative example In the next section we will describe a property of probability distributions such that the max-utilizing allocation is also fair Proof of general property T Consider a set of continuous candidate distributions with and Suppose the set of candidate distributions has the following property Fj C C i Then under fractional allocation of resources the max-utilization allocation is already fair In other words i i Fj min C min C Intuitively this property means that the CDFs of all of the candidate distributions are versions of the same function scaled by the ratio of their expected values If this property holds it will turn out mathematically that the max-utilizing allocation is already fair First we can verify that the exponential distribution satisfies these properties The exponential distribution has the given expectation and CDF C C Fj e Substitution shows that the premise of the lemmas above holds Fj C C e ij e i Next we will explore why this property of probability distributions leads to As shown in Appendix C the max-utilization allocation has the property that i only if In this case i i otherwise We require so i i We additionally require which

0.9227 the arm pulled at time When arm is pulled at time the decision-maker receives a random reward We assume that for every the reward distribution is a Bernoulli distribution with expected value This is without loss of generality since we can reduce any instance with general -supported distribution to an instance with Bernoulli arms using the technique of Agrawal and Goyal We use to denote the vector of expected rewards ie We denote by the number of times arm is pulled by the end of round and let be the gap between the expected reward of the optimal arm and the expected reward of arm We now present the Reward-Opportunity MAB model An instance of MAB is represented by a tuple The tuple is an instance of standard stochastic bandit as described above The combination of and creates what we call the fairness policy The fairness requirements are expressed by a function receives as input a vector of expected rewards and outputs a vector of minimal fraction of times each arm has to be pulled in order not to be penalized We let denote the entry of We assume that and that is Lipschitz continuous with a Lipschitz constant with

0.9225 a factor of optimal where Theorem If the k-means problem admits a c-approximation in polynomial time then the fair k-means problem admits a c-approximation in polynomial time Proof Let A A B B This is basically the k-means objective when we consider a weight of A for the points in A and a weight of B for the points in B Let O be an optimal solution to and S be a c-approximation solution to ie Moreover max SUS A A SUS B B SUS A A SUS B B Hence Now let O be an optimal solution for Then O A A O B B max O A A O B B Also by optimality of O for we have Therefore This implies that US c GENERALIZATION TOm GROUPS Let U A Am Then the objective of fair k-means form demographic groups is to find a set of centers c that minimizes the following max A A Am Am Let U U Uk be a partition of U and i be the mean of ie the mean of members of subgroup in cluster i Then by a similar argument to Lemma one can conclude that for a fair set

0.9214 multiplicative factor of at most o and the constraint Equation by a multiplicative factor of at most high probability If p is large then x from Theorem can violate the constraints by a large amount However in this case it is NP-hard to even check if there is a solution to Program Denoised which violates the constraints by a constant additive factor let alone finds an optimal solution for Program Target see Theorem Algorithm crucially uses the Program Denoised it first solves the linear-programming relaxation of Program Denoised and then rounds this solution to integral coordinates In the next section we overview the proof of Theorem We defer the proof of Theorem to Supplementary Material A due to space constraints Remark We can strengthen Theorem to guarantee that Algorithm finds an x m which does not violate the lower bound fairness constraint left inequality in Equation and violates the upper bound fairness constraints by at most without changing other conditions In particular this shows that if one places only lower bound fairness constraints then subset found by Algorithm would never violate the fairness constraints Algorithm Algorithm for Program Target Input A number n N probability matrix q mp utility vector

0.9161 a partition AB Then C c is a fair set of centers with respect to U if i A i x i B i li where x is an optimal solution to the following convex program min st A A i MB U B B i li xi xi li i We can solve this convex program with standard convex optimization methods such as gradient descent However as we show in the next section we can solve it with a much faster algorithm Computing Fair Centers via Line Search We first need to review a couple of facts about subgradients For a convex continuous function we say that a vector u is a subgradient of at point x if y x uT y x for any y We denote the set of subgradients of at x by x Fact Let be a convex function Then point x is a minimum for if and only if x Fact Let fm be smooth functions and x max m fj x Let m fj x x Then the set of subgradients of at x is the convex hull of union of the subgradients of fj s at x for Let fAx A A

0.9155 respect to the norm That is for all it holds that Intuitively satisfying the Lipschitz condition implies that two similar expected reward vectors get similar fairness requirements For simplicity from here on we call the fairness function The difference between the fairness requirement and the number of times an arm was pulled represents the deviation from the fairness constraint If the deviation is positive it means the arm was not pulled enough times ie the subpopulation did not receive enough opportunities according to the fairness function In such a case the decision-maker pays a cost The paid cost for a single arm pulls deviation from the fairness requirement is given by the transfer cost for arm If arm was pulled less than times the reward will be deducted by The transfer cost is known to the decision-maker in advance To account for all cases the possible cost which stems from the deviation is max For simplicity we use for all but stress that our results hold with minor modifications in the general case as well The utility of the decision-maker is denoted by It is an additive utility of the reward minus the total deviation from the fairness requirement Notice

0.9134 to the optimal algorithm from Subsection The expected utility increases with the horizon since the proportion of approximation rounds Lines and decreases with the horizon For and uni use all the rounds for allocating opportunities The difference in the average utility stems from the allocation differences uni allocates evenly while allocates more opportunities as the arm expected reward grows Observe that uni lin for every and the described thus the utility of lin is greater than uni for Figure investigates the crux of the execution of Algorithm for The proportion of rounds devoted to phase Line phase Line phase Line and phase invoking 𝐴𝐿𝐺 Line For Algorithm always invokes the black box algorithm 𝐴𝐿𝐺 For and the proportion of approximation rounds phases and decreases as the horizon increases The proportion of phase rounds increases as the horizon grows The reason is that the proportion of approximation rounds decreases while the required number of opportunities grows linearly with the horizon For the algorithm dedicates all rounds for opportunities However for it realizes that for arms and with paying the transfer cost per each withheld opportunities and hence the remaining rounds are devoted to phase Figure compares the average utility of Algorithm

0.9124 t max Y t max and t B t median t Conditioning on them we can show a one-step lower bound on at Lemma Conditional lower bound on at For all s N if then Es s implies as as ab a b Proof outline First we show that U s Y s X conditioned on Es Then we show an upper bound on the median of the beta distribution such that given s B s median s it holds t t b a b Now conditioning on Es and s and using the above we get s t t U s Y s X t t b at b The lemma follows by using the update rule and simplifying Next we extend Lemma along with other facts to show a lower bound on at alluded to earlier Towards this note that Pr t We show that by analyzing an equivalent urn model Let Z t be the indicator random variable that Et t occurs Since Et and t are independent t B t t Note that the event t only depends on the draw of t and that E t only depends on X t and Y t Thus

0.9119 that stopped the loop in Line The term the clean event is similar to the clean event defined in Subsection a If given the clean event This implies that Thus the first part of the regret is bounded by b If given the clean event This implies that and therefore the first part of the regret is bounded by To summarize if arm with is pulled in the third phase the regret associated with this arm is bounded by Eventually Phase invoking 𝐴𝐿𝐺 contributes the regret of 𝐴𝐿𝐺 The regret from the approximation phases is bounded by log The regret from filling the fairness requirements is also bounded by this term Ultimately the total regret is bounded by log assuming that this term is an upper bound on the regret of 𝐴𝐿𝐺 with respect to a vanilla stochastic bandit setting Lemma Fix any arbitrary instance of MAB and let Then after at most rounds for every either or Proof With high probability after pulling each arm log times with high probability log Therefore and hence at least one of the following happens Algorithm Self-regulated Utility Maximization Input Black-box bandit algorithm 𝐴𝐿𝐺 allowed approximation error parameters and Initialize arms data for

0.9114 U A U A Uk A The goal of fair k-means is to minimize so as to minimize the higher average cost As illustrated in Figure right minimizing this objective results in a set of centers with equal average cost to individuals of different groups In fact as we will soon see the solution to this problem equalizes the average cost of both groups in most cases Next we present the fair k-means algorithm or Fair-Lloyd in Algorithm Algorithm Fair-Lloyd Input A set of points u A B and N Initialize the set of centers C c repeat Assign each point to its nearest center in C to form a partition U U Uk Pick a set of centers C that minimizes U C Line search u U until convergence return C c The second step of each iteration uses a minimization procedure to assign centers fairly to a given partition of the data While this can be done via a gradient descent algorithm we show in Section that it can be solved very efficiently using a simple line search procedure see Algorithm due to the structure of fair centers Section In Section we discuss some other properties of the

0.9103 We will argue separately the clean event and the bad event the complement of the clean event The regret is clean event P clean event bad event P bad event We now bound the probabilities for the clean event and for the bad event at the end of the exploration phase That is after each arm was pulled times Using Hoeffdings inequality P Using the union bound the probability for the clean event is Since the bad event complements the clean event the probability of the bad event is at most That the regret of the bad event is bounded by the highest possible regret Combining the two last statement together we obtain that bad event P bad event The clean event implies that for all log Thus with respect to norm since is 𝐿-Lipschitz log Next we examine several cases at the end of the exploration phase and their effect on the regret For arm if the optimal algorithm pulls arm exactly times If Algorithm will pull arm max times If the regret is bounded by log Otherwise the regret is bounded by If Given the clean event it implies that log In this situation Algorithm will not pull

0.9087 search are shown in Figure See Algorithm for a precise description Using this line search algorithm we can solve the convex program described in to error in O i log li time Fair k-means is well-behaved In this section we discuss the stability convergence and approximability of Fair-Lloyd for groups As we will show in Section these results can be extended tom groups Stability The line search algorithm finds the optimal solution to This means that for a fixed partition of the points eg the last clustering that the algorithm outputs the returned centers are optimal in terms of the maximum average cost of the groups However one important question is whether we can improve the cost for the group with the smaller average cost The following proposition shows that this is not possible assuring that the solution is pareto optimal Algorithm Line search u U Input A set of points u A B and a partition U U Uk Compute B i li M A MB See Definition for t T do xi li for i Compute fAx and fB x if fAx fB x then else if fAx fB x then else break end end li xi A

0.9056 to obtain rewritten terms for every i term with i The above lemma allows us to rewrite the entire term as i i x i i i bh i x fh Fh Next our approach for bounding the will be to bound each term within the sum in the numerator independently L where x fh Fh bh i x fh Fh P We can apply Lemmas and to obtain R and We additionally use the fact bi These facts allow us to bound the equation as bh bh Rh bh bh Rh bh Rh bh In the last step to over-bound we under-bounded a term in the denominator replacing bh i with At R this fraction is undefined We denote the numerator as s the denominator as t and use LHôpitals rule to investigate the limit as R We note that s bh and t bh Rh bh As R s bh and t bh so s t which tells us that as R Next we will show that is decreasing in R proving that is an upper bound on everywhere To do this we will use LHôpitals Monotone rule which states the following L Given a c and s

0.9016 the following auxiliary LP min subject to for all for all for all k-med-approx for all The constraint now involves a new term k-med-approx which is an approximation to the optimum k-median objective of the set For our purposes we do not care how this approximation is achieved it can be via an LP relaxation local search or any other method We assume that if is the optimum k-median objective for then for all k-med-approx for some constant From the works above we can even think of as being Lemma Suppose there is a rounding procedure that takes a solution to FairLP-RelError and outputs a set of centers with the property that for some parameter k-med-approx for all groups Then this algorithm provides an approximation to RelError-fair clustering Proof Let Opt be the optimum value of the ratio-fair objective on the instance The main observation is that the LP provides a lower bound This is true because any solution to ratio-fair clustering leads to a feasible integral solution to fairlp-relerror where the of the constraint is replaced by Since k-med-approx it is also feasible for fairlp-relerror showing that the optimum LP value is Opt Next consider a rounding algorithm that

0.9016 term constraint vectors R p Solve x Find a basic feasible solution to LP-relaxation of Program Denoised with inputs Set x i B xi for all i m Round solution Return x Proof overview and hardness results In this section we overview the proof of Theorem The complete proof and an extension of Theorem for multiple protected attributes ie s appear in Supplementary Materials A and B The proof of Theorem has two broad steps First we show that solving Program Denoised even approximately gives us a good solution to Program Target and then we develop an approximation algorithm along with matching hardness results for Program Denoised To prove the former we bound the difference between the true and the expected number of candidates from any one group Lemma For all and x m st m i xi n p i xi i m holds with probability at least p over the noise in the protected attributes of each item The proof of this lemma appears in Supplementary Material A Using Lemma we can show that any solution that violates the constraints of Program Denoised by a small amount with high probability also violates the constraints of Program Target by

0.9006 pay for opening This is the motivation for the well-known facility location problem Definition Let be a set of clients and L be a set of locations in a metric space with distance function For each location we have an opening cost which is the cost of opening a facility at The objective is now a sum of the connection costs and the opening costs Formally the goal is to select a subset of L so as to minimize Now consider the setting in which clients fall into different demographic groups We propose our objective based on an equal division of the facility opening costs among all the clients Thus every client pays an opening cost of Our definition of fair facility location aims to ensure that the average total cost opening plus connection is small for all the groups Definition be a set of clients composed of groups and let L be a set of locations The goal of fair facility location is to select a subset of the locations so as to minimize max We remark that the second term in the objective is independent of the group as this is the cost paid by every client Our

0.8996 probability of Algorithm a succeeding can be lower-bounded as Pi i ti tj ti l i tj i l Union bound c i l Lemma c Lemma Let c l so that Algorithm a succeeds with probability at least Since the algorithm performs a binary search over an interval of size l with step size at most times it uses at most log l queries Next we will simplify the expression for the number of queries First set l Then solving for using the expressions for l and yields c We can then upper-bound the number of queries used as log l log log O log Therefore Algorithm a succeeds log queries with probability Step two recovering the signs of the normal vectors Algorithm a recovers unsigned weighted normal vectors wiAi or wiAi for i But to identify the function we still need the sign of these vectors In Algorithm b we recover a vector s that encodes this sign information Precisely Algorithm b returns a vector s such that x s where si i wi Ai i wi Ai i wi Ai i wi Ai It is clear that if Algorithm b returns the vector s then the function

0.898 at least one center from them should be opened Therefore while there are at least two unmatched points in we match the corresponding bundles of the two closest unmatched points in Sampling Given the matching generated in the last phase we iterate over its members and consider the bundle volumes as probabilities to open centers in expectation The centers picked in the sampling phase are returned as the final centers LS-Fair k-median In this section we propose a heuristic local search algorithm in addition to LP-Fair k-median Arya et al proposed a local search algorithm to approximately solve the k-median problem Their algorithm starts with an arbitrary solution and repeatedly improves it by swapping a subset of the centers in the current solution with another set of centers not in it We modify this algorithm to minimize the maximum average cost over all groups Assuming were given a cost function as groups where LS-Fair k-median is presented in Algorithm Algorithm LS-Fair k-median cost an arbitrary set of centers from old cost new cost max cost cost while there is and st max cost cost old cost do old cost max cost cost return Unlike the LP-Fair K-Medians we do not

---------------------------------------------------------------------------
TOPIC 9: law/rights

---------------------------------------------------------------------------

0.8927 added What makes processing not only transparent but also fairly transparent is that the data subject has an actual knowledge of the data processing concerning him or her and of its main characteristics see recital rephrasing recital of the Directive The link between transparency and fairness is also rapidly confirmed at Article about code of conducts Code of conducts are self-regulatory and voluntary tools required to specify the application of the GDPR such as with regards to fair and transparent processing see also recital of the GDPR Lastly recital also mentions fairness in connection with transparency but under a new perspective in order to ensure fair and transparent processing in respect of the data subject taking into account the specific circumstances and context in which the personal data are processed the controller should implement technical and organizational measures appropriate to prevent errors and inaccuracies taking into account potential risks involved for the interests and rights of the data subject such as discrimination eg based on sensitive data Here for the first time fairness is not merely linked to substantial transparency but to the organizational measures for the prevention of any adverse effect to interests and rights of the data subject

0.8668 this background in mind the next two sections of this paper go into more detail on both the individual rights and systemic governance elements of the GDPRs approach to algorithmic accountability before turning to the role the Data Protection Impact Assessment DPIA plays in linking the two Individual Rights in the GDPR and the multi-layered Explanation The GDPR gives data subjects several important rights with respect to algorithmic decision-making The GDPR contains both general data protection rights and rights specific to profiling that also apply to algorithmic decision-making On top of this it establishes rights specific to algorithmic decision-making which include a right to be notified of solely automated decision-making Arts a right of both notification and access to meaningful information about the logic involved Arts a right to be informed of the significance of and envisaged effects of solely automated decision-making Arts and a right not to be subject to solely automated decision making Art with safeguards for the limited cases in which automated decision-making is permitted Those safeguards include but are not limited to a right to contest a decision to express ones point of view and to human intervention Art We do not intend to revisit the

0.8325 of Algorithmic DPIA and individual rights can create a virtuous circle of multi-layered explanations Algorithmic Accountability in the GDPR The GDPR has significant implications for algorithmic decision-making At first the legal debate focused on whether the GDPR created an individual right to an explanation of an individual algorithmic decision Subsequent legal analysis however began to focus instead on other accountability tools either required in the text of the GDPR or recommended in interpretative guidelines These tools include third-party auditing the appointment of Data Protection Officers DPOs Art and the requirement of Data Protection Impact Assessments DPIAs Art As one of us has argued the GDPR combines a series of individual rights Arts with a systemic governance regime overseen by regulators Arts throughout These two systems interact and overlap An individual right is often also a companys duty But even if individuals data subjects fail to invoke their rights companies data controllers have significant obligations both procedural and substantive under the GDPR For example a data subject has a right to contest an individual algorithmic decision Art to receive notice of solely automated decision-making Art and to request access to meaningful information about the logic involved Art Should she fail to

0.8196 fairness as the first principle of personal data processing together with lawfulness and transparency The strong link with lawfulness and transparency is evident the three different principles are mentioned together as three interdependent notions This double link of fairness with lawfulness and transparency is confirmed also at recital which first declares that any processing of personal data should be lawful and fair emphasis added and then affirms that the transparency principle concerns in particular information to the data subjects on the identity of the controller and the purposes of the processing and further information to ensure fair and transparent processing emphasis added The other references to fairness in the GDPR are either related to the link between fairness and lawfulness or to the link between fairness and transparency Fairness as transparency in the GDPR As regards transparency Articles and similar to Articles and of the Data Protection Directive foresee that data controllers in addition to some basic information about data processing identity of the controller contact details purposes etc need to disclose to the data subject some additional information eg storage periods data subjects rights etc necessary to ensure fair and transparent processing in respect of the data subject emphasis

0.7915 in automated decision-making and how the algorithmic profiling is built Analogously the controllers duty to assess necessity and proportionality of the processing operations in the DPIA is similar to the algorithmic transparency duty to explain the pertinency of personal data used and the relevance of the profiling The controllers duty to assess the data processing risks and the impacts on individuals is similar to the transparency duty to explain the impact of the profiling use in automated decision-making Lastly the controllers duty to find safeguards of individual rights in case of automated decision-making under Article and GDPR is similar to the duty to find and describe measures envisaged to address the risks in DPIA In other words in the case of automated decision-making the DPIA steps might correspond to transparency duties as interpreted by Article Working Party in Annex I Towards Multi-layered Explanations from Algorithmic DPIA The DPIA process combined with the GDPRs extensive individual transparency rights and Guidelines on suitable safeguards of individual rights suggests what we call multi-layered explanations for automated decision-making Edwards Veale have similarly suggested that data subjects should be given what they call model-centric and subject-centric explanations Model-centric explanations they suggest should include the family

0.7874 subjects The GDPR approach of fairness is thus effect-based what is relevant is not the formal respect of procedures in terms of transparency lawfulness or accountability but the substantial mitigation of unfair imbalances that create situations of vulnerability

0.7715 invoke any of these rights however the GDPR still puts in place significant obligations on data controllers using automated decision-making whether solely automated or not The GDPR requires an array of systemic accountability tools including third-party auditing the appointment of Data Protection Officers DPOs Art and Data Protection Impact Assessments DPIAs Art These obligations arise from the text of the law but also in accompanying Recitals and in the Guidelines on Automated Individual Decision-making and Profiling Guidelines on ADM released in October and revised in February by the Article Working Party It is also crucial to understand the mode through which the GDPR governs The GDPR largely governs both in the sense of coming up with the substance of data controllers duties and in the sense of monitoring compliance with them through an approach known in legal literature as collaborative governance the use of public-private partnerships Because the GDPR often effectively outsources governance decisions to single data controllers accountability takes on added significance Accountability in the GDPR is not just about protecting individual rights It is about ensuring that this process of co-governing with private parties receives appropriate oversight from the public and both expert and interested third parties With

0.7581 on DPIAs DPIA Guidelines mandate DPIAs for any automated decision-making creating a categorical requirement that applies in the case of decision-making including profiling with legal or similarly significant effects that is not wholly automated as well as solely automated decisionmaking defined in Article Moreover as Casey Farhangi Vogl have noted demonstrating that a DPIA is not necessary will in many instances itself require a DPIA We note too that at least one Member State see the Slovenian Data Protection Act that implements the GDPR requires algorithmic impact assessments as a specific safeguard in case of automated decision-making under Article of the GDPR In this section we address what the purpose of the DPIA is in the GDPR and what it must include Understanding the DPIAs purpose clarifies what the content should be and points to several shortcomings in the current conception of the DPIA What is Required in a DPIA The GDPR describes a DPIA as an assessment of the impact of the envisaged processing operations on the protection of personal data Art That assessment per the text of the GDPR must include a description of the processing operations in this case the algorithm and the purpose of the processing

0.7369 procedural fairness Fair balancing is based on proportionality between interests and necessity of purposes while procedural fairness is based on transparency duties timeliness and burden of care of data controllers The notion of fair balancing comes from several provisions in the GDPR As aforementioned Article refers to measures to ensure lawful and fair processing of personal data in specific processing situations such as the cases described at Chapter IX GDPR which regulates complex balancing between conflicting interests eg freedom of expression and information Article public access to official documents Article archiving and research purposes Article freedom of religious entities to process personal data Article The idea of fairness as fair balancing between conflicting interests is also confirmed in the jurisprudence of the European Court of Justice as we will see in the following Section At the same time the GDPR usually relates the notion of fairness to specific measures Article and recital procedures recital or information duties Arts and recitals and that the data controller needs to adopt That is why Clifford and Ausloos refer to procedural fairness fairness should be practically implemented through specific procedures that can improve the level of transparency and lawfulness of a certain data processing

0.7365 of data subjects in particular when it connects the idea of fairness to predictability and legal certainty The Explanatory Memorandum also mentions fairness in the balancing with the interests of law enforcement authorities processing data under a necessity and proportionality principle In sum in these first sections we observed how the notion of fairness in the EU Data Protection framework is related to the notion of significant imbalance between companies data controllers and individuals data subjects Fairness in the GDPR is linked both to transparency and to lawfulness the principles of fair transparency and fair lawfulness impose to the data controller adopting specific procedures in order to reach a fair balancing between different interests Fairness as non-discrimination The notion of fairness is also often interpreted as nondiscrimination in particular in the FAT discourse Indeed Oxford English Dictionary defines fair as impartial and just treatment or behaviour without favouritism or discrimination emphasis added The relation between discrimination and unfairness can be found also in the very first international documents on data protection In particular both Resolutions of Council of Europe on the protection of privacy in electronic data banks refer to unfair discrimination While in the Data Protection Directive there is

0.7296 Algorithmic Accountability requirements under the GDPR Thus far few commentators have linked the Guidelines on Automated Decision-Making to the DPIA process Here we connect the GDPRs text to these Guidelines to show how the required content of DPIAs in fact easily serves as the basis for disclosures controllers must make to data subjects Article GDPR requires that a DPIA should contain a systematic description of the envisaged processing operations and the purposes of the processing an assessment of the necessity and proportionality of the processing operations in relation to the purposes an assessment of the risks to the rights and freedoms of data subjects and the measures envisaged to address the risks The Guidelines on DPIAs clarify that a systematic description of processing should include the nature scope context and purposes of the processing categories of personal data recipients and storage a functional description of the processing operation and the assets on which personal data rely If we compare algorithm accountability requirements with DPIA content there are multiple interesting similarities In particular the data controllers duty to systematically describe the processing operations in a DPIA is similar to the algorithmic transparency duty to clarify the categories of personal data used

0.7206 the GDPR To understand the meaning and usefulness of fairness we need to understand the origins of the fairness principle and its future developments The notion of fairness has always been present in the data protection framework since the very first international documents The very first Council of Europe Resolutions and referred to fair collection of data and unfair discrimination The OECD guidelines on data processing also mentioned lawful and fair data processing of personal data Analogously the Council of Europe Convention at Article stated that personal data undergoing automatic processing shall be a obtained and processed fairly and lawfully Subsequently the Data Protection Directive already adopted the principle of fairness in data protection at Article a personal data must be processed fairly and lawfully and recital any processing of personal data must be lawful and fair to the individuals concerned In all the aforementioned provisions fairness is mentioned in conjunction with lawfulness as two inherently connected principles However in several other provisions fairness is linked to transparency duties Articles and of the Data Protection Directive clarified that in addition to the information about the identity of data controllers and the purposes for data processing the data subject should receive

0.7113 DPIA fails to trigger both of these mechanisms This failure could be drastic The GDPR puts a lot of faith in the behaviour of data controllers As discussed it often tasks data controllers with coming up with the substance of a how individual rights will be implemented and b how to address unfairness biases and discrimination concerns about algorithms The counterbalance is regulatory oversight of DPAs Data Protection Agencies But the GDPRs enforcers have not historically been well-resourced compared to the data controllers they regulate Tasking regulators with extensive monitoring also forgoes some of the touted benefits of governing through public-private partnerships including lowered costs By failing to require public disclosure of Impact Assessments the GDPR fails to activate necessary third parties in its governance regime such as civil society actors or civic-minded experts who might not be recruited for auditing purposes Similarly the DPIA fails to involve serious stakeholder input unless data controllers understand the Guidelines emphasis on expert boards and third-party audits to include impacted stakeholders and to be mandatory As an alternative individual notification and access rights could do some work If data controllers indeed link their DPIA content to what they disclose to data subjects for

0.7036 additional specific provisions on automated individual decision-making are introduced Data subjects have the right not to be subject to a decision exclusively based on automated processing if such decision affects him/her legally or significantly in any other way unless any of the exceptions foreseen in Paragraph applies necessary for a contract authorised by Union or Member State Law or based on data subject explicit consent Article emphasises the requirement to implement appropriate measures to safeguard the rights and freedoms of data subjects In those cases where automated decision-making takes place it does so by granting data subjects the right to obtain human intervention to express their point of view and to be able to contest the decision taken The guidelines released on this topic by the European Data Protection Board state that human intervention implies that the human-in-the-loop should refer to someone with the appropriate authority and capability to change the decision It is clear how a requirement of technical explainability is relevant for these envisaged safeguards Human supervision can only be effective if the person reviewing the process is in a position to assess the algorithmic processing carried out This implies that such processing should be understandable Further explainability

0.6993 significance and the envisaged consequences of automated decision-making Arts A DPIA must contain as mentioned above a systematic description of the envisaged processing operations and the purposes of the processing Art If data controllers are already internally describing automated decision-making at a systematic level as part of the DPIA process those internal descriptions could be disclosed to data subjects or at least serve as the basis for these disclosures In addition they might be released to the public in the form of summaries Similarly a DPIA must include an assessment of the risks to the rights and freedoms of data subjects and data subjects have a right in the context of automated decision-making to be informed of the significance and envisaged consequences of such decision-making Arts Again as a company conducting automated decision-making must conduct a DPIA it should consider how the information it produces in that process might also feed into or even satisfy the individual rights requirements under the GDPR Second despite other commentators dismissal of DPIAs as failing to put in place individual due process the Guidelines explicitly envision the DPIA as an essential part of establishing suitable measures to safeguard individual rights including a version of

0.6985 by subjecting its logics and performance to inspection and assessment as to whether they are socially acceptable or even illegal Rejecting individual rights as we discuss below also ignores the symbiosis between the GDPRs two regimes Individual rights can play a crucial role in the GDPRs system of collaborative governance The GDPRs dual approach to algorithmic accountability has the potential to answer important questions in the literature about the value in practice of individual rights in algorithmic governance Collaborative Governance in the GDPR The other side of algorithmic governance in the GDPR is its systemic governance regime It aims largely to address instrumental goals preventing algorithmic error bias and discrimination This governance regime as one of us discusses at length elsewhere is largely constituted through collaborative governance or public-private cooperation We here illustrate two examples of how this works in the GDPR Article s suitable safeguards on automated decisionmaking are one example of this in practice The GDPRs text does not comprehensively dictate what data controllers must do to protect individual rights when they use solely automated decision-making Art Instead the GDPR lists examples of safeguards contestation expression human intervention and leaves to both data controllers and regulators to determine

0.6888 Title VII of the Civil Rights Act of forms the basis of regulatory oversight regarding discrimination in employment It prohibits discrimination with respect to a number of protected attributes race color religion sex and national origin establishing the Equal Employment Opportunity Commission EEOC to ensure compliance The EEOC in turn issued the Uniform Guidelines on Employment Selection Procedures in to set standards for how employers can choose their employees According to the Uniform Guidelines the gold standard for pre-employment assessments is validity the outcome of a test should say something meaningful about a candidates potential as an employee The EEOC accepts three forms of evidence for validity criterion content and construct Criterion validity refers to predictive ability do test scores correlate with meaningful job outcomes eg sales numbers An assessment with content validity tests candidates in similar situations to ones that they will encounter on the job eg a coding interview Finally assessments demonstrate construct validity if they test for some fundamental characteristic eg grit or leadership required for good job performance When is an assessment legally considered discriminatory Based on existing precedent the Uniform Guidelines provide two avenues to challenge an assessment disparate treatment and disparate impact Disparate treatment

0.6857 in paragraph X also the European Court of Justice has reaffirmed the notion of procedural fairness as disclosure of personal data recipients in particular in the public sector Similarly the fairness principle imposes the data controller to explicitly mention all third countries to which the data will be transferred According to WP the notion of fairness is also relevant in case of further processing of personal data Indeed the fairness principle imposes that the more intrusive or less expected the new data processing is the earlier the data subjects should be informed in advance In case the data controller processes data for further purposes in compliance with the compatibility test at Article essential requisites of accountability and fairness under the GDPR would impose that data controllers should provide data subjects with further information on the compatibility analysis carried out under Article All these practical examples of fairly transparent and fairly lawful data processing are forms of what Clifford and Ausloos name procedural fairness Fairness as fair balancing based on procedural fairness Analyzing the GDPR provisions Clifford and Ausloos notice that that the notion of fairness combined both to transparency and lawfulness can have two main concurring interpretations fair balancing and

0.6853 including discrimination If Articles and reveal only that the notion of fairness should give a substantial approach to transparency duties considering specific circumstances and effective awareness of the data subject recital gives a new additional perspective fairness should be also aimed at preventing significant effects of the algorithmic-driven environment on individuals eg discriminatory biases We will address the idea of fairness as non-discrimination in a specific section below Fairness as lawfulness in the GDPR As regards the link between fairness and lawfulness we need to analyze first Article whose title is lawfulness of processing Article foresees that Member States may maintain or introduce more specific provisions in relation to data processing for legal obligations Article c or for public tasks Article e In these cases Member States can determine specific requirements for the processing and other measures to ensure lawful and fair processing including for other specific processing situations as provided for in Chapter IX of the GDPR As aforementioned the concept of fairness is often linked to specific circumstances and practical cases specific processing situations In addition the reference to Chapter IX is also noteworthy Chapter IX of the GDPR Arts refers to particular cases of conflicting interests which

0.684 measures traits eg grit and presence and claims that in all panels since the Pre-Hire assessment does not show bias against women or minority respondents Other vendors actively intervene in their learned models to remove biases One technique that we have observed across multiple vendors eg HireVue pymetrics PredictiveHire is the following Figure Description of the pymetrics process screenshot from the pymetrics website employers/ Vendor name Assessment types Features Custom Target Training data Validation info Validation Adverse impact Fairness and Above phone video S bias mentioned ActiView VR assessment C validation claimed bias mentioned Assessment Innovation games questions bias mentioned GoodCo questions C P multiple studies adverse impact Harver games questions S HireVue games questions video C P rule impressai questions S Knockri video S bias mentioned Koru questions S some description adverse impact LaunchPad Recruits questions video bias mentioned myInterview video compliance Plumio questions games S validation claimed bias mentioned PredictiveHire questions C rule pymetrics games C small case study rule Scoutible games C Teamscope questions S P bias mentioned ThriveMap questions C bias mentioned Yobs video C S adverse impact Table Examining the websites of vendors of algorithmic pre-employment assessments we answer a number of questions regarding their

0.6819 scholars begun to focus on the GDPRs systemic accountability tools Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability The aim of this paper is to address how Data Protection Impact Assessments DPIAs Art link the GDPRs two approaches to algorithmic accountability individual rights and systemic governance and potentially lead to more accountable and explainable algorithms Indeed we argue that algorithmic explanation should not be understood as a static statement but as a circular and multi-layered transparency process based on several layers general information about an algorithm group-based explanations and legal justification of individual decisions taken We argue that the impact assessment process plays a crucial role both in connecting internal company heuristics and risk mitigation to outward-facing individual rights and in forming the substance of several kinds of explanations Section introduces the algorithmic accountability tools in the GDPR while Section explores the individual rights of data subjects Section develops the idea of collaborative governance of algorithms in the GDPR The subsequent sections address the concept of an Algorithmic Impact Assessment and how the DPIA in the GDPR can serve as an AIA The final section explains how the combination

0.68 at Article and at recital algorithmic auditing ensuring the right to context to have a new decision and to a human in the loop but also the right to explanation but these are not closed lists and the Guidelines on ADM suggest additional techniques Accordingly in case of more intrusive and riskier automated decision-making processes the data controller should implement all possible safeguards including the right to explanation of an individual decision However there have been legitimate concerns voiced in the legal literature about the capacity of data subjects to both invoke their rights and execute oversight over algorithmic decision-making These range from concerns about access to justice to concerns about individual capacity and expertise Consequently most policy proposals call either for a dual regime like the GDPR that mixes individual rights with other more systemic forms of accountability or for foregoing individual accountability in favour of expert and external oversight Foregoing individual accountability ignores the dignitary and legitimizing value of such rights Individual rights allow data subjects to exhibit autonomy and exert control and to protest or reject their objectification by profiling or decisionmaking machines Individualized explanations also serve to establish the legitimacy or illegitimacy of a decisionmaking system

0.68 Accordingly the data subject has a right to receive meaningful information about the logic the significance and the envisaged effects of the automated decision when she first answers the questionnaire see Article g or upon her request at a later stage see Article In addition we can affirm that this automated decision-making system might be allowed because either it is based on the explicit consent of the data subject c or it is eventually authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subjects rights and freedoms and legitimate interests b Accordingly the data controller or the national law eventually allowing such automated decision-making system needs to implement suitable measures to safeguard the data subjects rights and freedoms and legitimate interests at least the right to obtain human intervention on the part of the controller to express his or her point of view and to contest the decision Article Given that COVID- automated detection/diagnosis might be considered a data processing producing high risks for the rights and freedoms of individuals according to the risk-based approach Article the data controller might be required to implement higher

0.676 duties are formally respected but that the data subjects in practical circumstances is concretely in a position to learn of the existence and of the details of data processing concerning him or her The principle of fairness in data processing is also mentioned in the EU Charter of Fundamental Rights Article and state as follows Everyone has the right to the protection of personal data concerning him or her Such data must be processed fairly for specified purposes and on the basis of the consent of the person concerned or some other legitimate basis laid down by law Everyone has the right of access to data which has been collected concerning him or her and the right to have it rectified emphasis added The legislator of the Charter had in mind the principle of fairness of the Data Protection Directive and its interpretation should be linked to the general interpretation of secondary law the directive and now the GDPR Fairness in the GDPR the two nuances ie transparency and lawfulness Compared to the aforementioned legal texts the GDPR is the legal text with the highest number of references to fairness Article a rephrasing Article of the Data Protection Directive mentions

0.674 requires employers to take affirmative action to increase the representation of women and minorities in their workforces The order enforced by the Department of Labors Office of Federal Contract Compliance OFCCP also outlines related requirements around the documentation of recruitment activities including the collection of demographic information about job applicants and employees in order to facilitate the detection of discrimination at different points in the recruitment pipeline Title VII requires employers and other covered entities to make and keep such records relevant to the determinations of whether unlawful employment practices have been or are being committed as defined by the Equal Opportunity Employment Commission EEOC which enforces the law Since employers may be liable for employment practices that result in disparate impact on the basis of protected categories including race and gender EEOC guidance points to Title VII as a legal basis for requiring the collection of applicant data as necessary to detect mitigate or defend against claims of disparate impact The Uniform Guidelines on Employment Selection Procedures which reflects the US governments unified position on employment tests detail how employment tests must be evaluated for unjustified adverse impact on the basis of race sex or ethnicity The EEOC may

0.6738 right to explanation but on a varied set of tools including right to contest human in the loop and algorithmic auditing Beyond Article and recital the GDPR contains several notions that might influence the interpretation of accountability duties also in case of automated profiling the fairness principle the lawfulness principle the risk-based approach the data protection impact assessment model etc In our view in cases where the possible explanation three-layered audience-based moment-based is not satisfactory the data controller should anyway take every reasonable step to provide such an explanation but at the same time other complimentary tools should be implemented These might include disclosing meaningful information about a Data Protection Impact Assessment DPIA on the algorithmic decision-making system the DPIA as mentioned in Article of the GDPR is a process to assess and mitigate the impact of data processing operations on fundamental rights and freedoms of data subjects or the result of a justification effect-based test on the algorithm where the data controller explains why the algorithm analysed on the aggregated final effects on different data subjects but also analysed in its purposes intentions etc is not unfair unlawful inaccurate beyond the purpose limitation etc see the legibility test in

0.6715 may reflect the duty to assess the proportionality and necessity of data processing required at Article d GDPR If the data controller must prove the legal proportionality and necessity of automated decisions taken they would be creating not merely an explanation but a justification of both data used and profiling mechanisms Conclusion There is a growing literature suggesting that Algorithmic Impact Assessments are a crucial tool in establishing algorithmic accountability This paper addresses that tool as implemented in the GDPR We find that the GDPRs version of Impact Assessments serves as a central connection between its two approaches to regulating algorithms individual rights and systemic governance That framing allowed us to identify both value in and shortcomings of the GDPRs Impact Assessment regime as applied to algorithmic governance This analysis we hope will have value for other discussions of Algorithmic Impact Assessments beyond the GDPR In particular moving from individual transparency rights and governance accountability duties in the field of automated decision-making we suggest a model of Multi-layered Explanations drawn from Algorithmic Impact Assessments Since there are several layers of algorithmic explanation required by the GDPR we recommend that data controllers disclose a relevant summary of a system produced in

0.6661 observe that this requirement effectively expands the suitable safeguards to protect individual rights from the harms of algorithmic decision-making from the series of individual due-process-like protections enumerated in the GDPR text to a far broader set of systemic accountability measures including third-party auditing Art The Guidelines list of best practices for suitable safeguards over algorithmic decision-making supports this interpretation including recommendations that data controllers use both audits and external review boards When a company that deploys algorithmic decision-making conducts its DPIA it will refer to the Guidelines list of best practices in establishing risk mitigation measures that are already regulator-approved This means that in practice a company running through the cyclical DPIA process discussed above will likely incorporate external oversight and input at the risk mitigation stage bringing external input into the cycle despite the fact that it is not a formal procedural requirement for DPIAs in general Conceptually the implications of this are even broader By characterizing third-party and expert oversight as a suitable safeguard or suitable measure to protect individual rights the Guidelines link individual rights protection with collaborative governance techniques Data controllers are tasked with coming up with ways to prevent error bias discrimination and other harms

0.666 process as running multiple times The GDPR also lays out procedural requirements for the DPIA Differing from the Impact Assessments imagined in the literature DPIAs do not involve a period of public comment or input They do require consultation with an internal but independent Data Protection Officer if a company has one Many data controllers that are required to perform impact assessments will have Data Protection Officers in place Art In lieu of public or formal stakeholder consultation the GDPR requires consultation where appropriate with impacted data subjects Art In the original proposal of the Commission consultation with data subjects was mandatory Article The Parliaments text argued that this represents a disproportionate burden on data controllers amendment Accordingly the approved Article requires consultation only where appropriate and without prejudice to the protection of commercial or public interests or the security of the processing operations This puts in place one method for external input from impacted data subjects rather than external experts or the public The DPIA Guidelines envision that this input could be for example in the form of surveys crafted by data controllers and sent to future customers which would make it less meaningful than say deep consultation with

0.6602 is also key to ensure that data subjects are able to express their point of view and are capable of contesting the decision As it is stated in the European Data Protection Board guidelines data subjects will only be able to do that if they fully understand how the automated decision was made and on which bases Even further the European Data Protection Board provides in Annex a set of good practice recommendations for data controllers with respect to the several rights and provisions of the GDPR that are of relevance for profiling and automated decision making On top of the generic transparency requirements as commented in data controllers have the specific duty at Articles to provide the data subjects with meaningful information about the logics the significance and the consequences of automated decision-making but also to adopt specific safeguards right to contest right to human intervention right to express ones view including the explanation of the decision reached recital Since the GDPR entered into force the academic community has debated about whether in actual fact this constitutes a new right to explanation Considering the different levels of risk that automated decisionmaking can imply for rights and freedoms of data

0.6542 in a specific context Actually the GDPR does not always describe in details such fair procedures the data controller is asked to choose and adopt her own procedures in order to make a data processing fairly transparent and fairly lawful in particular looking at the specific circumstances and context in which the personal data are processed recital and or the specific processing situations Article and In other words the difference between mere transparency or lawfulness and fair transparency or lawfulness is the adoption of additional safeguards that can effectively rebalance the unfair imbalance between the data controller and the subject in specific circumstances For this reason it seems reasonable to affirm that the two components of fairness procedures and balancing are not separate but connected as means to ends procedural fairness tends to fair balance In other words procedural fairness might be an obligation of results rather than an obligation of means the data controller is not asked to merely apply specific measures but to reach a fair substantial balancing between interests in specific situations Actually some scholars argued that fairness at Articles and GDPR is an ex ante assessment on the average data subjects while data subject rights such

0.6539 Multi-layered Explanations from Algorithmic Impact Assessments in the GDPR Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability The aim of this paper is to address how Data Protection Impact Assessments DPIAs Art in the European Union EUs General Data Protection Regulation GDPR link the GDPRs two approaches to algorithmic accountability individual rights and systemic governance and potentially lead to more accountable and explainable algorithms We argue that algorithmic explanation should not be understood as a static statement but as a circular and multi-layered transparency process based on several layers general information about an algorithm group-based explanations and legal justification of individual decisions taken We argue that the impact assessment process plays a crucial role in connecting internal company heuristics and risk mitigation to outward-facing rights and in forming the substance of several kinds of explanations CONCEPTS Applied computing Law social and behavioral sciences Computing methodologies Machine learning KEYWORDS Law General Data Protection Regulation Impact Assessments Introduction The discussion of the GDPR General Data Protection Regulation and algorithmic accountability has largely focused on whether there is an individual right to explanation of an algorithmic decision Only more recently have legal

0.6489 defined in as the ability to make decisions by technological means without human involvement This refers to the broader notion of the application of algorithms for the purposes of decision-making which may or may not involve some form of profiling in doing so Several examples on this topic can be found in Recital contextualises the set of data subject rights relevant to both profiling and automated decision-making that are developed in the several articles of the regulation including the right to information to obtain human intervention to express his/her point of view to obtain an explanation of the outcome of a decision and to challenge such decision Article and of the GDPR require that data subjects are informed about the existence of automated decision-making processes including but not limited to profiling Further the articles require that data controllers provide data subjects with information about the underlying mechanisms logics behind the automated decision-making performed and the significance and potential consequences of such processing The right of access Article also includes similar provisions granting data subjects the right to access their personal data and obtain such information about its processing from the data controllers The aforementioned articles refer respectively to Article where

0.6485 of consent while actually some other lawful basis is relied on would be fundamentally unfair to individuals Also basing personal data processing on the consent of data subject does not allow the collection of data which is not necessary in relation to a specified purpose of processing and this would be fundamentally unfair As regards the relation between fair and transparent WP Opinion on Transparency often refers to fairness In particular fair transparency seems to require additional efforts if compared to merely formal transparency since it takes into account also reasonable expectations of data subjects Fairly transparent data processing means expectable and foreseeable data processing even considering the circumstances and context of each situation We can make an example of the difference between formal transparency and fair transparency While Articles e and d states that in principle data controllers should communicate merely the categories of data recipients the fairness principle imposes something more to provide information on the actual named recipients of the personal data and if the controller decides to disclose just the categories of recipients she would be asked to demonstrate why in that case it was fair not to disclose the name of recipients As we show

0.6448 to individual rights and external oversight is imposed over how they choose to address these problems That external oversight itself is simultaneously conceptualized as a crucial aspect of individual rights in the GDPR standing in for data subjects to ensure that they are not subjected to an unfair arbitrary discriminatory or erroneous system A simpler way to say this is that expert oversight in the DPIA process serves two roles it watches the data controllers as they come up with ways of addressing problems with algorithmic decision-making and it reassures data subjects that their dignity and individual rights are being respected by a fair system As the mechanism through which this external oversight is implemented the DPIA thus connects the two approaches to algorithmic governance in the GDPR Shortcomings of the DPIA The biggest shortcoming of the DPIA is that it does not include a mechanism for mandatory disclosure to the public Public disclosure is understood by many to be an essential element of Impact Assessments as a policy tool of collaborative governance Public-facing disclosure enables public feedback both in the form of market feedback and regulatory feedback over the longer term By failing to mandate public disclosure the GPDRs

0.6428 individual due process The GDPR requires data controllers that fall under the exceptions to its ban on solely automated decision-making to still implement suitable measures to protect individual rights Art The Guidelines counsel that data controllers should use DPIAs to identify what measures they will introduce to address the risks involved As discussed suggested measures include a number of individual rights informing data subjects about the logic involved explaining the significance and envisaged consequences of algorithmic decision-making providing a way to contest a decision and providing a way to express ones point of view This list effectively imports the various individual rights laid out in the GDPR that are restricted to solely automated decision-making into DPIAs that the Guidelines say go beyond the solely automated context that is algorithmic decisions that involve a human decision-maker In other words the GDPR or really the interpreting Guidelines envisions DPIAs in the context of algorithmic decision-making as serving as a form of commitment-making to protecting or even enabling individual due process rights By characterizing these individual rights as risk mitigation measures it both provides a substantive backstop as to what must be included in a DPIA and tasks data controllers with constituting through

0.6423 the process of performing a DPIA what these individual rights will look like in practice Thus the DPIA serves as a collaborative governance mechanism used to constitute the substance of individual due process rights It also serves as a means of expanding company commitments changing company decision-making heuristics to include an assessment of individual due process rights Finally the DPIA has a role in linking the GPDRs system of collaborative governance to its individual rights regime through the imposition of systemic accountability measures such as audits or external review Remember the general DPIA Guidelines only suggest and do not mandate consultation with external experts In the context of algorithmic decision-making however external expert involvement and oversight is more like a requirement The use of external experts is framed as a necessary risk-mitigation measure for algorithmic decisionmaking functionally changing the required content of a DPIA in that context The reasoning goes as follows Recital requires in the context of algorithmic decision-making the use of technical and organisational measures appropriate to ensure in particular that factors which result in inaccuracies in personal data are corrected and the risk of errors is minimised and that prevents inter alia discriminatory effects Malgieri and Comandé

0.6372 models including LIME and SHAP These advanced statistical tools were not necessary for this audit since training data and source code were available to us and pymetrics uses interpretable models see ABOUT PYMETRICS In this audit we focus on the candidate screening aka preemployment assessment product offered by pymetrics pymetrics is a startup that offers a number of services in the context of employment Unlike services like Monstercom or Indeed pymetrics is not a marketplace where employers post jobs or job seekers post resumes Rather pymetrics uses gamified psychological measurement and applied ML to evaluate the cognitive and behavioral characteristics that differentiate a roles high-performing incumbents to make predictions about job seekers applying to that role pymetrics candidate screening service is designed to surface the applicants with the greatest potential and pass them on to the interview stage while simultaneously seeking to avoid disparate impact by abiding by the UGESPs four-fifths rule with respect to protected demographic groups At a high-level pymetrics candidate screening service can be summarized as follows An employer contracts with pymetrics to develop and deploy a predictive model for candidate screening We refer to these employers as clients A job analyst from pymetrics surveys the client

0.6324 years Finally we return to the context of the GDPR Mantelero discusses the idea of a Human Rights Social and Ethical Impact Assessment HRSEIA in the AI context A hybrid between a Human Rights Impact Assessment and a Privacy Impact Assessment the HRSEIA suggests that businesses voluntarily take into account ethical and social impact in addition to human rights Mantelero emphasizes the role of such an impact assessment in addressing the collective dimensions of data harms At its core the HRSEIA has three features it is participatory it is transparent and it is circular in nature Practically it consists of a self-assessment questionnaire sometimes leading to evaluation by an ad hoc committee of experts Stakeholder engagement is encouraged but not required Similarly public disclosure is encouraged Mantelero explains that while this proposal is in line with the declared intent of the GDPR he does not understand the HRSEIA to be required by the GDPR Several other commentators have recently discussed the DPIA and the role it plays in the context of algorithmic accountability more generally Most of these proposals for Impact Assessments centrally depend on release of information to the public This is necessary both to obtain external input into

0.6323 impacted data subjects Article GDPR in the continuous cycle of the DPIA framework see also recital merging an algorithmic DPIA and multi-layered explanation might serve as a suitable safeguard to protect fundamental rights and freedoms of individuals both under Article and under Article d of the GDPR developing an algorithmic DPIA and explanation safeguards in parallel intrinsically related to the right to contest a decision right to human-in-the-loop etc might be the best way to enrich transparency with accountability safeguards and overcome the transparency fallacy through a virtuous cycle of algorithmic auditing and continuous detection/mitigation of unfair effects The idea of merging at least partially algorithmic accountability duties and DPIAs also seems useful considering the most advanced literature on explanation Effective explanation is not only the result of an analysis but also a two-stage process both cognitive and social Multi-layered and multi-step explanations would be a continuous process not merely a static product Some scholars have remarked that what is needed is not merely an explanation but a legal justification of automated decisions taken Connecting an algorithmic DPIA to individual transparency rights might address this the information duties about the pertinence and relevance of decision-making processing required by the Guidelines

0.6297 how a system is developed trained or monitored and to gain public legitimacy and acceptance for the use of a system The kind of information released to the public can be more in the nature of a summary or an overview it is not necessarily source code Some suggest a tiered release of information with summaries released to the public and detailed or sensitive information released only to regulators or experts Thus more recent proposals also call for expert input and oversight by suggesting that companies or government agencies use Impact Assessments to come up with and stick to a plan for third-party expert oversight over a systems development and eventual use The Data Protection Impact Assessment DPIA as an Algorithmic Impact Assessment A version of an Algorithmic Impact Assessment might be derived from the GDPRs Data Protection Impact Assessment DPIA Article a requires a DPIA in case of inter alia a systematic and extensive evaluation of personal aspects relating to natural persons which is based on automated processing including profiling and on which decisions are based that produce legal effects concerning the natural person or similarly significantly affect the natural person Interpreting this provision the Article Working Party Guidelines

0.6244 also other information in so far as such further information is necessary having regard to the specific circumstances in which the data are collected to guarantee fair processing in respect of the data subject emphasis added Also Recital confirmed the link between fairness and transparency if the processing of data is to be fair the data subject must be in a position to learn of the existence of a processing operation and where data are collected from him must be given accurate and full information bearing in mind the circumstances of the collection emphasis added The distinction between mere transparency and fairness is probably based on the references to specific circumstances of the data processing at stake If transparency eg at Articles and is mainly based on specific formal procedures to respect in terms of information disclosure fair transparency takes into account substantial interests at stake in a specific data processing and the eventual need to communicate more information to the data subject In other words in the Data Protection Directive while the mere notion of transparency adopts a formalistic-procedural approach the notion of fairness applied to transparency required a substantial effect-based approach what matters is not that data controllers

0.6134 about a particular stand-alone decision but information about the algorithms treatment of groups and tendency towards bias This group-based explanation which we argue can be at least implied from if not required by the rights to contestation etc could be created based on information on bias and discrimination uncovered during a DPIA As a third layer of explanation the GDPR requires disclosure of meaningful information about the logic involved in automated decision-making on a systemic level Arts and The DPIA must include an assessment of necessity proportionality risks and safeguards A DPIA is not required to be made public but its public disclosure is highly recommended at least in the form of meaningful summaries If we compare DPIA requirements to the safeguards in automated decision-making as we do below in Table it seems reasonable and efficient to suggest that a relevant summary of the Algorithmic DPIA could be used as meaningful information about the overall system of decisionmaking that must be provided to data subjects Further levels of details could be tailored from this systemic layer Indeed the description of algorithmic processing including rationales and criteria might well be specified on both a group level and on individual level the

0.6131 in closely related fields We are however the first to discuss Algorithmic Impact Assessments not in isolation but as a central component among many components of the GDPRs two-prong approach to algorithmic accountability This changes the nature of the conversation Instead of examining impact assessments in isolation from other accountability tools it situates them within an overarching system of data governance Our GDPR-specific analysis then may have implications for proposals for algorithmic impact assessments in other legal systems It suggests that impact assessments best serve a role in conversation with other accountability tools as part of overarching governance design We suggest that impact assessments play a central role both as a source of and mediator between the multi-layered explanations indicated in the GDPR Proposals for Algorithmic Impact Assessments Algorithmic Impact Assessments have received a good deal of attention on both sides of the Atlantic as possible tools to address problems of algorithmic discrimination bias and unfairness including in at least one proposed US federal law We here briefly discuss several important precursors to the AIA Human Rights Impact Assessments HRIA Privacy Impact Assessments PIA Ethical Impact Assessments EIA and Surveillance Impact Assessments SIA The Human Rights Impact Assessment HRIA process

0.612 faith and proportionality although there is no textual link in the GDPR between these two principles several commentators have addressed the inherent link between proportionality and fairness as fair balance as reflected also in the CJEU case-law see supra and in the relationship between Article and in the EU Charter of fundamental right In other EU legal fields the link between fairness and equity or good faith is more explicit In consumer law Unfair Terms Directive defines unfair what is contrary to the requirement of good faith and causes a significant imbalance in the parties rights and obligations arising under the contract to the detriment of the consumer emphasis added The link between unfairness good faith and significant imbalance in consumer law is in line with the aforementioned reflections about bona fide and fair balance in the national legal histories see interestingly also Department of Justice of State of Victoria A term is unfair if it causes a significant imbalance in the parties rights and obligations under the contract it is not reasonably necessary to protect a legitimate interest of the trader it would cause detriment to the consumer but also in the data protection law In the field of

0.6078 an assessment of the necessity of processing in relation to the purpose an assessment of the risks to individual rights and freedoms and importantly the measures a company will use to address these risks and demonstrate GDPR compliance including security measures The GDPRs version of a DPIA must take place before a company implements a system That is a company must assess a system and propose risk-mitigation measures before data processing takes place Art But the GDPR also envisions iteration If the risk posed by a system changes a company must assess whether it is complying with its own Impact Assessment Art It should also review the DPIA itself The DPIA Guidelines suggest an even more dynamic view of DPIAs They suggest that DPIAs should as a matter of good practice actually be continuous updated throughout the lifecycle of the project and that they should be re-assessed or revised at least every years Carrying out a DPIA is a continual process not a one-time exercise state the DPIA Guidelines This continual process involves assessing risk deploying risk-mitigation measures documenting their efficacy through monitoring and feeding that information back into the risk assessment and ongoing process The DPIA Guidelines envision this

0.6029 in uncovering or combating bias than sectoral employment or equality rights yet US systems may unsurprisingly not be optimised to meet these rights Turning first to UK discrimination law similarly to US law it is based initially on the idea of a closed list of protected characteristics here laid out in the Equality Act s UK law also however currently Brexit may change this has to respect supranational EU law which includes a number of Directives relevant to equality and bias notably the Equal Opportunities and Equal Treatment Directive the Gender Equality Directive and the Framework Directive for Equal Treatment in Employment and Occupation Human rights standards under the European Convention of Human Rights ECHR are also relevant and refers to broader protection against discrimination on any ground art in relation to the rights and freedoms guaranteed by the ECHR The EU itself now has as a binding source of law its own human rights instrument the Charter of Fundamental Rights of the EU This complex legislative patchwork divided across common law and civilian approaches to law-making and interpretation possibly contributes to fewer hard and fast standards for measuring unlawful bias in UK employment law than in the US which

0.5974 prior Data Protection Impact Assessment DPIA see art a which should show inter alia that potential for unfairness and bias had been considered and steps taken to avert This might arguably be seen as implying a requirement for debiasing in AHS tools deployed in EU see also for a similar point in a US context However even if a right to algorithmic transparency does exist in the solely-automated hiring context what does it practically mean This has again been the subject of much academic debate Selbst and Powles argue for example that for a right to meaningful information about the logic involved to be sic meaningful it must be more than a simple regurgitation of source code The A Working Party recommend that the data subject should be provided with general information notably on factors taken into account for the decision-making process and on their respective weight on an aggregate level which is also useful for him or her to challenge the decision To date there has been no relevant case law in the UK and the provision is not expanded on in the Data Protection Act DPA which implements the GDPR in the UK in comparison to some other

0.5935 to understand the target role eg the job description seniority-level etc and the metrics that the client uses to assess job performance in that role The client has incumbent employees in the targeted role play pymetrics suite of games described further in The client also gives existing job performance data about these incumbents to pymetrics This performance and gameplay data are used as the training input for a predictive model A pymetrics data scientist uses a proprietary pymetrics tool to develop a predictive model for the client These models are evaluated for predictive performance and compliance with the UGESPs four-fifths rule using a separate held-out testing set with demographic information pymetrics deploys the best-performing predictive model that meets the fairness criteria Job seekers who apply for the targeted role are asked to play pymetrics suite of games Based on this gameplay data the model predicts which candidates have similar attributes to the clients high-performing incumbent employees Information about high-scoring job seekers are sent to the client who may then apply additional filters eg resume screening and interview candidates pymetrics performs longitudinal analysis of the predictive model This includes back-testing to re-evaluate whether fairness criteria are being met with respect to

0.5872 GDPRs version of the DPIA for inadequate public disclosure and stakeholder involvement not to look to how it connects to the broader system of both collaborative governance tools and individual rights in the GDPR Second as part of a larger system of governance there are unexplored connections between the GDPRs potential AIA and its underlying substantive individual rights and substantive principles It is true that many of those rights and principles are articulated in broad sometimes aspirational terms The GDPR version of the AIA has a substantive backstop The oddity is the GDPRs circularity the DPIA helps not just to implement but to constitute those individual rights Third impact assessments can serve as a connection between collaborative governance and individual rights only one other proposal to our knowledge suggests using Impact Assessments to establish something resembling individual rights a system of enhanced due process mechanisms for affected individuals The information a company creates during the Impact Assessment process can feed into what it provides to data subjects and to the public at large The procedures an Impact Assessment puts in place can serve not just to prevent error bias and discrimination but also to legitimize a system or even respect

---------------------------------------------------------------------------
TOPIC 10: uncertainty

---------------------------------------------------------------------------

0.8509 and potentially also a visa Across a vast range of sectors decisions that fundamentally affect peoples lives and their ability to engage in long-term planning are made by algorithms and bureaucracies Sometimes those decisions are unfavorable When they are the subject of the decision can only reasonably plan their subsequent course of action if they know what it would take to receive a more favorable decision After all a desired or hoped-for end can only become the target of a plan if the agent is able to select a means to that end Moreover this need to be able to plan applies not just to one-off cases but generally over the course of ones life As such Accessed May For a critical history of this phenomenon see someone can be positioned in such a way that they know or reasonably expect that were things to go wrong they would be able to set them right again Such positioning refers not only to the way things currently are but also to how they might be across a range of counterfactual scenarios As such we need some way to ensure that people both have some way of getting unfavorable decisions reversed and

0.7982 an emergency expense of without having to sell something or borrow money Someone who suffers from precarity in this way will find it hard to engage in temporally extended agency and to trust that their plans will come to fruition They face a life of constant worry and stress and such worry and stress can have knock-on effects that feed back into the precariousness of their existence For example stress and anxiety may lead someone to snap at their boss which could get them fired Elizabeth Anderson pg estimates that approximately of American workers essentially all those who are neither securely self-employed nor upper-level managers are just one arbitrary and oppressive managerial decision away from being fired demoted or otherwise mistreated by the pervasive authoritarian governance in our work and off-hours lives The modally robust good of recourse The examples described in the previous section and others like them suggest that people will often need some way to reverse unfavorable decisions that would otherwise impair their ability not only to accomplish one particular goal but also to accomplish all of the other goals that it is a means to For example someone who is counting on a loan in order

0.7962 to purchase a car in order to be able to drive to a well-paying job in order to take care of their family might be denied that loan In such a case the denial affects not just their immediate financial situation but their whole life plan If someone cannot trust that they will have some way of overcoming challenges that thwart the crucial means to their long-term ends they will have little reason to try to engage in the temporally-extended agency characteristic of mature adults We live in a world in which many decisions that significantly affect our ability to exercise temporally-extended agency are made by algorithms and bureaucracies These algorithms and bureaucracies establish a system of incentives and disincentives that apply to both the ends that people might pursue for their own sake and the means to those ends If you want to enter a profession you typically need to receive some sort of certification If you want to make a large purchase you may need to take out a loan that you can pay back at a reasonable interest rate in a reasonable amount of time If you want to travel internationally you need to get a passport

0.7583 non-robust good of favor and therefore also value the robust good of friendship which delivers favor in a range of counterfactual scenarios If someone is your friend not only do they favor you now but also they would be disposed to favor you in a range of nearby possible worlds Friends are disposed to put one another back on course rather than simply abandoning each other when the going gets tough and there are derogatory natural language expressions eg fair-weather friend for people whose favor cannot be counted on in a broad enough range of Thus we distinguish between particular token acts of exercising recourse reversing a single harmful decision and the general state of enjoying systematic access to the power to reverse harmful decisions knowing that if a harmful decision were to be made one would be able to get it reversed In philosophy robustness of this sort is understood in terms of counterfactual conditionals Unlike the material conditional If p then q the truth conditions for the counterfactual conditional if p were the case then q would be the case refer not only to the world as it actually is but also to various ways the world could

0.7422 Someone who enjoys recourse and knows that they enjoy it is better positioned to engage in long-term planning and to trust that if something were to go wrong on the way towards their long-term goals they would not fall victim to precarity but rather be able to set themselves right again We then pointed out that many of the decisions for which people might want recourse are made by algorithms and bureaucracies This in turn suggests that these socio-technical systems should be implemented in a way that automatically delivers suggested flipsets to people about who unfavorable decisions are made In the remaining two sections of the paper we raised some problems for the existing literature on algorithmic recourse then addressed these problems to the extent possible In closing we want to address an objection that we expect many readers may have namely that recourse is a red herring that will only distract us from the more fundamental problems of precarity After all one might think that if precarity were much less severe than it currently is there would be little need for recourse Moreover one might worry that focusing on recourse places yet an additional burden on individuals as they

0.7397 methods for de-biasing have also been shown to often be inadequate these circumstances and how would one test it The only way we can think about an unbiased training set in this circumstance is one where certain ideas are not automatically precluded from being included in any given corpus But knowing what perspectives have been omitted is difficult to determine and correct after the fact

0.7251 to take actions that are irrational or harmful What they mean more specifically is that there may be some recommendations that are indeed rational if the only goal is to obtain a positive decision from the model but irrational with respect to other goals in a persons life A common-sense example for this proposition is that an explanation should never recommend that a person seek to make less money eg While we believe it unrealistic that an actual credit model would ever be allowed to learn such a relationship the example still holds value It is self-evident that no one would want to make less money even if doing so would improve their access to credit Or consider an example that reverses this dependency a person contemplating applying to a new job for its superior health insurance is unlikely to remain at his current job because an explanation for a failed credit application told him to increase the value of his length-of-employment feature In this case acting on the recommendation would impose an opportunity cost on the consumer by forcing him to forgo benefits in other domains When other aspects of ones life depend on some of the same features

0.7205 decision subject But by attempting to return power to the decision subject via an explanation that for her own sake cannot be a complete explanation we grant a new form of largely unanticipated power to the decision-maker Furthermore the requirement to make certain assumptions about the real world also grants power to the decision maker Whenever there is ambiguity in the individuals preferences the decision maker has the power to resolve the ambiguity however it sees fit This leaves the decision maker with significant room to maneuver the choice of when and where to further investigate and more degrees of freedom to make choices that promote their own welfare than we might realize This new power can be used for good or ill Consider for example a decision maker providing a counterfactual explanation for why an individual did not qualify for a loan As discussed in Section this decision and therefore explanation is not simply binary in its explanation the decision maker must give the decision subject a counterfactual that would result in a specific interest rate At best it might allow the subject to choose their target interest rate Alternatively it might somewhat paternalistically choose the interest rate that

0.7091 called search engine optimization or SEO or that if we revealed the algorithm behind a credit scoring system then an individual could inspect the algorithm and improve their attributes strategically to get a good score The debate over whether an individual is seeking to exercise recourse or merely strategically manipulating a classifier rests on the answer to an ambiguous question are the set of attributes being changed relevant to success at the task or not If we construe relevance descriptively then this question just asks whether changing a particular set of attributes would result in a different decision by the algorithm By contrast if we construe relevance normatively the question asks what attributes should make a difference For example a risk assessment algorithm trained on historical data is likely to treat the attribute of race as relevant However we might agree that race should not be taken into account when deciding whether to grant bail In the ideal case an algorithm would take into account all and only the attributes that should make a difference Of course this is rarely if ever the case Instead algorithms almost always use proxies for attributes of interest and the gap between the attribute

0.7048 feature or the action required to make such a change affects other aspects of a persons life positively or negatively And assuming the decision maker could achieve a general sense of these facts the decision maker further does not know how they vary from person to person All this means that the choice of features to disclose can have unintended effects for decision subjects which would have been avoided with a different disclosure Given the informational position of the decision maker there is simply no way to fully realize its commitment to respecting a decision subjects autonomy One might suggest that these problems can be solved with even more data If explaining a credit decision the decision makers choice of which features to highlight might be affected by information about a persons health family situation or future educational plans And a decision subjects willingness to look for a new job will be influenced by whether they are sick have a new baby or aging relative to care for or are saving to go back to school This information can directly or indirectly be mined from other sources in the world such as social media data If a decision maker understands

0.7031 reasoning involved If your ultimate aim is for instance to vacation next year in Hawaii you might go about it by saving money in order to be We focus on the least well off because this is arguably the most defensible principle from an ethical and political point of view able to purchase a ticket And you might go about saving money by getting a second job And you might go about getting a second job by receiving certification to work that job And you might go about receiving certification by taking vocational training courses In this scenario you take training in order to receive certification in order to get a second job in order to save money in order to purchase airfare in order to go to Hawaii Such plans are only likely to succeed in a sufficiently well-ordered system in which the reasons things might go wrong are foreseeable and understandable and in which errors can be identified and rectified If you could not trust that the vocational training would be sufficient to get certified it would not make sense to plan in this way Likewise if you could not trust that hyperinflation would not destroy your savings

0.6952 things actually are and as they would be under certain variations p For this reason when we are assured that someone embodies a robust good we can live free from anxiety and fear that the correlative non-robust benefit and everything that depends on it will suddenly be snatched away without notice or warning Robust goods thus systematically deliver as a side-effect peace of mind and warrant for trust While Pettits account focuses primarily on robust goods as they are embodied in individual humans it is also possible for a social group or an institution to embody a robust good For example a fail-safe nuclear reactor is a complex socio-technical system in which multiple layers of safeguards are put in place When such a reactor is working as designed it delivers two non-robust benefits in a range of counterfactual scenarios namely electrical power and safety from radiation If something were to go wrong either mechanically or via human error in a fail-safe reactor multiple alerts and protective actions would be triggered that would at least if it works as designed set the reactor on a course towards equilibrium In addition whereas Pettits account focuses only on robust goods it is possible

0.6936 I think we should stick but we should have a broader understanding of where they are coming from in order to understand how far they need to come I think this could be what the third space is talking about its just saying just discussing about you know in my culture this is not a concept but this is not even applicable from where I come from so having that space and kind of protected space where there is more of a collaborative feeling where you know everyone can share what they feel like is going to be quite interesting Through consideration of being mindful of the different cultural assumptions we and our students might be bringing to the classroom we also recognised that by deepening FATE/CDS integration we would likely introduce more potentially contentious topics into the classroom Our discussions highlighted the need to be more confident of our own boundaries about what was acceptable Speaker Would there ever come a point where you would actually put your foot down and say No thats unacceptable what someone was saying because it just jarred so much with your beliefs Just say if someone says something particularly sexist or racist and

0.6802 its potential uptake as a justification but I can point to some aspects of non-robust explanations that are unsatisfactory from the point of view of acting as possible justifications We generally expect justifications to be based on real patterns in the world eg the pattern that people with a history of repaying loans on time in the past have social and economic circumstances that make them more likely to repay loans in the future but as Ive argued the non-robustness of certain explanations makes it unclear that their content reflects real patterns Discursively if I ask for an explanation of a phenomenon and just one explanation is provided with no indication that there are perhaps other explanations that would have served just as well I would feel that crucial information was being withheld from me Plausibly the Rashomon Effect occurs when many features interact with one another in complicated ways so that some models are able to imitate the contribution of a feature or a combination of features by another seemingly distinct feature or combination of features The different explanations provided are thus different views on the same interactional phenomenon Providing a summary of these different views for example through

0.6775 potential into realized abilities is not determined entirely by external circumstances that are matters of brute good or bad luck Apart from the influence of good and ill fortune our realized abilities reflect our personal history ie the choices we make every day If human agency is not an illusion we are partially at least responsible for at least some of our choices Therefore at least some LB is not morally problematic It is therefore unclear why one should treat inequalities arising in the as a result of such LB on a par with inequalities arising in the OS due to MB Just LB at group-level In the discussion above we are saying that not all LB is necessarily unjust because some LB might simply be caused by personal choices However one may question whether it is possible for personal choices to cause unequal outcomes not only between individuals but also at the group level even if there were no inequalities in the PS For example two people with the same potential with institutions that only let inequalities reflecting their individual choices exist could still end up with different realized abilities Yet on a group level it seems improbable that

0.6769 the following Sorry but in your case recourse does not seem to be a feasible option because even the least burdensome avenue to reversing the decision is probably too onerous for you to pursue In particular the easiest way for you to receive a favorable decision is details of recourse You are of course welcome to pursue this option but please do bear in mind that it may be excessively costly In this way the individual would still see their best flipset which if all they need to do is appeal rather than exercise recourse narrowly That is laws that take effect retroactively conceived may in fact be feasible but the flipset would not be provided as a recommendation so much as an explanation of why it may not be worth pursuing recourse One interesting advantage of using upper bounds on costs is that we can count the number or proportion of times this upper bound on cost is reached A large value would indicate systemic society-wide problems in the application of recourse and indicate a need for deeper structural reforms Changing the geometry of the intervention space The problem with features that might vary jointly is that the assumption

0.6738 know in general that they will have a way of getting unfavorable decisions reversed Let us define the enjoyment of recourse as being in such a position Recourse systematically delivers the benefit of reversing harmful decisions by algorithms and bureaucracies across a range of counterfactual scenarios If someone enjoys recourse then not only are they able to get a single decision reversed but they also enjoy the power to reverse decisions across a range of counterfactual scenarios As such someone who enjoys recourse need not passively suffer the slings and arrows of outrageous fortune but is positioned to take up arms against a sea of troubles They do not suffer from what Condorcet considered one of the most debilitating aspects of poverty the idea of being counted for nothing of being delivered up without defense to all vexations and all outrages It is illuminating in this context to refer to recent work by Philip Pettit who has argued that a wide range of ethically important values are modally robust For a good to be modally robust in Pettits sense it must systematically deliver some other benefit in a range of counterfactual scenarios For instance according to Pettit people value the

0.6675 would affect the utility of an explanation Worse yet the cost of making certain changes will not be consistent across different people Changes that might be rather inexpensive for one person to make might be costly for another person to make Thus when we use explanations to identify the easiest or most difficult features for someone to change to achieve a different decision from a model the explanation must be sensitive not only to how these changes involve different costs but how these costs vary across the population Different subsets of features may be appropriate for different people with different life circumstances This complication cuts against the very desirability of these explanations the idea that we can automatically determine what is easiest or hardest to change Features may be relevant to decision making in multiple domains Feature-highlighting explanations may interact with facts about a persons life that are invisible to the model In particular the supposition of a counterfactual explanation is that it is offering advice about the kinds of changes that it would be rational for a person to make to achieve better results in future decisions Some commentators and scholars have cautioned that explanations should never encourage people

0.6645 no credit history when in fact they have taken out and repaid several loans In the case of a correct-but-unfavorable decision the recommended flipset would enumerate the actions that the agent would need to undertake to receive a more favorable decision from the algorithm Call this the exercise of recourse narrowly conceived In the latter case the recommended flipset would enumerate rectifications that need to be made to the dataset so that the person will receive the correct favorable decision Call this appeal Recourse narrowly conceived and appeal both promote and protect peace of mind and warrant for trust Someone who inhabits a socio-technical system rife with errors and no prospect of appeal is just as badly off as and perhaps worse off than someone who inhabits an error-free socio-technical system that offers no understandable ways to have unfavorable decisions reversed Both lack a fundamental robust good and are thus liable to all the stress and anxiety and inability to trust that lack of recourse entails Thus while the distinction between appeal and the exercise of recourse narrowly conceived is external to the workings of the mathematical model it is important to bear in mind when considering how to implement

0.6618 a new job ask for a raise or take on more hours As Figure illustrates these actions are not as simple as increase income or increase length of employment Individuals cannot in general instantaneously change the features suggested by an explanation Some features like length of employment are inherently time-dependent Other changes may take varying degrees of time to implement Suppose for instance the decision subject could obtain credit either by increasing their income by or by increasing the length of their employment by months If it would take months of especially grueling work to secure a raise it would seem unnecessary for him to do so instead of simply waiting months to qualify On the other hand if the subject needs credit immediately and is able to change to a higher-paying job right away then we might view income as the right feature to provide the subject in an explanation Because the relationship between features and actions is time-dependent explanations that do not consider temporal aspects will fail to highlight the right features But these examples still assume a relatively direct relationship To act on explanations that instruct us to change certain feature values we need to know what

0.6611 writing down our design intent in natural language is that what we do not write says as much about our design and ourselves as what we do Leaving out crucial information could be perceived as carelessness or even a deliberate attempt to hide something By requiring that we expressed our decisions and rationale through writing it became easier to find conceptual gaps and inconsistencies either they accidental or deliberate Having identified these cases we could revisit our thinking behind these decisions gaining insights on the problem on our design process and on ourselves Commitment to a First-person Perspective Once we understand how explicitly writing down our intent makes us face our own decisions we may seek to hide behind other entities Attributing actions to the system for example without acknowledging our major role as its creators might allow us to deflect some of the blame for any negative consequences that arise from its use In that way the system becomes the agent and we fade into the background Detecting these cases was important since they gave us insights about our mindset and our own uncertainties when creating the artifact This was an important advantage of having the guiding questions frame

0.6603 argue that it is unjust and not merely inefficient for a decision about the individual to be taken when LB exists This suggests that there is a common moral reason for why decisions influenced by MB or LB are unfair We will now attempt to identify this common moral cause For this we will follow the philosophical analysis which is explicitly invoked by Binns This analysis will explain why both MB and LB cause unfair predictions and decisions It appeals to the question of responsibility which asks whether individuals are responsible for predictions others make about them that impact their well-being This responsibility may fail to obtain either because the measurements that decision-makers have the OS do not reflect peoples choices ie people are not responsible for MB or because the construct that is measured the does not reflect peoples choices or both Thus one should ask whether the OS or the is the result of choices or of circumstances individuals cannot control When MB exists the individuals who are judged in a biased manner do not control the bias and are not morally responsible for it Intuitively it is unfair that individuals are imposed costs due to factors for

0.6547 recourse is to ensure that people can reasonably expect that were they to receive an unfavorable decision they would have some way to get that decision reversed If the algorithm that recommends flipsets serves up non-compossible combinations of actions they cannot reasonably expect this On a related note recall that the bureaucratic and algorithmic decisions for which we might seek recourse are ubiquitous Thus we cannot treat recourse as something that one might seek in isolation from all the other things happening in ones life An example from the academic job market should illustrate this In many disciplines there is a stark contrast between the desirable CV of someone applying for a job at a research university and the desirable CV of someone applying for a job at a small liberal arts college Consider the case of someone who completely strikes out on the job market When they ask interviewing committees from research universities what they could do to improve their chances ie when they ask for a flipset they are told to publish more and in more prestigious venues When they ask interviewing committees from small liberal arts colleges what they could do to improve their chances they are

0.6521 may have changed in such a way that the individual who received the recommendation no longer qualifies This is frustrating and makes it difficult to plan Furthermore such scenarios are likely to crop up more and more as algorithms become embedded in a wide range of mundane decision making processes If recourse is to be employed in such contexts it needs to suggest flipsets that have indefinite temporal stability or clear expiration dates One option would be to follow the precedent set in United States law according to which ex post facto laws that disadvantage the accused are not binding Translated to the context of algorithms and recourse this would mean that if someone receives a flipset recommendation it should be valid forever or come with an expiration date Then if the person satisfies the criteria in the flipset before the expiration date they automatically gain whatever benefit they were pursuing even if the rules for others have changed in the meantime An alternative option would be to follow the precedent set in European law namely lex mitior or milder law according to which the milder rule is the one that applies In the case of the GPA cutoff sketched

0.6489 above if the admission criteria become more strict then anyone who was recommended a flipset prior to the change should be eligible under the old rules By contrast if the admission criteria become less strict then the new standards should be used for everyone In cases where there is no well-defined ordering of strictness individuals should be able to choose for themselves whether they are judged by the old or new criteria Implementing either of these options assumes that the flipset is recommended by those with enough institutional power to ensure that either the ex post facto or lex mitior rule is applied If instead the flipset is recommended by a third party such as a consulting firm or independent coach the flipset should at least be accompanied by a very clear warning that the rules may change Introducing an upper bound on costs As we pointed out above sometimes even the cheapest or least onerous way to reverse an unfavorable decision is to expensive or burdensome To handle this problem we suggest introducing an upper bound to the cost threshold when calculating a flipset If even the cheapest option exceeds this threshold the individual would receive a recommendation like

0.6399 no way to extrapolate a strategy for obtaining a low-interest loan from the counterfactual explanation that gets her to a high-interest loan Indeed she may not even know that the counterfactual explanation that tells her how to get a loan is specific to a high-interest loan instead seeing the interest rate on offer as the only option and concluding that she cannot get a better rate UNAVOIDABLE TENSIONS We have argued so far that the need to disclose a limited subset of features infuses feature-highlighting explanations with subjective choices and creates a number of challenges that makes their promise harder to realize in practice than advocates of such techniques would have us believe They also present a number of unavoidable normative tensions Decision makers start with a great deal of power over decision subjects and the purpose of explanations and the legal requirements for themis to restore some degree of power to the decision subjects Yet the fact that decision makers must by necessity withhold information creates three unavoidable tensions First in order to generate genuinely helpful explanations decision makers must be both paternalistic and privacy-invasive That is they must interfere with decision subjects autonomy to offer some back to

0.6396 RECOURSE From the perspective of recourse as a modally robust good the specific algorithmic recourse proposals fall shy in the ways described above In this section we articulate a revised approach to recourse that responds to the criticisms canvassed in the previous section Each of the subsections here addresses its correlative subsection from Section above Disjunctive instructions for flipsets Because datasets contain errors and omissions any flipset recommended to an individual might specify a set of attributes that they already satisfy In such a situation the individual would need to appeal for rectification of the dataset rather than exercise recourse narrowly conceived For this reason flipsets should come with disjunctive instructions For example We are sorry that you did not receive a favorable outcome Our model indicates that if your profile were changed in the following way attributes X Y Z you would receive a favorable outcome If you believe that your profile should already characterize you in this way you can appeal the decision by contacting relevant authority Otherwise you have until date to make these changes and then seek to exercise recourse This way of framing the flipset recognizes that the dataset may contain errors or omissions Stakeholder

0.6339 candidates for credit while those who have stayed five or more are not Likewise carrying more debt might render applicants less attractive until they start earning more income at which point additional debt might make them more attractive Decision subjects will not necessarily be able to alter the value of these features through some sudden step change Instead they may have to make incremental changes in the direction of the specified value And despite their best efforts decision subjects might struggle to hit the specified feature value their efforts could move the value of these features in the right direction but ultimately fail to get the decision subject all the way there Similarly decision subjects might lack precise control over the value of a feature making it difficult to avoid overshooting the mark when they take some action Unless the model exhibits monotonicity with respect to the highlighted features the decision subject might find herself in a worse position as she moves toward the specified value or if she exceeds it Similarly principal-reasons explanations can lose their utility without monotonicity In some cases highlighted features like length of employment do not suggest an obvious direction for improvement and indeed models

0.6336 very private information and often gets answers back that she cannot understand But it is doubtful that anyone would consider hiring a lawyer to be a loss of autonomy At the same time we would immediately recognize that furnishing lenders with detailed information and relinquishing control over decision making is not an obvious mechanism for enhancing ones autonomy Notably there are no requirements that they act in your best interest while such fiduciary obligations do apply to lawyers This is a difficult tension to resolve and may depend on the relative power of and constraints upon the decision maker rather than the quality of the explanation The burden and power to choose One of the reasons feature-highlighting explanations are so appealing is that they appear to offer complete automation whenever a decision is made an explanation can be provided without any further human intervention But this veneer of mechanization belies the fact that such explanations cannot be completely formulaic They require decisions about what to disclose and assumptions about the real world The need for partial disclosure grants new power to the decision maker Of course a decision maker by virtue of being one has always had power over the

0.6322 bankrupt them another example of appeal rather than of the exercise of recourse narrowly conceived We suggest that the only normatively acceptable conception of recourse in this scenario is one in which the family and friends are also able to take actions on behalf of the patient In an individualistic system someone might reasonably refuse to undergo medical treatment because they are worried that they will be unable to exercise recourse on their own behalf during recovery Moreover we note that this sort of case is going to be especially common among children the elderly people with disabilities people who are undocumented people who are less well-educated and so on In order to ensure that the most disadvantaged members of society can sincerely and reasonably expect to be able to get unfavorable decisions recourse must be conceived of less individualistically and more communally Strategic manipulation versus undoing unfair outcomes As mentioned earlier An implicit critique of recourse appears in papers that concern themselves with strategic manipulation of a classifier to reverse an otherwise-justified outcome Examples include the idea that if we revealed the algorithm behind search ranking entities will manipulate the algorithm to place themselves higher on the list often

0.6292 options each of which might involve some investment of time and effort Later at time t t the individual returns and expects a favorable decision to be made But now the classifier has changed and the attributes that would have given the individual success in the original classification will no longer do so For example there might be a GPA cutoff to gain entry into a computer science major at a university A student learns what the cutoff is determines how much they need to improve their GPA to gain admittance the flipset and retakes a few classes to get an improved grade However by the time their coursework is complete say a year later the threshold has increased again for example due to increased demand and their new GPA is still not sufficient for entry into the major Since the goal of recourse is to ensure that people can plan and can reasonably trust the socio-technical system that makes decisions about them this temporal dimension needs to be taken into account Qualification Sometimes perhaps often the cheapest or easiest way for someone to ensure a more favorable decision from an algorithm is just too expensive or onerous For example

0.6269 relatively recent phenomenon that has till now proceeded without being firmly embedded in a philosophical framework such as Pettits The first paper that explicitly addresses it is the work of Ustun et al They define recourse as the ability of a person to change the harmful decision of the model through actionable input variables and then present an algorithm that generates candidate changes of variables that would reverse an algorithms decision these sets of variables are called flipsets in their work Two aspects of their definition are noteworthy Firstly the word actionable is an important part of their definition By this they mean that recourse is defined in terms of features that a can be changed by the individual and b are or should be relevant to the decision that they are Thus they focus on token acts of exercising recourse but they are also concerned with the general state of enjoying systematic access to the power to reverse harmful decisions More on this ambiguity below trying to achieve For the first aspect an example is that expecting an individual to change their age or race in order to get a job is not actionable and therefore would not be

0.6267 two sets of concerns may actually have a common normative source and that any conflict between the measures is more likely to be the result of a failure to clarify what kinds of injustice are assumed to exist and are being addressed The first task is to decide what objectives they have in awarding places to applicants One criteria might be giving places to applicants most likely to succeed in their college studies This is a laudable aim but in most cases it is not and should not be the only aim They might also aim echoing the luck egalitarian perspective to avoid making decisions which would lead people to suffer disadvantages arising from circumstances outside of their control Some applicants might appear more or less likely to succeed at college due to factors like whether or not they had parents who could pay for expensive test preparation They may also actually be more or less likely to succeed after being admitted due to factors outside their control such as systemic racism or sexism in the higher education institution In such cases it would be unjust to distribute college places on the basis of features which predict success at college

0.6217 in so far as such features and such success are in part a function of circumstances that should be excluded as would be suggested by the egalitarian perspective If the college admissions panel can agree upon a set of assumptions about what factors lie outside an applicants control and the probable influence of those factors on the distribution of features in college applications they might then be in a position to formulate some kind of fairness measure to apply to their system Of course this is not an easy task and the legitimacy of any assumptions The use of this example should not be taken to condone the use of algorithmic decision-making to make such important decisions it is for illustrative purposes only as in other works eg We also note that unfortunately such systems are already in use made could rightly be challenged However assuming for the sake of argument that the college has a legitimate and accountable process by which they can reach an agreeable set of assumptions the first question they should ask is not should they use individual or group fairness Rather it is what kinds of injustice do we believe may be in operation in

0.6187 an explanation on why the loan was rejected for example due to poor credit history However such an explanation does not help the person decide what they should do next to improve their chances of being approved in the future Critically the most important feature may not be enough to flip the decision of the algorithm and in practice may not even be changeable such as gender and race Thus it is equally important to show decision outcomes from the algorithm with actionable alternative profiles to help people understand what they could have done to change their loan decision Similar to the loan example this argument is valid for a range of scenarios involving decision-making on an individuals outcome such as deciding admission to a university screening job applicants disbursing government aid and identifying people at high risk of a future disease In all these cases knowing reasons for a bad outcome is not enough it is important to know what to do to obtain a better outcome in the future assuming that the algorithm remains relatively static Counterfactual explanations provide this information by showing feature-perturbed versions of the same person who would have received the loan eg you would

0.6179 causes features to change value in the real world This might not be obvious we may struggle to identify the actions that would cause a feature value to change or change in a predictable way Recent work on actionable explanations has focused on avoiding explanations that tell people to make changes that are impossible placing the burden on decision makers to give advice that is sensitive to the actual steps that decision subjects would need to take to achieve the change in feature values Avoiding these potential explanations is a matter of identifying the lack of any possible causal mechanism in the real world that would have the necessary effect on the value of some feature Gaming is just a special case of this disconnect between feature changes and actions When a decision maker instructs someone to change certain features the decision maker will often assume that the person will take a specific desirable sequence of actions because that is the causal mechanism that the decision maker has in mind But there are often many other ways to change feature values that dont require taking these steps Highlighting certain features as those that need to change to obtain a different

0.6145 a result people will have incentives to change or hide their social relationships in order to get better credit This will harm the social contexts in which friendships flourish for the sake of credit Ironically then while in Section we argued that explanations would not be useful unless the decision maker understood facts about peoples lives beyond those considered in the model contextual integrity would suggest that the fact of decision makers knowing this information is itself harmful to autonomy But imagine a decision subject who facing an adverse credit decision is shown the complete model and finds it overwhelming Such a person may instead prefer the counterfactual explanation even if it requires the decision subject to disclose all the information necessary for the decision maker to offer an appropriately tailored set of instructions This exact scenario motivates much of the work on counterfactual explanations so we should not discount that a more informed explanation can still be autonomy-enhancing on balance In a sense this tension is reflective of a common concern in discussions of autonomy When can giving up information and agency be autonomy-enhancing For example if a person hires an attorney she outsources some important decisions gives up

0.6115 concerns albeit a highly significant and perhaps paradigmatic subset In so far as egalitarianism seeks to equalise the distribution of outcomes between such groups it might seem to be the natural corresponding normative principle that would justify group fairness However just as the principle of consistency can actually be commensurate with group fairness despite its natural affinity with individual fairness so egalitarian concerns can also be couched in terms of individual fairness despite their natural affinity with group fairness Consider that when specifying an individual fairness metric the policymaker will need to consider certain features eg exam scores and ignore others eg first language when assigning distances between pairs of individuals Those choices reflect normative assumptions which may well correspond to the egalitarian principles above This make sense if we see the notion of task-relevance as already incorporating normative aspects For instance a policymaker operationalising the luck egalitarian framework could base their determination of task-relevant similarity on norms and causal assumptions which account for the attributes for which individuals can be held responsible for In some cases they might adjust the distance metric to account for features which reflect circumstances outside the individuals control which would otherwise make them appear

0.6095 in similar fashion to define modally robust ills as ills that deliver non-robust harms in a range of counterfactual scenarios For example malevolence towards someone is a robust ill because it delivers harm in a range of counterfactual scenarios If someone harbors malevolence towards you then not only are they going to harm you in the actual world when it is easy for them but also they will go out of their way to harm you in nearby possible worlds where they face obstacles to harming you And just as robust goods can be embodied by both individuals and institutions so robust ills can be embodied by both individuals and institutions For example Kate Manne argues that misogyny is a set of institutionalized social norms and expectations and related behaviors that function to enforce patriarchal oppression Women who deviate from patriarchal norms and expectations are punished by misogynistic actions and emotional reactions whereas women who conform to such norms and expectations are rewarded A deontological argument for the value of recourse Pettits account of robust goods presupposes a consequentialist normative ethics For those more sympathetic to deontological or Kantian ethics a closely related argument may be more appealing In particular

0.6094 evaluation metrics interventions remediations etc to a particular decision That is to say courts are generally not concerned with whether a particular outcome is right but whether the process that produced that outcome was correct In administrative law there is no general duty to give reasons or explanations but decision-makers must act in line with long-established principles of good administration throughout that process In administrative law various aspects of the decision-making process are considered both discretely and together applying principles to those aspects to in theory ensure good decision-making For instance nominated decision-makers cannot delegate decisions entirely to someone else though can take advice into account Decision-makers must consider all information relevant to a decision and cannot consider any irrelevant information nor can they consider factually inaccurate information Decision-makers cannot give even the appearance of bias in making a decision nor can they unlawfully discriminate on a protected characteristic Although not giving reasons for consequential decisions can sometimes be unlawful reasons given unless inadequate are usually not themselves the basis for finding that a decision was made unlawfully though they can give insight into how a decision-maker approached a decision Judicial review instead assesses various aspects of the process for

0.6076 explanations for how to get the desired outcome in one aspect of your life may conflict with those in another We can reason about this the other way around as well From the point of view of a counterfactual explanation an applicant might be best off trying to change a number of other features besides income Yet from the perspective of the applicant increasing her income might have ancillary benefits in other parts of her life that make this change more attractive and indeed rational than those suggested by the explanation Increasing her income would grant her improved access not only to credit but to improved quality of life generally In the first case a change in feature might benefit the decision subject in one domain while hurting her in others In the second case a change in a feature might benefit the decision subject in multiple domains not just one These spillovers both negative and positive complicate the process of determining which features would be most useful to highlight in an explanation Ideally feature-highlighting explanations would allow decision subjects to avoid negative spillovers and identify opportunities for positive spillover But a decision maker will lack information about the many

0.6054 decision also implicitly relies on the belief that everything else can be held constant while making these changes In reality actions may affect multiple features simultaneously Changes in the value of one feature may also affect the value of another feature if the two features interact As Figure demonstrates whether one increases his income by finding a higher-paying job or waiting for his performance review to get his raise the action will affect both income and length of employment a separate feature in the model In the case of a job change length of employment will be negatively affected Thus increasing income may not be enough to get credit which is why point is on the left of the decision boundary In the case of waiting for a raise a smaller increase in income might be needed than the explanation would say because length of employment increases at the same time This is why point is on the right side of the decision boundary despite not increasing income as much as the explanation suggests When considered in the context of feasible real-world actions it becomes clear that features may not be independent changing one feature may impact others Insisting that

0.6004 of interest and its proxy opens up the possibility of strategic manipulation If this gap is too large or too easily exploited we may question whether the algorithm itself is valid Consider the example of an African-American job-seeker changing their name in order for their CV to be taken seriously or a female academic job seeker muting any indicators of her gender in order to get a favorable assessment by a hiring committee One can easily argue that the attributes being changed eg the name are not relevant to the task success at the job However the classifier treats these attributes as relevant Because the classifier appears to be blatantly unfair such manipulation may be justified all-things-considered In any case this determination rests on which attributes are considered relevant or irrelevant which itself can be controversial and touches on deeper arguments about the difference between the world-as-it-is and the world-as-it-should-be Changes in the classifier over time Classifiers change As more and more data is incorporated into a model the rule for classification might change its dependence on input attributes Suppose at some time t an unfavorable decision is made about an individual They request and are provided with various flipset

0.6004 of such intentions on the discriminated-against individual such as humiliation as a result of lack of respect On such accounts the decision-makers intent is key to discrimination A decision-maker with no such intent who nonetheless accidentally and unwittingly created such disparities would arguably not be guilty of wrongful discrimination even if the situation is morally problematic on other grounds Such cases often called indirect or institutional discrimination in the UK or in the US disparate impact might still count as discriminatory on a mental state account of discrimination if the failure of decision-makers to anticipate such disparities or to redress them when they become apparent is sufficiently similarly morally objectionable to the failure to treat people equally in the first instance However if such conditions do not obtain then indirect discrimination while it may be wrong may not usefully be described as an instance of discrimination at all Eidelson This line of thinking suggests a potential challenge to the notion of algorithmic discrimination If the possession of certain mental states by decision-makers is a necessary condition of a decision being discriminatory one might argue that algorithmic decision-making systems can never be discriminatory as such because such systems are incapable of

0.5986 While existing literature at first sight often seems to reason about independence in moral terms a lot of the arguments are either not backed up by moral philosophy or turn out to be purely mathematical We note that the need for enforcing independence is rarely justified from a philosophical perspective and that the two spaces philosophy and mathematics are often conflated in part also due to terminology In this paper we want to make a contribution towards resolving this ambiguity and to highlight the relation between mathematical justifications for choosing independence and the corresponding moral significance We begin by defining independence mathematically in Section and then reconstruct arguments on when independence is considered the correct fairness metric in Section We will show that these arguments while at first sight appearing to hold moral value are actually purely mathematical if taken literally However since they suggest that there are moral reasons for choosing independence we will provide a natural extension for the arguments found in the literature Section We will then argue that this natural extension is not always in line with our moral intuitions about fairness in specific cases Section We conclude in Section that the question whether independence should

0.592 other goals that a person might have in her life and the features that are relevant in those domains This also highlights an additional risk due to other life goals decision subjects may change undisclosed features unless otherwise instructed For example if a counterfactual explanation tells someone to increase her income and lower her debt but fails to mention that she should not reduce her length of employment she may have no idea that she should avoid any career change while attempting to address these other issues stumbling accidentally into point in Figure Indeed she might not even know that length of employment figured into the credit decision in the first place Thus by failing to disclose what a decision subject must not change an explanation may lead her to take an ultimately unsuccessful action Models may not have certain properties stability monotonicity and binary outcomes Those advocating in favor of feature-highlighting explanations tend to assume that the underlying models have certain properties stability monotonicity and binary outcomes Credit scoring models are routinely retrained to react to changes in the overall environment and to changes in borrowers behavioral patterns Perhaps there is another recession Or perhaps borrowers change their behavior

0.592 of those features would be overwhelming As a result both the law in the form of principal reasons and the emerging technical literature in the form of counterfactual explanations seek to produce sparse explanations that present the decision subject with only a small subset of features When opting to use feature-highlighting explanations there is no natural way to choose between principal-reasons and counterfactual explanations Yet rarely is the choice to use one method over another discussed explicitly or indeed even recognized as a choice in the first place These methods produce different explanations and serve fundamentally different goals Focusing on features that are furthest from the average value of the features in the credit-receiving or general population casts the problem of identifying principal reasons as one of identifying extreme deficiencies that would seem to rule out the applicant completely rather than near-misses that applicants might readily address before applying for credit again in the future While the former may strike us as a less attractive or sensible approach to Though they are written into the regulation it is not clear that firms actually use these methods to generate principal reasons explanation there may be good reason to favor an explanation

0.5912 rather than its mean this is a robust estimator for the population median because the median of a collection of numbers does not change significantly if even a fraction of the points are corrupted Referring back to the idea of a robust good we can think of a robust estimator as one that is valid in nearby worlds where only small amounts of data corruption exist note that the notion of near refers to the number of points that are corrupted rather than the amount of corruption counterfactual scenarios Beyond friendship Pettit argues people value a variety of other robust goods The virtue of honesty is a robust good that delivers the non-robust benefit of truth-telling in a range of counterfactual scenarios If someone is honest you can trust them to tell you the truth when they have no incentive to lie but also to tell you the truth were lying to be to their benefit Likewise the robust good of respect delivers the non-robust benefit of non-interference in a range of counterfactual scenarios According to Pettit robust goods are valuable because they are resilient enough to survive situational shifts p and thus deliver their correlative non-robust goods both as

0.5896 the wrongness of mental states or of generalisations Some of these difficulties are internal to the philosophical accounts of discrimination and others stem from the dis-analogy between human and algorithmic decision-makers If the wrongness of algorithmic discrimination does not consist in the morally suspect intentions of decision-makers or in failing to treat people as individuals then what might it consist in A more general set of egalitarian norms might provide a better footing for a theory of algorithmic fairness Egalitarianism Broadly speaking egalitarianism is the idea that people should be treated equally and sometimes that certain valuable things should be equally distributed It might seem entirely obvious that what makes discrimination wrongful is something to do with egalitarianism However this connection has perhaps surprisingly been resisted by many of the previously mentioned theorists of discrimination with one even claiming that any connection between antidiscrimination laws and equality it is at best negligible and in any event is insufficient to count as a justification Holmes Meanwhile others argue the opposite that only a direct appeal to egalitarian norms can satisfactorily account for everything that is wrong about discrimination Segall For our current purposes this debate can be safely sidestepped This is

0.5891 model clearly implies a choice to act or not act If by act one means adopt or consent to be subject to AI the public cannot be said to make such choices Toreini et al attempt to resolve this by noting that there can be dissonance between beliefs decisions and actions resulting in as if trust though this leaves unresolved the issue of why if the final result is the same trust beliefs or decisions even matter Also to the extent that accountability relies on parties being able to act eg sanction untrustworthy actions on the part of the trustee the irrelevance of action here removes a critical mechanism to facilitate better behavior see also which would otherwise feed a virtuous cycle for trust For these reasons a new model is needed to account for public trust/distrust in AI In what follows we provide a foundation upon which to construct a more appropriate understanding for this altogether different context of pervasive AI where the assessment of trustworthiness simply does not scale Toreini et al acknowledge limitations of individuals capabilities with respect to assessing ability and benevolence and propose that individuals accomplish this indirectly through assessment of the ability and benevolence

---------------------------------------------------------------------------
TOPIC 11: fairness/algorithm

---------------------------------------------------------------------------

0.8115 of sociotechnical systems We make the following contributions We review definitions of fairness and discrimination in machine learning and mechanism design highlighting historical differences in the way fairness has been defined and implemented in each Section We define several lessons that can be learned from mechanism design and machine learning in order to create an encompassing framework for decision-making Specifically we highlight the gap between fairness and welfare the potential of long-term assessment of decision making systems group versus individual assessment of fairness and the effect of human perception of fairness among others Section Finally we highlight different application domains and survey relevant works in which both mechanism design and machine learning tools have been deployed such as advertising education labor markets and the gig economy criminal justice health insurance markets creditworthiness and social networks We discuss advances and limitations of current techniques and implementations in each of these domains relating to the lessons from the previous section Section DIFFERENCES BETWEEN MECHANISM DESIGN AND MACHINE LEARNING Machine learning has been increasingly used to supplement human decisions drawing attention to biases rooted in learning from historically prejudiced data Fair machine learning often defines fairness conditions eg parity for legally protected groups

0.7473 Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness Decision-making systems increasingly orchestrate our world how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance one that is substantially complicated by the context-dependent nature of fairness and discrimination Modern decision-making systems that involve allocating resources or information to people eg school choice advertising incorporate machine-learned predictions in their pipelines raising concerns about potential strategic behavior or constrained allocation concerns usually tackled in the context of mechanism design Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity in some complex decision-making systems neither framework is individually sufficient In this paper we develop the position that building fair decision-making systems requires overcoming these limitations which we argue are inherent to each field Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making teasing out the lessons each field has taught and can teach the other and highlighting application domains that require a strong

0.7469 or optimize a pre-defined metric on such data points Thus machine learning definitions of fairness tend to ignore complex preferences long-term effects and strategic behavior of individuals On the other hand as mechanism design often assumes known preferences and more generally that information comes from a fixed and known distribution without further critique and measures utility as a proxy for equality it tends to miss systematic patterns of discrimination and human perceptions see also Section While recent works have started to address these gaps between machine learning and mechanism design approaches to fairness by embedding welfare notions in measures of accuracy and fairness and using learning algorithms to elicit preferences many open questions remain on what each field can learn from the other to improve the design of automated decision-making systems In this paper we formalize these ideas into a set of lessons that each field can learn from the other in order to bridge gaps between different theories of fairness and discrimination In doing so we aim to provide concrete avenues to address some of the limitations of machine learning and mechanism design under the acknowledgement that bridging these fields is only an initial step towards a comprehensive analysis

0.7291 both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs thus making mitigation of such biases a non-trivial task Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making Code to reproduce all our results can be found here CONCEPTS Computing methodologies Neural networks General and reference Evaluation INTRODUCTION Automated decision-making systems that are driven by data are being used in a variety of different real-world applications In many cases these systems make decisions on data points that represent humans eg targeted ads personalized recommendations hiring credit scoring or recidivism prediction In such scenarios there is often concern regarding the fairness of outcomes of the systems This has resulted in a growing body of work from the nascent Fairness Accountability Transparency and Ethics FATE community that drawing on prior legal and philosophical doctrine aims to define measure and attempt to mitigate manifestations of unfairness in automated systems Most of the initial work on fairness in machine learning considered notions that were one-shot and considered the model and data distribution to be static Recently there has been more work

0.7043 domains utilize probabilistic models for example labor market models from mechanism design often consider two-stage processes in which noisy signals provide information about whether a worker is qualified or not Importantly while these models do incorporate uncertainty the designer knows the true relationship between observed signals and true outcomes even though this relationship is probabilistic This style of analysis is less suited to deal with cases where the relationship between signals and ground truth is unknown and can only be learned about through data The lack of ground-truth information greatly complicates any analysis of the impact of a mechanism but it is precisely this lack of information that machine learning techniques are designed to handle Many of the challenges that arise during learning including data scarcity for certain groups feedback loops preference elicitation and explore-exploit trade-offs implicate serious fairness concerns By integrating lessons from machine learning on how to define and measure disparities that learning produces mechanism design can gain a deeper understanding of real-world systems Using fairness definitions as a diagnostic tool for potential harms and societal issues is a powerful application of computing as Abebe et al argue As such the various group fairness definitions from machine learning

0.7011 Fairness Is Not Static Deeper Understanding of Long Term Fairness via Simulation Studies As machine learning becomes increasingly incorporated within high impact decision ecosystems there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics This is understandable long term dynamics are hard to assess particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets To address this structural difficulty in the eld we advocate for the use of simulation as a key tool in studying the fairness of algorithms We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans college admissions and allocation of attention By analyzing how learning agents interact with these systems in simulation we are able to extend previous work showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research

0.6996 pair was seen as fair and unbiased by a majority of participants we did not observe consensus that a machine learning model was preferable to a human judge Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations CONCEPTS Human-centered computing Empirical studies in interaction design Computing methodologies Machine learning KEYWORDS Fairness Accountability Machine Learning Survey Data Science INTRODUCTION As machine learning ML is used to automate increasingly significant decisions such as predicting criminal defendants risk of recidivism activists and journalists have raised the alarm about issues of bias and a lack of fairness Researchers have responded by attempting to define fairness mathematically There are dozens of competing definitions that embed different notions of what is fair Recent work has noted that these fairness definitions address a relatively narrow set of considerations In light of this some have suggested that these definitions can be deployed to understand and address specific problems Unfortunately some key definitions of fairness are incompatible Therefore in reality building models that embed some of these definitions will require making difficult trade-offs between competing notions of fairness We conducted an empirical user study to understand attitudes about

0.6951 available at CONCEPTS Computing methodologies Simulation environments Supervised learning by classification INTRO BEYOND STATIC FAIRNESS Much of the literature on fairness in machine learning is motivated by the concern that high impact decisions made by machine-learned systems may have negative consequences for vulnerable populations eg reviewed in However much of this prior work has focused on the fairness implications of decisions made in a static or one-shot context in which long-term effects and system level dynamics are not considered Despite recent work that has shown that long-term implications can be at odds with fairness objectives optimized in the static setting long-term implications remain relatively under-studied In this work we propose that simulation studies can serve as a framework for systematically exploring the long-term implications of deploying a machine learning based decision system henceforth a learning agent Simulation studies address a gap between currently-favored evaluation of fairness policies on static realworld datasets and more recent forays into simple analytically tractable models of dynamics Simulations allow access to counterfactual information about how the data would have varied if a different data collection or decision-making policy had been in place a dimension that is missing from static datasets For example the COMPAS and

0.6916 harms caused by decision-making algorithms This overlooks that when the system hosting the algorithms optimizes its operation it may gather more inputs and outputs than those of the algorithm Therefore focusing on the algorithm may miss effects on the world that can go beyond those generated by the outputs of the algorithm actions When phenomena in the machine domain are subject to optimization unfairness can arise from optimization programs fulfilling the specification fair but not the requirements fair in the application domain For instance prediction techniques to optimize targeted advertising can create discriminatory effects and exploration strategies to optimize fair may gather inputs from the application domain that put some users unfairly at risk Discriminatory effects are not the only concern for building just systems In Jacksons terms considering only discriminatory effects constrains the requirements fair to a particular class of harms This approach risks missing other harms caused by the system when evaluating the performance of the specification fair in the environment We assume the introduction of a machine in an environment aims to improve specific phenomena The fact that this machine follows a specification fair that does not discriminate according to fair does not guarantee that this machine

0.6833 to handle cognitive blind spots such as patterns in large complex datasets This framework centers human-algorithm interactions as the locus of study and Although previous studies have used the phrase algorithm-in-the-loop they have it in the context of simulation and modeling rather than in relation to human-in-the-loop computations and human-algorithm interactions prioritizes the humans decision over the algorithms as the most important outcome An algorithm-in-the-loop perspective can inform essential sociotechnical research into algorithms Recent work related to interpretability provides one important direction where progress is already being made Future analysis should focus on how to develop and present algorithms so that people can most effectively and fairly incorporate them into their deliberative processes with particular attention to improving evaluations of algorithm quality and reducing disparate interactions This may involve altering the algorithm in unintuitive ways previous research suggests that in certain situations a seemingly suboptimal algorithm actually leads to better outcomes when provided to people as advice It will also be important to study the efficacy of different mechanisms for combining human and algorithmic judgment across a variety of contexts Most algorithm-in-the-loop settings involve simply presenting an algorithmic output to a human decision-maker relying on the person to interpret and

0.6815 Berkel et al study the validity of using certain input features in the decision-making process in order to achieve procedural fairness Finally how should the individual preferences be aggregated Even though most of the literature has followed some variant of majority rule for this Noothigattu et al and Kahng et al have argued for tools like score-based bloc voting or Borda count from voting theory for better representation of participants choices These studies have also shown the need of model explainability transparency and context-specific feature selection in improving societal fairness perceptions which mechanism design has traditionally considered as out of scope or assumed to be known leading to a recent surge in explainability and transparency studies in machine learning Future work in mechanism design can learn from such studies in challenging current assumptions about preferences perceptions and values APPLICATION DOMAINS In this section we discuss several application domains of machine learning and mechanism design to illustrate the lessons of Section underscore the complex interplay between these domains point out gaps as well as potential ways of bridging these gaps We note that many of the applications are open to critique One might object to the idea of deciding which students

0.6796 fairness literature draw attention to the ways that machine learning systems interact with and often reinforce the power structures that generate the inequities that this literature is meant to address By these accounts notions of fairness and justice can only be addressed if this constructivist feedback is taken into account Approaches such as systems dynamics have been proposed before to model and simulate some of these social dynamics Aspirationally despite the simplicity of the examples we present here we hope our framework can extend these approaches in a way that allows practitioners to integrate their machine learning procedures and draw broad lessons about how they might interact with a social environment Our open-source library ml-fairness-gym is built on the OpenAI Gym framework which simulates a Markov Decision Process MDP between agents and environments Our choice of this framing is discussed in more detail in the next section A FRAMEWORK FOR SIMULATIONS ENVIRONMENT METRICS AGENTS To implement all of our simulations we developed an open-source library ml-fairness-gym that extends OpenAIs Gym In the Gym framework agents interact with simulated environments in an alternating loop At each step the agent chooses an action which affects the environments state The environment then reveals

0.6737 and reinforcement learning This framing is flexible and puts dynamical analyses of fairness into a language that is more familiar to many ML researchers than the economic concepts highlighted in previous work Finally along with code to reproduce the results in this paper we provide a general library ml-fairness-gym for specifying new simulation environments and agents with a unified interface for easy extensibility and development at ml-fairness-gym Related Work The simulation approach that we advocate for in this paper is meant to complement several lines of work in fair machine learning First simulation complements the bulk of the fairness literature that focuses on classification in static settings eg These static approaches to fairness are often evaluated on real datasets This approach has a number of virtues chief among them being a relevance to real problems as opposed to the toy nature of simulated environments However as discussed above the scope of fairness questions that can be addressed in the static setting is limited so we believe that both sorts of investigations are valuable Secondly simulation can be used to expand analytical investigations into fairness with dynamics that have recently appeared in papers on lending resource allocation and college admissions Finally

0.6627 mechanism design tools such as large market models mean-field equilibria analysis and dynamic programming techniques seems to be a promising direction for the design of effective and fair policies in machine learning-driven systems Finally most machine learning models focus solely on algorithmic bias and are oblivious to the existence of the social bias that is coming from human agents making complex dynamic decisions as a response to the systems algorithmic decisions The interplay between social and algorithmic bias over time may in fact prove itself useful in explaining dynamic patterns of discrimination in sociotechnical systems Bohren et al introduce the discrimination theory of mis-specification and show both theoretically and empirically that contradicting patterns of discrimination against womens evaluations in online platforms can be well explained by users mis-specified bias in sequential ratings Monachou and Ashlagi build upon this theory to study the long-term effects of social bias on worker welfare inequality in online labor markets while Heidari et al also use observational learning to study the temporal relation between social segregation and unfairness MD ML Strategic agents The economists basic analytic tool is the assumption that people are rational maximizers of their utility and most principles of mechanism design are

0.6624 the problem of intersectionality occurs again in complex decision pipelines where different fairness metrics may be required yet it may be infeasible to satisfy all simultaneously the question of what is an acceptable trade-off between utility maximization and various notions of fairness persists Labor markets and gig economy Discrimination has been a perennial problem in labor markets Decades of research has shown that hiring decisions are subject to bias against disadvantaged communities More recently techniques from both machine learning and mechanism design have been brought to bear in the labor market and in particular the gig economy leading to a fresh wave of concern that the persistent discrimination found in traditional labor markets will manifest itself in new and unexpected ways In particular we focus on two use cases employee selection and employee evaluation Both of these use cases blend techniques from machine learning and mechanism design and as we will argue it is impossible to adequately deal with issues of discrimination and bias without drawing upon ideas from both fields Emerging data-driven techniques for employee selection have begun to employ techniques from machine learning to evaluate and sort candidates While some contend that quantitative tools might help to reduce

0.6601 in Section through examples In Section we present an empirical application of these insights In Section we present a step-by-step guide for algorithmic auditing estimating the causal impact of algorithmic changes on measures of inequality or welfare We close with a discussion on the importance of inequality- and power-based frameworks in algorithmic decision-making Related Work Many now-classic bodies of work study discrimination and harms caused by machine learning systems on historically disadvantaged groups in settings ranging from ad delivery to facial analysis to search engine bias and provision of public services Barocas and Selbst provide a framework for understanding the negative consequences of such automated decision-making systems For general overviews and discussions see also With a growing set of findings of algorithmic discrimination in the backdrop researchers across numerous fields have sought to formalize and define different notions of fairness as well as analyze their feasibility incompatibility and politics We direct the reader to for an overview and extensive discussions around various definitions of fairness as well as their relationship with other algorithmically-defined desiderata Our work draws on the economics literature on discrimination causal inference social choice optimal taxation and on inequality and distributional decompositions Definitions of fairness correspond to

0.6545 collaboration between these disciplines INTRODUCTION Centralized decision-making systems are being increasingly automated through the use of algorithmic tools user data is processed through algorithms that predict what products and ads a user will click on student data is used to predict academic performance for admissions into schools and universities potential employees are increasingly being filtered through algorithms that process their resume data and so on Many of these applications have traditionally fallen under the umbrella of mechanism design from auction design to fair allocation and school matching to labor markets and online platform design However recent pushes towards data-driven decision-making have brought together the fields of mechanism design MD and machine learning ML creating complex pipelines that mediate access to resources and opportunities Increasingly learning algorithms are used in the context of mechanism design applications by adopting reinforcement learning techniques in auctions or general machine learning algorithms in combinatorial optimization and transportation systems As such applications do not directly focus on fairness and discrimination they are not the central focus of this paper The growing impact of these decision-making and resource-allocating systems has prompted an inquiry by computer scientists and economists are these systems fair and equitable or do they

0.6528 in decision making However both communities must acknowledge that making the pipeline fair from a technical perspective does not mean the system is ipso facto perfect or just More interdisciplinary work is needed beyond mechanism design and machine learning to create interventions that improve access to sociotechnical systems and design algorithms for critical application domains

0.6474 a test-bed to facilitate direct comparisons of algorithms with respect to measures on a variety of datasets Our open-source framework allows for the easy addition of new methods measures and data for the purpose of evaluation We show how to use our test-bed for determining not only which specific algorithm has the best performance under a fairness or accuracy measure but what types of algorithmic interventions tend to be the most effective In addition to the impact of these algorithmic choices we examine the impact of different preprocessing techniques and different measures for accuracy and fairness that have an important and previously obscured impact on the results of these algorithms Our goal is to provide a comprehensive comparative analysis of existing approaches that is currently lacking in the literature In this paper we use the term intervention to refer to how the choice of algorithm used impacts the fairness of the overall system We are not studying causal definitions of fairness Our results Our evaluation yields the following major findings Fairness-accuracy tradeoffs depend on preprocessing Section Different algorithms tend to have slightly different requirements in terms of input how are sensitive attributes encoded Are multiple sensitive attributes supported Does the

0.646 model is then rigorously analyzed with the hope that the formal analysis provides new insights about alternative policy choices and their counterfactual effects These insights can in turn inform policymakers at a qualitative level We conclude this article by acknowledging that all mathematical models are by definition highly simplified representations of the phenomena at hand and as such it is important to understand and interpret them keeping their limitations and scope of applicability in mind We have grounded our work in a broad literature from economics so as to draw on the insights that earlier modelers have brought to this setting But we emphasize that theoretical models and their implications should never be taken as exact representations of the way complex societal processes operate and evolve As such it is important to not draw policy interpretations on the basis of such models alone

0.6413 Fairness and Abstraction in Sociotechnical Systems A key goal of the fair-ML community is to develop machine-learning based systems that once introduced into a social context can achieve social and legal outcomes such as fairness justice and due process Bedrock concepts in computer science such as abstraction and modular design are used to define notions of fairness and discrimination to produce fairness-aware learning algorithms and to intervene at different stages of a decision-making pipeline to produce fair outcomes In this paper however we contend that these concepts render technical interventions ineffective inaccurate and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems We outline this mismatch with five traps that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them Finally we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions and by drawing abstraction boundaries to include social actors rather than purely technical ones CONCEPTS Applied computing Law social and

0.6403 can produce suggested terms which could be viewed as racist sexist or homophobic image search results are gender-biased depending on the search term used and racially-biased towards Black individuals There has been an increasing focus in the research community from various disciplines on promoting and understanding fairness in algorithmic decision-making While much effort has been devoted to developing frameworks of fairness and algorithmic models to alleviate biases there is a need to understand how algorithmic fairness is perceived by people wwwpropublicaorg/article/machine-bias-risk-assessments-in-criminal-sentencing While related work has looked into how the end users and/or the general public perceive algorithmic fairness it is important to understand how the people who are developing or will soon be involved in developing algorithmic decision-making systems perceive algorithmic fairness To our knowledge perception of fairness in algorithmic decision-making of students from fields adjacent to computing has not been studied previously To explore how future developers perceive algorithmic fairness we conducted an online survey with students in fields adjacent to algorithm development who will potentially be involved in the development of an algorithmic decision-making system We presented participants with three scenarios of algorithmic decision-making systems describing different contexts and asked them to indicate their agreement regarding six statements

0.6341 There has been rapidly growing interest in the use of algorithms in hiring especially as a means to address or mitigate bias Yet to date little is known about how these methods are used in practice How are algorithmic assessments built validated and examined for bias In this work we document and analyze the claims and practices of companies offering algorithms for employment assessment In particular we identify vendors of algorithmic pre-employment assessments ie algorithms to screen candidates document what they have disclosed about their development and validation procedures and evaluate their practices focusing particularly on efforts to detect and mitigate bias Our analysis considers both technical and legal perspectives Technically we consider the various choices vendors make regarding data collection and prediction targets and explore the risks and trade-offs that these choices pose We also discuss how algorithmic de-biasing techniques interface with and create challenges for antidiscrimination law CONCEPTS Social and professional topics Employment issues Computing methodologies Machine learning Applied computing Law KEYWORDS algorithmic hiring discrimination law algorithmic bias INTRODUCTION The study of algorithmic bias and fairness in machine learning has quickly matured into a field of study in its own right delivering a wide range of formal definitions

0.6336 domain This related work provides an excellent report on various considerations that are important for model development as well as strategies for effectively engaging with agency leadership Organization of this paper We begin in the next section with some background on the model development As part of this discussion we describe both the tool that is currently deployed in Allegheny County and the competing models that are being developed The term actuarial has fallen out of fashion and has in many cases been replaced with machine learning as part of an ongoing redesign process Then in Section we investigate the predictive bias properties of the current tool and a Random forest model that has emerged in the redesign as one of the best performing competing models Our predictive bias assessment is motivated both by considerations of human bias and recent work on fairness criteria that has emerged in the algorithmic fairness literature Section discusses some of the challenges in incorporating algorithms into human decision making processes and reflects on the predictive bias analysis in the context of how the model is actually being used We discuss some of the concerns that have arisen as part of the redesign and propose

0.632 to re-embed chosen words without putting impedance on the practitioners downstream algorithms This is beneficial over further downstream debiasing algorithms that require the practitioner to change the learning algorithm or classification thresholds This paper starts by reviewing related works in Section In Section we formalize the notion of sentiment polarity and present our adversarial learning algorithm for debiasing word vectors with respect to sentiment Section evaluates the effectiveness of our method We show that our method increases fairness for sentiment applications and even toxicity prediction applications via benchmarks created in and In Section we discuss possible future directions for our work Finally we conclude this work in Section RELATEDWORK Unintended bias in NLP is often subtle and can emanate from many different sources Many researchers have focused on understanding the forms and sources of unintended bias in standard NLP systems Researchers have also made progress towards formally measuring and mitigating some of this bias at different stages in the NLP pipeline Starting with word embeddings works like have made progress towards identifying and mitigating many different types of bias with respect to gender and race However identifying and mitigating unintended bias is still a vast and complicated problem as argues

0.6297 tools and how they are used in practice which can lead to undesirable or ineffective results The LJAF a foundation that focuses on addressing societal issues through data driven approaches argues for a risk assessment tool that can be adopted by judges and jurisdictions anywhere in America and has released such a tool that has been widely used We observe that minor demographic differences in the distribution of data can lead to broad effects on statistical notions of group fairness on fairly trained machine learning models These results could impact the ways in which fair risk prediction tools are used because such results suggest the proliferation and transfer of such methods between precincts can lead to their unreliability The most related work to the proposed questions is fairness applied to transfer learning and the covariate shift problem in machine learning Covariate shift deals with situations where the distribution of data in application differs from the distribution of data in training Covariate shift is a well studied field and there are numerous methods that attempt to train supervised learning classifiers that are robust to test distribution shifts with respect to accuracy Related methods have been developed to address fairness in the

0.6185 increasing concerns about algorithmic bias and unfairness in AI systems eg see As a result significant effort in the fair ML community has been devoted to the development of algorithms toolkits and systems to aid ML development teams in assessing and addressing potential biases eg see For example Googles People AI Research group PAIR developed the open-source What-if tool to help practitioners who are not formally trained in machine learning visualize the effects of fairness metrics Microsoft has also developed the Fairlearn toolkit fairlearngithubio based on the work of and IBM has developed the AI Fairness toolkit to help ML developers assess and mitigate certain kinds of harms in ML models More recently researchers at Google developed ML-fairness-gym to simulate the potential long-term impacts of deploying machine learning-based decision systems in social environments offering ML practitioners the chance to conduct a dynamic analysis instead of a single-step or static one A different line of research has explored the use of detailed and multidisciplinary documentation techniques to enhance transparency in model and data reporting For example the Model Card approach details information such as the model type intended use cases performance characteristics of a trained ML model Datasheets focuses more on

0.6166 the electronic hardware industry provides datasheets with detailed characterizations of components performances under different test conditions By contrast despite the broad reach and impact of machine learning models there are no standard stress tests that are performed on machine learning based systems nor standardized formats to report the results of these tests Recently researchers have proposed standardized forms of communicating characteristics of datasets used in machine learning to help users understand the context in which the datasets should be used We focus on the complementary task for machine learning models proposing a standardized method to evaluate the performance of human-centric models Disaggregated by unitary and intersectional groups such as cultural demographic or phenotypic population groups A framework that we refer to as Model Cards can present such evaluation supplemented with additional considerations such as intended use Outside of machine learning the need for population-based reporting of outcomes as suggested here has become increasingly evident For example in vehicular crash tests dummies with prototypical female characteristics were only introduced after researchers discovered that women were more likely than men to suffer serious head injuries in real-world side impacts Similarly drugs developed based on results of clinical trials with exclusively male participants

0.6124 Biases in Generative Art A Causal Look from the Lens of Art History With rapid progress in artificial intelligence AI popularity of generative art has grown substantially From creating paintings to generating novel art styles AI based generative art has showcased a variety of applications However there has been little focus concerning the ethical impacts of AI based generative art In this work we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design Viewing from the lens of art history we discuss the socio-cultural impacts of these biases Leveraging causal models we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases We illustrate the same through case studies in particular those related to style transfer To the best of our knowledge this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history We hope our work sparks interdisciplinary discussions related to accountability of generative art CONCEPTS Social and professional topics Computing methodologies Artificial intelligence KEYWORDS generative art style transfer biases AI socio-cultural

0.6116 Years of Test Unfairness Lessons for Machine Learning Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over years including in education hiring and machine learning We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century exploring the cultural and social context in which different fairness definitions have emerged In some cases earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research and foreshadow current formal work In other cases insights into what fairness means and how to measure it have largely gone overlooked We compare past and current notions of fairness along several dimensions including the fairness criteria the focus of the criteria eg a test a model or its use the relationship of fairness to individuals groups and subgroups and the mathematical method for measuring fairness eg classification regression This work points the way towards future research and measurement of unfairness that builds from our modern understanding of fairness while incorporating insights from the past CONCEPTS General and reference Surveys and overviews Metrics Social and professional topics History of

0.6106 the underlying distribution is constantly shifting one could imagine incorporating that fact into the runtime eg by maintaining a sliding window where older less-representative observations are discarded RELATED WORK The algorithmic fairness literature has been rapidly expanding in breadth and depth In this section we focus on the most related works from the fairness literature and relevant works from software engineering and verification Enforcing and Checking Fairness We focus on two types of work on algorithmic fairness i works on enforcing fairness and ii works developing techniques for checking fairness We have shown that our language-based approach can capture a range of properties from the literature These include forms of group fairness which have appeared in a various forms in the literature eg Some notions of fairness like equalized odds work exclusively in the context of supervised learning where we are ensuring fairness with respect to some labeled data In this work we consider the setting in which we are observing decisions as they are being made on unseen data and therefore we are not targeting notions of fairness tied to supervised learning In Section we discussed some of the notions of fairness that our language cannot capture In the

0.6043 Recent works increasingly adapt group fairness methods inspired from machine learning to design fair voting procedures and advertising bridging the gap between the individual perspective of mechanism design methods and group-level definitions of fairness from machine learning Beyond transferring lessons from machine learning to mechanism design we argue that future design must encompass perspectives other than the purely computational one from sociological understandings of harm and power to economic discrimination and theories of justice MLMD Human perceptions and societal expectations of fairness Early studies on fairness in both mechanism design and machine learning propose various mathematical formulations of fairness and normatively prescribe how fair decisions should be made However given the impossibility to simultaneously satisfy multiple fairness notions decision-making systems need to be restricted to only selected principles of fairness a process that becomes challenging in certain applications such as criminal justice finance and lending self-driving cars and others Given such applications and their potential for harm it is essential for the chosen design and principles to be socially acceptable Thus there is a need to understand how people assess fairness and how to infer societal expectations about fairness principles in order to account for all voices in a democratic

0.603 Airbnb recently launched an initiative to study racial discrimination on their platform Crucially this body of work combines insights from economics mechanism design and machine learning to better understand how discrimination can manifest in the gig economy Criminal justice Recent popularity of the use of machine learning techniques in prescriptive settings has motivated several attempts to analyze the fairness aspects of predictive and statistical models especially in the context of a critical application domain like criminal justice Unsurprisingly relying on such models in practice can end up reinforcing underlying racial biases as it has been shown in studies about neighbourhood surveillance and recidivism prediction The latter ProPublica study has raised a heated discussion leading many to advocate that the deployed system independent of the larger criminal justice system in which it is situated is plainly unfair While that was apparent in this instance a rigorous explanation was not trivial several responses argued that their claims of discrimination were mainly caused by differences in methodology like the statistical measure of discrimination Since the deployment of predictive models in the criminal justice system is a contested idea knowledge about their potential advantages and pitfalls regarding fairness is crucial in order to perform

0.6013 Fairness Equality and Power in Algorithmic Decision-Making Much of the debate on the impact of algorithms is concerned with fairness defined as the absence of discrimination for individuals with the same merit Drawing on the theory of justice we argue that leading notions of fairness suffer from three key limitations they legitimize inequalities justified by merit they are narrowly bracketed considering only differences of treatment within the algorithm and they consider between-group and not within-group differences We contrast this fairness-based perspective with two alternate perspectives the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power We formalize these perspectives drawing on techniques from causal inference and empirical economics and characterize when they give divergent evaluations We present theoretical results and empirical examples which demonstrate this tension We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making CONCEPTS Computing methodologies Philosophical/theoretical foundations of artificial intelligence Social and professional topics Computing technology policy KEYWORDS Algorithmic fairness inequality power auditing empirical economics INTRODUCTION A rich line of work within computer science examines the differential treatment by algorithms of historically disadvantaged

0.5968 our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination CONCEPTS Human-centered computing Visualization application domains KEYWORDS human annotation data ethics race discrimination sampling bias data labeling machine learning INTRODUCTION In the last decades machine learning systems are widely spreading in different academic domains as well as many public and private sectors are increasing the exploitation of these systems Their widespread and pervasiveness is mainly driven by the exponential growth of computational power and the extensive availability of large amounts of data Supervised machine learning models are also particularly widespread and now deeply rooted in different sectors due to their usage versatility The predictive ability of supervised machine learning systems is deployed in disparate areas of application credit reliability justice system job recommendations university selection process cultural contents and purchases recommendations The key ingredient that supervised machine learning models have in common is the availability of a set of labeled data used to train the model in elaborating a response related to past events Since the known properties of the available set of data is used to create a classifier that makes predictions about new entities of the same type

0.5936 further insights into that information the participants find essential to judge and understand an algorithmic decision-making systems fairness Finally the level of academic education has a role to play in the perception of fairness and justice in algorithmic decision-making CONCEPTS Human-centered computing Empirical studies in HCI Also with the Research Centre on Interactive Media Smart Systems and Emerging Technologies Nicosia Cyprus INTRODUCTION Algorithmic decision-making is widely used for contributing to decisions affecting peoples lives Job hiring healthcare education finance and criminal justice are just a few of the examples where algorithms are taking on what previously were human decision-making tasks An algorithm is even deciding on the posts and news people will see on social media While the use of algorithmic decision-making has prospects to make decision-making more efficient and reliable concerns have been raised about the fairness and justice of such decisions Algorithmic decision-making systems do not always behave as they should making decisions that may discriminate against certain groups of people There are many examples in different domains that show the misbehavior of these systems gender discrimination has been detected in a recruitment system for reviewing and ranking applicants resumes and in resume search engines auto-complete search terms

0.5901 Balanced Neighborhoods for Multi-sided Fairness in Recommendation Fairness has emerged as an important category of analysis for machine learning systems in some application areas In extending the concept of fairness to recommender systems there is an essential tension between the goals of fairness and those of personalization However there are contexts in which equity across recommendation outcomes is a desirable goal It is also the case that in some applications fairness may be a multisided concept in which the impacts on multiple groups of individuals must be considered In this paper we examine two different cases of fairness-aware recommender systems consumer-centered and provider-centered We explore the concept of a balanced neighborhood as a mechanism to preserve personalization in recommendation while enhancing the fairness of recommendation outcomes We show that a modified version of the Sparse Linear Method SLIM can be used to improve the balance of user and item neighborhoods with the result of achieving greater outcome fairness in real-world datasets with minimal loss in ranking performance Keywords Recommender systems fairness multi-sided platform sparse linear method INTRODUCTION Bias and fairness in machine learning are topics of considerable recent research interest Pedreshi et al Dwork et al Bozdag A standard approach

0.5874 of prior knowledge when modeling data and provide a framework for inference that balances these beliefs with evidence from the data This is a powerful fairness idea we may believe a priori that a dataset should look a certain way if not for some bias In the context of a fair machine learning pipeline that considers many datasets this relates to the AutoML task of learning distributions over datasets that share global parameters In automated decision making the focus on intervention over classification suggests the more equitable deployment of machine learning when only biased data are available but also raises significant technical challenges We believe causal modeling to be an invaluable tool in addressing these challenges and hope that this paper contributes to the discussion around how best to understand and make predictions from existing datasets without replicating existing biases

0.5854 that can occur conceptualizing and addressing the problem has been challenging Our contributions We address the problem of learning fair models under mismatch in train-test distributions when either limited or no data is available from the test distribution We consider the setup of causal domain adaptation where possible shifts are expressed using causal graphs with the goal of learning models with stable performance under the specified shifts Our main contribution is to formulate the fair learning problem in this setup and provide sufficient conditions that enable estimation of model accuracy and fairness metrics in the test domain For a subset of covariate shifts and for several well-known group-fairness metrics we show that the resulting solution is worst-case optimal We operationalize the sufficient conditions in an algorithm based on a reduction to the standard fair learning problem Finally we present a case study on a medical decision-making task which demonstrates applicability of the approach RELATEDWORK Domain adaptation and fair machine learning are both widely studied problems Thus we primarily focus the discussion on literature at their intersection Fairness A number of fairness metrics have been proposed that make different normative statements on the machine learning models output see for a review

0.584 learning Software and its engineering Software libraries and repositories KEYWORDS Source code including instructions for adding your own algorithm or dataset can be found at and installed via pip install fairness INTRODUCTION As the use of machine learning to make decisions about people has increased so has the drive to make fairness-aware machine learning algorithms A considerable body of research over the past ten years has produced algorithms for accurate yet fair decisions under varying definitions of fair for goals such as non-discriminatory hiring risk assessment for sentencing guidance and loan allocation And yet we have not yet seen extensive deployment of these algorithms in the pertinent domains The primary technical obstacle appears to be our ability to compare methods effectively across different evaluation measures and different data sets with consistent data preprocessing and testing methodologies Such comparisons would not just reveal best-in-class methods they would also suggest which measures are robust and how different algorithms are sensitive to different kinds of preprocessing As pointed out by Lehr and Ohm such considerations of the data processing pipeline are not just important for efficient implementation but also have legal ramifications for the resulting automated decision-making process In this paper we present

0.5834 against the use of certain models instead of verify that models will behave fairly A final limitation to our work is that we assess Fair-MAML when there are many related training tasks to learn from In reality there may only be a few related training tasks available We leave assessing how useful Fair-MAML is on domains with only a few related training tasks to future work

0.5834 choice of objectives Such a question foregrounds how power gets allocated and what is the process that leads some groups to have more control over data in decision making processes These alternative perspectives focused on inequality and power are not intended to entirely solve the above fairness concerns but rather to elucidate them and bring to the forefront concerns that havent been adequately considered in the literature thus far In doing so we add to a recent line of work aiming to broaden discussions on the social impact of algorithmic decision-making

0.5813 Detecting discriminatory risk through data annotation based on Bayesian inferences Thanks to the increasing growth of computational power and data availability the research in machine learning has advanced with tremendous rapidity Nowadays the majority of automatic decision making systems are based on data However it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data In fact in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded Although the process of rigorous data collection and analysis is fundamental in the model design this step is still largely overlooked by the machine learning community For this reason we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set In particular our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available We empirically test

0.5781 in these domains To achieve the desired change we need descriptions of the phenomena before the machine is introduced known as domain assumptions and statements about the desired situation once the machine is introduced to the domain known as requirements A specification is a set of requirements providing enough information for engineers to implement the machine typically describes phenomena shared between the machine and application domain A program derived from the specification is a description of a machine If implemented correctly programs satisfy the specification If the specification is derived correctly programs generate phenomena that attain the desired effects in the application domain ie they fulfill the requirements Jacksons explicit treatment of the application domain and its interaction with the machine helps us to project known problems with algorithms to a systems view It enables us to distinguish problems due to badly derived requirements description of the problem from those due to badly derived specifications description of the solution and those due to badly implemented programs how solutions are implemented Application domain does not refer to a class of applications like health or banking domain but to actors and things in the environment where the machine is introduced The focus

0.5741 have been defined for example using counterfactuals or preference between group or aggregate-level models We plan to investigate fair domain adaptation under broader notions of fairness We have motivated the approach on healthcare tasks due to the importance of ensuring reliable model performance under distribution shifts in this domain We note that the approach is more broadly applicable to other domains involving high-stakes decisions CONCLUSION AND FUTURE WORK In absence of data from new environments in which a machine learning model will be deployed giving performance guarantees regarding predictive performance and fairness is challenging We find that methods to address distribution shift while controlling for decay in accuracy can result in fairness violations As a countermeasure we show that it is possible to obtain accurate and fair predictors for widely-studied fairness definitions and under a large class of shifts particularly prevalent in healthcare tasks Future work includes studying fair domain adaptation under parametric assumptions on shifts adaptation for counterfactual definitions of fairness and finite sample properties of the estimators We hope that the problem setup presented here will enable further work at the intersection of fairness and causal inference

0.5729 detection diffusion mitigation fairness sexism racism homophobia INTRODUCTION The growing ubiquity of algorithms in society poses questions about their social political and ethical consequences One of the issues research focuses on is algorithmic bias which denotes the deviation of the algorithmic results from specific social expectations based on epistemic or normative reasons Prior research has shown that algorithmic bias might result in unfair or discriminative decisions and statements initiating a multilevel debate on the ethical use of algorithms Under that framework researchers decision makers and institutions try to answer the following questions What definitions of fairness and discrimination are appropriate and under what conditions At which part of an algorithm does bias emerge and in what form What are the actual consequences of biased algorithms and who is accountable for them How can researchers and decision makers mitigate the detected bias Problem Statement This study investigates bias in word embeddings a set of natural language processing techniques for the mapping of words into numerical vectors These vectors can then be used for the improvement of the predictions and inferences of other machine learning models Previous work has proven that word embeddings contain bias and researchers have already developed methodologies for

0.5705 behavioral sciences Computing methodologies Machine learning KEYWORDS Fairness-aware Machine Learning Sociotechnical Systems Interdisciplinary INTRODUCTION On the typical first day of an introductory computer science course the notion of abstraction is explained Students learn that systems can be described as black boxes defined precisely by their inputs outputs and the relationship between them Desirable properties of a system can then be described in terms of inputs and outputs alone the internals of the system and the provenance of the inputs and outputs have been abstracted away Machine learning systems are designed and built to achieve specific goals and performance metrics eg AUC precision recall Thus far the field of fairness-aware machine learning fair-ML has been focused on trying to engineer fairer and more just machine learning algorithms and models by using fairness itself as a property of the black box system Many papers have been written proposing definitions of fairness and then based on those generating best approximations or fairness guarantees based on hard constraints or fairness metrics Almost all of these papers bound the system of interest narrowly They consider the machine learning model the inputs and the outputs and abstract away any context that surrounds this system We contend

0.5681 who those systems are actually being used is huge Our paper aims to bridge that gap Fourth we provide an account of observed biases within policing institutions and their transfer to technical systems that operate within these formal structures Policing is an institution historically known for its problematic practices reflected in its data and systems By observing the link between historical practice and technical solutions we avoid the ripple effect trap identified by Selbst et al while at the same time demonstrating the importance of analysing sociotechnical systems in the context that they are designed developed and deployed In the absence of accountability mechanisms and publicly available data our research points to the value in focusing on the institutional and human aspect of machine learning in the public sector This paper proceeds as follows Section will provide a closer look at the current status of predictive policing in New Delhi Section describes our methodology for conducting research and field work Section provides insights from our field work and walks the reader through current data practices Section provides key points of analysis and Section concludes with reflections learnings and recommendations and Section reflects on limitations and future research CURRENT STATE OF

0.5679 that seek to achieve observational parity can lead to disparities on the relevant counterfactual metrics and may further compound inequities in initial treatment assignment The counterfactual approaches to learning evaluation and predictive fairness assessment introduced in this paper provide more accurate and relevant indications of model performance

0.5649 Biases in word embeddings Biases in Sentiment Analysis Artificial Psychophysics Two-alternative forced choice task INTRODUCTION Recent artificial intelligence models have shown remarkable performance in a variety of tasks that were once thought to be solvable only by humans With such promising results companies and governments begun deploying such systems for increasingly critical tasks including job candidate screening justice system decisions and credit scoring Due to training with data that might contain biases however deep learning models inadvertently fit those biases and create decisions that discriminate against gender and other protected statuses If we were to find those biases in humans we could interrogate them and determine whether such biases have occurred Several researchers have attempted to develop methods for detecting biases in AI models but these methods are specific to the task eg data see or type of model hindering their potential adoption Here we entertain the idea of using Experimental Psychology to develop novel and coherent methods for probing AI systems Experimental Psychology has a very rich tradition of treating human consciousness as a black box developing and extracting potential biases from subjective judgements in behavioral tasks We hypothesize that we can adapt these methods to uncover biases in

---------------------------------------------------------------------------
TOPIC 12: health

---------------------------------------------------------------------------

0.8449 a dataset with more than M comments in videos of channels and with more than M video and channel recommendations Importantly our recommendations do not account for personalization We analyze this large dataset extensively We look at the growth of the IDW the Alt-lite and the Alt-right throughout the last decade in terms of videos likes and views finding a steep rise in activity and engagement in the communities of interest when compared with the media channels Moreover comments per view seem to be particularly high in more extreme content reaching near to comment for every views in Alt-right channels in Sec We inspect the intersection of commenting users across the communities finding they increasingly share the same user base Analyzing the overlap between the sets of commenting users we find that approximately half of the users who commented on Alt-right channels in also comment on Alt-lite and on IDW channels Sec We also find that the intersection is not only growing due to new users but that there is significant user migration among the communities being studied Users that initially comment only on content from the IDW or the Alt-lite throughout the years consistently start to comment on Alt-right

0.8336 on the website It could be for example that new users who join the website go on to consume content from all three communities To better address this question we find users who did not comment in Alt-right content in a given year and track their subsequent activity Notice that we do not have the users entire activity history and thus we track their activity only in the channels whose videos we collected For four time brackets we track four sets of users those who only commented on videos of the Alt-lite or the IDW those who did so only for videos on the Alt-lite those who did so only for videos on the IDW and those who commented only on videos of the media channels Then for subsequent years we track the same users Notice that when users are tracked for one year they are not eligible for selection in upcoming years We consider these users to be exposed if they commented on light mild or severe Alt-right videos The results for this analysis are shown in Fig We show the percentage of users we managed to track that were exposed The number of users tracked and exposed at

0.8291 The top figure for each column shows the Jaccard Similarity and the bottom one shows the Overlap Coefficient Column b in Fig shows the similarity measures for a community with itself a year before which here we name self-similarity We find that the retention of users among the three communities is growing with time for both metrics However for media channels we find that the Jaccard similarity is plateauing since and that the overlap coefficient only recently started to grow perhaps due to the sharp increase in commenting users since Commenting users from the communities of interest seem to go back more often than those in media channels Column c in Fig shows the pairwise similarity between the three communities Notably in the Jaccard Similarity between the Alt-lite and the IDW reached almost which is more than the self-similarity between the two communities Moreover the Overlap Coefficient of the Alt-right with the Alt-lite and the IDW is high reaching around in This means around half of the users who commented in Alt-right channels commented in the other communities Lastly column d in Fig shows the similarity of the three communities with the media channels We have that the Jaccard similarity

0.8078 in the communities of interest and show results about their growth in the last years setting the stage to more in-depth analyses in later sections Tab shows the most viewed YouTubers for each of the communities and for the media channels and Figure shows information on the number of videos published channels created likes views and comments per year as well as several engagement metrics Recent rise in activity Figs ae show the rise in channel creation video publishing likes views and comments in the last decade The four latter are growing exponentially for all the communities of interest and for the media channels Noticeably the rise in the number of active channels is much more recent for the communities of interest than for media channels as shown in Fig a In mid for example out of the of the media channels were active posted their first video while less than of the Alt-lite Alt-right and IDW channels had done so This growth in the communities of interest during may also be noted in Fig i which shows the CDF of number comments per videos and can also be seen between early and late in Figs fg which show the

0.8005 number of likes and views per video respectively Notice that the number of likes and views is obtained during data collection and thus it might be that older videos from those channels became popular later Altogether our data corroborates with the narrative that these communities gained traction in and fortified Donald Trumps campaign during the presidential elections Engagement A key difference between the communities of interest and the media channels is the level of engagement with the videos as portrayed by the number of likes per video comments per video and comments per view shown in Figs i and respectively For all these metrics the communities of interest have more engagement than the media channels Although media channels have more views per video as shown in Figs g these views are less often converted into likes and comments Notably Alt-right channels have since become the ones with the highest number of comments per view with nearly comment per views by Dormant Alt-right Channels Although by approximately the same number of channels of all three communities had become active as it can be seen in Fig a the number of videos they published by the Alt-right was low before This can

0.7912 communities looking closely at the users who commented on them M p er Y ea r a Commenting Users Alt-right Alt-lite IDW Media ac ca rd b Self-Similarity c Similarity among Communities Alt-right IDW Alt-right Alt-lite Alt-lite IDW d Similarity with Media O ve rl ap C o C om m en se r Figure In a the number of unique commenting users per year in the top plot and the CDF of comments per user for each one of the communities in the bottom plot In bd we show two similarity metrics Jaccard and Overlap Coefficient for different pairs of sets of commenting users across the years In b these pairs are the sets of users of each community in subsequent years In c these pairs are the sets of users of each one of the communities of interest In d these pairs are the sets of users of the communities compared with the users who commented in media channels USER INTERSECTION We begin our in-depth analysis of users who commented on the channels of interest by analysing the intersection between the users in different channels and communities In that context we use two set similarity metrics the Jaccard

0.7845 Fig we find a substantial difference between the IDW and the Alt-lite Whereas in Sec we find that the intersection between them both and the Alt-right are similar here we see that users who initially commented only on IDW channels constitute a much less significant percentage of the Alt-right consumer base in upcoming years For all levels of exposure at all times the number of exposed users that can be traced back to commenting exclusively on IDW channels is around times lower So while in of users who were lightly exposed can be traced back to users who commented on Alt-lite channels in previous years only can be traced back to IDW channels Overall in both analyses users who consumed only IDW channel seem to behave more similarly to the users in the media channels Yet as we see in Sec the intersection between the Alt-lite and the IDW is increasing with time which means this population is becoming less significant The experiments performed show that not only the commenting user bases are becoming increasingly similar as shown in Sec but that systematically users who commented only on IDW or Alt-lite content go on to comment on Alt-right channels This

0.7815 each step may be found in Appendix C Consider for example users who on commented only on IDW or Alt-lite content users as shown in the subplot in the first column and the first row By around were lightly exposed and roughly severely or mildly so which amounts to approximately users in total From the ones who in commented only on Alt-lite or IDW videos users as shown in the last column of the first row approximately of them were exposed more than users altogether We also find that media channels present lower exposure rates as can be seen in the last row of the figure The difference is particularly large for the last three time brackets Less than of users in media channels were mildly or severely exposed against to for Alt-lite or IDW users and roughly were lightly exposed versus approximately for Alt-lite or IDW users When teasing apart users that commented only on Alt-lite or only on IDW content we find that not only users who commented only on IDW get less exposed but increasingly less so The same applies to the media channels For example the exposure rates of users who watched only Alt-lite second row

0.772 user base A it e or I D Start Start Exposure Light Mild Severe Start Start A it e I D M ed Figure We show how users migrate towards Alt-right content For users who consumed only videos in the communities indicated by the labels in the rows Alt-lite or IDW Alt-lite IDW and Media we show the chance that they go on to consume Alt-right content We consider three levels of exposure light commented in to Alt-right videos mild to and severe Each column tracks users on a different starting date Initially their exposure rates are as they did not consume any Alt-right content As time passes we show the exposure rates in the y-axis for each of the years in the x-axis Line widths represent confidence intervals USER MIGRATION In the previous section we showed that the commenting user bases among the IDW the Alt-lite and the Alt-right are increasingly similar and the effect is stronger than for media channels This indicates that there is a growing percentage of users consuming extreme Alt-right content on YouTube while also consuming content from other milder communities Alt-lite/IDW Yet it does not per se indicate that there is a radicalization pipeline

0.7278 for searching by name or screen name and an intelligence discovery tool that presents links referencing the geographic region of interest from the last hour A drop down menu gives access to posts captured by automated searches The use logs show that the Corvallis Police Department did not access the results of the automated searches As we see from Figure DigitalStakeout seems to inconsistently access all social media platforms except for Twitter From colleagues l Aug Sep N ov O D Ja n Feb M ar Apr M ay n l Aug Figure Number of posts per week in the DigitalStakeout search logs from the Corvallis Police Department categorized by social media platform total in brackets at the Brennan Center we understand that the access to the Facebook API was pulled for all social-media monitoring platforms in Spring of Description of the search logs The data furnished by the Corvallis Police Department that we study here consists of spreadsheets in comma-separated form broken into groups corresponding to different sets of search terms LE Law Enforcement Terms Terror Report Narcotics The explanation accompanying the records request was that these are the results of search terms and according to the Corvallis Police

0.7207 this section we discuss how the insights of our analyses shed light into our research questions We also talk about the limitations and potential implications of this work How have these channels grown on YouTube in the last decade The three communities studied sky-rocketed in terms of views likes videos published and comments particularly since coinciding with the presidential election of that year as shown in Sec However this seems to be the case not only for these communities but also for the larger channels in the media group A key difference between the communities and media channels lies in the engagement of their users The number of comments per view seems to be particularly high for extreme content Sec and users in all three communities are more assiduous commentators than in the media channels Sec To which extent do users systematically gravitate towards more extreme content We find that the commenting user bases for the three communities are increasingly similar Sec and considering Alt-right channels as a proxy for extreme content that a significant amount of commenting users systematically migrates from commenting exclusively on milder content to commenting on more extreme content Sec We argue that this finding provides

0.697 to be collected with Twitters place search The argument to a place search is either an ID or a place name A place ID is a precise identifier for a unique geographic region Given an ID Twitters place search will return Tweets with geotags in that region tagged with a place within that region or from a user with a profile location within that region if the Tweet has no place or coordinate geotag On the other hand a place name is imprecise and Twitter will match that name to any geographic region that closely matches it It appears that the LE Terms search was configured with a descriptor relating to Benton County as the LE Terms data set contains Tweets with place and profile locations in Benton City Washington and Bentonville Arkansas Further the LE Terms data set includes many retweets which in the past had geotags but no longer do The current Twitter API will not pull retweets through a place search The poor configuration of the LE Terms search and change in the Twitter API has prevented us from being able to reproduce DigitalStakeouts geographical query for the LE Terms search A comparative dataset of Corvallis Geotagging

0.6941 to become notice that all lines begin at because initially they did not consume any Alt-right content users at each year can be traced back to users who initially watched content from other communities In other terms for each year we calculate of the users who are exposed ie who watched Alt-right videos which percentage belongs to each one of the sets of tracked users we just described The results for this analysis are shown in Fig We find that these users are a considerable fraction of the Alt-right commenting audience In for all kinds of exposure roughly of commenting users can be traced back from cohorts of users that commented only on Alt-lite or IDW videos in the past This can be seen in the first row of the plot Moreover we can observe that consistently users who consumed Alt-lite or IDW content in a given year go on to become a significant fraction of the Alt-right user base in the following year This number is much more expressive than the number of users which came from media channels in the last row which never surpasses for any level of exposure Looking at the second and third row of

0.6885 Auditing Radicalization Pathways on YouTube Non-profits as well as the media have hypothesized the existence of a radicalization pipeline on YouTube claiming that users systematically progress towards more extreme content on the platform Yet there is to date no substantial quantitative evidence of this alleged pipeline To close this gap we conduct a large-scale audit of user radicalization on YouTube We analyze videos posted on channels which we broadly classified into four types Media the Alt-lite the Intellectual DarkWeb IDW and the Alt-right According to the aforementioned radicalization hypothesis channels in the IDW and the Alt-lite serve as gateways to fringe far-right ideology here represented by Alt-right channels Processing M comments we show that the three channel types indeed increasingly share the same user base that users consistently migrate from milder to more extreme content and that a large percentage of users who consume Alt-right content now consumed Alt-lite and IDW content in the past We also probe YouTubes recommendation algorithm looking at more than M video and channel recommendations between May/July We find that Alt-lite content is easily reachable from IDW channels while Alt-right videos are reachable only through channel recommendations Overall we paint a comprehensive picture of user

0.686 search logs that originate from Twitter as Twitters API allows us to sample comparative data sets as we describe in Sections and We only analyze data that is available on the date that we collect comparative samples Table tweets users available available tweets users Terror Narcotics LE Terms Table Available data for analysis We only use data that is publicly available on the date that we collect comparative samples Geography of the DigitalStakeout Tweets In conversation with the Corvallis Police Department we learned that DigitalStakeout was calibrated to search social media posts originating from Benton County Oregon where Corvallis is located but returned results outside of this county notably from Benton County Washington We examined the geotags of the Tweets and profile locations of the corresponding users All the Tweets in the Narcotics and Terror Report sets are geotagged with coordinates which lie within the mile radius of Corvallis We presume that for these searches DigitalStakeout is limiting the search of Twitter to Tweets with coordinate geotags in this region Note that Tweets can also be tagged with a place which is a more general geographic region On the other hand the Tweets in the LE Terms data set seem

0.6749 or only IDW third row content are much more similar for those tracked in first column than for those tracked in last column For users who were tracked in around were exposed in both scenarios while for those tracked in this difference grew farther apart Alt-lite vs IDW The previous study suggests that the pipeline effect does exist and that indeed users systematically go from milder communities to the Alt-right However it does not give insight into how expressive the effect is in terms of what part of the Alt-right user base has gone through it We address this question by tracking users exactly as we did before and then analyzing what percentage of exposed A it e or I D Light Exposure Mild Exposure Start Start Start Start Severe Exposure A it e I D M ed Figure We show how expressive the tracked users are in terms of the Alt-right user base Each row shows a different condition for tracking users and each column shows a different level of exposure Each line corresponds to users tracked at a different starting date in the x-axis and the y-axis shows the percentage of the total Alt-right commenting users they went

0.6716 Similarity AB AB and the Overlap Coefficient AB min A B Notice that the overlap coefficient is particularly useful to compare communities of different sizes For example a small subset of a large set may yield low Jaccard Similarity but will necessarily yield an Overlap Coefficient of Column a of Fig characterizes commenting users The top plot shows the absolute number of commenting users per year while the bottom one shows the CDF of the number of comments per user per community It is interesting to compare these plots with that of Fig e as we can see that the communities of interest have many more highly active commenters This supports the hypothesis that users who consume content in the communities of interest are more engaged than those who consume the content from the media channels Notice also that although the Alt-right commenters have on average fewer comments than those in Alt-lite or the IDW the community is much younger as discussed in Sec and thus it is hard to tell whether their users are less engaged In columns bd of Fig we consider the intersection between the commenting users of the IDW the Alt-lite the Alt-right and media channels

0.6711 social networks Across the globe individuals are skeptical of traditional media vehicles and growingly consume news and opinion content on social media In this setting recent research has shown that fringe websites eg chan and subreddits eg /r/TheDonald have great influence over which memes and news are shared in large social networks such as Twitter YouTube is extremely popular especially among children and teenagers and if the streaming website is actually radicalizing individuals this could push fringe ideologies like white supremacy further into the mainstream A key issue in dealing with topics like radicalization and hate speech is the lack of agreement over what is hateful or extreme A work-around is to perform analyses based on communities large sets of loosely associated content creators here represented by their YouTube channels For the purpose of this work we consider three communities that have been associated with user radicalization and that differ in the extremity of their content the Intellectual Dark Web IDW the Alt-lite and the Alt-right While users in the IDW discuss controversial subjects like race and IQ without necessarily endorsing extreme views members of the Alt-right sponsor fringe ideas like that of a white ethnostate Somewhere in the middle

0.6689 active in addition to the month period for which we have no DigitalStakeout data such a gaps in time between search log files To most conservatively represent the possible input accessed by DigitalStakeout we down-sample the set of historical tweets to those with time-stamps between the first and last time stamps in a given search log for the Narcotics search This is imperfect as there may be times during the creation of a search log in which the DigitalStakeout software is not active or in which the Twitter API is down Reverse engineering keywords In order to reverse engineer the keywords used by DigitalStakeout we compare the the presumed input to DigitalStakeout to the output from DigitalStakeout Tin The set of Tweets obtained with the same presumed geographical filter over the same period of time as the DigitalStakeout Narcotics search as described in Section Tout The set of Tweets in the DigitalStakeout Narcotics search log that are still publicly available Let P be the set of words that appear only in DigitalStakeout Tweets that is words that appear in a Tweet of Tout but not in a Tweet of Tin Tout P is the set of possible keywords If the

0.6632 content These users are a significant fraction of the Alt-right commenting user base This effect is much stronger than for the large traditional and alternative media channels we collected Sec Lastly we take a look at the impact of YouTubes recommendation algorithms running simulations on recommendation graphs Our analyses show that particularly through the channel recommender system Alt-lite channels are easily discovered from IDW channels and that Alt-right channels may be reached from the two other communities Sec This is to our best knowledge the first large scale quantitative audit of user radicalization on YouTube We find strong evidence for radicalization among YouTube users and that YouTubes recommender system enables Alt-right channels to be discovered even in a scenario without personalization We discuss our findings and our limitations further in Sec We argue that commenting users are a good enough proxy to measure the user radicalization as more extreme content seems to beget more comments Moreover regardless of the degree of influence of the recommender system in the process of radicalizing users there is significant evidence that users are reaching content sponsoring fringe ideologies from the Alt-lite and the Intellectual Dark Web BACKGROUND Contrarian communities We discuss three of YouTubes

0.6546 media channels as a sanity check to capture general trends among more mainstream YouTube channels We summarize the dataset collected in the Tab Data collection was performed during the of May and the collection of the recommendations between May-July Table Overview of our dataset Channels Video rounds Videos Video Comments Channel Rounds Commenting users Channel b Videos Published M M c Like Count Alt-right Alt-lite Intellectual Dark Web Media M B d View Count M e Comment Count Likes/Video g Views/Video Videos Pub i Comments/Video Comments/View Figure On the top row figures ae for each community and media channels we have the cumulative number of active channels that posted at least one video of videos published of likes views and of comments In the bottom row we have engagement metrics accumulated over time figures g i and and the of videos published zoomed in the range on the y-axis figure Notice that for comments we know only the year when they were published and thus the CDFs granularity is coarser years rather than seconds The raw numbers of views likes videos published and more are shown in Appendix C THE RISE OF CONTRARIANS We present an overview of the channels

0.6516 social media post is accompanied by a set of keywords that match or closely match a word in the Tweet This seems to employ Twitters keyword search which is more general than exact keyword matching eg searching Twitter for hop will return Tweets containing the word hopped but not hope We cluster keywords into keyword variant groups if they are variants of each other such as rock rocked rocking rocks We use the simplest version in the group as a root representative although it may not be a formal linguistic root There are known keywords across variant groups listed in Table that appear in the meta-data of the Narcotics dataset The known keywords explain of the available Narcotics Tweets We aim to uncover keywords that explain the remaining Tweets and develop a process that would reliably identify keywords should such meta-data not be available A comparative dataset of historical Tweets To understand how DigitalStakeout identifies Tweets in their Narcotics search we collected the historical Tweets geotagged within the mile radius of Corvallis over the same time period as the DigitalStakeout data using Twitters Premium API The DigitalStakeout search logs suggest that there are time periods when the searches are not

0.6516 Tweeters To understand the demographics of the users whose Tweets appear in the DigitalStakeout data we collect a sample of Tweets geotagged within the mile radius of Corvallis OR matching the inferred geographical constraint for the Narcotics and Terror Report DigitalStakeout search results We were restricted to doing so with the public Twitter API as our our interest in studying the demographics of the account users violates Twitters Premium API agreements Twitters public API geotag filter takes as input an input polygon and returns any Tweet whose geotag intersects that polygon The geotag of a Tweet is either a point eg a GPS coordinate or a polygon rectangular bounding a city state or country for example For our filter we used the smallest rectangle encompassing Benton Co OR We refiltered the collected tweets to those whose geotags were points within Corvalliss mile radius and matching the behavior of the Twitter Premium API geotag filter we infer was used to curate the DigitalStakeout data for the Narcotics and Terror Report searches We collected Tweets between March and May From this we sampled a set of Twitter accounts to use in our comparative analysis each account was selected for this final set

0.6456 the former as here we ignore the possibility of the random walker being in a video we are not aware of Overall we find that in the channel recommender system it is easy to navigate from the IDW to the Alt-lite and vice-versa and it is possible to find Alt-right channels From the Alt-lite we follow the recommender system times approximately out of each times we will have spotted an Alt-right channel as seen in Fig a In the video recommender system Alt-right channels are less recommended but finding Alt-lite channels from the IDW and IDW channels from the large media channels in the media group is also feasible Considering the sheer amount of views the channels in the Alt-lite the IDW and the Alt-lite these percentages although low may result in a very significant number of views towards fringe content This process may also be amplified when taking personalization into account Notice that we depict the two graphs in which we performed our experiments in Appendix E DISCUSSION We performed a through analysis of three YouTube communities the IDW the Alt-lite and the Alt-right inspecting a large dataset with millions of comments and recommendations from thousands of videos In

0.6419 random walker to go towards it Notice that the scenario for the channels is more realistic and we give more weight to the conclusions drawn there The two aforementioned metrics at each step given different starting conditions are shown in Fig for channel and video recommendations For channel recommendations we have that the reachability of Alt-right channels is of approximately for the simulations starting from Alt-lite for IDW channels Moreover starting from an IDW channel users have approximately of chance of being in an Alt-lite channel at the next step and in steps there is of chance that the user has found at least one Alt-lite channel Starting from the media channels reachability of IDW channels is of and of slightly less than for Alt-lite channels These can be seen on the bottom row of Fig a For video recommendations reaching Alt-right channels from other communities is less likely From the Alt-lite reachability is of around Going from the IDW to the Alt-lite is more difficult the reachability is roughly More relevant though starting from media channels the reachability of IDW and Alt-lite channels is of around and respectively It is worth recalling that this experiment is less realistic than

0.6285 in the form of links to social media posts that are created by individuals of the Terror Report Tweets and of the LE Terms Tweets are labeled English-language by Twitter An Audit of a Social-Media Monitoring Tool via Log Files FAT January Barcelona Spain Table Reverse-engineered and known keywords for the Narcotics search Necessary Root Root Root Root snow face trip waste hop cheese burger gang high bag cook hustle line jack dope rip party treat blow smoke blast load bowl fried wreck rock crystal bake Likely Root Root Root pie indica pot mash burn zone dank keg bud hip malt fade jam melt angel bang addict deal roll Missed Known Root broken yay hookup stuck munchies stash track tweed Bold words are roots of known keywords Frequency is the number of available Narcotics Tweets implies the corresponding Tweets are now deleted Missed Known words are those that were not discovered by reverse engineering many of whom associate their account with their real identity and formulating our research questions we embarked on gaining IRB approval for our research We did so prior to pulling comparative data sets through the Twitter API Discussions with colleagues regarding this work received mixed opinions

0.6274 Department all the search terms are preset proprietary lists of terms DigitalStakeout searches for The columns of each spreadsheet include URL and TIME From July through early September the Narcotics search logs include keywords for each social media post We describe this in more detail in Section We note that for LE Terms and Terror Report in many weeks exactly search results are listed Figure Indeed in each of these weeks the search logs seem to indicate that the search process collects social media posts until results are returned then deactivates for the remainder of the week The vast majority of the social media posts are unprotected arising from public accounts so we believe DigitalStakeout to only l Aug Sep N ov O D Ja n Feb M ar Apr M ay n l Aug Figure Number of Tweets per week in the DigitalStakeout search logs from the Corvallis Police Department categorized by search term set total in brackets be accessing publicly available data We posit that those accounts that are currently not publicly available were made protected in the time since the posts were collected by DigitalStakeout Herein we focus on the subset of social media posts in the

0.6086 the same time Park et al developed a mixed methods approach to understand how Facebook use corresponded to clinical scales for depression In Coppersmith et al used self-reported disclosures of depression diagnosis on Twitter I was diagnosed with depression on to classify individuals suffering with depression contrasting their language with those who do not self-report such diagnoses De Choudhury et al also sought to identify new mothers who might be suffering from postpartum depression using Facebook and Twitter data After these works researchers began to replicate extend generalize and improve on these findings and in different cultural contexts and social media sites beyond just English-speaking Twitter From these seminal works on depression new studies have investigated new psychiatric disorders new social network platforms and new modalities Research has examined other disorders such as post-traumatic stress disorder anxiety schizophrenia eating disorders and suicidal ideation Work also now explores the symptomatology of mental disorders such as the severity of mental illness and stress connected to mental health Datasets too have expanded to social networks other than Twitter and Facebook like SinaWeibo Instagram Tumblr and Reddit Modalities other than text are now analyzed for their signals of mental health status Automated image analysis

0.5943 phenomenon is significant both in terms of the percentage of the users tracked as in Fig and in terms of the total Alt-right commenting user base as in Fig We present the raw numbers associated with these figures in Appendix D THE RECOMMENDATION ALGORITHM In this section we inspect the impact of YouTubes recommendation algorithm Unfortunately we have only a snapshot of the recommender system which does not take into account personalization Thus it is hard to reach significant conclusions on what was the role of the recommender system in the radicalization process we depicted in Sec Yet we argue that analyzing these data is relevant for it is a blueprint of how the influence of the recommender Table Percentage of edges in-between communities in the recommendation graphs normalized per weight Video recommendations are in bold Rows indicate the source of edges columns indicate their destination IDW Alt-lite Alt-right Media Other IDW Alt-L Alt-R Media A t Start Alt-lite Start IDW Start Alt-right Alt-right Alt-lite Intellectual Dark Web Media reachability Start Media Steps O th er s Steps Steps Steps C an n el s a A t Start Alt-lite Start IDW Start Alt-right Start Media Steps O th er

0.5791 between the IDW and the Alt-lite and the media channels is not so different from the similarity between these communities and the Alt-right This is a subtle finding On one hand it means that individuals in these communities make up a significant portion of the massive media channels we collected which gather billions of views These communities do not exist in a vacuum but are part of the existing online information environment On the other it shows that the Alt-right a group of channels with order of magnitudes fewer views subscribers and comments are actually on par with these large channels Inspecting the Overlap Coefficient however we get a different view there we have that the communities overlap more with themselves than with the media channels particularly since However in there is a sharp growth in the similarity with media channels A hypothesis for this is that as these channels grew more popular as previously discussed in Sec they became more mainstream These analyses take us one step further in understanding the communities being studied We again see that their users are more engaged and notably find that the IDW the Alt-lite and the Alt-right increasingly share the same commenting

0.5772 in the data collection and weights are normalized so that outgoing edges of each node sum up to The percentage of edges between communities normalized by their weight is shown in Tab for channel and video recommendations For channel recommendations we have that media channels are recommended scarcely by the communities of interest In fact there are more edges flowing out of media channels towards Altlite/IDW channels than the other way around Alt-lite and IDW channels recommend channels from the same community around of the time and recommend each other around Alt-L to IDW and IDW to Alt-L of the time Alt-right channels are only recommended by Alt-lite channels For video recommendations there is a high prevalence of recommendation to videos we were not able to track more than of outgoing edges from all communities pointed towards the Other node We also find that media channels are more often recommended in this setting for all communities while the Alt-lite and the IDW recommend each other roughly of the time Lastly Alt-right videos are not significantly recommended here Given these graphs we experiment with random walks The random walker begins in a random node chosen with chance proportional to the number

0.5685 media monitoring surveillance police demographics keywords audit INTRODUCTION Law enforcement use of social media monitoring software has been in the news for several years and usually it is not good news The ACLU of Northern California reported that MediaSonar used by the Fresno Police Department encouraged police to track BlackLivesMatter and related hashtags to identify threats to public safety After it was revealed that MediaSonar marketed itself as a way for police to avoid the warrant process Twitter cut off the companys access to their enterprise API Twitter also cut SnapTrends API access after the release of details of law enforcement use of their software SnapTrends closed shop shortly thereafter Geofeedia was notably used during the Freddie Gray uprisings to arrest protesters directly from the crowd aided by social media posts and face recognition technology shortly after this revelation from the ACLU of Northern California Facebook Twitter and Instagram all revoked API access from Geofeedia Both SnapTrends and Geofeedia are known to have enabled undercover accounts that befriend Facebook super-users in order to bypass users privacy settings During a trial period of DigitalStakeout an agent of the Oregon Department of Justice searched for BlackLivesMatter discovered that an Oregon DOJ attorney

0.5665 of each patient in the training set on a specific test point is plotted as a point in that patients column Influence of training points is bounded in the high privacy setting red dotted line Privacy Level Average Survived Influence Average Died Influence Most Helpful Group Most Harmful Group Influence None Died Survived Low Survived Survived High Survived Survived Table Group influence summary statistics of training data by class label in all privacy levels for all test patients Privacy changes the most helpful group the patients who died minority to the patients who survived majority DP learning minimizes the helpful influence of minority groups resulting in worse utility influence variance and is entangled with the poor privacy-utility tradeoff Influence-Fairness Tradeoff We approximate the collective group influence of different ethnicities in the training set on the test loss in Fig and Table We show that group privacy results in white patients having a more significant influence both helpful and harmful on test patients in the high privacy setting DISCUSSION On Utility Robustness and Trust in Clinical Prediction Tasks Poor Utility Impacts Trust While some reduced utility in long tail tasks are known the extreme tradeoffs that we observe in Table are

0.5656 be seen in the in Fig while media and Alt-lite channels had published nearly of their content the Alt-right had published a bit more than This is not because the most popular channels did not yet exist out of the current top Alt-right channels accumulating approximately M views had already been created by Moreover it is noteworthy that many of the channels now dedicated to Alt-right content have initial videos related to other subjects Take for example the channel The Golden One number in Tab Most of the initial videos in the channel are about working out or video-games with politics related videos becoming increasingly occurring The growth in engagement metrics such as likes per video and comments per video of the Alt-right succeeds that of the IDW and of the Alt-lite resonating with the narrative that the rise of Alt-Lite and IDW channels created fertile grounds for individuals with fringe ideas to prosper Although our data-driven analysis sheds light on existing narratives on the communities of interest it is still impossible to determine from these simple CDFs whether there is a radicalization pipeline To do so in the following two sections we dig deeper into the relationship between these

0.5618 obtained in Table Top YouTube channels with the most views per each community and for media channels Alt-right Views Alt-lite View Intellectual Dark Web Views Media Views James Allsup M StevenCrowder M Powerful JRE B vox B Black Pigeon Speaks M Rebel Media M JRE Clips M gq magazine B Thulean Perspective M Paul Joseph Watson M Prager University M vice news B Red Ice TV M Mark Dice M The Daily Wire M wired magazine B The Golden One M Sargon of Akkad M The Rubin Report M vanity fair M AmRen Videos M Stefan Molyneux M ReasonTV M the verge M NeatoBurrito Productions M hOrnsticles M Jordan Peterson Videos M glamour magazine M The Last Stand M MILO M Bite-sized Philosophy M business insider M Millennial Woes M Styxhexenhammer M Owen Benjamin M huffington post M Mark Collett M OneTruthLife M Agatan Foundation M today i found out M Australian Realist M No Bullshit M Essential Truth M cbc news M Jean-François Gariépy M SJW Central M Ben Shapiro M the guardian M Prince of Zimbabwe M Computing Forever M YAFTV M people magazine M The Alternative Hypothesis M The Thinkery M joerogandotnet M big think M Matthew

0.5618 nurse screen alerts and make a phone call The RRT nurse would call front-line physicians to confirm sepsis diagnoses and there was a clear directive that the nurse could not independently diagnose or treat sepsis Sepsis Watch is not a diagnostic device and was never intended to drive clinical care Sepsis Watch identified patients for further evaluation and the attending physician caring for an individual patient made the final diagnostic determination to start treatment for sepsis The Sepsis Watch workflow was designed to rapidly identify patients requiring treatment for sepsis Risk scores are calculated every hour and patients with a high risk score above are displayed in red At this risk score threshold the positive predictive value of the model was Sepsis criteria are evaluated every five minutes and patients who meet sepsis criteria are displayed in black Patients meeting sepsis criteria and high risk patients are displayed at the top of the Triage page The RRT nurse is instructed to call the ED attending physician to discuss every high risk or septic patient If the attending physician confirms that the patient does not require treatment for sepsis the patient is moved to the Screened page If the attending physician

0.5606 Whose Tweets are Surveilled for the Police An Audit of a Social-Media Monitoring Tool via Log Files Social media monitoring by law enforcement is becoming commonplace but little is known about what software packages for it do Through public records requests we obtained log files from the Corvallis Oregon Police Departments use of social media monitoring software called DigitalStakeout These log files include the results of proprietary searches by DigitalStakeout that were running over a period of months and include social media posts In this paper we focus on the Tweets logged in this data and consider the racial and ethnic identity through manual coding of the users that are therein flagged by DigitalStakeout We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region however our sample size is too small to determine significance Further the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region with an apparent higher representation of Black and Hispanic people We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police

0.5493 the first two weeks of the Sepsis Watch pilot innovation team staff conducted one-on-one training sessions in the hospital to support RRT nurses during shifts Sepsis Watch was rolled out to a small group of RRT nurses and formal and informal lines of communication were established between the nurses clinical experts and innovation team staff After a month feedback from this small group of RRT nurses was presented to the governance committee and prompted a handful of workflow changes To reach ED attending physicians the Sepsis Watch team presented at multiple faculty meetings ED physicians did not directly see the Sepsis Watch interface so communication focused on the role of the RRT nurse and the incoming phone calls inquiring about treatment for sepsis There was a lack of awareness during the first few weeks of the pilot and additional communication and messaging was promoted by both the ED medical and nursing directors Throughout the design and development of Sepsis Watch RRT nurses and ED physicians were valued and involved as expert clinical professionals Sepsis Watch was meant to support clinicians by ensuring that nothing slipped through the cracks The ways in which professional expertise shaped the impact of Sepsis Watch

0.5405 significant evidence that there has been and there continues to be user radicalization on YouTube and our analyses of the activity of these communities Sec is consistent with the theory that more extreme content piggybacked on the surge in popularity of IDW and Alt-lite content We show that this migration phenomenon is not only consistent throughout the years but also that it is significant in its absolute quantity Noticeably the findings related to this research question make the implicit assumption that commenting users are a good enough proxy for radicalization and that comments in YouTube channels are supportive of the videos they are associated with We established the validity of these assumptions as follows First the sheer number of comments and high prevalence of comments per views in Alt-right videos suggest that commenting users are a population worth studying especially when in Sec we found that Alt-right channels have a very high percentage of comments per view Secondly during the three week annotation period it was noted that the number of opposing comments is rather small as we found by manually checking randomly selected comments for each community of interest finding that only could be interpreted as criticisms to the

0.5338 search is active for all the periods of time that cover T in and keywords are matched consistently then a keyword must be in P Unfortunately data is rarely perfect We find that of the known keywords yay broken trip tracks are not in P Broken is in Tweets of T in Tout yay trip and tracks are each in Tweet Tout This could be explained by the Twitter API or DigitalStakeouts services being down during this time period and not making data available for collection at the time of DigitalStakeouts collection That a Tweet with the word broken is missed times is not surprising as broken is overall a very high frequency word indeed broken is contained in of the available Narcotics Tweets Each Tweet in the Narcotics set contains at least one word of P For a Tweet in the Narcotics set that contains exactly one from P we presume is the keyword that returned this Tweet We call the set of all such words in P the set of necessary keywords and denote it N Given perfect data and exact keyword matching all words in N must be keywords For more general keyword matching but otherwise perfect

0.5321 was tweeting support and wrote a memo describing the posts as possible threats towards law enforcement the agent who wrote the memo was later found to be in violation of state law The usefulness of social media monitoring has been called into question Conarck reports that social media monitoring in Jacksonville FL by Geofeedia included largely protected free-speech activity and useless miscellanea Relevant to the monitoring of social media in Corvallis OR in February an individual was arrested for Tweets threatening a shooting on the Oregon State Universitys Corvallis campus However the Tweets were not discovered through surveillance of social media but through an anonymous tip line Indeed our work echoes that of Conarck uncovering that DigitalStakeout uses simple keyword search at least on the topic of Narcotics and that almost all the keywords have benign drug meanings that uncover useless miscellanea Police increasingly utilize social media A survey of over US police departments found that of agencies had used social media in some capacity to notify the public recruit employees gather intelligence manage reputations or other The survey found that of agencies had used social media tools to further criminal investigations Further a report by the Brennan Center for

0.5316 individuals of the Alt-lite deny to embrace white supremacist ideology although they frequently flirt with concepts associated with it eg the Great Replacement globalist conspiracies Present work In this paper we audit whether users are indeed becoming radicalized on YouTube and whether the recommendation algorithms contribute towards this radicalization We do so by examining three prominent communities the Intellectual Dark Web the Alt-lite and the Alt-right More specifically considering Alt-right content as a proxy for extreme content we ask How have these channels grown on YouTube in the last decade To which extent do users systematically gravitate towards more extreme content Do algorithmic recommendations steer users towards more extreme content We develop a data collection process where we i acquire a large pool of relevant channels from these communities ii collect metadata and comments for each of the videos in the channels iii annotate channels as belonging to several different communities and iv collect YouTube video and channel recommendations We also collect traditional and alternative media channels for additional comparisons We use these as a sanity check to capture the growth of other content on YouTube rather than trying to obtain similar users in other channels These efforts resulted in

0.5232 AUROC across all shifted test sets to quantitatively measure the utility tradeoff We measure the privacy-utility tradeoff based on the difference in performance metrics as the level of privacy increases The average performance across years is used because it incorporates the performance variability between each of the years due to dataset shift Results for AUPRC for MIMIC-III can be found in Appendix Both our AUROC and AUPRC results show extreme utility tradeoffs in health care tasks Both metrics are commonly used to evaluate clinical performance of diagnostic tests Imaging Utility Metrics For the NIH Chest X-Ray experiments the task we experiment on is multiclass multilabel disease prediction We average the AUROC across all disease labels For the MNIST and FashionMNIST vision baselines the task we experiment on is multiclass prediction labels for both where we evaluate using accuracy Health Care Tasks Have Steep Utility Tradeoffs We compare the privacy-utility tradeoffs in Table DP-SGD generally has a negative impact on model utility The extreme tradeoffs in MIMIC-III mortality prediction and NIH Chest X-Ray diagnosis exemplify the information DP-SGD looses from the tails because the positive cases are in the long tails of the distribution There is a and drop in the

0.5218 year-olds use Twitter compared to only of The data omit Oregon State Police which has jurisdiction over the Oregon State University campus those over In a college town like Corvallis this issue will be compounded Finally we note that the Twitter users represented in Table are gathered purely based on geotags and that geotags will pick up Tweets from users who are simply visiting the area and not resident in Corvallis KEYWORDS In order to understand how the social media posts are being identified by DigitalStakeout we attempt to reverse engineer the search We do so only for the Narcotics search Of the Tweets that are still available not deleted in the Terror Report data set contain videos or images and contain urls one Tweet contains only an image Given the limited and type of data and the likelihood that this search is not simply defined by a keyword search we are not able to explain how the Terror Report is generated For LE Terms as previously noted we are unable to reproduce the geographic filter The Narcotics dataset includes partial meta-data that suggests a simple keyword search is being employed for the first months of Narcotics search results each

0.5169 Privacy-robustness tradeoffs may also be high cost in health care as the data distribution is constantly changing in response to new conditions clinical practice shifts and underlying EHR systems changing Privacy-fairness tradeoffs are perhaps the most pernicious concern in health care as there are well-documented prejudices in health care Importantly the data of patients from minority groups also often lie even further in the data distribution tails because lack of access to care can impact patients EHR presence and leads to small sample sizes of non-white patients In this work we investigate the feasibility of using DP methods to train models for health care tasks We characterize the impact of DP learning in both linear and neural models on accuracy robustness and fairness First we establish the privacy-utility tradeoffs within two health care datasets NIH Chest X-Ray data and MIMIC-III EHR data as compared to two vision datasets MNIST and Fashion-MNIST We find that DP models have severe privacy-utility tradeoffs in the MIMIC-III EHR setting using three common tasks mortality long-length of stay LOS and an intervention vasopressor onset Second we investigate the impact of DP on robustness to dataset shifts in EHR data Because medical data often contains dataset

0.5071 method of surveilling citizens perhaps differentially it poses new challenges for how to define individual group membership and subsequently measure aggregate levels of surveillance among different sub-populations DESCRIPTION OF THE DATA The search logs used in our audit contain the results of automated searches defined by DigitalStakeout rather than the police department They consist of links to social media posts with some additional meta-data over a period of months with a month gap see Figure Also furnished by the Corvallis Police Department were additional use logs documenting officer-initiated inquiries in DigitalStakeout Our IRB denied our request to address research questions towards these additional use logs However the use logs show that the Corvallis Police Department used DigitalStakeout infrequently and did not access the results of the automated searches defined by DigitalStakeout DigitalStakeout did not respond to our request for a demonstration but the Corvallis Police Department did describe the system to us DigitalStakeout is provided as a subscription software that the Corvallis Police Department accesses through a web portal It provides three main ways to navigate social media all within the predefined geographic region a map of the region with pins corresponding to recent posts of interest a search box

0.5069 radicalization on YouTube CONCEPTS Human-centered computing Empirical studies in collaborative and social computing KEYWORDS Radicalization hate speech extremism algorithmic auditing INTRODUCTION Video channels that discuss social political and cultural subjects have flourished on YouTube Frequently the videos posted in such channels focus on highly controversial topics such as race gender and religion The users who create and post such videos span a wide spectrum of political orientation from prolific podcast hosts like Joe Rogan to outspoken advocates of white supremacy like Richard Spencer These individuals not only share the same platform but often publicly engage in debates and conversations with each other on the website This way even distant personalities can be linked in chains of pairwise co-appearances For instance Joe Rogan interviewed YouTuber Carl Benjamin who debated with white supremacist Richard Spencer According to Lewis this proximity may create radicalization pathways for audience members and content creators Examples of these journeys are plenty including content creator Roosh Vs trajectory from pick-up artist to Alt-right supporter and Caleb Cains testimony of his YouTube-driven radicalization The claim that there is a radicalization pipeline on YouTube should be considered in the context of decreasing trust in mainstream media and increasing influence of

0.5042 channels that have content from these creators and with a similar spirit to also belong in this category To distinguish between the Alt-right and the Alt-lite read and It is important to stress the difference between civic nationalism and racial nationalism in that case Please consider the Alt-right label only to the most extreme content You are encouraged to search on the internet for the name of the content creator to help you make your decision The annotation process lasted for weeks In case they disagreed they had to discuss the cases individually until a conclusion was reached Interannotator agreement was of We ended up with IDW Alt-lite and Alt-right channels Media We also collect popular media channels These were obtained from the mediabiasfactcheckcom For each media source of the categories on the website Left Left-Center Center Right-Center Right we search for its name on YouTube and consider it if there is a match in the first page of results Some of the channels were not considered because they had too many videos and we were not able to retrieve them all which is important because our analyses are temporal In total we collect channels that way We use these

0.5038 Department We chose DigitalStakeout as a case study because it is a social media monitoring software package that was not reported to be subject to API restrictions by social media platforms as MediaSonar SnapTrends and Geofeedia were is still actively used and had the largest number of listed subscribing agencies in the Brennan Center report Initially these requests were not made with a specific research question in mind but more generally seeking to understand the use of social media monitoring software As part of the public records request we asked for logs of searches that have been input into DigitalStakeout and debug logs produced by DigitalStakeout Several departments have claimed criminal investigatory material exemptions to public records laws for which we are still seeking research exemptions to that exemption and at least two agencies did not have records to release Oregon DOJ did not subscribe to DigitalStakeout after their trial run and now reports a policy of not subscribing to social media monitoring software and the Yakima Police Department reports that their officers did not use the software and no longer subscribe The Corvallis Police Department did furnish logs in the form of csv files which consist of links to

0.4995 much worse than expected Machine learning can only support the decision making processes of clinicians if there is clinical trust If models do not perform well as or better than Figure Group influence of training data by class label in no privacy A and high privacy B settings on test patients with highest influence variance In the no privacy setting patients who died have a helpful influence despite being a minority class High privacy gives the majority group the most influence due to the group privacy guarantee White Test Patients Privacy Level Average White Influence Average Black Influence Most Helpful Ethnicity Most Harmful Ethnicity None White White Low White White High White White Black Test Patients Privacy Level Average White Influence Average Black Influence Most Helpful Ethnicity Most Harmful Ethnicity None Black White Low White White High White White Table Group influence summary statistics across all privacy levels for white majority and Black minority training patients on both white and Black test patients in MIMIC-III Privacy changes the most helpful group from Black patients to the majority white patients and minimizes their helpful influence This needs careful consideration as the use of ethnicity is still being investigated in medical practice

---------------------------------------------------------------------------
TOPIC 13: social-sciences/ethics

---------------------------------------------------------------------------

0.8636 Towards a more representative politics in the ethics of computer science Ethics curricula in computer science departments should include a focus on the political action of students While ethics holds significant sway over current discourse in computer science recent work particularly in data science has shown that this discourse elides the underlying political nature of the problems that it aims to solve In order to avoid these pitfalls such as co-option whitewashing and assumed universal values we should recognize and teach the political nature of computing technologies largely through science and technology studies Education is an essential focus not just intrinsically but also because computing students end up joining the companies which have outsize impacts on our lives At those companies students both have a responsibility to society and agency beyond just engineering decisions albeit not uniformly I propose that we move away from strict ethics curricula and include examples of and calls for political action of students and future engineers Through such examples calls to action practitioner reflections legislative engagement direct action we might allow engineers to better recognize both their diverse agencies and responsibilities CONCEPTS Social and professional topics Codes of ethics Computing education Accreditation Socio-technical systems KEYWORDS

0.8426 From Ethics Washing to Ethics Bashing A View on Tech Ethics from Within Moral Philosophy The word ethics is under siege in technology policy circles Weaponized in support of deregulation self-regulation or handsoff governance ethics is increasingly identified with technology companies self-regulatory efforts and with shallow appearances of ethical behavior So-called ethics washing by tech companies is on the rise prompting criticism and scrutiny from scholars and the tech community at large In parallel to the growth of ethics washing its condemnation has led to a tendency to engage in ethics bashing This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards self-governance schemes or stakeholder groups The misunderstandings underlying ethics bashing are at least threefold a philosophy and ethics are seen as a communications strategy and as a form of instrumentalized cover-up or façade for unethical behavior b philosophy is understood in opposition and as alternative to political representation and social organizing and c the role and importance of moral philosophy is downplayed and portrayed as mere ivory tower intellectualization of complex problems that need to be dealt with in practice This paper argues that the

0.7888 Whose Side are Ethics Codes On Power Responsibility and the Social Good The moral authority of ethics codes stems from an assumption that they serve a unified society yet this ignores the political aspects of any shared resource The sociologist Howard S Becker challenged researchers to clarify their power and responsibility in the classic essay Whose Side Are We On Building on Beckers hierarchy of credibility we report on a critical discourse analysis of data ethics codes and emerging conceptualizations of beneficence or the social good of data technology The analysis revealed that ethics codes from corporations and professional associations conflated consumers with society and were largely silent on agency Interviews with community organizers about social change in the digital era supplement the analysis surfacing the limits of technical solutions to concerns of marginalized communities Given evidence that highlights the gulf between the documents and lived experiences we argue that ethics codes that elevate consumers may simultaneously subordinate the needs of vulnerable populations Understanding contested digital resources is central to the emerging field of public interest technology We introduce the concept of digital differential vulnerability to explain disproportionate exposures to harm within data technology and suggest recommendations for future ethics

0.7831 then discuss current issues in tech ethics primarily its insular focus on techno-solutionism that continues to prioritize computer science expertise and center the system itself in ethical fixes as well as a promotion of the ideal of ethical unicorns or tech saviours ie technologists with shallow socio-technical understanding intent on playing the primary role in delivering complete solutions Historical Retrospective on Education Norms In starting with computer science as a discipline broadly there is a heavy focus on what Eden identifies as three paradigms technocratic rationalist and scientific At its simplest the technocratic paradigm might be described as an engineering or programmatic approach which centers the skills to build computer programs the rationalist paradigm might be described as a mathematically theoretical approach focusing more heavily on a priori knowledge about the underlying mathematical reality of computer systems and the scientific paradigm might be best characterized by its focus on empiricism seeking to more deeply understand the behaviors of computer programs These three paradigms shape how computer science operates through enculturation in computer science education and in industry cultures at both the level of attitudes and values and at the level of behaviors and practices While there are often debates on

0.7794 public life including the state and extending to activism and civil society pg In their work on relational service models use the term civics similarly p p and offer a more in-depth exploration of technological determinism such as the difference between hard and soft determinism See for a list of some projects Digital media may also promote learning of civics concepts which is the use of civics in In this sense the politics of civic engagement are related to the work of computer scientists In this section I have reviewed literature on ethics in computer science literature professional ethics codes ethics education critiques of ethics in computer science and civics in computer science We see that current ethics courses face tension in whether to be integrated in the curriculum or stand-alone how to engage engineers and how to broaden the narrow frame of professional ethics The political and societal engagement offered by liberal arts and civic education present these tensions in an alternative light particularly given recent critiques of ethics in computing POLITICS AND ETHICS My argument rests on a number of normative commitments I ground my argument in social justice as operationalized by the capabilities approach I take a

0.7587 of authority As Peters and Wendland argue studying up indicates a relative direction between the researcher and subject to study someone who has what Edward Said referred to as the relative upper hand in terms of the amount of agency and authority they have in a given context Naders provocation came at a time when the social sciences were undergoing what historians have called the epistemological revolution of the s whereby the predominant post-war model of social science as an objective and value-neutral enterprise modeled after the natural sciences was called into question These developments occurred amidst high-profile controversies such as Project Camelot which brought to light the various ways that scholarly practice in the social sciences was deeply intertwined with larger social and political agendas both at home and abroad These movements pushed anthropologists to critically examine the political nature of their work and to reflect on the blindspots they had developed while working in a dominant tradition that generally preferred studying the underdog While the call to study up was a particularly influential provocation Nader was part of a larger movement within anthropology in the late s and early s which called for the discipline to develop methods

0.7555 heart rate or facial expression can say little about a persons emotional state while for others these forms of data are key indicators of such The current paradigm around affective computing was sparked largely by the work of MITs Rosalind Picard whose book Affective Computing argued for emotion as a topic worthy of examination by computer scientists Picards approach focused on as noted physiological signals such as heart rate and blood flow as proxies for emotions and sought to translate these signals into data a computer could find legible and tractable for analysis treating human emotional expression as another form of digitizable information Scholars in social computing and critical HCI contested this approach almost immediately noting such an informational reading systematically ignores a second set of concerns which focus on emotion as it is interactionally and culturally constituted This interactional approach to the digital mediation of emotional expression and interaction laid out in a series of papers by critical HCI scholars including Kirsten Boehner Phoebe Sengers Katherine Isbister and Kia Höök among others emphasizes the centrality of supporting human emotive interaction through a diverse array of digital technologies instead of focusing narrowly on the sensing tracking quantification and analysis of

0.7518 contract of specialization is using those skills for the betterment of all Indeed he suggests professional curricula should reflect the constrictions of practitioners action such development particularly in applied sciences and reflection of practitioners on their organizational settings p In Computer Science In engineering and particularly in computer science and its sub-field human-computer interaction civics takes on a number of meanings Civics in this literature is used with regard to the development of explicit civic tech digital civics or the new part of civil society which exists online and in the sense of education the culture of engineers Civic tech So-called civic tech has arisen as an alternative to the capital-driven domain of Silicon Valley The rise of big data prompted discussion of how regulation might encourage Data-Driven Innovation in the building of civic infrastructure the computational tools produced by various forms of government From interviews with government officials hackers and community groups in Atlanta evokes various understandings of civic tech Through a series of speculative prototypes elicit themes for civic tech like Mediated Civics Computational Civics and Proxied Civics They take a pluralistic view of civics which I follow To them civics are the structures practices and experiences of

0.7498 inherent in the existence of the products themselves the funding of work on fair machine learning systems which positively obscures deeper questioning around the broader impacts of those systems on society On the other hand the technology communitys criticism and scrutiny of instances of ethics washing often borders into the opposite fallacy which we call ethics bashing This is a tendency common amongst social scientists and non-philosophers to trivialize ethics and moral philosophy by reducing more capacious forms of moral inquiry to the narrow conventional heuristics or misused corporate language they seek to criticize Equating serious engagement in moral argument with the social and political dynamics within ethics boards or understanding ethics as a political stance which is antithetic to instead of complementary to serious engagement in democratic decision-making is a frequent and dangerous fallacy The misunderstandings underlying ethics bashing are at least three-fold a philosophy and ethics are seen as a communications strategy and as a form of cover-up or façade for unethical behavior b the role and importance of moral philosophy is downplayed and portrayed as mere ivory tower intellectualization of complex problems that need to be dealt with in practice and c philosophy is understood in opposition

0.7498 this space where its technical artifacts have a global reach yet its methods of analysis concepts values and assumptions reflect only a narrow subset of the social world that it finds itself embedded in Thus in a way as a discipline is inflicted with a crisis of representation that is revealed by the narrow scope of its techniques and the modes of analysis that shape the discipline The continuation of this narrow disciplinary approach which reproduces the aforementioned mechanisms of disciplinary exclusion is not only costly but also harmful If as a discipline continues this trend we need to reflect on the consequences of not engaging with other disciplines and in particular the social costs of not engaging with HSS To put it differently what are the values interests and goals that guide HSS and are lacking in If we follow Habermas HSS is framed around humanistic goals of emancipation participation and respectful inclusion of all stakeholders present or absent powerful or not These are thus the priorities that the field will fundamentally neglect once they disengage with HSS Such a disassociation would be tragic given that these are all considerations that represent key ideals commitments and values for responsible

0.7478 on artificial intelligence robotics and computer science more broadly However BET has also come under sustained critique from a number of quarters including from proponents of what might be best termed hybrid theories of emotion mixing elements of the experiential evaluative and motivational traditions Both James Russell and Lisa Feldman Barrett have vigorously critiqued Basic Emotion Theory arguing that while core affect is an important component of emotional experiences emotion itself emerges out of human evaluative and experiential assessments of affective states in particular social contexts Scarantino and de Sousa note that philosophical and psychological theories understanding emotion as a form of evaluative assessment and as felt experience are increasingly intertwined in hybrid approaches with the former now identifying emotions as evaluative perceptions with a distinctive phenomenology and the latter identifying emotions as evaluative feelings with a distinctive intentionality s p Prinzs perceptual theory of emotion is one such hybrid approach treating emotions as evaluative perceptions grounded both in physiological affective changes and in the particular social context of the object or person being perceived Sociologist Arlie Russell Hochschild who developed the concept of emotional labor likewise presents a hybrid theory articulating emotions as a mix of physiological and social

0.7458 Algorithmic Realism Expanding the Boundaries of Algorithmic Thought Although computer scientists are eager to help address social problems the eld faces a growing awareness that many well-intentioned applications of algorithms in social contexts have led to cant harm We argue that addressing this gap between the fields desire to do good and the harmful impacts of many of its interventions requires looking to the epistemic and methodological underpinnings of algorithms We diagnose the dominant mode of algorithmic reasoning as algorithmic formalism and describe how formalist orientations lead to harmful algorithmic interventions Addressing these harms requires pursuing a new mode of algorithmic thinking that is attentive to the internal limits of algorithms and to the social concerns that fall beyond the bounds of algorithmic formalism To understand what a methodological evolution beyond formalism looks like and what it may achieve we turn to the twentieth century evolution in American legal thought from legal formalism to legal realism Drawing on the lessons of legal realism we propose a new mode of algorithmic thinking algorithmic realism that provides tools for computer scientists to account for the realities of social life and of algorithmic impacts These realist approaches although not foolproof will better equip

0.7417 as extensions of classical discussions of ethics This argument proceeds in four parts First I review literature on the topics of ethics civics and employee responsibility Second I offer an argument for why we should cover political engagement in addition to ethics Third I explore how to implement such a change and offer recommendations Fourth I consider critiques one might have for my approach LITERATURE REVIEW Here I conduct a literature review that consists of three parts First I review ethics in computer science and education Second I review current critiques of ethics Third I establish the responsibilities of engineers and review discussions of civics In conducting this review I used the ACM digital library IEEExplore and Google Scholar with terms such as ethics civics politics and activism Ethics in Engineering Ethics is the branch of knowledge or study dealing with moral principles Computer science already engages with some versions of ethics Most university degree programs offer courses or components on ethics The Accreditation Board of Engineering and Technology ABET requires at least an awareness of ethics In Computer Science Let us begin with Moor whose widely cited paper What is Computer Ethics established through example computer ethics as its

0.7409 You Cant Sit With Us Exclusionary Pedagogy in AI Ethics Given a growing concern about the lack of ethical consideration in the Artificial Intelligence AI field many have begun to question how dominant approaches to the disciplinary education of computer science and its implications for AI has led to the current ethics crisis However we claim that the current AI ethics education space relies on a form of exclusionary pedagogy where ethics is distilled for computational approaches but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking This results in indifference devaluation and a lack of mutual support between and humanistic social science HSS elevating the myth of technologists as ethical unicorns that can do it all though their disciplinary tools are ultimately limited Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in training and explore evidence for the practical mechanisms through which this exclusion occurs We then propose a shift towards a

0.7389 doubtful its moral value becomes for society Ethics washing and ethics bashing are instrumental understandings of ethics in that both positions or tendencies envision or experience ethics as a means to an end and nothing more Further what is at stake in recent controversies around the weaponization of ethics rhetoric are competing thinner and thicker moral conceptions of technology companies role the former arguably being promoted through narrow instrumental understandings of the role of ethical work the latter arguably being promoted through greater participatory democracy and activism Yet this understanding obscures the potential role of ethics within a thicker conception of technology policy The narrower the lens one uses to look at an ethical problem the narrower and more limited the response one is willing to offer to address it As will be argued in what follows it is important to maintain a critical outlook on the instrumentalization of ethics in technology settings while also recognizing and respecting its moral worth as an exercise and mode of inquiry capable of expanding our horizons and thickening our moral commitments This paper has three goals First it aims to articulate the weaknesses of both the ethics washing and ethics bashing fallacies explaining

0.7343 social constructivist approach and complicate a simple narrative of technological determinism eg I disagree with and who excluded from their analysis courses covering extra-organizational implications of computing technologies particularly as Wajcman evokes p I follow actor-network theory in emerging the agency of students and future engineers in their associations with each other their employers technological artifacts etc I use the term politics both in terms of civic participation and theory following Winner who describes politics as the arrangements of power and authority in human associations as well as the activities that take place within those arrangements p I follow in defining ethics primarily as social phenomena and not as primarily philosophical abstractions p which accords with ethics as both personal and societal I call for greater political engagement in ethics to increase the representation of other histories So does Hoffman with regard to antidiscrimination Brey calls for computer technology itself to be an object of moral analysis Benjamin p quotes as saying we have to decentralize our idea of where solutions and decisions happen where ideas come from This argument proceeds in five parts First I establish the necessity but also limits of ethics Second I make the case that

0.7342 and in anecdotal evidence from my own experience All of this work demonstrates that there is more room to engage with politics in computer science Such engagement might build on classic approaches like Value-Sensitive Design to acknowledge the inherent political nature of technologies while at the same time presenting engineers with tangible solutions working on problems according to their theory of change and advocating for the computing discipline to transform and not just recognize structural problems Fifth the politics of computer science better corresponds with what is happening in the real world As mentioned in the introduction software engineers and computing professionals already exercise their ethical values politically These exercises have come through direct action general advocacy bodies intra-organizational advocacy and a growth of more socially oriented problem areas such as civic tech Examples of direct action include the GoogleWalkout and the TechWontBuildIt movement General advocacy bodies include the Tech Workers Coalition the defunct Computer Professionals for Social Responsibility and various not-for-profits like more recently the Center for Human Technology and older organizations like the Electronic Frontier Foundation Intra-organizational efforts include how employees of Facebook convinced Zuckerberg to change the newsfeed Civic tech and public interest technology have begun to

0.7327 is an expansion of computer science to embrace realist orientations alongside formalist ones not a wholesale rejection of formalism It is precisely the formalism of algorithmic methods that has enabled many of computer sciences most exciting advances Algorithmic realism provides complementary approaches that make sociotechnical considerations legible and commonplace within computer science thinking This expanded epistemic and methodological toolkit can help computer scientists to address existing problems more fully and to see new questions Nor does the distinction between algorithmic formalism and realism fully characterize the behaviors of computer scientists In practice computer scientists are diverse and ambivalent characters who blend formalist and realist methods engaging in nuanced contextualized and re practices as they continuously straddle the competing demands of formal abstraction and empirical contingency Some computer science sub fields such as CSCW have long histories of engaging with sociotechnical practices while others such as FAT are actively developing such methods We aim to highlight examples of realist-aligned work to help shift such work from exception to standard practice Nonetheless computer scientists recognize that the insights of STS and critical algorithm studies fall beyond their own interpretive frames Even within the FAT community critical evaluations of the mathematization of fairness

0.7286 social justice Leydens and Lucena find engineering curriculum detached from literature like science and technology studies a lack of focus on the macroethical or societal concerns little coverage of sustainability or inclusivity and no theoretical grounding such as in social justice They advocate for a number of criteria to judge curricula among which is Acknowledging political agency/mobilizing power p They find that contextual listening and appropriate teaching of this mapping of power reveals the degree to which citizens in a community are is how much agency they have in shaping their own future Riley and Nieusma et al both provide further grounding for this unification of engineering and social justice Such a framework is useful because social justice tempers technical imperatives by directing attention to social power imbalances surrounding technology decision making as well as inequitable material outcomes p I follow after their suggestions in advocating for educational interventions Karwat et al seek to fundamentally redefine contemporary engineering practice by exposing the political and value-based nature of engineering by applying socio-ecological learning to technological design by imbuing a different sense of responsibility in engineers and by moving the scope of engineering beyond solely technological development p In line with the

0.7286 Onward for the freedom of others Marching beyond the AI Ethics The debate on the ethics of Artificial Intelligence brought together different stakeholders including but not limited to academics policymakers CEOs activists workers representatives lobbyists journalists and moral machines Prominent political institutions crafted principles for the ethical being of the AI companies while tech giants were documenting ethics in a series of self-written guidelines In parallel a large community started to flourish focusing on how to technically embed ethical parameters into algorithmic systems Founded upon the philosophical work of Simone de Beauvoir and Jean-Paul Sartre this paper explores the philosophical antinomies of the AI Ethics debate as well as the conceptual disorientation of the fairness discussion By bringing the philosophy of existentialism to the dialogue this paper attempts to challenge the dialectical monopoly of utilitarianism and sheds fresh light on the -already glaring- AI arena Why is the AI Ethics guidelines a futile battle doomed to dangerous abstraction How this battle can harm our sense of collective freedom Which is the uncomfortable reality that remains obscured by the smoke gas of the AI Ethics discussion And eventually whats the alternative There seems to be a different pathway for discussing and

0.7263 political nature of data scientists Green offers steps for how to realize these politics through practitioners interest reflection applications and practice Rogaway presents how cryptography is inherently political with an intrinsically moral dimension He specifically calls out the implicit and overt politics present in cryptographic work and argues that practitioners have an ethic of responsibility Agre an AI researcher writing in the s walks the line between a critical perspective and that of a practitioner evoking some may bristle at critique Nonetheless critical analysis legitimizes moral and ethical discussion and encourages connections with methods and concepts from other fields p Moore explores whether given these critiques AI technologies can be deemed as good at all particularly in light of the growing trend of the use of terms like AI for social good That technologies are political is not a new argument From mentions a similar danger of co-option of language and the kind of technological determinism involved Even earlier in mainstream computer ethics suffered from two problems they focused too narrowly on publicly recognized moral dilemmas and tended to downplay computer technology itself as an object of moral analysis pg I add a third problem to mainstream computer ethics they

0.7236 authority within the US court system Finally we reflect on the challenges we have encountered in implementing data science projects that aim to shift the gaze upward In the process we develop new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority in the field To do this we draw on a feminist tradition of rigorously examining the micropolitics of research to unpack the ways that our positionality as researchers who are interacting with powerful institutions impacts the production of specific knowledge claims in our work Our hope is that in doing so we will expand the conversation about ethical engagement in data science and open up new lines of inquiry that until now have been left unexamined by the algorithmic fairness community THE CALL TO STUDY UP Anthropology In her article Up the Anthropologist Perspectives Gained from Studying Up Laura Nader called on her fellow anthropologists to move beyond the study of people and cultures at the peripheries of Western society who comprised the bulk of the anthropological canon in favor of studying up to excavate the ways that power operates through elite institutions and positions

0.7234 is the first policy-focused toolkit for communities to self-determine algorithmic governance through policy engagement BACKGROUND The toolkit we present in this work is the product of a particular policy context which afforded opportunities for public engagement and policy action Here we share the background of this policy context and how it shaped the AEKit as the product of a direct political encounter between community groups and government employees Community organizations and civil rights groups concerned about the discriminatory risks of public sector technology adoption have pushed for the accountability and transparency of public sector information technologies through the implementation of municipal ordinances in several US cities Closely related local policy efforts in Berkeley and Oakland California Cambridge Massachusetts Nashville Tennessee and Seattle Washington among others have led to the passage of surveillance ordinances that manage the acquisition and use of surveillance technologies and other automated decision systems by disclosing their use and subjecting them to political oversight The AEKit was created in Seattle Washington where the first municipal surveillance ordinance was passed in By the American Civil Liberties Union of Washington ACLU-WA had begun working to increase the community control of local surveillance technologies in a campaign that shaped a

0.7189 the proper role for discretion ways to combat discrimination and determinations of the legitimate bases for decision making Moreover the recent surge of enthusiasm for public interest technology explicitly follows in the footsteps of and indeed takes its name from a prior movement in legal education Of course our goal is not to claim a neat one-to-one correspondence between computer science and law there certainly are substantial di but to point to how the lessons of law can inform computer science Like computer science the law involves training in a methodological practice that structures how its practitioners create and evaluate social interventions Modes of legal thought influence legal interventions in much the same way that modes of algorithmic thought influence algorithmic interventions Legal scholars have long considered the relationship between the intended and actual impacts of social interventions Thus we see the parallel to legal formalism/realism as a way to identify a bridge between the deconstructive critique of algorithmic formalism from STS and a new mode of computer science practice algorithmic realism that productively engages with these critiques Following the history of law the distinction between algorithmic formalism and realism does not re a rigid dichotomy the evolution toward realism

0.7165 insights Elish and Boyd argue that reconceptualizing data science as a novel form of qualitative inquiry opens up a new set of methodological frameworks that can guide data-driven practices of modeling the world and illuminate the ways that power shapes and operates through the work of data science The algorithmic fairness community can benefit greatly from ongoing methodological debates and insights gleaned from fields such as anthropology and sociology where scholars have been working for decades to develop more robust frameworks for understanding their work as situated practice In this paper we expand on Elish and Boyds call for the development of more reflexive data science practices by examining a specific methodological debate within the field of anthropology frequently referred to as the practice of studying up In what is now considered a classic anthropological text Laura Nader called for her fellow anthropologists to expand their field of inquiry to include the study of elite individuals and institutions who remained significantly underexamined in the anthropological cannon Rather than study exotic cultures in far-flung lands Nader appealed for a critical repatriated anthropology that would shed light on processes of exploitation and domination by refocusing the anthropological lens on the cultures of

0.7141 and that emotions are largely contingent on social contexts These evaluative theories define emotions as primarily cognitive phenomena and as being or involving distinctive evaluations of the eliciting circumstances s p Central to this view is that emotions have intentionality that humans direct our emotions at particular objects a view synthesized and popularized in the s by the German philosopher Franz Brentano Whether understood philosophically as either simple judgments or complex appraisals the evaluative tradition understands emotion as in some way connected to human judgment though there is further broad debate as to of what judgment itself consists In psychology and affective science Scarantino and de Sousa reviewing and connect the evaluative tradition to the rise of appraisal theory Appraisal theory is concerned with developing accounts of the structure of the processes that extract significance from stimuli and differentiate emotions from one another s p though such differentiation does not conflict with understanding emotions as evaluative experiential or motivational per se Emotion as Felt Experience A third tradition in the philosophy of emotion is to treat emotions as primarily experiential as Scarantino and de Sousa put it this Feeling Tradition takes the way emotions feel to be their most essential

0.7129 This is a required first step towards the genuine interdisciplinary collaboration necessary to meaningfully address the ethical issues that continue to arise The way in which we teach AI ethics informs the way in which practitioners are trained and reflects academic practice Rather than exploring strategies to retrain AI scholars or practitioners exposing them to a sprinkle of ethics and social science and centering interventions on how to incorporate social considerations into technical expertise we instead discuss the need to think more deeply about what it would mean to reset the pedagogy and practices of the field to shift away from this exclusionary default Through a systematic analysis of over AI ethics syllabi we map the current situation and characterize some suggestions for an educational reset towards a more collaborative pedagogy with hopefully more direct consequences on improving both industry practice and academic norms HOW COMPUTER SCIENCE PEDAGOGY LED US TO THE ETHICS CRISIS We first examine the literature on how the culture of computer science has led to the current state of ethics discussions in the field focusing specifically on computer science education research epistemological analyses of computer science and empirical studies of computer science classrooms and cultures We

0.7107 focused on research regulation and executives practitioners too have a role Indeed some tech employees have already begun to act beyond their organizationally prescribed scope Given the issues with ethics as it is used in computer science and students likely receptivity to action-based teaching we should teach the politics of computing Doing so would provide students opportunities to explore not only values in technologies and normative frameworks but also means for redressal

0.7085 rhetoric of ethics and morality should not be reductively instrumentalized either by the industry in the form of ethics washing or by scholars and policy-makers in the form of ethics bashing Grappling with the role of philosophy and ethics requires moving beyond both tendencies and seeing ethics as a mode of inquiry that facilitates the evaluation of competing tech policy strategies In other words we must resist narrow reductivism of moral philosophy as instrumentalized performance and renew our faith in its intrinsic moral value as a mode of knowledge-seeking and inquiry Far from mandating a self-regulatory scheme or a given governance structure moral philosophy in fact facilitates the questioning and reconsideration of any given practice situating it within a complex web of legal political and economic institutions Moral philosophy indeed can shed new light on human practices by adding needed perspective explaining the relationship between technology and other worthy goals situating technology within the human the social the political It has become urgent to start considering technology ethics also from within and not only from outside of ethics KEYWORDS Moral Philosophy Ethics Technology Ethics Regulation Self-regulation Technology Law AI Introduction On May th Google announced that it would put in

0.7068 social economic and political origins of these problems not optimizing around them This debate has happened against the backdrop of a long history of critical reflection on the values embodied in technical artifacts and the need to design with such values in mind Scholars have spent decades working to draw attention to the normative commitments expressed in and through technology This call for recognition of values has been a core project of science and technology studies Values-in-Design legal scholarship and allied disciplines for years and remains an active area of research Algorithms have become objects of particular scrutiny over the past decade Within computer science the machine learning and mechanism design communities have been particularly active in taking up these concerns Both fields have been heeding the call for attention to values politics and social good more generally holding more than twenty technical workshops and conferences between them on some variation of fairness bias discrimination accountability and transparency in the last five years eg And yet these recent efforts have been met with concern that computer science has failed to target the right point of intervention In focusing on changes to decision-making and allocation these endeavors risk obscuring the background

0.7033 the larger conceptual frame of the paper He also conducted a large portion of the literature review foundation to the argument The third author input social and political theory perspectives aided in imagining future directions of the work and contributed to editorializing the work POSITIONALITY STATEMENT Reflexivity in research has become established practice in anthropology sociology and feminist studies and increasingly computing domains like human-computer interaction eg Each of the authors on this work have been challenged to engage with their own experiences and backgrounds in researching the interdisciplinary space of FAccT The authors perspectives are shaped by a collection of backgrounds in social and political theory engineering computer science gender studies and human-computer interaction This collection of disciplinary backgrounds has shaped both our pragmatic and philosophical approaches to computing approaches Our experiences as students and teachers at different aspects of our careers has led us to care deeply about educational approaches in computing disciplines including how to embrace new approaches for preparing future students on handling ethical dilemmas While we each have histories in different countries each authors education is rooted in Western scholarship which has shaped our approach to AI ethics pedagogy We each have our own set

0.7008 in the classroom will facilitate the development of more inclusive instruction and participation in the field for both academic and industry contexts Transversality in AI Ethics pedagogy on its own is not able to elaborate the disciplinary norms and create conditions for stable comprehensive and socially beneficent technical artifacts In other words the creation of theories tools and methods in AI ethics need to be understood as transversal problems involving methods theories and collaborators across several traditional disciplines Not only would this new pedagogy play a crucial role in addressing the divide between and HSS but it would focus on developing solutions in a different way from our current default of developing methods and technical artifacts first and only weighing their impacts later it would allow for technologists to imagine their role as a collaborative one with peers whose perspective and approach could be key to addressing what continues to be major challenges in the field The power of acting in concert in pedagogy would involve the acknowledgement of a requirement of transversality This requirement is a blind spot both in the design of the collaborations themselves and in the way we diagnose problems The field of has not yet

0.7007 Their study abroad experience promoted identity transitions that more fully account for design as a social good p Monteiro et al conduct a case study of engineering courses in Portugal exploring how engineers mindsets place importance on ethics and civics education Invariably they find that the engineer is seen more as an executor of technical orders than as a socially involved actor p Interestingly for this paper they almost always use ethics and civics in conjunction suggesting a connection between theoretical reasoning and political action Drawing on science and technology studies in the context of scientific literacy Hodson advocates for a focus on civic science literacy which encompasses the knowledge skills attitudes and values necessary for making decisions on matters such as energy policy use of natural resources environmental protection and moral-ethical issues raised by technological innovations p He establishes a number of levels of engagement with such literacy from appreciating social impacts of technology to taking action about those impacts which would inform a curriculum It is through such political engagement that a curriculum can allow students to learn how to participate in sociopolitical action and to encourage others to participate too p Bucciarelli and Drew in proposing a

0.6991 underlie both ethics washing and ethics bashing Whats Wrong with Ethics Washing Having examined some of the opportunities and limits of moral philosophys role in informing technology policy we must now ask what makes ethics-based practices as practiced in technology policy circles instead particularly problematic Can moral philosophy help us evaluate the acceptability of these efforts And should we call these efforts ethics washing As Googles ATEAC episode or the employment of ethicists by a number of companies show companies such as Google Apple Microsoft OpenAI Palantir are increasingly making efforts from an ethical standpoint and are particularly concerned about their ethical reputation in the face of new technological developments in AI and beyond Putting in place boards of external experts and hiring moral philosophers who can engage in ethical thinking about the techniques and products being developed in-house indicates willingness on their part to add an internal layer of accountability and governance and to subject themselves to preemptive checks and internal constraints The intentions behind these initiatives are often good but they beg for further scrutiny Notwithstanding good intentions embedding philosophers or ethicists within technology companies is a double-edged sword and could shield these companies from capacious regulation more

0.699 are co-located within a single department Nonetheless as MacLeod observes such institutional challenges are only part of what makes working at the intersection of disciplines difficult and more understanding is needed about the likely cognitive difficulties that pervade constrain and even block collaborative interdisciplinary work p As Pederson similarly observes while something of a consensus has emerged in the literature around the necessity for integrating knowledge from different disciplines the differing ontological and epistemological regimes of disciplines means that integration can become the Achilles heel of a project Such barriers to researching and teaching at the intersection of disciplines are observed in the literature with multiple concerns raised that when Social Science and Humanities SSH are brought into interdisciplinary teams it is often as an add on or as an independent project within the overall project architecture risking that SSH becomes merely an appendix to the techno-scientific disciplines Building upon such insights we became interested in opening up a conversation amongst academics teaching on the Data Science programme about how our Data Science team had evolved its approach to working at the intersection of disciplines what this means for how topics related to FATE/CDS are integrated into the curriculum and

0.6966 Integrating FATE/Critical Data Studies into Data Science Curricula Where are we going and how do we get there There have been multiple calls for integrating topics related to fairness accountability transparency ethics FATE and social justice into Data Science curricula but little exploration of how this might work in practice This paper presents the findings of a collaborative auto-ethnography CAE engaged in by a MSc Data Science teaching team based at University of Sheffield UK Information School where FATE/Critical Data Studies CDS topics have been a core part of the curriculum since In this paper we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines and our progress and future plans for integrating FATE/CDS into the curriculum We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community CONCEPTS Social and professional topics Computing education KEYWORDS data science FATE critical data studies higher education INTRODUCTION The number of Data Science degrees has expanded significantly in the last five years eg largely in response to

0.6929 new undergraduate degree program to better enmesh liberal arts with engineering focus on how current engineering programs fail to teach civics Indeed ABET requires no civics courses and instead their ethics requirement often boils down to an ethics lecture on how to avoid negligence there is little said about virtue ethics or social/civic responsibility of the individual as member of a profession p In their proposal as in this one they hope to demonstrate the compatibility of education in science and technology engineering with education in the humanities and social sciences in particular with learning civics p This is akin to what Schuler evokes as a necessary part of engineering culture that civic engagement with the world be brought to greater levels of visibility and appreciation and practice Hekert uses the language of macro and microethics to describe a focus on respectively the social or the individual and professional Like Moor he makes the case that engineering ethics education focuses too much on microethics eliding societal concerns As I do he advocates for a greater coverage of science and technology studies and corresponding curriculum revision Social justice Such a visibility of social implications has increasingly come in the form of

0.6929 do algorithmic interventions Third we consider a possible path forward An epistemic and methodological evolution is a daunting task and it is not obvious how such a shift could occur or that it would be productive With this in mind we draw on our characterization of algorithmic formalism to explore a parallel to formalism in another eld law and to how legal formalism was addressed with a methodological evolution toward legal realism From around through the beginning of the twentieth century American legal thought was characterized by legal formalism a project to systematize law around c and deductive principles Because this mode of thought adhered to objective principles but did not consider those principles actual impacts its application upheld highly unequal social conditions These impacts provoked critiques that led to a methodological evolution toward legal realism Legal realism did not wholly supplant formalism but instead provided lawyers and judges with additional tools to account for the realities of social life and of the laws impacts This shift which expanded the terrain on which law could be evaluated and debated suggests both a path toward reforming computer science and the utility of such a path Fourth drawing on the lessons of

0.6929 centers I propose we also focus on those who end up becoming integral to technology companies Third given practitioners responsibility universities should act to encourage it Some may think that political engagement is separate from the knowledge-creation ambit of universities A social constructivist position shows us that we produce knowledge through interactions with each other that knowledge does not exist by itself In this sense knowledge is already produced and acted upon as partially determined by an agents normative positions neither knowledge nor science is value free Therefore a university acting to refrain from covering the use of and arguments about knowledge in the world would have as and others have made clear already adopted a political position a conservative one Fourth a greater focus on the political nature of the organizations and problems which computer scientists face will better address the problems raised about it The restriction just to ethical issues such as the trolley problems drowns out concern over the realization of ethically less ambiguous areas such as the treatment of marginalized populations Along these lines Green calls for data scientists to recognize the politics in their work and cast off the current use of ethics Greene et

0.6916 I finished by covering limitations to my approach Stark and Hoffman aptly quote Langdon Winner as testifying to Congress to deplore those who conduct research about the ethical dimensions of emerging technology to gravitate toward the more comfortable even trivial questions involved avoiding issues that might become a focus of conflict This work aims to support the increased proliferation of ethics courses It also focuses on the bigger picture political questions at stake Ethics education in computer science has many ways to improve before even reaching the point of advocacy As Ethan Zuckerman a prominent MIT computer scientist has said My guess is that courses that force us to have these sorts of arguments are critical to unpacking the intricacies of emerging technologies and their implications To be clear theres the field of science and technology studies which makes these questions central to its debates Values and discussions about them under-gird the work of computer scientists Recent uses of ethics particularly regarding technologies like artificial intelligence appear to ameliorate the concerns of computing technologies but in practice ethics does not go far enough to recognize the inherently political nature of these technologies While past works on ethics in computing have

0.6908 the powerful As Nader argued If ones pivot point is around those who have responsibility by virtue of being delegated power then the questions change This paper is organized into two parts In part one we reflect on the contributions that the call to study up has made to the field of anthropology Naders provocation came at a time when anthropologists were grappling with the epistemological and methodological limits of their tradition The call to study up was a call for scholars to move beyond their default orientations their tendency to study the underdog in isolation from larger structural forces in order to deal directly with issues of power and domination in their work We draw parallels between this debate in anthropology and similar issues that data scientists are grappling with today in their pursuit of fair accountable and transparent algorithmic systems We then make the case for why the field of algorithmic fairness would benefit from such a reorientation upward The political and social impacts of algorithmic systems cannot be fully understood unless they are conceptualized within larger institutional contexts and systems of oppression and control Data science projects that study up could lay the foundation for more robust

0.6895 computing professionals are responsible to society more generally Third I present the role of the university as not just a knowledge creator but also to encourage students action Fourth I incorporate current critiques of ethics in computing to argue for practitioners political engagement Fifth I cover the actual political engagement and agency of computing professionals Following these I argue that computing ethics education should cover political engagement to a greater degree First as is widely established ethics are necessary for engineers They are required for engagement with values but are not sufficient to lead to action At the same time as find engineers commonly perceive their role as limited This is often called the engineering mindset Computer science students often limit their opportunities to shape or influence the world as occurring only through technology or the accumulation of capital Along these cites Collins in defining social justice as an organized long-term effort to eliminate oppression and empower individuals and groups within a just society expands this in terms of the capability approach as do lines examines three very common responses to the politics of AI technologies This is to say current ethics education is both limited in scope and appears

0.6874 certain basic emotions were reliably recognizable in facial expressions in different populations around the world As such Ekman suggested there should be bodily signatures for each basic emotion consisting of highly correlated and emotion-specific changes at the level of facial expressions autonomic changes and preset and learned actions s p In other words Basic Emotion Theory argues human emotional expression is universal reliably legible and critically difficult to falsify because emotions are understood as motivational drives deriving from biophysiological processes they are difficult to conceal from the expert eye The ethologist Alan Fridlund has developed a notable alternative paradigm for understanding emotion as motivational known as the Behavioral Ecology view in this theory all externalized forms of human emotional expression are better understood as social displays always suggesting some sort of social motivation or function eg a performative suggestion but which provide no evidence regarding the interior mental or motivational states of the expressor Emotion as Evaluative Signal A second tradition grounded largely in the cognitive revolution of the s treats emotions either as directly constitutive of cognitive states or caused by them Evaluative theories suggest that human emotions are one important category of evidence underpinning human thought and action

0.6841 in both design and action In particular the focus on applied ethics blunts the political nature of the situations in which computer scientists find themselves Furthermore ethics carries its own history and should have us ask whose ethical theories are we using and whom might they leave out Relevant are not only ethical theories but also their professional realization While scholars have historically found that both ethics codes and classes appear to have little impact on the actions of engineers as discuss this may be due to the limits of the framing of such ethics Nevertheless despite the limits of ethics as they exist in computer science practitioners have begun to act Reporting from summer details the actions of Stanford computing students to resist the big tech companies who the students see as perpetuating injustice by supporting groups such as the US Immigration and Customs Enforcement agency At Google in fall thousands of workers walked out to protest the companys sexual harassment policies That came a year after tech workers organized to oppose building infrastructure to support the Trump administrations Muslim ban The Tech Workers Coalition aims to organize both gig workers and software engineers to advance more just policies

0.6828 Data Science for Social Good UW DSSG team Our collaborators at the time in the ACLU-WA had previously shared their interest in technical expert support in deepening their advocacy efforts We held joint planning conversations to determine what process our co-design should follow Our collaborators at the ACLU-WA were interested in continuous engagement in our process We therefore engaged in a process that was participatory with practitioners throughout its design lifecycle By the summer of the team behind the AEKit gained institutional and financial support from the ACLU-WA and the UW DSSG program where the team joined by student fellows data science experts community partners and policy advocates Our community partners included two additional civil rights organizations that advocate on behalf of historically marginalized communities Densho an organization dedicated to preserving the history of World War II incarceration of Japanese Americans and advocating to prevent state violence and the Council on American-Islamic Relations of Washington CAIR-WA a prominent Islamic civil liberties group who defends the rights of American Muslims The ACLU-WA Densho and CAIR-WA had already been engaged in a long-term collaboration for tech fairness and advocacy work They expressed interest in the AEKit as a resource to equip their

0.6817 the exercise of moral reasoning and inquiry can add to existing technology policy debates Here the focus is not on how moral philosophy is instrumentalized but on the worth of moral philosophy as a practice and an exercise that is taken seriously from within Philosophical work is valuable and can add value in at least four ways First philosophical reasoning and deliberation can act as a meta-level perspective from which to consider any disagreement relating to the governance of technology Instead of taking arguments narrowly intuitively or personally at face value philosophical reasoning provides us with a framework for stepping back situating any problem within its broader context and understanding it within or in relation to other relevant or analogous debates As such the practice or method of engaging in moral argument allows us to broaden our perspective on a debate to look at it from a wider lens overcoming confusions filling in gaps correcting inconsistencies and drawing clarifying distinctions In the debate on surveillance examined above for instance a philosophical method can help us rethink our reasons for rejecting or promoting surveillance it can help us clarify points of agreement with a variety of opponents and focus the discussion

0.6806 al find a lack of grounding and co-option in prominent AI ethics principles Wagner finds that industry groups use ethics as a means to avoid regulation and do not provide means of assessment on those ethics Stark and Hoffman discuss how norms around equality and justice must supplant processual professional ethics Metcalf et al and Benjamin find that the current presentation of ethics is too narrow Likewise ethics courses for engineers are lacking Colby and Sullivan find engineers less willing to engage in social issues and Bucciarelli et al discuss how these courses fail to be meaningful for students At the same time there is a growing of interest in civic tech For human-computer interaction students at least cultural experiences increase desire to engage with civic tech This complements Colby and Sullivans other finding that engineers engage more when presented with active pedagogies and when part of institutions with clear missions that which might have been lacking for students in the technical track found by Monteiro This dovetails s call that science literacy include sociopolitical action as similarly evoked by Likewise a focus on political action appears to show that the engineering mindset may not be recalcitrant as find directly

0.6802 reflection applications and practice Signs of Success A greater coverage of politics in computer science ethics curricula might succeed in a number of forms In line with my aforementioned normative commitments I cover how those changes might occur in both courses and in students Ethics Courses First the constitution of ethics courses might change Courses might assign more critical perspectives to reduce the focus on dilemmas and employ discussion-based methods eg using as a reference They might focus on action and provide examples of students and engineers who have acted politically in contemporary tech companies Critical Perspectives Literature from a number of disciplines can explicate the political nature of computing Some of these disciplines include critical data studies science and technology studies surveillance studies race and ethnic studies sociology feminism and the philosophy of science some of which human-computer interaction covers For example one might cover the shifts in power in our datafied society and the political implications of modern AI systems To do so as Hoffman does in her course one might start out with theory such as Foucaults coverage of the Panopticon weave through Desrosières discussion of statistics and tie these together with Deleuzes analysis on the transition

0.6799 intervention In a realist mode of reasoning all of these questions are seen as integral to rigorous algorithmic work rather than as beyond the scope of algorithmic design These realist practices will enable the eld not just to avoid harmful impacts but also to identify new research questions and directions to pursue As in law algorithmic realism is not meant to provide a wholesale rejection of formal methods nor will it provide a wholesale solution to the intractable challenges of designing just algorithmic systems Even to the extent that the turn to algorithmic realism is motivated by a broader program of social reform à la the turn to legal realism new epistemic and methodological tools cannot by themselves achieve a vision of the good let alone determine which vision of the good to work towards Nonetheless algorithmic realism can help computer scientists reflexively approach their work in light of their larger normative commitments and the impacts of algorithmic systems As such algorithmic realism enables computer scientists to reason well about doing good

0.6779 mirrors wider discussions beginning to take place in the FATE/CDS community for example in relation to the decision to embed a program on Critiquing and Re-thinking Accountability Fairness and Transparency CRAFT in the FAT Conference and Taylor and Purtovas recent paper Acknowledging and understanding this political dimension to FATE/CDS integration will be important for Data Science degree teams such as ours that aim to deepen integration of FATE/CDS into the curriculum As a community we can help teams better understand these issues through the sharing of short accessible and engaging materials that illuminate the power dynamics at play in such efforts and which can be used in reading and discussion groups Collaborate across borders In advancing an agenda around FATE/CDS integration in Data Science we need to deepen our understanding of the cultural shaping of FATE discourse and work with international students and collaborators and experts in curriculum decolonisation efforts to address these issues In our teams case this particularly means enhancing our understanding of Chinese perspectives on issues of ethics and social justice Existing networks and organisations such as the iSchools Consortium and ACM FAT may be well placed to help foster such collaboration The image of Data Science

---------------------------------------------------------------------------
TOPIC 14: explainability

---------------------------------------------------------------------------

0.7822 not producing the same explanation result in this context does not refer to model predictions Indeed the examples Im about to discuss are cases where the predictions of the underlying ML models are pretty robust but the explanations associated with them are not Non-Robust Local Explanations Locally Interpretable Model-Agnostic Explanations LIME and Shapley values are popular local explanation methods that take a complex black-box model and construct locally interpretable models at specific input points They fall in the class of local explanations because the constructed interpretable model differs at each input point and a local explanation constructed for one input point does not necessarily hold at another input point Alvarez-Melis and Jaakkola showed that LIME and Shapley values lack robustness for non-linear black-box models in the following sense They conducted experiments where they slightly perturbed input values that were entered into the black-box model They found that the surrogate models and the original black box models produced stable output values in response to the perturbed inputs but that the perturbations often caused the explanations provided by LIME and Shapley to change drastically In short explanations generated by LIME and Shapley values are not robust to small changes in input values

0.7779 that the fidelity of the decision tree trained using the ontological pipeline with ontological perturbation red boxplot is generally higher compared to all the other explanation pipelines This observed tendency confirms that exploiting the ontological information during the synthetic neighborhood creation allows the decision tree to better approximate the local black-box decision boundary Figure Fidelity distribution for the non-ontological pipeline at different values of and training set In Figure we show the fidelity sample distributions at different values of for the decision trees trained using the non-ontological explanation pipeline ie the pipeline that selects the first dataset neighbors of the instance to be explained using the Jaccard similarity between patients visits We developed this explanation pipeline that does not use the semantic information encoded into the ICD codes as a baseline to prove that an approach that does not exploit this information has lower performance This is true if we compare this explanation pipeline with the fully-ontological one the ontological pipeline with ontological perturbation However the fidelity performance of this non-ontological pipeline is comparable to the ones of the ontological pipeline with normal perturbation The high values of fidelity achieved by this pipeline prove that we developed a trustable explainability

0.7649 prediction The first two exploit the ontological information encoded into ICD- codes whereas the last one can also be used to explain sequential data classification if an ontology is missing We aim to show that exploiting the ontological information in the data increases the explanation quality Ontological pipeline with ontological perturbation DrXAI This pipeline fully exploits the knowledge encoded into the ICD- ontology to create the synthetic neighborhood Given a patient whose black-box decision we want to explain it selects its first neighbors in the dataset using the ontological distance described in section and then it generates the synthetic neighborhood by perturbing them using the ontological perturbations described in section This pipeline corresponds to the blue path of Figure using the Ontological similarity Ontological pipeline with normal perturbation This pipeline selects the first real neighbors of the instance to explain using the ontological distance but then it creates the synthetic neighborhood by perturbing these instances using the normal perturbation described in section This pipeline corresponds to the red path of Figure using the Ontological similarity Non-ontological pipeline with normal perturbation This pipeline does not use the semantic information encoded in the ICD codes It first selects the real neighbors of

0.756 information gain Each conjunct of the rule follows the pattern ICD-_code observed_value threshold_value The observed value is the value of that ICD- code for the patient whose decision we want to explain Recall that the temporal encoding or flattening procedure described in Section assigns to each ICD- code a weight according to the visit in which it was observed diagnosed The threshold value is the split value assigned by the decision tree to that ICD- code Both these values can be interpreted as the presence of the ICD- code in a set of visits The patient under examination had four visits The ICD- codes describing the diagnoses associated with each visit are represented in the timeline just below the decision rule Recall that we are explaining the top- -codes predicted by Doctor AI The ICD- codes considered meaningful by the black-box have been colored to enhance the readability The explanation of each real and threshold value can be found in the list below the timeline For example the ICD code has an observed value of which means that it was observed in the second-to-last visit visit Its threshold value is whose closest value among those generated in the temporal encoding

0.7445 in Section it is desirable that a method produces diverse and proximal examples and that it can generate valid counterfactual examples for all possible inputs Ultimately however the examples should help a user in understanding the local decision boundary of the ML classifier Thus in addition to diversity and proximity we propose a metric that approximates the notion of a users understanding We do so by constructing a secondary model based on the counterfactual examples that acts as a proxy of a users understanding and compare how well it can mimic the ML classifiers decision boundary Nevertheless it is important to emphasize that CF examples are eventually evaluated by end users The goal of this work is to provide metrics that pave the way towards meaningful human subject experiments and we will offer further discussion in Section Validity Proximity and Diversity First we define quantitative metrics for validity diversity and proximity for a counterfactual set that can be used to evaluate any method for generating counterfactuals We assume that a set C of counterfactual examples are generated for an original input Validity Validity is simply the fraction of examples returned by a method that are actually counterfactuals That is they

0.7431 or BII This method faithfully captures how feature interactions influence model decision-making Not only does our proposed model explanation satisfy a set of desirable properties it is the only model explanation that satisfies all these properties Our Contributions In this work we propose a method for high-dimensional explanations for black-box models Our main goal is to axiomatically capture higher-order feature interaction Our main contribution is twofold first we propose a high-dimensional black-box model explanation method axiomatically capturing higher-order feature interaction Our characterization uses properties that are very natural in the ML context such as feature symmetry and effect monotonicity Section we obtain a new axiomatization of the Banzhaf interaction index which uniquely satisfies symmetry limit condition general-efficiency and a newly proposed axiom in the context of Banzhaf indices monotonicity Monotonicity is a rather general property which essentially means that the model explanation should change in a manner faithful to the underlying data This is very fundamental property for an interaction measure which states that the net contribution of the subset of features for the machine learning model is more than that for the model then the interaction measure for those features for model should be more than the interaction measure

0.7288 learned by the black-box in the feature space can be arbitrarily complex it can always be locally approximated by a simpler more interpretable model In the LIME approach the explanation is the set of weights of a sparse linear model Other examples of agnostic approaches that focus on explaining the black-box behavior around a specific instance are SHAP and ANCHORS The SHAP approach evaluates local features importance using a game theory approach while ANCHORS formulate the problem using a multiarmed bandit approach Our work shares some of the features of these approaches for example we mine our explanations using a perturbation-based strategy However unlike any other method we also exploit the domain knowledge encoded in the ontology to generate relevant perturbations for the specific problem under study By doing so we increase the quality of the generated synthetic instances which is of pivotal importance for the quality of the explanation provided Similarly to LIME we train a local surrogate model able to mimic the black-box behavior around the data point and similarly to ANCHORS our approach produces rule-based explanations However we mine our explanations from a multi-label decision tree which allows us to deal with a complex output space the

0.7274 In such a graph a node represents a random variable and an edge between each pair of nodes represents a direct causal relation between the Kilbertus et al use causal models to analyze fairness We focus our discussion on Kusner et al but our criticism also applies to their work corresponding random variables for instance X is a direct cause parent of Y is represented by X Y Nodes with no incoming edge are said to be exogenous To find causal relations via a causal model requires establishing well-defined connections between some aspects of the sample data and a causal model The main connections are often captured by two causal assumptions the causal Markov condition and faithfulness The causal Markov condition ensures that a variable is independent of its non-descendants given its parents The causal faithfulness condition requires that all inter-dependencies in the observational data are non-accidental and structural the result of the structure of the causal graph To counterfactually think via a causal modeling approach in a specific machine learning domain requires an in-depth interpretation of the mapping of the random variables on the elements of the domain and the satisfaction of the causal assumptions If the domain of

0.7235 model functions but rather how it behaved and why According to Lipton approaches to post-hoc interpretability include verbal natural language explanations eg McAuley and Leskovec visualisations and interactive interfaces eg Simonyan et al Tamagnini et al local explanations or approximations eg Fong and Vedaldi Ribeiro et al and case-based explanations eg Caruana et al Kim et al Natural language explanations can consist of textual or visual artefacts that provide qualitative understanding of the relationship between features of an input eg words in a document and the models output eg a classification or prediction Ribeiro et al Visualisation techniques can visually demonstrate the relative influence of features or particular pixels eg in the case of an image classifier or provide an interface for users to explore textual or visual explanations Poulin et al Tamagnini et al Local explanations seek to explain how a fixed model leads to a particular prediction either by fitting a simpler local model around a particular decision Ribeiro et al or by perturbing variables to measure how the prediction changes Adler et al Datta et al Simonyan et al Case-based explanation methods Caruana et al Kim et al for non-cased based machine learning involve using the trained

0.7225 subsume both retrospective/subjunctive/counterfactual what would have been the value of and prospective/indicative/predictive what will be the value of conditionals as long as we assume that the laws governing the world are stationary Definition CFE-based actions Given an individual in world M the solution of and the set of indices of observed variables that are acted upon a CFE-based action refers to a set of structural interventions of the form ACFE B Using Definition we can derive the following key results that provide necessary and sufficient conditions for CFE-based actions to guarantee recourse Proposition A CFE-based action ACFE where performed by individual in general results in the structural counterfactual scf cfe and thus guarantees recourse ie scf if and only if the set of descendants of the acted upon variables determined by is the empty set Corollary If the true world M is independent ie all the observed features are root-nodes then CFE-based actions always guarantee recourse While the above results are formally proven in Appendix A we provide a sketch of the proof below If the intervened-upon variables do not have descendants then by definition scf cfe Otherwise the value of the descendants will depend on the counterfactual value of

0.7211 an interpretable classifier able to mimic the decision boundary of the black-box that is relevant to the decision taken for a particular instance More formally Given an instance x and its black-box outcome y an explanation is extracted for this individual decision from an inherently interpretable model c trained to mimic the local behavior of b For our approach we follow the general pipeline of generating a set of synthetic instances called neighborhood surrounding the instance x we want to explain labeling them utilizing the black-box b training an interpretable model c on the labeled neighborhood and finally extracting an explanation in the form of a symbolic rule However we have developed specific modules in order to deal with the temporal dimension in the data and exploit linked structural knowledge representation Figure illustrates our explanation pipeline The explanation pipeline The starting point is the data point whose black-box prediction we are interested in explaining As the first step we select the data points that are closest to the instance to be explained in the available dataset these points are called the real neighbors of the instance We can either select the closest data points according to a standard distance metric

0.7115 such as the Jaccard one or exploit ontology-base similarities We describe the latter in Subsection In both cases we obtain a set of real neighbors each of which is represented as a sequence We then generate the synthetic neighborhood perturbing the first real neighbors to ensure the locality of the augmented neighborhood The synthetic neighbors sampling is crucial to the purpose of auditing black-box models Ideally the synthetic instances should be drawn from the true underlying local distribution Unfortunately this distribution is generally unknown and how to generate meaningful synthetic patients is still an open question While most state-of-the-art agnostic explainers employ random perturbations we use the domain knowledge encoded in the ICD- ontology to generate more meaningful synthetic instances as explained in Subsection It could be argued that the interpretable model could be trained directly on the closest real neighbors However the rationale behind the generation of synthetic neighbors is that we want to build a dense training set for DATA POINT to be explained Real neighbors sequence Real neighbors flattened Synthetic neighbors flattened Interpretable Model Rules Synthetic neighbors sequence Labels for synthetic neighbors Ontological Perturbation Jaccard Ontological Distance Selection Feeding as input Extraction Temporal Encoding Temporal Encoding Temporal

0.7037 Marital Status MS Yes No No Yes No Yes Capital Gain CG Age A Education Level EL Prediction Monotonicity Matters Having established the importance of measuring feature synergy let us turn to evaluating the importance of our proposed axioms To do so we consider a modified version of the tree depicted in Figure The output in the second-to-last leaf is changed from to ie married people with high Education Level but low Capital Gain are now classified as not having a high paying job This change does not affect the prediction of the two points discussed above However this increases the interaction between Capital Gain and Marital Status having a lower Capital Gain or being unmarried both lead to a negative outcome More formally we denote the cooperative game induced by the original tree by and the one induced by the modified tree by Under the joint marginal gains increase Age Education Level Age Education Level for any other coalition According to the monotonicity property the interaction index value among Age Education Level should be higher under than This change is reflected in Shapley interactions and BII as both measures satisfy monotonicity see Figure column b for actual interaction and

0.6985 is not captured in the current definition of cost and hence Assumption does not hold either Therefore except in trivial cases where the model designer actively inputs pair-wise independent features to generating recommendations from counterfactual explanations in this manner ie ignoring the dependencies between features warrants reconsideration Next we formalize these shortcomings using causal reasoning Actions as Interventions Let M denote the structural causal model SCM capturing all inter-variable causal dependencies in the real world M is characterized by the endogenous observed variables X X the exogenous variables U U and a sequence of structural equations U X describing how endogenous variables can be deterministically obtained from the exogenous variables Often M is illustrated using a directed graphical model G see eg Figure From a causal perspective actions may be carried out via structural interventions A which can be thought of as a transformation between SCMs A set of interventions can be constructed as A B where contains the indices of the subset of endogenous variables to be intervened upon In this case for each the do-operator replaces the structural equation for the endogenous variable in with B Correspondingly graph surgery is performed on G severing graph edges incident on

0.693 a different explanandum from a global explanation because the latter includes the entire range of inputs and outputs for the model in its explanandum whereas a local explanation focuses on only a single input-output mapping I focus on another difference in explananda that has not been explicitly discussed the extent to which model-world relations are included in the explananda To understand how the world enters into explanations consider Figure which depicts relationships between the world the data input into the model the model and the models output I constructed this diagram to show that there are many potential explananda that we could pick out from among the diagrams components The following are an incomplete list For a given input how outputs are generated in the local neighborhood of that input This is the kind of explanandum targeted by a local explanation In Figure these explanations have only the data input model output relationship as the explananda and each local explanation covers only a specific data input point How the model transforms inputs to outputs in a global sense Global explanations of interpretable models have this type of explanandum In Figure these explanations cover the data input model model output relationship

0.6915 that in every graph in Fig there is an open path between C and S the path is direct in Fig a in Fig b the path CMS includes only terminal variables and a mediator and thus open when the conditioning set is empty in Fig c the path CMS includes only terminal variables and a common cause and so is also open given an empty conditioning set and the path CUMS includes U as a common cause and M as a mediator and so is unconditionally open In a and b there is a direct path CB is open unconditionally in both graphs consequently C and B will be associated in representative data given such causal structures However in Fig a there is only one path between C and B CUMUB M is a collider in that path and so relative to an empty conditioning set C and B will be unassociated put formally C and B are probabilistically independent in any probability density faithful to Fig a and in any representative data set they will be statistically unassociated on any measure of association MEASURES OF BIAS Measures of algorithmic bias can be classified by the conditional dependencies to

0.688 correspond to a different outcome than the original input Here we consider only unique examples because a method may generate multiple examples that are identical to each other Valid-CFs unique instances in C st c Proximity We define distance-based proximity separately for continuous and categorical features Using the definition metrics from Equations and we define proximity as Continuous-Proximity i x Categorical-Proximity i x That is we define proximity as the mean of feature-wise distances between the CF example and the original input Proximity for a set of examples is simply the average proximity over all the examples Note that the above metric for continuous proximity is slightly different than the one used during CF generation During CF generation we transform continuous features to for reasons discussed in Section but we use the features in their original scale during evaluation for better interpretability of the distances Sparsity While proximity quantifies the average change between a CF example and the original input we also measure another related property sparsity that captures the number of features that are different We define sparsity as the number of changes between the original input and a generated counterfactual Sparsity i d l l i where d

0.6804 diversity and proximity no method will be able to maximize both Therefore evaluation of a counterfactual set will depend on the relative merits of diversity versus proximity for a particular application domain Approximating the local decision boundary The above properties are desirable but ideally we would like to evaluate whether the examples help a user in understanding the local decision boundary of the ML model As a tool for explanation counterfactual examples help a user intuitively explore specific points on the other side of the ML models decision boundary which then help the user to guess the workings of the model To construct a metric for the accuracy of such guesses we approximate a users guess with another machine learning model that is trained on the generated counterfactual examples and the original input Given this secondary model we can evaluate the effectiveness of counterfactual examples by comparing how well the secondary model can mimic the original ML model Thus considering the secondary model as a best-case scenario of how a user may rationally extrapolate counterfactual examples we obtain a proxy for how well a user may guess the local decision boundary Specifically given a set of counterfactual examples and the

0.6783 technique applicable to any black-box that takes as input any sequential data even when there is no ontology associated with the items of the sequence Furthermore it is important to notice that also for this pipeline the values of fidelity to the black-box increase after the synthetic neighborhood augmentation the orange boxplot Figure Explanation complexity for the ontological pipelines In Figure we show the sample distribution of explanation complexity ie the number of premises in the rule-based explanations at different values of for the two ontological explanation pipelines As expected we see how the length of the explanation increases as increases This happens because if we start from a high number of first real dataset neighbors we are trying to approximate a larger portion of the decision boundary of the black-box with the interpretable classifier We could say that we are not restricting ourselves to the local decision boundary close to the instance whose decision we want to explain Therefore since we are trying to approximate a more complex decision boundary the dimensionality/complexity of the decision tree grows and consequentially the length of the rule increases From this plot it is also possible to see that the explanation length of

0.6779 the instance to be explained using Jaccard similarity between each patient visit and then it perturbs them by using normal perturbations This pipeline corresponds to the red path of Figure using the Jaccard similarity By comparing the two ontological pipelines we want to show that exploiting the semantic information encoded in the ICD- ontology is also useful to create the synthetic neighbors We developed the non-ontological pipeline as a baseline for explanation quality However this last pipeline is also the most general one because it can be applied to sequential data that does not have an associated ontology Furthermore we wanted to show that increasing the density of the feature space around the instance to be explained by creating the synthetic neighbors actually increases the interpretable models ability to mimic the black-box locally For this reason for each instance to be explained we trained two decision trees One decision tree is trained directly on the real neighbors of that patient from the dataset and the other one is trained on a fraction of the augmented synthetic neighborhood We then compare the performance of these decision trees on an out-of-sample set of synthetic neighbors We utilize the following metrics to evaluate

0.6755 graphical model or the graph Nodes are the variables edges represent a causal relationship between them While the graph does not include the details of its structure alone is enough to identify patterns and causes In an SCM exogenous variables denoted are external to the model meaning that we chose not to explain how they are caused They are the root nodes of the causal graph and are not a descendant of any other variable Endogenous variables denoted are descendants of at least one exogenous variable and model components of our system and the world for which we want to explain causes describes the relationships between all those variables If we knew the value of every exogenous variable we could use to determine the value of every endogenous variable In a graphical model every node represents an endogenous variable and arrows represent functions from between those variables Actual Causality Type causal models make predictions on how events will unfold This is useful because we want to build systems in a way that they will probably be accountable To achieve this we try to make sure that the causal effects within a system are clearly understood and that the structure ensures

0.6746 a result depending on the choices for the similarity criteria and the ordering we can obtain contradictory judgments about the truth or falsity of counterfactuals Hence the vagueness and the multiplicity of orderings pertain to the problems of using counterfactuals in machine learning A causal modeling approach uses a causal model as a representational tool for exploring the space of alternative causal hypothesis Following Pearl from a causal modeling perspective the world is described in terms of random variables and their values The random variables are either exogenous or endogenous and they might take continuous or categorical values The exogenous variables are determined by factors outside of the causal model and serve as fixed background assumptions to the causal reasoning The endogenous variables may have a causal influence on each other This influence is modeled by a set of structural equations that are functions for capturing the potential causal effects of functional dependencies on the endogenous variables A set of exogenous and endogenous variables their values and a set of structural equations form a causal model can be graphically visualized by a directed acyclic graph This graph facilitates cognitive efforts in thinking about potential causal sources effects and causal relations

0.6737 for those features for model Second we extend the idea of feature-based model explanations which can be thought of as a local linear approximations of blackbox models to higher-order polynomial approximations Especially we show that our proposed measure can be obtained by approximating the black-box model by a higher-order polynomial Section We evaluate our approach on real datasets In particular our experiments show that feature-based model explanations may fail to capture feature synergy moreover we show that other model explanations which fail to satisfy our axioms tend to behave in an undesirable manner Section Related Work While some model explanations provide record-based explanations or generate explanations from source code the bulk of the literature on the topic focuses on feature-based model explanations Ancona et al offer an overview of feature based-model explanations for deep neural networks The connection to cooperative game theory has been widely discussed and exploited in order to generate model explanations with a particular focus on the Shapley value and its variants Interaction Indices for Cooperative Games Two widely accepted measures of marginal influence from cooperative game theory the Shapley value and the Banzhaf value are uniquely derived from a set of natural axioms see also These

0.672 note that while a variable See eg may not be directly action able it may still change as a result of changes to its parents For example the financial credit score in Figure may change as a result of interventions to salary or savings but is not itself directly intervenable Therefore for a non-action able but mutable variable the constraint is sufficient and does not induce any other constraints action able In the most general sense the action able feasibility of an intervention on may be contingent on a number of conditions as follows a the pre-intervention value of the intervened variable ie b the pre-intervention value of other variables ie c the post-intervention value of the intervened variable ie SCF and d the post-intervention value of other variables ie SCF Such feasibility conditions can easily be encoded into consider the following scenarios a an individuals age can only increase ie SCF age age b an individual cannot apply for credit on a temporary visa ie visa PERMANENT SCF credit TRUE c an individual may undergo heart surgery an additive intervention only if they wont remiss due to sustained smoking habits ie SCF heart REMISSION and d an individual may

0.6713 Y This corresponds to evaluating whether the counterfactual If X had not occurred Y would not have occurred is true In machine learning practice there are several technical ways to generate and evaluate counterfactuals such as feature-based explanations prototype explanations example-based explanations or causal explanations These approaches are most often rooted implicitly or explicitly in either of the two prominent conceptual approaches for evaluating counterfactuals the close-enough-possible-worlds approach inspired by Lewis and Stalnaker and the causal modeling approach developed by Spirtes et al and Pearl among others To evaluate a counterfactual the close-enough-possible-worlds approach compares the actual world in which X and Y occur with those similar-enough worlds to the actual world in which X does not occur eg comparing a data instance Strictly speaking develop the closest-possible-worlds approach to make sense of counterfactuals With a bit of weakening the set of closest-possible-worlds can be interpreted as the set of close-enough-possible-worlds where due to practical considerations those possible worlds that are close enough to the actual world rather than the closest possible worlds are selected For a recent alternative to evaluating conditionals relative to a causal model see to a similar data instance or to a prototype when generating example-based

0.6692 cause cancer explicit and that it can be tested against actual data It might also be refined over time Figure c shows a causal model that assumes that texting does not directly cause accidents but that it does so via a mediator namely distraction Such details are often very important as they improve our understanding of the problem and allow us to develop ways to affect and often prevent specific outcomes by targeting the mediators directly Such causal relations can be formalized in so-called Structural Causal Models SCMs They are are derived from structural equation models SEMs eg but their relations have a direction Following Pearl an SCM consists of two sets of variables U and a set of functions that assigns each variable in V a value based on the value of the other variables in the model Formally Definition Structural Causal Model A structural causal model M is a tuple M where U is a set of exogenous variables V is a set of endogenous variables associates with each variable X V a function that determines the value of X given the values of all other variables Every SCM is associated with a graphical causal model called the

0.6659 an intervened variable Thus performing the actions A in a world M yields the post-intervention world X U X B U X B X U M X X U X B X B X U M X X B U X B M X X X B X B M Figure Given world model M intervening on X and/or on X result in different post-intervention models M corresponds to interventions only on X with consequential effects on X M shows the result of structural interventions only on X which in turn dismisses ancestral effects on this variable and M is the resulting independent world model after intervening on both variables ie the type of interventions generally assumed in the CFE-based recourse problem model mA with structural equations FA B Structural interventions are illustrated in Figure Structural interventions are used to predict the effect of actions on the world as a whole ie how M becomes MA In the context of recourse we aim to model the effect of actions on one individuals situation ie how becomes scf to ascertain whether or not the desirable outcome is achieved ie scf We compute individual-level effects using structural counterfactuals Assuming causal sufficiency of

0.663 approvals which are the cases that our work focuses on Explanation through Examples The most relevant class of explanations to our approach is through examples An example-based explanation framework is MMD-critic proposed by Kim et al which selects both prototypes and criticisms from the original data points More recently counterfactual explanations are proposed as a way to provide alternative perturbations that would have changed the prediction of a model In other words given an input feature x and the corresponding output by ML model The trained model obtained from the training data Original input x The feature vector associated with an instance of interest that receives an unfavorable decision from the ML model Original outcome The prediction of the original input from the trained model usually corresponding to the undesired class Original outcome class The undesired class Counterfactual example c i An instance and its feature vector close to the original input that would have received a favorable decision from the ML model CF class The desired class Table Terminology used throughout the paper a ML model a counterfactual explanation is a perturbation of the input to generate a different output y by the same algorithm Specifically Wachter et al

0.6629 of Marital Status and Capital Gain with merged feature deviates from the expected interactions For example for Point the SII of Education Level with Marital Status is and Education Number with Marital Status is therefore the interaction after merging Education Number and Education Level should be less than however the SII interaction value between Marital Status and the merged feature is more than Table Failure to satisfy generalized-efficiency leads to counterintuitive interactions after features are merged We merge Education Level and Education Number which are actually identical Shapley interaction values with Marital Status left and Capital Gain right before and after the features are merged tend to overestimate the expected merged value as compared to the expected value under generalized-efficiency Interaction with Marital Status Interaction with Capital Gain Pt Pt Pt Pt Pt Pt Pt Pt Pt Pt Education Level EL Education Number EN Expected Difference Negative interaction No interaction Positive interaction It It isnt t the th e greatest gr ea te st scifi sc flick Ive Iv e every er y seen se en but t it it is is not no t a a bad ba d movie m ov ie Negative interaction No interaction Positive interaction

0.6626 are probabilistically dependent and which independent of one another given a possibly empty set of variables on which one is conditioning Doing so requires the use of the so-called D-separation Theorem on the assumption that the Causal Markov and Faithfulness conditions hold readers are referred to Spirtes for detailed discussion the theorem and the axioms from which it follows Here we simply recount the rules of thumb by which one may read from a graph facts about which variables will and will not be probabilistically independent from one another Variables in a path ie a sequence of variables connected by arrows come in four varieties Terminal variables with only one arrow in or out C and B are terminal variables in the path CMB in Fig b Mediators which have one arrow in and one arrow out M is a mediator on the path CMB in Fig b Common causes which have two arrows out M is a common cause on the path CMB in Fig c and Colliders which have two arrows in S is a collider on the path CSMB in Fig a Two variables connected by a path are associated provided that the path is open A

0.6605 the set of feasible actions to be performed for minimally costly recourse with cost X R and scf FA denotes the resulting structural counterfactual We recall that although scf is a counterfactual instance it does not need to correspond to the nearest counterfactual explanation cfe resulting from see eg Example Importantly using the formulation in it is now straightforward to show the suboptimality of CFE-based actions as shown next proof in Appendix A Proposition Given an individual observed in world M a family of feasible actions and the solution of A Assume that there exists CFE-based action ACFE that achieves recourse ie cfe Then costA cost ACFE Thus for a known causal model capturing the dependencies among observed variables and a family of feasible interventions the optimization problem in yields Recourse through Minimal Interventions MINT Generating minimal interventions through solving requires that we be able to compute the structural counterfactual scf of the individual in world M given any feasible action A To this end we consider that the SCMM falls in the class of additive noise models so that we can deterministically compute the counterfactual scf FA by performing the Abduction-Action-Prediction steps proposed by Pearl et al Working example

0.6516 derived from the input data They do this without reference to the original model that produced the output by essentially constructing a surrogate model around the particular input point that produces the expected output Global explanations in contrast explain how the whole range of outputs is generated from the whole range of inputs Interpretable models typically provide global explanations It is also possible to construct surrogate models for black-box models that are global explanations Counterfactual explanations have been proposed as a particularly useful form of explanation for communicating ML-generated decisions to impacted users because of their resemblance to everyday explanations in human conversation Counterfactual explanations state what would have happened had the input variables been changed in certain ways These explanations are particularly useful when you want to help the user understand how they can change inputs under their control in order to achieve a different outcome WHAT WERE EXPLAINING Before discussing desiderata for explanations its helpful to first clarify what it is that ML explanations aim to explain The lack of consensus about desiderata stems partly from a lack of clarity on what the appropriate explanandum is what phenomena were trying to explain For example a local explanation has

0.6495 by fitting a decision-tree classifier to approximate the non-linear model and then tracing the decision-tree paths to generate explanations Similarly Lundberg and Lee present a unified framework that assigns each feature an importance value for a particular prediction Such explanations however lie about the machine learning models There is an inherent tradeoff between truthfulness about the model and human interpretability when explaining a complex model and so explanation methods that use proxy models inevitably approximate the true model to varying degrees Similarly global explanations can be generated by approximating the true surface with a simpler surrogate model and using the simpler model to derive explanations A major problem with these approaches is that since the explanations are sourced from simpler surrogates there is no guarantee that they are faithful to the original model Explanation through Visualization Similar to identifying feature importance visualizing the decision of a model is a common technique for explaining model predictions Such visualizations are commonly used in the computer vision community ranging from highlighting certain parts of an image to activations in convolutional neural networks However these visualizations can be difficult to interpret in scenarios that are not inherently visual such as recidivism prediction and loan

0.6449 knowledge in our proposed method Users can provide their domain knowledge in the form of pairs of features and the direction of the causal edge between them Using this we construct constraints that any counterfactual should follow For instance any counterfactual that changes the cause without changing its outcome is infeasible Given these constraints we apply a filtering step after CF examples are generated to increase the feasibility of the output CF set As an example we consider two infeasible changes based on the causal relationship between educational level and age Education Age and on the practical constraint that educational level of a person cannot be decreased Education As Figure shows over one-third of the obtained counterfactuals that include a change in education level are infeasible and need to be filtered most of them suggest an individual to obtain a higher degree but do not increase their age In fact this fraction increases as we look at CF examples for highly educated people Masters Doctorate and Professional as high as of all CFs suggest to switch to a lower education degree Though post-hoc filtering can ensure feasibility of the resultant CF examples it is more efficient to incorporate causal constraints

0.6418 to satisfies the monotoncity and fails to capture the interaction between for any Set-QII for all Similarly the Shapley-Taylor interaction index for and is as it does not follow the monotonicity property however for it satisfies monotonicity and interaction value for is The BII value for is In Section we show that BII can be interpreted as a polynomial approximation offering additional intuition as to why our explanation method is optimal GEOMETRICAL INTERACTION AND BII The geometry of model explanations is relatively well understood for attribute-based methods Ribeiro et al view linear explanation methods as local linear approximations of around a point of interest Linear model explanations can be thought of as functions taking the following form where captures the importance of feature Taking this interpretation linear model explanations can be objectively bad explanations they are a poor local approximation of the underlying model as black-box model can be highly non-linear around In order to better capture the behavior of a black-box model we approximate it using higher-order polynomials For better visualization we first assume that the black-box model R takes a binary input vector mainly referred to as the humanly understandable feature representation First we start with quadratic approximation

0.6407 we show the fidelity sample distributions at different values of for the decision trees trained using the ontological explanation pipelines ie the pipelines that select the first dataset Table Mean values of fidelity Explanation Pipeline Fidelity Ontological pipeline with ontological perturbation Ontological pipeline with normal perturbation Non-ontological pipeline with normal perturbation Table Mean values of hit Explanation Pipeline Hit Ontological pipeline with ontological perturbation Ontological pipeline with normal perturbation Non-ontological pipeline with normal perturbation neighbors of the instance to be explained using the ontological distance The first observation is that the decision trees trained directly on the real neighbors blue and green boxplots generally have a lower fidelity to the black-box compared to the ones trained on the augmented synthetic neighborhood orange and red boxplots This trend is true for all values of and for both the ontological pipeline with ontological perturbation and the ontological pipeline with normal perturbation The fidelity values of each decision tree have been evaluated on an held-out test set of synthetic neighbors This trend confirms that increasing the local density of points in the feature space around the instance to be explained helps the interpretable model to understand the black-box behavior The second observation is

0.638 baseline methods Remarkably DiverseCF has the highest number of continuous features changed too Cont-Count-Diversity even though it was not explicitly optimized for this metric The only exception is on COMPAS data where NoDiversityCF has a slightly higher Cont-Count-Diversity for but is unable to generate any valid CFs for higher Proximity To generate diverse CF examples DiverseCF searches a larger space than proximity-only methods such as RandomInitCF or NoDiversityCF As a result DiverseCF returns examples with lower proximity than other methods indicating an inherent tradeoff between diversity and proximity However for categorical features the difference in proximity compared to baselines is small up to of the baselines proximity Higher proximity over continuous Ad ul nc om e Le nd in gC b M PA S CFs Valid CFs CFs Categorical-Diversity Continuous-Diversity CFs Cont-Count-Diversity CFs Categorical-Proximity Continuous-Proximity CFs Continuous-Sparsity DiverseCF DiverseCF-Sparse MixedIntegerCF Figure Comparisons of DiverseCF with MixedIntegerCF on Valid CFs diversity proximity and sparsity on linear ML models For a fair comparison we compute average metrics only over the original inputs where MixedIntegerCF returned the required number of CF examples Thus we omit results when for COMPAS since MixedIntegerCF could not find more than four CFs for any original input Results

0.634 vectors Then the normal perturbation is applied in order to obtain a synthetic neighborhood and this kind of data can be fed to an interpretable model In order to obtain the labels for the synthetic data points however we have to decode them back into sequences so that we can feed them to our black-box model for labeling Once we have both the synthetic neighborhood and the corresponding labels we can train the interpretable model and finally extract symbolic rules Similarly to we chose a multi-label decision tree as inherently interpretable classifier c From such decision tree we extract rule-based explanations in the form p y where y The explanations are extracted by including in the rule premise p all the split conditions on the path from the root to the leaf node that is satisfied by the instance x The blue path involves the ontological perturbation In this case we can apply the perturbation directly on sequential data obtain a synthetic neighborhood as a set of sequences and feed them to the black-box model for labeling However as it was for the red path the interpretable model requires a tabular input so we proceed to flatten time-encode the synthetic

0.6337 datasets both precision and recall is higher for DiverseCF compared to LIME Approximating local decision boundary As a proxy for understanding how well users can guess the local decision boundary of the ML model see Section we compare classifiers based on the proposed DiverseCF method baseline methods and LIME We use precision recall and for the counterfactual outcome class Figure as our main evaluation metrics because of the class imbalance in data points near the original input To evaluate the sensitivity of these metrics to varying distance from the original input we show these metrics for points sampled within varying distance thresholds Even with a handful of training examples generated counterfactuals and the original input we find that -NN classifiers trained on the output of DiverseCF obtain higher score than the LIME classifier in most configurations For instance on the CFs C ou nd All Education Levels CFs Education Masters Prof-school Doctorate Any Change in Education Decrease in Education infeasible Increase in Education feasible Increase in Education infeasible Figure Post-hoc filtering of CF examples based on causal constraints The left figure shows that there are nearly CFs that include any change in education out of which more than one-third are

0.6328 Figure we plot the explanation generated by BII and LIME for individuals with Marital Status yes Capital Gain and Education Level from the adult dataset for the simple tree classifier see Figure LIME suggests that Capital Gain is the most important feature and the other two features have minor importance However for any coalition of features activated features are set to their POI value and the others to their baseline value Marital Status No and Capital Gain changing only Marital Status or Capital Gain to their POI value does not significantly change the prediction value but changing both of them concurrently to their POI value results in a significant increase in prediction probability see Table BII successfully captures these insights in Figure and suggests that the high positive interaction between Capital Gain and Marital Status is responsible for the positive label When considering feature interaction there is high positive interaction between Capital Gain and Marital Status an insight that linear explanations do not capture as seen in Table Figure summarizes the explanations generated by BII SII LIME and QII for a married woman with a Capital Gain CG of with Education Level in the adult test data for the simple

0.6325 the explanations extracted from the ontological pipeline with ontological perturbation orange boxplot is more variable than the ones extracted using the ontological pipeline with normal perturbation for large values of Aggregated statistics of fidelity and of hit for all the explanation pipelines are shown in Tables and we can observe that the value of hit is consistently high for all explanation pipelines and across all values of Explanation example We show in Figure an explanation example extracted with the ontological pipeline with ontological perturbation with In order to make it more comprehensible for readers not familiar with ICD- codes we enriched the rule-based explanation with the ICD codes semantic The original decision rule extracted from the decision tree can be seen at the top of the figure with the fidelity of the decision tree and its hit value There are several ways to read this rule since it contains many layers of information The decision rule is the decision tree pathway that leads from the root of the tree to the leaf containing the black-box decision for this reason all inequalities are to be considered in conjunction furthermore the ICD- codes occurring in the rule are ranked in order of

0.6236 om e Le nd in gC b Ge term an re di t M PA S CFs Valid CFs CFs Categorical-Diversity CFs Continuous-Diversity CFs Cont-Count-Diversity CFs Categorical-Proximity Continuous-Proximity CFs Continuous-Sparsity DiverseCF DiverseCF-Sparse NoDiverseCF NoDiverseCF-Sparse RandomInitCF RandomInitCF-Sparse SingleCF Figure Comparisons of DiverseCF with baseline methods on Valid CFs diversity proximity and sparsity SingleCF only shows up for Ally-axes are defined so that higher values are better while the x-axis represents the number of requested counterfactuals DiverseCF finds a greater number of valid unique counterfactuals for each and tends to generate more diverse counterfactuals than the baseline methods For COMPAS dataset none of the baselines could generate CF examples for any original input therefore we only show results for DiverseCF in COMPAS when Quantitative Evaluation We first evaluate DiverseCF based on quantitative metrics of valid CF generation diversity and proximity As described in Section we report results with hyperparameters and from Equation Figure shows the comparison with SingleCF RandomInitCF and NoDiversityCF for explaining the non-linear ML models while Figure compares with MixedIntegerCF for explaining the linear ML models All results are based on random instances from the test set Explaining a non-linear ML model Figure Given that nonlinear ML models are common

0.6208 the number of used features translates into less variability in the explanations This also ends up reflecting on the occurrence rate of the most popular feature ie the feature used the most times to explain an instance which in LIME occurred in of the transactions as opposed to the most common feature in TreeInterpreter which only occurred of the explanations The agreement between explainers is calculated by how many features two given explainers choose to integrate the explanation normalized by the length of the explanation For example if in an instance LIME and SHAP had chosen features in common to explain the instance score and the other features were different for each explainer the agreement in that instance would be for that pair of explainers Comparing explanations between SHAP and TreeInterpreter produces an agreement of ie of the features used by SHAP for a given explanation were also used by TreeInterpreter Likewise the agreement for the other explainers pairs produces an agreement of between LIME and SHAP and between LIME and TreeInterpreter These results show that the output explanation for a given instance depends on the post-hoc method chosen to explain it ie different explainers will choose different features to

0.62 tree classifier Figure Column a is the additive expansion of the feature-importance versions of these measures and Column b shows the methods that explicitly consider interactions Both BII and SII indicate a negative synergy between Education Level and Capital Gain whereas all methods indicate positive individual importance Hence additive expansions fail to capture negative interactions even for relatively simple models Marital Status Capital Gain Education Level Education Level Age Capital Gain Capital Gain No Yes No Yes No Yes No Yes No Yes No Yes No Yes Figure A three-level decision tree trained on the adult dataset achieving test accuracy Table This example shows the joint importance of the interactions between Capital Gain and Marital Status in the coalition of Age Education Level and the monotonicity property in the tree Figure The prediction probability becomes when Marital Status and Capital Gain both join the coalition Age Education Level however when the Marital Status or Capital Gain join individually in the coalition Age Education Level the prediction probability becomes and respectively This shows that the features have synergistic effect which is not captured by linear explanations this holds for other coalitions not just Age Education Level Features POI Base Coalition AEL

0.6156 it can be applied to black-box models works for any input data type and generates multiple explanations in a single run of the algorithm However there are some shortcomings that Organization C is addressing One challenge of counterfactual models is that the counterfactual might not be feasible Organization C addresses this by using the training data to guide the counterfactual generation process and by providing a user interface that allows domain experts to specify constraints In addition the flexibility of the counterfactual approach comes with a drawback that is common among explanations for black-box models there is no guarantee of the optimality of the explanation since black-box techniques cannot guarantee optimality Through the creation of a deployed solution for this method the organization realized that clients would ideally want an explainability score along with a measure of fairness and robustness as such they have developed an explainability score that can be used to compare the explainability of different models Counterfactual Explanations Takeaways Organizations are interested in counterfactual explanation solutions since the underlying method is flexible and such explanations are easy for end users to understand It is not clear exactly what should be optimized for when generating a counterfactual or

0.6116 and compare the different explanation pipelines Fidelity to the black-box This metric compares the predictions made by the interpretable model with the predictions made by the black-box on a synthetic neighborhood of the instance It measures the ability of the interpretable classifier to locally mimic the black-box and therefore it is tested on a held-out subset of the synthetic neighborhood Since we are dealing with a multi-label classification task we calculate the fidelity the measure with micro-averaging Hit This metric compares the interpretable classifier prediction and the black-box prediction on the instance to be explained It tells us if the interpretable classifier predicts the same label as the black-box on the instance we want to explain Since the prediction we are trying to explain is a multi-label classification we calculate the hit as hamming-distance b Explanation complexity This metric measures the complexity of the explanation as the number of premises in the rule-based explanation This measure is important since we do not want to approximate the black-box with a model that loses its interpretability because of the high-dimensionality of the explanations it produces Results Figure Fidelity distribution for the ontological pipeline with different perturbation type and training/test set In Figure

0.6103 measures do not capture player interactions rather they assign weights to individual players Owen proposes the first higher-order solution for a cooperative game for pairwise players Grabisch and Roubens extend it to interaction between arbitrary subsets and axiomatically derive the Shapley and the Banzhaf interaction indices In a recent paper Agarwal et al propose a new axiomatization for interaction among players which is inspired by the Taylor approximation of a Boolean function Feature Interactions Feature interaction has been studied by different communities Statistics offers classic ANOVA based feature interaction analysis Some recent work in the deep learning literature discusses feature interaction Tsang et al learns interactions by inspecting the inter-layer weight matrices of a neural network Tsang et al construct a generalized additive model that contains feature interaction information In another line of work Cui et al Greenside et al compute the interaction among features by computing the expected Hessian this can be thought as an extension of gradient-based influence measures for neural networks Datta et al also propose an influence measure for a set of features called Set-QII It essentially measures the change in model output when we randomly change a fixed set of features Lundberg et al propose

0.6097 column c for interaction after the modification Set-QII however is unchanged it fails to satisfy monotonicity and even for this small example cannot distinguish between the two trees Similarly the interaction between Capital Gain and Education Number should also increase which is again reflected by SII and BII Set-QII fails to show this see Figure The Effects of Efficiency Table highlights how failure to satisfy generalized efficiency can lead to counterintuitive interactions when features are merged We calculated the SII values for Education Level and Education Number for a random forest with Marital Status left and Capital Gain right on some sampled points we recalculated them after merging the features to Education Level Education Number These features can be naturally merged as they are in fact identical Education Number is just a numerical representation of Education Level In some instances the interaction values of Education Level and Education Number with other features differ in sign ie one has positive interaction and another has a negative interaction In this case under efficiency the interaction value of the merged feature Education Level Education Number should lie somewhere between the two In Table we show some of the points where the Shapley interaction

0.6082 ICD- nodes c and c let L be their lowest common ancestor LCA and R be the root of the ICD- ontology also let y be the number of hops steps required to reach node y from node x following the ontology links The WuP similarity measure between c and c corresponds to c c for any couple of ICD- nodes The lower bound is obtained when that is when the LCA of c and c is the root node Conversely a node has WuP-similarity with itself By relying on the underlying ICD- ontology we can therefore use the WuP similarity to compute pairwise distances between ICD- codes This yields a much more fine-grained analysis compared to a coarse Hamming similarity Visit-to-visit distance Having defined a code-to-code distance the following step is to compute distances at the visit level since visits are defined as lists of occurring ICD- codes We adopted the weighted Levenshtein distance a string metric for measuring the difference between two sequences as the minimum number of single-character edits insertions deletions or substitutions required to change one sequence into the other The weighted version of the Levenshtein distance allows defining custom insertion/deletion/edit costs We have set c

0.6081 path is open provided each variable in the path is on A variable is on in a path relative to a possibly empty conditioning set V of variables in the graph provided it is a terminal variable a mediator or a common cause and not in the conditioning set or is a collider and either is in the conditioning set or has some effect direct or distal which is in the conditioning set If every variable in a path is on the path is open and the two terminal variables are said to be d-connected on the conditioning set D-connected variables are probabilistically dependent conditional on the variables in the conditioning set and therefore conditionally associated in representative data Variables that are not d-connected relative to a conditioning set are independent of one another conditional on the variables in the conditioning set Using the d-separation theorem one can see that in a and b C is independent of and therefore in representative data will be statistically unassociated with S any outcome O influenced by S and the target behavior B There is no path and thus no open path between C and S O or B Similarly one can see

---------------------------------------------------------------------------
TOPIC 15: education

---------------------------------------------------------------------------

0.9486 va lue r a t e r than You got s c o r e above average One way you ld have got s c o r e below average i s i l s a t took va lue r a t e r than Another way you ld have got s c o r e below average i s i i s b l a c took va lue r a t e r than You got s c o r e below average One way you ld have got s c o r e above average i s i l s a t took va lue r a t e r than i s b l a c took va lue r a t e r than You got s c o r e below average One way you ld have got s c o r e above average i s i l s a t took va lue r a t e r than i s b l a c took va lue r a t e r than You got s c o r e below average One way you ld have got s c

0.94 t e r than had taken va lue r a t e r than ta l had taken va lue r a t e r than i l i za t ion had taken va lue r a t e r than had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than MS t l days had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than N e t r a c t i o n I n s t a l l B u r d e n had taken va lue r a t e r than ta l had taken va lue r a t

0.9383 lue r a t e r than Ne t r a c t i l v i had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i had taken va lue r a t e r than had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i i l e had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i had taken va lue r a t e r than P e r c e n t I n s t a l l T r a d e s had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than Another way you ld have got s c o r e good i n

0.9383 a t e r than had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than Ne t r a c t i l v i had taken va lue r a t e r than had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than Pe r cen lq had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than N e t r a c t i o n I n s t a l l B u r d e n had taken va lue r a

0.9304 taken va lue r a t e r than Ne t r a c t i l v i had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i E x t e r n a l R i s E s t im a t e had taken va lue r a t e r than MS t l days had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i E x t e r n a l R i s E s t im a t e had taken va lue r a t e r than MS t l days had taken va lue r a t e r than had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i t i s a c t o s had taken va

0.9281 lue r a t e r than Ne t r a c t i l v i had taken va lue r a t e r than had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i t i s a c t o s had taken va lue r a t e r than had taken va lue r a t e r than Ne t r a c t i l v i had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i P e r c e n t I n s t a l l T r a d e s had taken va lue r a t e r than had taken va lue r a t e r than Ne t r a c t i l v i had taken va lue r a t e r than Another way you ld have got s c o r e

0.9281 lue r a t e r than MS t l days had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i E x t e r n a l R i s E s t im a t e had taken va lue r a t e r than MS t l days had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i t i s a c t o s had taken va lue r a t e r than had taken va lue r a t e r than Ne t r a c t i l v i had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i P e r c e n t I n s t a l l T r a d e s had taken va

0.9281 e r than i l i za t ion had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i had taken va lue r a t e r than had taken va lue r a t e r than P e r c e n t I n s t a l l T r a d e s had taken va lue r a t e r than Ne t r a c t i l v i had taken va lue r a t e r than had taken va lue r a t e r than i l i za t ion had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i had taken va lue r a t e r than N e t r a c t i o n I n s t a l l B u r d e n had taken va lue r a t e

0.9148 r than had taken va lue r a t e r than i l i za t ion had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than MS t l days had taken va lue r a t e r than N e t r a c t i o n I n s t a l l B u r d e n had taken va lue r a t e r than had taken va lue r a t e r than i l i za t ion had taken va lue r a t e r than had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i had taken va lue r

0.9045 i s i E x t e r n a l R i s E s t im a t e had taken va lue r a t e r than Another way you ld have got s c o r e bad i n s t e a d i s i Ne t r a c t i l v i had taken va lue r a t e r than Another way you ld have got s c o r e bad i n s t e a d i s i had taken va lue r a t e r than Another way you ld have got s c o r e bad i n s t e a d i s i t i s a c t o s had taken va lue r a t e r than Another way you ld have got s c o r e bad i n s t e a d i s i i l e had taken va lue r a t e r than had taken va lue r a t e r than Another way you ld have got s c o r

0.9045 e bad i n s t e a d i s i E x t e r n a l R i s E s t im a t e had taken va lue r a t e r than Another way you ld have got s c o r e bad i n s t e a d i s i t i s a c t o s had taken va lue r a t e r than Another way you ld have got s c o r e bad i n s t e a d i s i P e r cen lq had taken va lue r a t e r than You got s c o r e bad One way you ld have got s c o r e good i s i E x t e r n a l R i s E s t im a t e took va lue r a t e r than You got s c o r e bad One way you ld have got s c o r e good i n s t e a d i s i had taken va

0.8664 good i n s t e a d i s i P e r c e n t I n s t a l l T r a d e s had taken va lue r a t e r than had taken va lue r a t e r than had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i i l e had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i i l e had taken va lue r a t e r than Another way you ld have got s c o r e good i n s t e a d i s i had taken va lue r a t e r than P e r c e n t I n s t a l l T r a d e s had taken va lue r a t e r than had taken va lue r a t

0.8492 in va how to register to vote how to register to vote in va how to register to vote in virginia primaries primaries in virginia primaries virginia sample ballot sample ballot virginia sample ballot fairfax county voter registration voter registration virginia voter registration card va voting voting in virginia voting in va voting virginia where do i vote in primaries where to vote where to vote in virginia where to vote in virginia primary who is running who is running for virginia senate and Massachusetts on June the election day in Virginia Comparing the two sets of autocomplete phrases we find an overlap of While the overlap is substantial the fact that of phrases are different indicates that there is a degree of localization happening in both locations Given that our focus was on the election happening that day we further analyzed the seeds and suggested phrases that indicated an awareness about local elections The results for both locations are shown in Table and Table Comparing the results in the two tables shows that there are Table Out of seed queries received autocomplete suggestions for the location in Massachusetts There was no election happening in Massachusetts on the collection date

0.8197 ti ti for The score si is public ie the school and the university both observe it A school has access to a set of signals as before but now can design a signaling scheme that is a function of both the students grade and his test score ie the school designs Pr si for The university again makes accept/reject decisions that maximize its expected utility but now the university has access to the test score si and its distribution as well as the signal and the distributions p and q As before a strategic school chooses a signaling scheme that maximizes the fraction of students accepted whereas a revealing school simply transmits the grade to the university as the signal As in Section we introduce an assumption controlling the noise of the test Assumption The university has non-negative expected utility for accepting a student with a high test score and negative expected utility for accepting a student with a low test score p p We note that if the expected utility of accepting a student with a high test score were negative or the expected utility of accepting a student with a low test score were positive then in

0.7929 time-invariant confounding Assuming no selection bias or time-varying confounding we can In Appendix C we present similar placebo results for measurement validity Age quartile oldest R ac e ar e la rg es t p er ce nt n on hi te Coverage Percentage nonwhite C ov er ag e Voters Poll age elder young Figure Intersectional coverage effects by race and age The top panel presents the coverage rate by quartiles of age on the G-axis and race on the -axis The bottom panel plots the coverage rate on the -axis against percentage of nonwhite voters at the polling location on the G-axis for older polling locations yellow versus younger polling locations blue for ventiles of poll location by race Lines display linear smoothing of the individual poll locations Coverage is lowest among older minority populations and highest among younger whiter populations conclude that SafeGraph has disparate coverage by age and race two demographic risk factors for COVID age race Rank correlation co un t Election Regular Figure Distribution of placebo rank correlations between election-day demographics and marginal SafeGraph traffic on non-election days Under the empirical placebo distribution the election-day coverages negative correlations with age top panel and race

0.7808 worse for school due in part to school holidays or large-scale events such as sports games As a result we exclude schools from our analysis In what follows we use the generic variable to indicate either measure of demographics Time-invariant confounding Example Younger voting populations vote at places like community centers with large non-voter traffic whereas older populations vote at places like re stations with little non-voter traffic Testable see Time-varying confounding Example Younger voting populations vote at places that are open to non-voter traffic on election day whereas older populations vote at places that are closed to non-voter traffic on election day Untestable assumption Figure We distinguish between two types of confounding time-invariant versus time-varying confounding We test for time-invariant confounding but we cannot test for time-varying confounding Our results assume no time-varying confounding D D We will say that disparate coverage exists when the rank correlation between coverage on election day and voter demographics is statistically different from the rank correlation between coverage on non-election day and voter demographics For cor A A cor A A We evaluate this more robust notion of disparate coverage using weekdays in October and November of to generate a placebo distribution of

0.7733 Seed Suggestion Rank election day election day massachusetts how to register how to regiser to vote in ma how to register to vote how to register to vote in ma how to register to vote in mass primaries primaries in massachusetts sample ballot sample ballot brookline ma sample ballot massachusetts voter registration voter registration ma voter registration card ma where do i vote where do i vote ma where do i vote boston where do i vote brookline ma the suggestions generally appear towards the top of the SERP This can lead us to hypothesize that to some extent Googles algorithms are able to pick up the signal about Virginias elections and update the autocomplete suggestions accordingly However most of the seed queries didnt contain any localized suggestions for VA and for MA and it is difficult to predict which queries will display locally relevant results This is especially concerning for queries that are semantically similar but display minor syntactic differences For example the phrases who is running and whos running differ only in the contraction of the verb but all their suggestions but one are different Similarly suggestions for where can I vote where do i vote and where

0.7732 the expected utility may be non-monotone in q a b c d Figure The ratio Us of utilities of a strategic school vs a revealing school as a function of the grade accuracy q with and without test score We observe that the test score intervention may increase inequality

0.7704 util X Y U B util X Y Note that before selecting the candidates the decision-maker knows U but does not know U Our implicit bias update model assumes that the decision-maker sees U after selecting the candidates The rationale is that after the decision-maker selects the candidates n and observes their actual performance at the job or in an interview it can better estimate their latent utility U Implicit bias update model Inspired by the works that give mappings from beliefs to beta distributions in each iteration we model the implicit bias as a draw from the a beta distribution Beta ab where ab and roughly a is the evidence favouring the belief and b evidence against it In our setting a is roughly proportional to the belief that X candidates have the same latent utility as Y -candidates and b is roughly proportional to bias against it Notice that the larger a is the closer is to no bias and the larger b is the closer is to largest bias If U U then the decision-maker has evidence that the X candidates performed better than expected In this case the decisionmakers implicit bias reduces ie a would increase

0.7658 expected quality of a student is negative such that the university would reject students without any signal from the school set q q and all results carry through by symmetry considers all students for admission Assumption The universitys expected utility for accepting any given student absent any auxiliary information is negative ie p p and therefore p Next we assume the universitys expected utility of accepting a student with a high low grade is positive negative Assumption The university has non-negative expected utility for accepting a student with a high grade and negative expected utility for accepting a student with a low grade Pr t Pr t Pr t Pr t These can be rewritten as p q p q We note that if the expected utility of accepting a student with a high grade were negative then none of the schools students would be admitted by the university under any signaling scheme On the other hand if the expected utility of accepting a student with a low grade were positive then the university would always accept every student in which the utilities of revealing and strategic schools may differ The following easy consequence of these assumptions will be useful

0.7609 latent utility of selected blue red tiles as a fraction of the optimal strategy utility Figure The latent utility of RRGrp is less than that of ControlGrp but the latent utility derived from the blue tiles was higher for RRGrp than ControlGrp Latent utility mean standard deviation overall derived from blue derived from red RRGrp ControlGrp We find that in the long term the latent utility obtained by RRGrp is less than that of the ControlGrp by only Moreover the latent utility derived from the blue tiles was higher for the RRGrp than ControlGrp see Figure Hence even though the latent utility attained by RRGrp and ControlGrp were not substantially different a much greater proportion of RRGrp latent utility came from blue tiles The difference between the means of latent utility derived from blue tiles for the two groups is statistically significant at p See Supplementary Material C for details This suggests that implementing the Rooney Rule can increase the contributions from underrepresented candidates to the overall latent utility without substantially decreasing the total latent utility attained As another metric for how well a decision-maker subject to the Rooney Rule can fulfill its utility-maximizing objective we also analyzed optimal strategy

0.7301 and are updated more frequently than related searches Refer to materials on our webpage Table An overview of the four datasets used throughout this paper Three datasets were created by this papers authors and one was received by the authors of Hu et al Name Period Description Creator Voter Searches Oct Nov A list of search phrases collected through a survey of AMTs Authors Partisan Queries Jan Aug A list of phrases extracted from politicians speeches of democrats and republicans with a partisan bias score Hu et al RS-Candidates Oct Nov Googles related searches RS in SERPs for two groups of candidates Authors Virginia Election June Autocomplete phrases and SERPs for queries searched simultaneously in Virginia and Massachusetts Authors One way to think about these two groups of searches is that the former correspond to trending searches while the latter to most popular searches about a seed query After collecting SERPs for two groups of candidates for the US midterm elections we extracted the related searches RS from each page The groups of candidates were as follows Women candidates Women running for congress or governor Their names were gathered from a Washington Post article that was tracking the political fortunes

0.7296 in most of the voter choices On the other hand tags with low bias tend to be ranked higher by most of the voters This interesting observation explains why methods like or STV Gender Race Age D e m o g ra p B s STV a M a le e m a le M a le e m a le M a le e m a le M a le e m a le M a le e m a le o S e le e d T a g s STV b White Black Asian o S e le e d T a g s STV c Figure a Demographic bias b Gender and c Racial under-representation in tags selected by different methods p c c c c p c Ranked choices of the voters o v o te rs High Gender Bias Low Gender Bias a p c c c c p c Ranked choices of the voters o v o te rs High Racial Bias Low Racial Bias b p c c c c p c Ranked choices of the voters o v o te rs High Age Bias Low Age Bias c Figure Ranked choices

0.7192 for a revealing school is therefore constant in q For intermediate values of q the grades are still uninformative on their own but are informative coupled with a high standardized test score at this point only students with both a score and a grade of get admitted by the university and the schools expected utility suddenly drops when compared to smaller q The schools expected utility in that regime is increasing in q as under Assumption increasing the value of q increases the fraction of students with both high scores and high grades Finally when q is large enough the grades are significant enough on their own that only students with high grades are admitted this leads to a jump in expected utility compared to the intermediate regime In this regime for high values of q the schools expected utility is decreasing as a result of the fact that increasing the value of q now decreases the number of students with a high grade by Assumption as seen in Section Lemma Strategic schools utility The expected utility Us when a school signals strategically and is Us p when the expected utility is Us p q p q p p q

0.7189 the type distribution the accuracy of its grades and the accuracy of the standardized test score We defer all proofs to the Appendix For a revealing school the university always accepts high-grade high-score students If high grades are more informative than low test scores that is if which depends on p as well as q and and happens for instance if p q and then the university also accepts students with low test scores benefiting the school Alternatively if high test scores are more informative than low grades ie then the university also accepts students with low grades These conditions provide additional boosts to the utility of a revealing school Lemma Revealing schools utility The expected utility Ur of a revealing school with access to grades and a test score is Ur p q p p For the special case of a revealing school with accurate grades when q we have Ur p p As illustrated in Figure for fixed p and Ur may not be a decreasing function of q In fact when q is small enough the grades are completely uninformative and the university only admits students with a test score of In that regime the expected utility

0.7044 of women candidates Senate challengers Challengers running in the primaries and the Senate election for US states The names of candidates were collected from Ballotpedia in July by visiting the electoral information webpage of each US state holding a election An example of related searches for the candidate Alexandria Ocasio-Cortez is shown in Figure For our analysis we are interested in the words or phrases next to the candidate name in this case husband married age etc To have a more complete dataset we collected SERPs for both groups of candidates on different days on the period Oct Nov and compiled all unique phrases in related searches for each candidate Then we automatically removed the names of the candidates to focus on the additional words and phrases in the queries We found unique phrases for Women and unique phrases for Challengers These phrases were manually inspected to correct some of the inevitable errors of the automatic phrase extraction Figure The related searches for Alexandria Ocasio-Cortez a woman candidate running for the US Congress screenshot taken on Nov We automatically remove the candidate name from the related searches and aggregate the remaining words or phrases In this case these words are

0.6992 engine will be playing by directing voters to information sources in response to such queries is even more important given the scale that this might be happening The thematic coding in Table contains more insights The voters are interested not only in a wide range of topics but also in a variety of information kinds factual information where can I vote speculative information whos winning Florida and so-called problematic queries that might lead to data voids eg immigration based crime Given such variation to carry out comprehensive voter-centered audits we will need to design mechanisms to elicit such queries from voters in order to examine how the search engines treat these distinct epistemic categories factual speculative and problematic of information needs Ethnographic research by provides additional evidence for such a need Unreliable Localization The Virginia Election dataset contains SERPs and autocomplete suggestions that were collected on two different locations Virginia Table Out of seed queries received autocomplete suggestions for the location in Virginia related to the primary election happening that day in Virginia R Rank Seed Suggestion R candidates candidates in virginia primary election election day virginia election day election day virginia elections elections in virginia elections in virginia elections

0.699 which had the number of IITs as We allocate n seats using the simple constraints scheme Cons and Uncons and n seats using and Uncons and compare the average latent utilities of the candidates admitted by the scheme Here we define the average latent utility from scheme A which admits nA candidates as We vary from to ie from the fraction of women admitted if all candidates are admitted on basis of their scores to the value which corresponds to proportional the representation based on the number of candidates appearing in IIT-JEE We report the average latent utilities in Figure Figure Empirical Results Studying the Effect of Supernumerary Seats for Women We plot the average latent utilities of the ranking schemes we consider The y-axis represents the utility per candidate and the x-axis represents the lower-bound constraint Our constrained interventions outperform the unconstrained variants up to approximately female seats and always outperforms the existing supernumerary approach used in practice Results We observe that the always has an lower average utility that scheme We note that the and the always select the same number of candidates The difference is that could place all the underprivileged candidates at the end of the

0.6929 In based on the empirical observations of the setting where the decision-maker does not observe latent utilities and instead sees an observed utility which is its possibly biased estimate of the latent utilities is considered They consider the following model of observed utilities parameterized by an implicit bias parameter define the observed utilities of an X -candidate i as Xi B Xi The observed utility of a Y -candidate is assumed to be the same as its latent utility Notice that in the above definition if then Xi Xi for all i and the decision-maker evaluates X -candidates without bias It is sometimes useful to define the following vectors X B Xi X B Xi and Y B where i varies over and varies over GY Candidate selection problems and the Rooney Rule The utility of a subset of candidates S is defined as the sum of the utilities of all candidates in S Given a subset S n define its total observed utility as util s X Y B i Xi Similarly define the total latent utility of S n as the sum of the latent utilities of all candidates in S util Y where we replace X in

0.692 and executes any experiments ie treatments associated with those audiences This design enables us to record all audiences and experiments that are available on a website that uses Optimizely We crawled sites drawn from the Alexa Top-M that were previously observed including resources from Optimizely Of these sites were running experiments at the time of our crawls from January to March Using this dataset we examine what kinds of websites conducts OCEs how many audiences do We are not aware of any major website that prominently discloses the existence of Optimizely experiments to users or asks for affirmative consent although it is possible that websites may disclose this practice in their terms of use or privacy policy experiments do they run In total our analysis considers audiences and experiments Measurement Results We found that the usage of Optimizely is heavily distributed over top-ranked websites Most of these websites were conducting experiments on audiences while a small number of websites were running dozens of experiments with complicated audience structure eg Optimizely itself The New York Times and AirAsia We analyze how websites segment audiences overall and present detailed results for popular attributes such as location device and browser Furthermore we analyze

0.692 Connecticut Washington etc with of websites in our sample segmenting Californians Similar to above we cross reference this distribution with the technology index of each state from the State Technology and Science Index The scatterplot is shown in Figure and again we find a significant and positive correlation between technology index and audience distribution Pearson r Websites that heavily target locations include with audiences and Teacher Certification Map with audiences Experiments Next we shift focus to the experiments per se that are being run by websites Of the websites that used Optimizely in our sample we observed of them running experiments The remaining websites may just be using Optimizely for analytics purposes or they may have had a lull in experiments at the time of our crawls Experiment count Pe rc en ta ge o eb si te s Figure Distribution of experiments per website of websites are running experiments Nine websites are running experiments with the max being by Optimizely itself Audience count Pe rc en ta ge o e er im en Figure Distribution of audiences per experiment of of experiments have no audiences which corresponds to all site visitors of have a single audience Figure shows the

0.6786 website that include the Optimizely JavaScript library in Figure We observe that most websites of in our sample include Optimizely on all of their pages We hypothesize that this could be the result of templated web design The remaining websites in our sample either include Optimizely on a few of their pages of websites include Optimizely in of their pages or most of their pages of websites include Optimizely in of their pages The former observation suggests that there may be false negatives in our dataset ie websites that use Optimizely on a small number of pages that our crawler happened to miss The latter observation is unsurprising since even templated website designs sometimes have exceptional pages that do not follow the design language eg terms of use and privacy policies Note that just because a website includes the Optimizely library does not necessary mean it is actively running experiments with Optimizely We examine the distribution of experiments in Audiences We now turn our attention to the audiences that website operators have defined for their experiments We observe that of websites in our sample defined audiences for their experiments Note that this does not necessarily mean the remaining Longitude La

0.6786 coverage by modifying for the multiple regression setting We perform linear regressions to model coverage as a function of the percentage over and the percentage non-white for each weekday in October and November We test whether the election-day coefficients on age/race are different from the non-election day coefficients on age/race provides details using the notation that denotes the proportion of voters over age and denotes the proportion of voters who are non-white RESULTS Measurement Validity Election day brings a dramatic increase in traffic to polling locations relative to non-election days and any valid measure of visits should detect this outlier Figure shows the daily aggregate traffic across poll locations for October and November of and as expected we see a significant increase in both total traffic top panel and marginal traffic bottom panel on election day To assess the strength of this signal using the framework described above we present the correlation between marginal SafeGraph traffic on election day and actual voter turnout The rank correlation test yields a positive correlation cor A A with Algorithm Assessing Joint Disparate Coverage Input Voter data SafeGraph data Result -values for the election-day coefficients on race and age under the placebo distribution for

0.6766 and common knowledge to all agents A school observes noisy information about the types of each of its students To formally model this we assume student i has a grade which is observed by the school but is unknown to the university The for student i is drawn as follows ti ti q for q That is the students type is flipped with some probability q As q increases the grade becomes amore accurate estimate of the students type ti The grade is known to the school but not the university The distribution q of the grade however is public ie common knowledge to all parties A school has access to a possibly trivial or uncountably infinite set of signals and commits to a signaling scheme mapping grades to probability distributions over signals in For each student i the university makes an accept/reject decision based on the distribution of the types p the distribution of the grades q and the realization of the signal chosen by the school The goal of the university is to maximize the quality of the students it accepts over the set of students it accepts with utility for accepting a student of high type ti

0.6747 to as the grades become more inaccurate In the regime for high values of q the university admits students with a high grade only independently of what their standardized test scores are therefore the utility of a revealing school is the same with or without a standardized test On the other hand when the standardized test score becomes more inaccurate the strategic school can take advantage of the noise in said score to bundle in more students than if there was no standardized test the university loses in utility from accepting unqualified students with high scores but at the same time gains in utility from accepting qualified students with low scores allowing a strategic school to bundle more students when compared to the case with no standardized test As decreases and the standardized test becomes less and less accurate a strategic school starts losing fewer high-score students to rejection than it gains in admitted low-score students and its utility increases FURTHER DISCUSSION AND FUTURE DIRECTIONS Our paper in introducing the study of inequity induced by population-level signaling raises a number of directions for future work We discuss a few of them here First one might be interested in enriching the

0.6684 shows all SafeGraph traffic and the bottom panel shows the marginal traffic computed using the method in In both total and marginal traffic the election day dotted line shows a significant boost in traffic SafeGraph marginal election traffic te rs Figure Election day traffic as observed by SafeGraph Gaxis and actual voter turnout across polling locations axis Each dot represents a polling location in North Carolina in the general election Main results Figure shows that the negative election-day rank correlation between coverage and voter demographics is significantly outside the placebo distribution for non-election days Percentage over age C ov er ag e Voters Percentage nonwhite C ov er ag e Voters Figure Estimated SafeGraph coverage rates against age and race for North Carolina general election Each point displays a ventile of poll location by age top and race bottom The blue lines depict LOESS smoothing on the individual poll locations empirical one-sided -values are for both age and race respectively For our joint analysis of disparate coverage we nd that the negative coefficients for age and race are statistically outside the placebo distribution See Fig empirical one-sided -values are and for age and race respectively Our endings are robust to

0.6658 si te s the Optimizely library Most websites of include Optimizely in all of their web pages in our sample Optimizely JSON configuration data by executing the following function that is made available by the Optimizely JavaScript library exp_json optimizelygetdata If the Optimizely JavaScript library is included in the current web page and if the websites operator has configured experiments then exp_json will contain a reference to the JSON configuration data and the crawler saves this to a file Otherwise an exception is generated and we record that the web page did not contain any Optimizely experiments Our crawler visited all target domains once per week from January to March We visited the websites periodically to collect longitudinal data about their usage of Optimizely The crawler visited the homepage for each website as well as randomly selected links on the homepage that pointed to the firstparty domain Note that for any given domain it is possible for zero or more web pages to include Optimizely experiments One reason we might observe zero usage is that the website used Optimizely in but no longer does Another potential reason is that the website uses Optimizely for analytics eg as an alternative to

0.6624 positive and false negative rate achieved by a school depending on the accuracy of its grades and whether it uses the optimal strategic signaling scheme when transmitting information about its students to the university These lemmas will form the basis of our evaluation of the impacts of strategic signaling in Section Recall that we refer to a school that does not strategically signal and instead transmits its raw grades to the university as revealing The proofs of the following Lemmas follow by direct calculations We provide an exposition of the more involved calculations of Lemmas and in the Appendix Lemma Revealing schools utility The expected utility Ur of a revealing school is Ur p q For the special case of a revealing school with accurate grades when q we have Ur p p A revealing school gets exactly the students with high grades accepted as per Assumption in particular a q fraction of high-type students will have a high grade and be accepted while a q fraction of the low-type students will be accepted Lemma Strategic schools utility A schools expected utility Us when it signals strategically is given by Us p q p q p For the special case

0.6585 of a strategic school increases while that of a revealing school decreases in q as we have seen in Section The form in the presence of a test score can be explained as follows First when in the regime of small values of q only students with a high standardized test score are admitted by the university in which case admission decisions do not depend on how the schools act and both the strategic school and the revealing school have the same expected utility leading to a ratio of For intermediate values of q we have previously discussed that the utility for a strategic school remains constant the university still has positive utility for students with a score of and the strategic school can bundle all such students together regardless of grade while the utility for a revealing school suddenly drops only students with both a high grade and a high score are admitted and is increasing in q explaining the sudden drop in ratio of utilities at the change of regime and the decreasing monotonicity of the ratio in q within the intermediate regime When q becomes large enough we have seen that both the revealing and the strategic

0.6457 to vote dont overlap much This random variation makes it challenging to consider autocomplete suggestions as a reliable source for directing searchers toward valuable suggestions during an election period On Virginias election day we also collected SERPs in both locations Are SERPs personalized to show results based on location for simple queries such as elections As the screenshot in Figure shows there is evidence for such hypothesis too Nevertheless drawing a clear conclusion from the comparison of the organic links from all SERPs between the two locations was also challenging For of the queries the results are identical eg polls election polls republicans who is running etc but for of them the Jaccard similarity is less than little overlap between links Some of the search phrases with little overlap include voting locations elections poll results sample ballot how to vote etc For these queries many local links local government or local news are shown This is a desirable situation but the unpredictability of which queries lead to locally relevant SERPs and which do not makes this an unreliable feature similarly to autocomplete suggestions How consistently are results for important searches localized Does it depend on ones locality Does it depend

0.6444 shows how SafeGraph coverage varies with the proportion of voters over age The rank correlation test yields cor A A with -value We also show how coverage decreases as the proportion of non-white voters increases bottom panel The rank correlation of race and coverage is cor A A with -value The top panel of Figure presents a heat map of coverage with age bins quartiles on the G-axis and race bins quartiles on the -axis This bottom left cell for instance shows that precincts that are the most white and young have highest coverage rates The lowest coverage is for older minority precincts The lower panel of Figure similarly plots race on the G-axis against coverage on the -axis separating older precincts yellow and younger precincts blue Older precincts on average have lower coverage rates than younger precincts and coverage declines as the minority population increases The correlation is similar but slightly lower for unadjusted SafeGraph traffic cor A A with -value All M Oct Oct Nov Nov Dec Date Sa fe G ra ph tr Monday Tuesday Wednesday Thursday Friday Figure SafeGraph traffic by weekday over October and November for all polling locations in North Carolina The top panel

0.6426 evaluates underrepresented candidates without bias and its bias against them becomes more severe as approaches The decision-maker shortlists candidates out of n with the highest observed utility In the setting where n is much larger than characterize conditions on the proportion of underrepresented candidates B n and the distribution of latent utilities such that under these conditions applying the Rooney Rule for increases the total latent utility of the shortlisted candidates An important benefit of the Rooney Rule is that the decisionmaker has the opportunity to closely evaluate qualified underrepresented candidates see that their latent utility was greater than expected and learn to evaluate underrepresented candidates more accurately Indeed studies show that implicit biases can change over time and with changes in local-environments In particular it has been observed that exposure to other groups and counter-stereotypical evidence opposing the implicit beliefs can help reduce implicit bias Thus one would hope that as the decision-maker observes the latent utilities of more underrepresented candidates over multiple iterations of selection its implicit biases would change This is in line with work on belief and opinion formation which model how individuals update their beliefs and opinions based on the information they observe At a

0.6342 mean latent utility of RRGrp versus ControlGrp However while the mean total latent utility between RRGrp and ControlGrp was not very different a much larger proportion of the latent utility came from blue tiles for participants subject to the Rooney Rule We interpret this to mean that participants subject to the Rooney Rule learn to increase the latent utility derived from blue tiles while nearly matching the total latent utility of selections made by unconstrained participants These trends suggest that over the long term implementing the Rooney Rule can yield significant benefits for increasing representation of underrepresented candidates without substantially decreasing total latent utility LIMITATIONS Model and theoretical results We consider a model with two groups where the decision-maker is implicitly biased against one underrepresented group However in real-world applications there may be multiple and intersecting groups say those defined by race and gender and where the candidates at the intersection of two or more underrepresented groups may face a larger implicit bias Further we consider that the decision-maker evaluates the candidates selected say for an interview in aggregate it compares U t and U t and sees the latent utilities of all selected candidates without noise But the decision-maker

0.6317 per website Most websites of have defined audiences although Optimizely itself has defined Percentage of websites Platform Language IP Browser Location Cookies Customized Device Ty pe o a ie nc es Figure Popularity of audience attributes A majority of websites of are using customize JavaScript to define audiences Percentage of websites iPad Tablet iPhone Mobile Desktop D e for audiences Percentage of websites Opera Edge Firefox Chrome Safari IE ow se r for audiences not the experimental tools that we are interested in Third websites may use Optimizely on portions of their site that our crawler failed to reach due to the random nature of our crawls For the remainder of this study we focus on these websites In Figure we examine how the usage of Optimizely is distributed across websites sorted by popularity according to Alexa We observe that the likelihood of using Optimizely decreases drastically with Alexa rank Point Biserial of the usage we observe occurs on the top most popular websites This observation suggests that even though Optimizelys tools are designed for non-experts only websites with significant budgets and staff are able or willing to engage in OCEs Next we examine the percentage of web pages per

0.6295 husband married age etc Dataset Virginia Election When we analyzed the phrases from the Voter Searches dataset we noticed that many of them were short and ambiguous For example respondents wrote phrases like voting or elections as well as candidates near me We hypothesized that one reason that voters might formulate such queries is that they rely on the search engine to correctly interpret their intentions despite their searches being short and ambiguous Google users are familiar with searches like weather or pizza near me that correctly leverage the users location to personalize the search results Another reason for supplying short queries to the search engine might be that users are accustomed to taking advantage of Googles autocomplete feature to suggest longer and more appropriate queries In order to test such hypotheses we conducted a pilot study the day of the primary election for the State Legislature in the US state of Virginia June We selected query phrases from the Voters Searches dataset mostly short queries such as voting election day democrats as well as syntactic variations of semantically similar queries such as where can i vote where do i vote or where to vote We used two computers one

0.6291 where the B group is the audience members and the A group is all other site visitors The remaining experiments of include multiple audiences The experiments with the most complex audience specifications totally in our sample are mostly personalizations from AirAsia of and The New York Times of Finally we investigate variations per experiment ie the number of different treatments shown to subjects in each experiment As shown in Figure of of experiments in our sample have a single variation which corresponds to experiments were all site visitors experience the same treatment These may represent the final stage of successful experiments eg the operator determined that a specific treatment in an A/B/n experiments was most effective and they now apply it to all site visitors of have two variations which may correspond to A/B tests The remaining experiments of have more variations which may correspond to multivariate tests The experiments with the most variations totally in our sample are run by the websites iCrackedVapeWorld Shutterstock and the Department of Motor Vehicles DMV Variation count Pe rc en ta ge o e er im en Figure Distribution of variations per experiment of of experiments have a single variation which corresponds to

0.6288 a perfect standardized test or in fact a sufficiently good one ie high enough it is not hard to see that the university accepts exactly those students with a high test score from strategic as well as revealing schools Thus no matter the accuracy of the grades or distribution of types the standardized test results in equal expected utility and hence equal access to opportunity for revealing and strategic schools see Appendix for details Similarly if grades are accurate ie q then a revealing schools expected utility is fixed at p whereas a strategic schools expected utility is only diminished from p without the test by the extra constraints introduced by a standardized test Thus in this case as well a standardized test decreases the inequality between the utilities of a strategic and a revealing school making the ratio of utilities less than see Appendix for details Figure plots Us with and without test scores as a function of q for p and different values of The form of the utility ratio between a strategic and a revealing school in the absence of a test score follows from the fact that both utilities are continuous and that the expected utility

0.6264 EMPIRICAL OBSERVATIONS We enlisted participants on Amazon Mechanical Turk for an iterative candidate selection experiment in order to observe the effect of the Rooney Rule on their long-term behavior Participants were shown colored tiles representing candidates along with the tiles observed utilities and were instructed to select a small fixed number of tiles with the goal of maximizing the latent utility of the selection this was incentivized by tying their bonus payment to the latent utility attained They were informed that the observed utilities were noisy estimates of the latent utilities We required some participants to follow the Rooney Rule and did not constrain others In this section we give an overview of the experimental design and our analysis of the results Experimental design A total of participants located in the US were recruited using Amazon Mechanical Turk to complete a repeated selection experiment In each of T iterations participants were presented with n tiles representing candidates in order of highest to lowest observed utility Half of the tiles were red representing Y -candidates and half were blue representing X -candidates ie The latent utilities X t i Y t for each tile was drawn from the uniform distribution on

0.6241 evidence that the Rooney Rule has had a positive impact in various contexts it is a subject of much debate Proponents of the policy argue that it counteracts the effects of implicit bias while critics warn that it can lead to poorer selections Towards demonstrating the effectiveness of the Rooney Rule proposed a mathematical model of implicit bias and showed that the Rooney rule can improve the true utility of the decisionmaker in a single hiring decision More precisely they consider n candidates partitioned into two disjoint groups GY n where is the group of underrepresented candidates Each candidate has a true latent utility which is the value they would contribute if selected and an observed utility which is the decisionmakers potentially biased estimate of their latent utility They model the decision-makers implicit bias as a multiplicative factor such that the observed utility of underrepresented The code for the experiment in Section is available at To complete a version of the experiment similar to the one presented to participants see candidates those in is times their latent utility while the observed utility of all other candidates those in GY is the same as their latent utility Thus if the decision-maker

0.623 ranking while places at least underprivileged candidates every positions We find that for any decreases the average latent utility of the admitted candidates Finally we observe that for a range of from to Cons increases the average latent utility of the admitted candidates over Uncons and increases the latent utility over Uncons ie for all U U Uncons U U Uncons The optimal constraint for is argmax U Intuitively can increase the average utility by swapping a male candidate i Ga and female candidate Gb such that wi Note that in additional candidates selected from gb compete against the lowest scoring candidates from gb in the ranking instead of the average utility U as in Remark Since was not designed to optimize the average utility it is not surprising that outperforms in our experiment However the goal of this experiment is to study the effect our constraints approach against systematic biases different from implicit bias by qualitatively comparing them an existing scheme DISCUSSION AND LIMITATIONS One could also consider other approaches to mitigate implicit bias For instance in a setting where an interviewer ranks the candidates we could ask interviewers to self-correct rescale their implicit bias However anti-bias training which

---------------------------------------------------------------------------
TOPIC 16: race

---------------------------------------------------------------------------

0.9301 Drugs Miscellaneous Offenses Number of charges Number of charges brought against the defendant in the current case Attorney Type Categorical variable Public Defense Attorney Private Defense Attorney Attorney Waived Number of Prior Felonies Categorical variable of prior felony convictions zero prior convictions one to two prior convictions three to five prior convictions six to nine prior convictions ten or more convictions Number of Prior Misdemeanors Categorical variable of prior misdemeanors zero prior convictions one to two prior convictions three to five prior convictions six to nine prior convictions ten or more convictions Bail Status Categorical variable of the status of the defendants bail Bail Set Bail Posted Bail Forfeited Bail Revoked Anyone who has not had their bail status changed from Bail Set is treated as being detained pretrial Individuals whose status is Bail Posted Bail Forfeited and Bail Revoked are all treated as having posted bail and been released pretrial Bail Type Categorical variable of the defendants bail type Monetary Bail Unsecured Bail Non-monetary Bail Nominal Bail We only consider the bail set at an initial hearing and not subsequent changes to bail types and amounts Fail to Appear Binary variable describing whether or not the defendant did not

0.9087 consider booking charges relative to when we consider conviction charges as inputs The average level of recommended pre-trial supervision is also elevated under the calculation using the booking charges relative to that using the conviction charges Charges exclusions bump-ups nvca Conviction Booking Difference Table Percent of cases with exclusions bump-ups nvca flags and the average recommendation by input charges Table shows the proportion of people who received a charge-based exclusion a charge-based bump-up or an All p-values are significant at at least the level even after a Bonferroni correction for multiplicity NVCA flag when the PSA was calculated using the booking charges but not when it was calculated using the conviction charges It also shows the percent of people who received a recommendation for more restrictive conditions under the booking charges than under the conviction charges We find that a substantial portion of the cases nearly would have had a lower recommended level of supervision if their PSA had been based only on the charges they were ultimately convicted of exclusions bump-ups nvca Table Percent of cases for which each PSA component was higher under the booking charges than under the conviction charges Next we turn to understanding whether overbookings

0.8286 at a higher rate for Black defendants than non-Black defendants However there is little difference between the groups in terms of the impact of unsubstantiated charges on charge-based bump-ups or on the final recommendation To understand how this seemingly paradoxical result is possible we must first recognize that there are instances This is different than what is shown in Table as under the former calculation cases in which the individual was convicted of more severe charges than those under which they were booked offset cases where the reverse occurs Charges group exclusions bump-ups nvca Conviction non-black Conviction black Booking non-black Booking black Difference non-black Difference black Table Comparison of effects of charges on components of PSA by race for all disposed cases in data group exclusions bump-ups nvca non-black black Table Percent of cases by defendant race with a charge-based exclusion charge-based bump-up NVCA flag or higher recommendations due to unsubstantiated booking charges where an individual can have an unfair charge-based exclusion or bump-up that does not translate to an unfair recommendation Recall that each of these charge-based components results in an increase to the recommended level of supervision above and beyond the initial recommendation Consider for example an individual

0.8108 under the Clean Slate law This allows a more consistent measurement among court decisions and will hopefully minimize the effects of covariate shift from before to after the Clean Slate law Initial Data Statistics Again the data used in this analysis consists of court filings from Allegheny County Pennsylvania and Philadelphia County Pennsylvania from to in the Court of Common Pleas and Magisterial District Courts The Court of Common Pleas hears major civil and criminal cases as opposed to the Municipal Courts After removing cases with either missing or ignored features our datasets descriptive statistics are given below Descriptive Statistics Allegheny Philadelphia N Black White Male Female Public Defender Private Defender Defender Waived Felony Defendant Misdemeanor Defendants Assigned Cash Bail Posted Bail Failed to Appear Released Failed To Appear Detained Pretrial Failure In the dataset used here the number of cases in which defendants miss their court dates is noticeably lower than other analyses of cash bail which have failure to appear rates of Philadelphia only Release pretrial Philadelphia and Miami Dade County Released pretrial Philadelphia and Miami Dade County Detained pretrial Philadelphia only eligible under Philadelphia DAs No-Cash-Bail policy Philadelphia only ineligible under Philadelphia DAs No-Cash-Bail policy However for

0.7942 collected from each PSA the defendants unique identification number name date of birth arrest date date on which the PSA was conducted NVCA prediction NCA prediction FTA prediction a list of booked charges and corresponding charge codes the recommendation of the PSA an indicator of whether the recommendation was the result of charge-based exclusion and an indicator of whether a charge-based bump-up was applied to the recommendation We also recorded several of the individual risk factors listed on the PSA for each defendant age at current offense prior conviction misdemeanor or felony and prior violent conviction We separately obtained data from San Franciscos court databases on all defendants who interacted with the court system during the time period of the pilot program For each defendant we obtained a unique identification number name date of birth list of booking charges list of charges filed by the district attorney and the disposition code for each individual charge The disposition codes define whether each charge resulted in a conviction and are the basis on which we retrospectively calculate which charges were substantiated and which were not ANALYSIS Our analysis consists of three parts de-duplication and record-linkage validation and counterfactual analysis The goal of

0.7703 Amazon PLs less than in both the categories in the sponsored recommendations In-degrees of different types of products Figure shows boxplots of in-degree distributions of all the nodes having non-zero in-degree All nodes items have been divided into two groups Amazon PLs and P products In the battery category there is a considerable disparity in the in-degree distributions of PL and P products in the organic RIN however the disparity increases significantly in case of the sponsored RIN Interestingly in the organic RIN of backpack category the in-degree distributions of both P and PL products are very similar however when we move Avg in-degree Backpack Battery Organic Sponsored Organic Sponsored All nodes Product category Amazon PLs P products Table Average in-degree for different kinds of products across sponsored and organic RINs The average in-degree of PL products sees tremendous increase in the sponsored RIN The differences in the in-degrees were statistically significant students t-test to sponsored RIN there is distinguishable difference in the in-degree distributions These box-plots clearly show the rise in disparity of the in-degree or promotion of PL products in the sponsored RIN as compared to the organic RIN Moreover the difference between the in-degree distributions of P

0.766 tr ut io n Sponsored RIN Organic RIN a Core Numbers C um ul at iv e D is tr ut io n Organic RIN Sponsored RIN b Figure of the core numbers of PL products in a battery and b backpack category In general PL products are strategically placed closer to the innermost core in the sponsored RINs the organic RIN Ro Specifically in terms of the normalized discounted KL divergence measure explained in the previous section and respectively for the battery and backpack categories Also in both the battery and backpack categories more than of all Amazon PL products have higher rankings in the Sponsored RIN than their ranking in the Organic RIN Thus the ranked lists obtained from the sponsored RIN Rs shows significant bias by placing Amazon PL products in better ranks Representation in the core of the network In this methodology we calculate the core number of the PL and P products Note that higher core-numbers indicate that the corresponding nodes are located in the inner cores and are hence more strategically placed to derive higher exposure We observe that the PL products heavily occupy the inner cores in the sponsored RIN For the backpack

0.7586 absolute exposure in sponsored and organic RINs Amazon private label brands highlighted with a rectangle account for nearly and of the total exposure in sponsored RINs in the battery and backpack categories Distortion in exposure of producers brands As stated in Section the exposure of a brand producer is defined as the sum of exposures of all items of that brand We will now analyze the exposures received by various brands Figure a shows the exposure values of the top- brands as per the number of items they have in our battery dataset in both the sponsored and organic RINs A few of the top brands eg Generic Power one Panasonic see a significant drop in exposure in the Sponsored RIN as compared to that in the Organic RIN The drop for the first two brands is more than while for Panasonic it is nearly In contrast the exposure of Amazon PL brands highlighted within a rectangle and Duracell see a significant increase in the sponsored RIN In fact Amazon is the most exposed brand in the sponsored RIN in contrast to Duracell in the organic RIN Alarmingly more than of all P brands were found to be significantly under-exposed

0.7449 effect on the PSA differs by race group For this analysis we disaggregate the data into two race categories Black and non-Black This is an obvious over-simplification as is any racial categorization However based on our analysis of the consistency of racial classification within the court data we have determined this categorization scheme introduces the fewest problems with inconsistent classification A full discussion of how we arrived at this decision is available in the appendix Table shows equivalent quantities to those shown in Table now disaggregated into the two race groups We find that overbooking had a larger impact on the rate of charge-based exclusions and the assignment of the NVCA flag for Black people than non-Black people in this data It had a larger impact on charge-based bump-ups on the non-Black population However the impact of overbooking on the ultimate recommendation is roughly speaking similar between the two groups The proportion of cases for which charge-based overrides bump-ups NVCA flags were triggered or the recommendation was higher under the conviction charges than under the booking charges is given in Table disaggregated by race Under this summary of the data we again see that unsubstantiated charges led to charge-based exclusions

0.7317 items in the backpack category get significantly under-exposed and the percentage rises to in the battery category The overall consequences that such distortions can have on the producers of different items is also worth investigating categories of items bias Battery Backpack Battery Backpack Over Exposed Adequately Exposed Under Exposed Table Percentage of items over adequately and underexposed in the sponsored RIN along with their induced exposure bias in comparison to the organic RIN A m on D ur ac el l P an as on Si em en s M O B O C IP G en er P ow er O ne D O O R O FA SH IO N To y ou se T he B la St or e Ex po su re va es Sponsored RIN Organic RIN a A m on C O SM U S ar is so A m er an T ou te r G ea r Sk ag s cr t M O C A T O M M Y IL G E R Fa st ra E os ur e va es Sponsored RIN Organic RIN b Figure Top brands in a battery and b backpack categories and their

0.7134 whose initial recommendation is the highest level and who has exclusion or bump-up charges at booking that they are not convicted of This individual would be classified as having an unfair exclusion or bump-up However because their initial recommendation was maximal whether we calculate the PSA using the booking charges which would include an exclusion or a bump-up or we calculate it using only the conviction charges which would not include an exclusion or bump-up the recommendation is the same In the former case the initial recommendation was maximal and applying the exclusion or bump-up did not increase the recommendation as it could not further increase In the latter case we do not apply the exclusion or bump-up and the recommendation is still the highest category Thus even if exclusion or bump-up booking charges are unsubstantiated the ultimate recommendation does not change based on those charges for people whose initial recommendation is the highest level Individuals in this category contribute to the disparity shown under exclusions and bump-ups in Tables and and do not contribute to any difference in recommendations Similarly consider a second scenario where an individual has an initial recommendation of SFPDP-ACM the second highest level of supervision

0.7113 the most common magistrates Top Left Black vs White for each magistrate Top Right Proportion of Black defendants assigned cash bail vs proportion of White defendants assigned cash bail for each magistrate Middle Left Male vs Female for each magistrate Middle Right Proportion of Male defendants assigned cash bail vs proportion of Female defendants assigned cash bail for each magistrate Bottom Left Felony vs Misdemeanor for each magistrate Bottom Right Proportion of Felony defendants assigned cash bail vs proportion of Misdemeanor defendants assigned cash bail for each magistrate burn in steps and sampling steps were sufficient for all chains to converge Results from latent variable model Based on the average estimated produced when estimating the parameters of our model Figure shows that nearly every magistrate has a striking difference in the costs of missing a court date for Black defendants as opposed to White defendants Recall that our model defines as the ratio of the cost of missing a court date to the cost of being unable to post cash bail so a greater implies that a judge is more willing to assign cash bail to a group in a way that minimizes the risk of missing their court date

0.7001 outputs of the human-administered PSA when given the same inputs To do this we apply our PSA-reproduction code to the linked dataset described above This code takes the FTA and NCA predictions several variables measuring criminal history and the booking charges as listed in the court records It then outputs each of the components of the PSA a charge-based exclusion indicator a charge-based bump-up indicator a violence flag and a final recommendation We then compare each of these outputs to those same components as recorded on the original PSA forms A high rate of agreement assures that our code is accurately representing the PSA We find that our calculation of the NVCA flag agrees with that listed on the PSA form for of the cases in our dataset With regard to charge-based exclusions we find that our reproduction is in agreement with the original PSA data for of the records In the PSA calculating the charge-based bump-up is not necessary if an exclusion is determined because the recommended level of supervision cannot further be increased For this reason we compare our reproduction of charge-based bump-ups to the PSA data only for those records for which a charge-based exclusion was not

0.6993 for nearly million active sellers worldwide Special relationships The producers or brands who manufacture the products sold at the Amazon marketplace can be divided into two categories third party P brands and Amazon private label brands and the corresponding products are P products and Amazon private label PL products respectively P brands are standalone brands eg Apple Adidas Skybags whose products are sold on Amazon under their respective brand names On the other hand Amazon private label products are produced by Amazon itself Some of the Amazon-owned most successful private label brands are AmazonBasics Amazon Collection Amazon Essentials Pinzon Solimo etc These brands compete with traditional brands across multiple product categories such as electronics fashion jewelry etc Amazon currently offers approximately private label products some of which have additional variations such as color and size across brands in the Amazon store Note that as per Amazons response to the US House antitrust subcommittees questions all of the Amazon private label products are sold by Amazon only In this paper we focus on potential biases toward such products having special relationship with Amazon Apart from products some sellers also have special relationships with the Amazon marketplace notably Fulfilled By Amazon FBA

0.6892 in Table The results of this module show that the probability that two properties will occur simultaneously is related to the sample size Evidence of this can be found in the results of the Drug Consumption and Adult datasets where the highest probabilities of simultaneous events involve the Caucasian property The COMPAS dataset shows quite homogeneous probabilities especially with regard to the Black property while for the Caucasian property the highest probabilities are related to the simultaneous occurrence Table Summary of Inclusiveness Analysis Results COMPAS Drug Adult Consumption Dataset aie Asian Black Black/Asian Caucasian Hispanic Other White/Asian White/Black AIE Asian API Black Black/Asian Caucasian Hispanic NA Other White/Asian White/Black American-Indian/Eskimo Asian-Pac-Islander Native American Figure Example of Inclusiveness graphic visualization with the Non-recidivist property Since the simultaneous probabilities depend on the number of examples within the available data and the sample size this result alone is not sufficient to establish a priori the certain presence of serious data distortions although some evidence can already be seen Figure shows the graphical representation of the inclusiveness module that simplifies the display of simultaneously probabilities In the example is given the notation for a dataset where all the properties of the target variable and

0.6887 cases in the state of Pennsylvania The website provides a pdf for each docket and pdf summary file for each defendants criminal history The Pennsylvania judiciary also includes a separate API in which users can retrieve json formatted files of case information We use text-extraction tools in order to retrieve an individuals history from the summary pdf and combine this with the json docket information We performed this process on records from the Court of Common Pleas in Philadelphia from the court of Common Pleas in Allegheny county with a total set of cases from January to December over these two counties From each docket-summary pair we extract a feature vector for each case that consists of Magistrate Categorical identifier for which magistrate set the defendants bail Race Binary variable White/Black Sex Binary variable Male/Female Age Defendants age when the case was filed Lead Offense Type Categorical identifier of the defendants lead offense eg First/Second/Third Degree Felony or Misdemeanor Lead Offense Description Categorical identifier for the lead offense based on the lead offenses statute number Offenses against Public Administration Offenses against Property Offenses Involving Danger to the Person Drug Offenses Inchoate Crimes Vehicle Offenses Driving after Imbibing Alcohol or Utilizing

0.6819 If this person is booked under an exclusion charge that is reduced to a bump-up charge that they are convicted of in both cases the final recommendation will be the highest level of supervision To break this down further under the booking charges they receive an exclusion and are automatically moved to the highest category Release Not Recommended Under the conviction charges they receive a charge-based bump-up which because they began in the second-highest category also results in a Release Not Recommended recommendation Thus under both the booking charges and the conviction charges their recommendation will be the same though they will still be classified as having had an unfair exclusion Both scenarios where unfair exclusion charges do not materialize into unfair recommendations are only possible when the individual has an initial recommendation that is either the highest category or the second highest category In the population examined here the distribution of initial scores was shifted higher for the Black individuals than the non-Black individuals See Figure which shows the distribution of initial scores broken down into Black and non-Black people Depending on the definition of fairness adopted this group-wise distributional difference may itself be indicative of unfairness in the

0.6775 in the sponsored RIN Qualitatively similar observations are found in case of backpack category also as shown in Figure b Some of the top brands eg Skybags American Tourister Wildcraft etc see a significant drop in exposure in the sponsored RIN as compared to that in the organic RIN The drop for American Tourister is around while that for the other two brands is more than In contrast the exposure of Amazon PL brands and Cosmus have increased significantly in the sponsored RIN Similar to the observations for the battery category more than of all P brands were found to be under-exposed in the sponsored RIN in the backpack category as well Influence of the sensitive attribute All the analyses reported in this section show that Amazon PL products get much higher exposure via sponsored recommendations Attributes Backpack Battery Organic Sponsored Organic Sponsored Intercept Seller fulfillment PL membership Product quality Product popularity Seller quality Table Attributes along with their estimates as per the NBR model fitted on the dataset inferred from sponsored and organic RIN Seller fulfillment and PL membership attributes have the strongest influence on the in-degree of items indicates p and indicates p than what they get via

0.6749 Asian and Pacific Islander API American Indian and Alaskan Native only AIAN multiracial or Hispanic respectively We do not use AIAN and two or more races categories ie and as they do not occur in the income dataset Then for each last name i we define the probability vector as the normalized version of the vector Income dataset We use family income data aggregated by race This was compiled by the US Census Bureau from the Current Population Survey The dataset provides income data of families It has four races White Black Asian and Hispanic age categories and income categories For each set of race age and income categories the dataset has the number of families whose reference person see definition here belongs to these categories For each race r we consider the discrete distribution Dr of incomes of families with race r derived from the income dataset see Figure in supplementary material for some statistics of Dr Setup We consider race as a protected attribute with four labels p and target equal representation based on race and n For each choice of and we draw a last names uniformly from the entire population with replacement The i-th last name

0.6718 The ways that racial social and spatial segregation lead to the monopolistic group closure of social advantage on racial lines is discussed in Haynes and Hernandez Ancestral national and geographic origins The definitions of races used in the US census are rife with inconsistencies and lack parallel construction but have nevertheless become a de facto standard of racial and ethnic classification Blacks are defined as those with total or partial ancestry from any of the black racial groups of Africa Asian Americans are those which have ancestral origins in East Asia Southeast Asia or South Asia Hispanic Americans are descendants of people from countries of Latin America and the Iberian Peninsula and is considered by the census as an ethnicity and not a race Though phenotype and class may be social markers of race beliefs about race as a true intrinsic property are anchored in perceptions and facts about personal ancestry Ancestry is one of the main conduits of citizenship which determines which legal jurisdiction one is subject to These jurisdictions can influence what categories a person individually identifies with It is not only in the United States that racial categories are anchored in ancestry even though racial categories are

0.6689 category there are as many as of all Amazon PLs in the innermost core core number of the sponsored RIN in contrast there was no Amazon PL in the innermost core of the organic backpack RIN In case of battery category both the RINs contained of all Amazon PLs in their respective innermost core Further if we divide the cores into four quartiles we observe that in the sponsored RIN of PLs in battery category in backpack category are in the top quartiles ie coreNumber i maxcoreNumbers The percentage reduces to mere in the organic RIN in backpack category Figure shows the cumulative distribution of core numbers of PLs in both RINs of the a battery and b backpack categories The figures show that the Amazon PL products are placed closer to the innermost core in the sponsored RIN to increase their overall visibility as compared to the organic RIN Exposure bias The exposure bias scores bias are computed using the customised Random Surfer model as described in Section The exposure bias scores for both the categories battery and backpack are listed in Table Sponsored recommendations in battery category induce more exposure bias than backpack category We observe that almost

0.6688 is still in use in San Francisco In order to create a risk profile for a newly arrested person and pre-trial supervision recommendation the tool combines several separate predictions risk of failure to appear at a future court date FTA risk of re-arrest for new criminal activity NCA and risk of re-arrest for new violent criminal activity NVCA Each of the predictions are calculated as a function of some subset of the following age at current arrest whether there are pending charges at the time of the current offense whether the arrested person has any prior misdemeanor convictions whether the arrested person has any prior felony convictions whether the arrested person has any prior convictions misdemeanor or felony the number of prior violent convictions the number of prior failures to appear for court dates in the past two years whether the person failed to appear prior to two years before the offense whether the individual has been incarcerated as the result of a conviction in the past As we describe each component of the version of the PSA as it existed during the period under study we will highlight analogues to each component under the new version of the PSA

0.6623 perturbations of the booking charges and these predictions do not change as a function of the booking charges DMF our reproduction of the final recommendation is in agreement with that listed on the PSA form of the time A full discussion of the validation process is given in Section of the appendix Finally having verified that our code accurately reproduces the PSA we apply our PSA-reproduction code to a counterfactual scenario in which only the charges that resulted in a conviction are used in the calculation of the PSA This results in two sets of calculations to compare a the PSAs recommendation and each of its components based on the booking charges and b the PSAs recommendation and each of its components based only on conviction charges To evaluate the impact that unsubstantiated charges had on the PSA we compare calculations a and b RESULTS In this section we compare the results of the PSA calculated using the conviction charges to the results of the PSA calculated using the booking charges Throughout this section the results presented pertain only to the cases for which all charges had been disposed or settled at the time of the analysis which includes of

0.6594 and PL products were found to be statistically significant in the sponsored RIN according to students t-test p for battery and p for backpack Further to explore the promotion bias toward Amazon PL products in sponsored RIN we look into the average in-degree of different types of products based on their special relationship with the marketplace Table lists the average in-degree of different types of products in the RINs For the Battery category an Amazon PL is recommended from other items on average in the Organic RIN however in the Sponsored RIN of the same category an Amazon PL is recommended from as many as other items In sharp contrast a P battery product gets recommended from only other items in the Sponsored RIN on average which is actually lower than the average number of items recommending a P battery in the Organic RIN Similarly for the Backpack category Amazon PLs receive recommendations from many more items on average in the Sponsored RIN as compared to the Organic RIN These values clearly suggest that the increase in the degree of promotion of Amazon PL products is significantly higher in Sponsored RINS than that of the P products In other words

0.6574 defendant However if cash bail acts as a deterrent for a missed court appearance and the defendant can afford it then the magistrate will instead choose to set cash bail Alternatively if the defendant cannot afford bail and they are still unlikely to miss their court date under non-monetary release the magistrate will choose not to use cash bail We represent this dilemma as choosing the option that minimizes the cost of setting cash bail pr fta cash bail Pr post cash bail or the cost of not using cash bail pr fta cash bail Pr post cash bail Note if cash bail is not used then the defendant posts bail with probability For convenience in notation let pb Y PrY cash bail ta Race Magistrate Person Figure Generative model describing how magistrates set cash bail and how these decisions influence probabilities of failing to appear Not pictured here is the analogous model for individual probabilities of pretrial release both generative models use the same parameter from the magistrate plate b In this model cash bail is assigned if p release Rather than estimating both we simplify this equation to have a single latent variable C where C is the

0.6526 comes after step so chronologically in some sense is not initial By this we mean to say that this is the recommendation that would be given without considering any charge-based amendments to the recommendation This can be calculated even if an individual has a charge-based exclusion recognizance and receive court reminders OR-NAS Minimum Supervision release on own recognizance with court reminders and twice-weekly phone reporting OR-Minimum Assertive Case Management release on own recognizance or under supervision including court date reminders four times weekly reporting with two to four of those times reporting in person and an out of custody needs assessment SFPDP-ACM and Release not Recommended Finally in step two more determinations are made whether the current booked offense is among the following charges or the current booked offense is a solicitation conspiracy attempt or FTA for any of the charges on that list Violation of other Protective Orders Person to Person Sex Crime Arson Involved the Use of a Weapon Caustic Chemical Flammable Substance or Explosive Felony inflicting Great Bodily Injury Misdemeanor Domestic Violence Misdemeanor Stalking whether the current booked charge is not on the list of violent offenses but the NVCA flag was triggered If either of these

0.6515 or reproduces structures of domination based on racial significance and identities and anti-racist if it undoes or resists structures of domination based on racial significations and identities Example In the case of Facebooks ethnic affiliation feature Facebook engaged in a racial project to discover and represent the racial affiliations of its users Doing so was neither a racist nor an anti-racist project That it passed these representations on to How has the software designed Blind to race A As a racial project B Enabling users racial projects C Are the input data racialized Not explicitly A Explicitly by ascription B Explicitly by self-identification C Is the system output racialized Not at all A System ascribes race B By user interpretation C Figure Heuristics for analysis and design of algorithmic systems Systems of type A are blind to race and therefore risk learning and reproducing the racial inequality inherent in society Systems of type B explicitly use ascribed racial labels and so risk reifying racial categories by treating race as an intrinsic property of a person These systems are racial projects in the sense that they represent racial categories in a way that is relevant to resource allocation Systems of type

0.6513 This presents another set of problems as expanding the region we consider the home of a racial category also expands our borders and introduces another larger set of shared borders along which populations should be visually indistinguishable due to proximity admixture and shared culture Naive Interpretations of Racial Categories Are Incongruous Because racial categories are defined with respect to and largely correspond with geographic regions they include populations with a wide range of phenotypical variation from those most steretotypical of a racial category to racially ambiguous individuals or those who may appear to be of a different observed race altogether As an example the South Asian racial category should include populations in Northeast India who may exhibit traits which are more common in East Asia However including individuals which appear East Asian in a racial category distinct from East Asian would violate the common lay understanding of racial categories Racial Categories Erase Ethnic Differences and Identity Racial categories are continent-spanning groupings Every racial category subsumes hundreds of ethnic groups with distinct languages cultures separation in space and time and phenotypes Ethnic groups even span racial lines and racial categories can fractionalize ethnic groups placing some members in one racial category

0.6487 or decrease by the estimated number of units If the sensitive attribute is given a high numeric estimate relative to other attributes then the sensitive attribute can be said to have a selectively high influence on the values of PS i or PO i P PL In eg re e Organic RIN P PL In eg re e Sponsored RIN a P PL In eg re e Organic RIN P PL In eg re e Sponsored RIN b Figure Box-plots showing the median first and third quartile of in-degree distribution of Amazon PL products and that of P products in a battery and b backpack category RINs Note that the y-axes are in log scale The disparity in the in-degree distributions of PLs and P products increase significantly in the sponsored RIN across both categories RESULTS In this section we apply the proposed methodologies on the collected datasets to examine the relative bias in the sponsored RIN S with respect to the organic RIN O toward Amazon PL products Promotion bias In our first bias measure we investigate the promotion bias in the sponsored RINs toward Amazon PLs Note that promotion of an item in the RIN is its in-degree

0.6373 County and Philadelphia County and address prior analysis for this aspect of the judicial process The Process of Cash Bail In Philadelphia County after an arrest typically a video conference is set up between a magistrate and a defendant where the magistrate will assign a bail type and bail amount in a hearing that generally lasts one to three minutes Each shift of bail hearings takes place for approximately four hours every day with one of Philadelphias six arraignment court magistrates presiding over each shift Pretrial processes work much the same in Allegheny county where one of magisterial district judges will preside over a preliminary arraignment in which a defendants bail is set Bail need not be set only at the preliminary arraignment it may also be set during a preliminary hearing or bail hearing However in most criminal cases it is set at this preliminary arraignment Of the bail types assigned a magistrate has the option under the law to choose one of the following Release on recognizance ROR a written agreement from a defendant to appear on their court date This agreement may come with additional stipulations but does not require any monetary commitment Release on nonmonetary conditions

0.6296 remainder of this section qualitatively the results were the same Table shows the rate of charge-based exclusions bump-ups NVCA flags and the average recommendation level when calculating each of the components of the PSA The average recommendation level is based on mapping each level of supervision to its numeric rank the lowest level of supervision is mapped to one the second to two etc The Charges column gives the charges used to calculate the PSA either booking charges or conviction charges The difference between the rate calculated under the booking charges and the rate calculated under the conviction charges is also shown To test for statistical significance between the components of the PSA under the two input charge conditions we performed standard statistical hypothesis tests When comparing the rate of exclusions under booking charges to the rate of exclusions under conviction charges we perform a difference of proportion test To compare the recommendations we performed a Wilcoxon rank-sum test as the recommendations are ordered categorical Statistical significance at the level is indicated by for the differences shown in the results tables We see that the rate at which charge-based exclusions bump-ups and NVCA flags occur is much higher when we

0.6265 by the American Civil Liberties Union ACLU found that cash bail was assigned in as many as of cases with some magistrates setting cash bail in up to of cases In Philadelphia a review by the ACLU found that upwards of of defendants were assigned cash bail Other studies have also found cash bail assignment rates ranging from to over all magistrates Beyond biases along racial lines a class action suit brought against Harris County Texas argued that the county had been using money bail on misdemeanor defendants who could not pay The district court whos ruling was later affirmed by the Fifth Circuit Court of Appeals concluded that Hearing Officers were aware that by imposing a secured bail on indigent arrestees they were ensuring that those arrestees would remain detained In essence from jurisdiction to jurisdiction we see trends that show that cash bail has been used as a barrier to justice for individuals of a certain race or financial status The Problem of Infra-Marginality in Cash Bail Assignments Among the studies on both the disparate rate of cash bail assignment and the downstream effects of cash bail there has been a growing emphasis that the methods employed in

0.6154 the records There is some nuance around which charges should count as convictions For example sometimes multiple cases are bundled into a single plea agreement Should all charges associated with that bundle be counted as conviction charges or only those conviction charges that are part of the case originally associated with the administration of the PSA Ultimately we decided that only charges that pertain directly to the arrest that triggered the administration of the PSA ought to be eligible though we acknowledge that others might disagree with this definition Thus for the purposes of this analysis we define conviction charges to be those charges associated with the arrest that triggered the administration of the PSA for which the arrestee was found or plead guilty This definition creates some situations where a case outcome indicates that the individual pleaded guilty to other charges but none of the charges to which they pleaded guilty We do not compare to the original PSA results directly to isolate the effect of altering the input charges If we compared to the original PSA components some of the differences we identify may in fact be due to some of the differences in interpretation we highlighted in

0.6123 three determinations are made whether the person was extradited for the current booked offense whether current booked offense is among the following offenses or is a conspiracy attempt solicitation or FTA of any for those offenses Murder Voluntary Manslaughter Aggravated Mayhem Torture Felony Sexual Assault Robbery Carjacking Felony Domestic Violence Felony Stalking Violation of a Domestic Violence Protective Order Escape whether the current booked offense is deemed violent according to the California PSA List of Violent Offenses and the NVCA flag calculated in step is indicated If any of these conditions are true the individual is automatically given a Release Not Recommended the most restrictive possible recommendation and the assessment need not continue This is called a charge-based exclusion and we refer to charges that trigger a charge-based exclusion as exclusion charges If no charge-based exclusion is made the assessment continues to step In step the six point NCA and FTA predictions are combined using the Decision Making Framework DMF shown in Figure to arrive at what we refer to as an initial recommendation Each cell in the matrix corresponds to a recommended level of supervision For example if an individual has an FTA prediction of and a NCA prediction

0.612 a magistrate may release a defendant on nonmonetary bail if they believe that set conditions such as restricting travel or requiring the defendant to report in are sufficient for their court appearance Release on nominal bail a magistrate may release a defendant on nominal bail by requiring a small amount of money eg one dollar and another entity a person or organization to act as surety Release on unsecured bail a magistrate may release a defendant on unsecured bail by requiring a written agreement that the defendant becomes liable for the set amount of money in the event of non-appearance This does not require any money or other form of security in order to be released Release on monetary bail a magistrate may release a defendant on cash bail if they believe that a defendant will be unlikely to comply with the conditions of release without an immediate guarantee In Allegheny County the bail authority may require that individual to pay no more than of the full bail amount in order to secure their release If a defendant is unable to pay they will be detained until trial In Allegheny County cash bail should not be set unless a magistrate

0.61 regardless of an Figure Difference in among seasons individuals ability to post bail While a few magistrates have less then for white defendants implying that they want to make sure that these defendants are released pretrial no magistrate has less than for black defendants Most magistrates place an outstandingly high emphasis on a defendant missing their court date for all defendants but this trend is exacerbated based on racial differences which is in line with the higher rate at which these magistrates assign cash bail to Black defendants Over these magistrates the cash bail assignment rate sits at an average decrease in the likelihood of being assigned cash bail when comparing their treatment of black defendants as opposed to white defendants While such an change is indicative of a direction of bias the results based on give a more actionable response to this behavior The majority of judges have high values for Black defendants with the median value being whereas White defendants have a median In this case it is much more clear that the action for improving pretrial release rates requires addressing magistrate bail assignments or assigned bail amounts for Black defendants Similarly when comparing for men and women

0.6096 model However because our goal is to study the effect of overbooking in isolation we do not further delve into this other than to note this disparity in the rate at which Black versus non-black people are recommended for pre-trial detention Regardless because Black defendants were more likely to fall into the highest or second highest category before any charge-based amendments were made Black defendants who had unfair charge-based exclusions were more likely to not have those unfair charge-based exclusions impact their final recommendation Figure Distribution of initial recommendation disaggregated into Black and non-Black people It is important to note that this conclusion only holds for this particular DMF the matrix that translates raw predictions of FTA and NCA into initial recommendations In a jurisdiction with a different DMF under which fewer combinations of risk scores correspond to the highest or second-highest level of supervision it is possible that the racial disparities in unwarranted charge-based exclusions bump-ups and NVCA flags might in fact translate to disparities in the recommendations as well Thus jurisdictions seeking to amend their DMF or similarly their release recommendation matrices should be aware of this possibility changes to the release recommendations that are intended to result

0.6088 more likely that this individual pleads guilty or is convicted of a crime In a similar analysis Dobbie et al use a measure of judge leniency based on the residuals of release decisions in a probit model in order to estimate the influence of the assigned magistrate on pretrial release and its subsequent effects on future crime court appearances and employment They find that while pretrial release increases the likelihood of failing to appear release both decreases the likelihood of rearrest over the next two years and increases the likelihood of having any income over the next two years In particular such models generally follow the rationale that the impact that the presiding authority has on a case can be isolated and that we can estimate regression coefficients for outcomes in ways that glean new insights as in the example below For example Gupta et al propose a regression model nct ki bail nct ki bail Outcome ict Bail bail ict X where bail ict is an indicator for whether or not cash bail was set for defendant i with offense o in court c for year t by judge and X are defendant controls such as age race gender

0.6088 of a racialized interpretation of a system to the system itself A thorough analysis of the system inputs software and outputs is necessary to determine where racial intent or social racial categories caused the output or ascription Some systems whose input and output data represent people may not be explicitly racialized at all However since racial categories structure inequality pervasively throughout society these systems Ascription Formation Disparity Figure Schematic of vicious cycle of racial formation Bodies are ascribed into racial categories then sorted socially and in space based on those ascriptions These sorted bodies are then exposed to disparate outcomes Racial categories are then formed on the basis of those unequal outcomes and their distribution across people based on phenotype ancestry and class indicators Those categories are then ascribed to bodies repeating the cycle will likely reproduce racial inequality anyway The difficulty of designing a system that neither reproduces racial social inequality nor reifies racial categories which are inherently unfair motivates an alternative design discussed in the next section AN ALTERNATIVE DESIGNING FOR SOCIAL INTEGRATION System designers of machine learning systems that determine resource allocation to people face a dilemma They can ignore racial inequality in society and risk having

0.6065 where is the sigmoid and N is bias term After estimating these probabilities each magistrate determines whether to set cash bail based on a tradeoff between the probability of the defendant appearing if released pretrial with/without cash bail and the probability of them being able to post their bail if cash bail is assigned Each magistrate has a measure of societal harm induced by a defendant not coming to their trial We represent this cost of not appearing as the latent variable Should the presiding authority believe that cash bail is an appropriate assignment for a defendant they legally must still ensure that the bail amount is reasonable While reasonable may mean different things to different magistrates we represent the cost of the defendant facing pretrial incarceration ie having cash bail imposed and being unable to pay via the latent variable Cost Model In the model here these underlying beliefs determine whether cash bail is an effective tool for bringing a specific defendant to their trial date If a defendant is likely to not appear for a court appearance regardless of the type of bail set instead of setting cash bail a magistrate will opt to deny bail to the

0.6065 conditions are true the initial recommendation is increased one level For example if the initial recommendation was OR-NAS it is increased to OR-Minimum This recommendation is then the final pre-trial supervision recommendation We refer to this as a charge-based bump-up and charges that trigger a charge-based bump-up as bump-up charges In summary abandoning jargon and simplifying to the extent possible we conceptualize the process as follows an individuals initial recommended level of supervision is determined by combining predictions about their likelihood of FTA and NCA predictions that do not rely on charge-based information If there are very serious booking charges ie exclusion charges or the booking charges are violent and the person is predicted to have a higher likelihood of rearrest for a violent crime then the individual is automatically What is referred to as the decision-making framework DMF in this section has a direct analogue in the new version of the PSA called the Release Conditions Matrix RCM One important differentiator between the versions is that in the version described here the DMF may recommend pre-trial detention Under the new version with the RCM according to Detention is not included in the matrix because eligibility for detention is based

0.6054 attributes and hence to see which attribute most influences the in-degree promotion of items Observations The attributes and their estimated correlation values as output by NBR are listed in Table The seller being an FBA fulfillment attribute has the most significant effect in determining the in-degree of a product in both RINs of the backpack category however for battery category the most significant attribute is the PL membership of the product PL membership is overall the most significant attribute of all in estimating the in-degree of items The estimates of PL membership feature tell an interesting story on the two RINs of the backpack category Being a PL product increases the chance of being recommended a higher number of times in the sponsored RIN estimate while it has a negative estimate on the organic RIN This indicates that in practice customers do not tend to visit the PL backpacks more while surfing for backpacks however the same have been highly promoted in the sponsored RIN Note that among all features the relative importance of PL attribute increases the most when we compare organic to sponsored RIN within the category Among other attributes the quality of the product and that of

0.6033 presiding authority all available information x Rd such as severity of the charged crime and number of prior crimes dictates the probability of this defendant not appearing on their trial date We model this probability of failing to appear as a logistic model with a Normal prior on the weights N I To model the effect of cash bail on FTA rates we introduce an additional coefficient with prior where N is the half-normal distribution making the reasonable assumption that adding cash bail should not increase the probability of failing to appear for a court date Thus the probability of missing a court date is given as where is the sigmoid function b is the indicator for whether or not cash bail was assigned and N is a bias term again with a Normal prior Secondly we consider the case where the presiding authority may be assigning the bail amount in excess of what the defendant is able to reasonably pay We model the probability of a defendants release for any type of bail by a second logistic model with a Normal prior release N I and the probability of being released pretrial given by Pr release b release x

0.5995 charges priors etc is a measure of judge leniency calculated as the difference in leave-one-out means of a judges rate of cash bail assignments compared to the rate of cash bail assignments for court c A litany of prior literature uses such methods based on probit/logistic regression models with or without residual measures of judge leniency/severity in order to model how our observed pretrial data influences defendant outcomes after their preliminary arraignments/bail hearings with most work coming to similar conclusions Disparate Rates of Cash Bail Assignments Due to the harmful long-term impact of cash bail use further work has sought to verify whether or not magistrates assign cash bail uniformly among demographic groups For cases in which a judge assigned cash bail prior work has reviewed pretrial decisions for instances of judge bias and found that across the state of Pennsylvania the proportion of Black defendants who are assigned cash bail is significantly greater than the proportion of their non-Black counterparts who were assigned cash bail with additional work finding that even after controlling for the types of crimes judges still assign cash bail to Black and White defendants differently for the same crime in of cases Furthermore a investigation

0.5916 the years the average rate of missed court appearance for the Common Pleas Court in Philadelphia was From our review there is no clear and consistent notice that a defendant did not appear at their court date within the publicly available dockets We flag a defendant as having failed to appear at their court date if one of two cases is satisfied within the docket if the related docket both lists their bail as having been revoked/forfeited and the reason for being revoked explicitly lists that either the defendant did not appear or that a bench warrant was put out if a bench warrant was authorized and if the comments within the case registry entry explictly states that the defendant did not appear Failure to appear rates are only a single aspect of pretrial failure Defendants may fail their pretrial release conditions by being arrested for a new crime while waiting for their trials Again the dockets do not provide a clear and consistent notice that an individual was arrested while awaiting trial While the court summaries do provide a view of all arrests for an individual in many cases the same incident was recorded multiple times and it may

0.5896 the seller have positive estimates on the in-degree of products whereas product popularity does not have any positive estimate on the in-degree All the results were found to be statistically significant However in the sponsored RIN their relative importance in comparison to the aforementioned sensitive attributes is very low From this analysis it is clear that being a PL product specifically influences the number of sponsored recommendations received by an item Takeaways from the section We used multiple orthogonal methods and measures to estimate the relative bias toward Amazon PL products Across all the analyses we consistently observe that Amazon PLs are significantly more advantaged in the sponsored recommendations as compared to the organic recommendations Often these advantages for Amazon PL products come at the cost of disadvantages toward other P products CONCLUDING DISCUSSION Our analysis using five different network-based biased measures suggests that Amazon PLs enjoy a significantly high promotion in the sponsored recommendations compared to the organic ie view similarity based recommendations on Amazon Now promotion of private label products is not illegal in fact many tech-giants regularly follow this practice However opaque sponsored private label recommendations can be abused by platforms to systematically evade competition in online

0.5871 in the corresponding RIN A higher in-degree is likely to drive more customers toward the given item Not all P products are promoted in Sponsored RIN Intuitively we would expect a much smaller fraction of products to be sponsored advertised on a platform Indeed we observe that a very small fraction of the entire number of products that we collected have non-zero in-degree in the sponsored RINs ie were advertised at least once Specifically more than of the P products do not even get a single inward recommendation in the sponsored RINs in both backpack and battery categories However almost all Amazon PLs are recommended in the sponsored RIN in both the categories Thus Amazon does significant self-promotion for almost all their PL products in the sponsored RIN Amazon PLs get sponsored recommendation from half of the product space We observe that Amazon PLs get recommended from approximately distinct products in the entire product base in both categories in contrast to approximately in organic recommendations In other words almost half of the entire product space recommends at least one Amazon PL in its sponsored recommendations Whereas a very small fraction of the entire product space is recommended back from the

0.5859 the validation section above According to the codebook we received this is all disposition codes greater than Additionally by manual review we have found that cases in which the case is listed as resolved if some charges associated with a case number have disposition code plead guilty to other charges and others charge codes associated with that same case number have disposition code those with disposition code are the ones the individual was convicted of This was confirmed on several cases by looking at alternative sources of information available in other systems that are not in a database form amenable to statistical analysis are associated with the original case In this scenario the conviction-charge-PSA is calculated as though there were no charges eligible to trigger charge-based exclusions bump-ups or to be considered violent despite the fact the individual was convicted on some charges just none that were filed as part of the case associated with their PSA form We performed additional analysis removing all cases for which a guilty plea was indicated but the individual did not plead guilty to any of the charges associated with the original case While the exact numbers were lower than those presented in the

0.5812 Figure Bail Amounts Finally it is worth noting that while the set cash bail amount is available in our data we do not include it in this model In order to Figure A histogram and the fitted normal distribution of log bail amounts for defendants accused of felony drug offenses and who saw any of the five most common magistrates in our dataset The variance of the bail assignments in these cases is too wide for meaningful inference so we do not include it in our analysis The stochasticity of a magistrates assigned cash bail amounts may be instructive in its own right as a metric for understanding judicial treatment estimate the cost of setting/not setting cash bail we need counterfactual information on bail assignments The probability of failing to appear for a court appearance is designed so that the counterfactual cash bail assignment is easily computable however due to the inherent stochasticity of the bail amounts that judges choose we found that a counterfactual assignment of cash bail will not provide a reasonable estimate of what a defendants bail amount would be had the judge set cash/non-monetary bail If we consider the defendants who were assigned cash bail by

0.576 products that have special relationships with Amazon receive disproportionately more sponsored recommendations than organic recommendations Ranking bias To quantify the ranking bias of the RINs we require a centrality measure of choice and a sensitive attribute To this end we performed our analyses across different choices of network centrality measures For brevity we explain the results for in-degree centrality measure and sensitive attribute based on the PL membership of the products ie whether a product is an Amazon private label PL or a P product In-degree centrality In-degree centrality of a node in a network is the fraction of inward edges coming to it Hence a better rank in this centrality measure suggests that a particular node can be reached with higher likelihood from the rest of the network In the context of RINs better in-degree centrality is synonymous to higher likelihood of customers visibility for products Observations Amazon PLs get significantly better centrality ranks in the sponsored RINs than in the organic RIN In both categories battery and backpack the ranked list obtained from the sponsored RIN ie Rs is almost times as biased as the ranked list from Core Numbers C um ul at iv e D is

---------------------------------------------------------------------------
TOPIC 17: data-collection/privacy

---------------------------------------------------------------------------

0.8882 you expect to be paid for contributing your data to an AI dataset If so how much do you think you should be paid for contributing various types of data or what other information about the purpose of data collection they would want to know Stage Concerns about contributing to an AI dataset Finally we asked our participants about their concerns with contributing to Table Self-Descriptions of Types of Disability That Our Participants Experienced Physical Disability Hearing Loss or Impairment Blindness or Visual Impairment Cognitive Disability P Spinal Cord Injury P Mild Hearing loss P Blindness P ADHD and Dyslexia P Cerebral Palsy P Profound hearing loss P Blindness P ADHD and Dyslexia P Spinal Cord Injury P Profound hearing loss P Blindness P Dyslexia P Rheumatoid Arthritis P Mild Hearing loss P Congenital Glaucoma P Autism P Shaking Hands P Profound hearing loss P Blindness P Dyslexia P Cerebral Palsy P Profound hearing loss P Blindness P ADHD and Dyslexia P Torsion Dystonia P Mild Blindness P Spinal Cord Injury P Blindness P Paralyzed P Visually Impaired an AI dataset For this stage we narrowed down the context of data contribution by applying a scenario-based method We presented to

0.8599 moves through its many iterations RESEARCH SITE AND METHODS This paper builds on six months of ethnographic fieldwork with DataVector a multi-billion-dollar US-based e-commerce and new media organization Established in the s DataVector owns several companies in domains such as health and automotive Many of these are multi-million-dollar companies with several thousand clients each DataVector has a core data science team based on the west coast of the United States that works with companies across different domains There are multiple teams of data engineers software developers and business analysts both at DataVector and its subsidiaries One of us worked as a data scientist with the organizations core data science team between June and November serving as the lead scientist on two corporate data science projects not reported in this paper and participating in many others During ethnographic research the data science team had eight to eleven members including one of the authors The team is headed by CliffDataVectors Director of Data Science with years of industry experience in major technology firms Cliff and the team report directly to BillDataVectors Chief Technology Officer with years of experience in the technology industry During the six-month period one of us conducted interviews with

0.8597 open data For instance the African Development Bank Group developed a statistical open data portal known as Africa Information Highway AIH portal AIH links all African countries with the aim of increasing public access to statistics across the continent while also helping these countries improve data management data quality and sharing/dissemination Similarly Kenya Open Data is a portal that provides public access to several governmental datasets curated in different categories at no cost This platform aims to support the governments commitment to transparency and data sharing between government offices and various stakeholders Despite extensive calls for data sharing and open data backed by various initiatives and organizations data access remains relatively limited within the continent Bangani and Moyo for example compare data sharing practices among researchers in South Africa to researchers in countries such as France the United Kingdom and Turkey The authors state that most researchers in South Africa are not interested in sharing their data and prefer to use data generated by others only of South African researchers indicated that they currently share their research data with others As reasons for their reluctance these South African researchers cited worries ranging from unethical use or misuse to lack of

0.767 who need special financing submit their data so that interested dealers can contact them People requiring special financing face several challenges ranging from the lack of knowledge about available credit sources to difficulties in negotiating interest rates As liaisons between borrowers and lenders companies such as CarCorp and its affiliates act as important sometimes necessary intermediaries for people requiring special financing CarCorp serves several dealers across the country Few dealers collect their own lead data as the money effort and technical skills required to do so is enormous This is a key reason why dealers pay companies such as CarCorp to buy lead data CarCorps technology development and project manager Brian wanted to leverage data science to improve the quality of leads Improving lead quality Brian argued will ensure that existing dealers do not churn ie they continue to give their business to CarCorp Brian project manager The main goal is to improve the quality of our leads for our customers We want to give actionable leads That is what helps us make money makes customers continue to use our services Interview November Initial discussions between the business and data science teams revolved around two themes a defining lead quality

0.759 gain economies of scale Groups of libraries can make expensive purchases such as subscriptions to academic journals pool resources for large scale projects such as HathiTrust and DPLA and reduce technological overhead costs Also by communicating holdings with other institutions consortial members can collectively reduce redundant collections instead focusing on increasing the size of unique collections Smaller institutions can also benefit from joining prestigious consortia by increasing visibility to their shared unique holdings Many libraries participate in multiple consortia North Carolina State University is a member of nine consortia Effective consortia benefit both smaller and larger participating institutions by enlarging the size of the consortial collection and making otherwise infeasible projects possible But history has shown that consortia are not without shortcomings In the s and s consortia came under criticism for creating bureaucracy unnecessary committees delay and new forms of power imbalance among consortial members Consortia are funded by membership contributions where members could have varying degrees of financial capacities prescribing potential power inequities The additional challenge of consortia in the realm of ML data is the intricate link between profit and data Many large tech organizations have proprietary datasets they may not share in consortial settings The ML

0.7579 following definition of quality a good lead for a dealer was a lead financeable for that dealer This in turn framed the problem as one of matching leads to dealers that were most likely to finance them CarCorp had a large amount of historical lead data In alone the company had processed close to two million leads CarCorp however had relatively less data on which leads had been approved for special financing let alone data on why a lead was approved The business team asked the data science team to contact data engineers to identify and assess the relevant data sources The data science team after investigating the data sources however declared that there wasnt enough data on dealer decisions without adequate data they argued it was impossible to match leads with dealers Few dealers in CarCorps network shared their approval data with the company The scarcity of this data stemmed from the challenge of creating business incentives for dealers to provide up-to-date feedback The incentives for dealers to share information about their approval process with CarCorp were too attenuated While the data science team instructed the business team to invest in the collection of up-to-date data on dealer decisions

0.7394 agencies and researchers These data providers have varying degrees of technical maturity and cannot all be assumed to follow best practices in data quality or format However the trust cannot reject data that does not conform to desired standards as the mission is to provide secure and policy-aware access to all data since even storing the raw data under our legal framework offers some value To accommodate the heterogeneity in data and provider the architecture has loosely coupled tiers A triage tier for unstructured data a scalable lake tier for semi-structured data and a warehouse tier for structured data All tiers are implemented as thin administrative layers on top of existing cloud services The Triage tier supports ingest of raw files that may exhibit unfamiliar format structure content or quality This tier provides no analysis capabilities and essentially offers only secure and policyprotected storage The Triage tier is implemented as a policy authentication and auditing layer over Azure Blob storage using API ETL ETL Lake tier semi-structured Triage tier unstructured Warehouse tier structured raw files keyvalue tuples Rideshare Bikeshare Transit City Researchers Public Mobile IoT synthetic aggregate Figure A three-tier architecture to capture heterogeneous data sources raw files semi-structured data

0.7231 vulnerable when sharing these experiences As such these stories convey and conceal truths at the same time On Good and Harm Themes power imbalance data colonialism Context DrinkUS West is a Europe-based non-government organization NGO working on sustainable access to water and health DrinkUS hopes to support people in Buranda home to the yetetebeke community by improving access to clean potable water It also plans to deploy and test new water accessibility technology and online monitoring of resources Data needs To provide adequate services and assess its impact DrinkUS seeks data about peoples day-to-day water use habits and experiences through surveys In the survey DrinkUS researchers ask individual respondents about their demographics age gender income marital status number of household members and children discretionary spending habits ownership of computers mobile phones leisurely activities and water use habits such as how water and utilities are accessed and the respondents knowledge of potable water accessibility Additionally DrinkUS seeks country-wide data on demographic patterns as well as outcomes related to health education and poverty levels over the past decade It gains access to this data from the Buranda Health Ministry By using these two data sources DrinkUS aims to estimate demand for potable

0.7163 among the different countries on the continent many countries are now producing high-quality data in a more reliable and regular manner through statistical offices national efforts and involvement of other public agencies Likewise non-public sectors actors such as private-sector companies academic institutions civil society organisations development agencies and individuals or communities are now encouraged and incentivized to be involved in open data management The future of open data management and data sharing and their contribution to the advancement of science and technology in Africa will continue to increase despite the slow pace caused by the lack of funding redundant policy frameworks and limited infrastructures The unique African landscape and especially the existing challenges and how they can be addressed will continue to play a big part in African participation in open science and open data global projects As explained by Bangani and Moyo to understand the sharing practices of researchers it is essential first to know where the data comes from and the lives cultures and communities it represents Bangani and Moyo also conclude that most of the emerging researchers in South Africa would prefer to use data from other sources and are not interested in sharing their own data

0.7046 what concerns and challenges they might have for contributing data to an AI dataset Finally we synthesized what we learned into guidelines for designing such an infrastructure and posed questions that need to be considered when soliciting data from PWD We hope our findings will help the creation of inclusive AI datasets that benefit PWD by directly and ethically sourcing data from this community

0.6906 to an AI dataset as it relates to ethics and privacy And what challenges would they face when collecting and uploading their data online We approach these questions from the perspectives of PWD to inform the design of online infrastructures that can be used to collect data from PWD to create more representative AI datasets METHOD To better understand PWDs concerns challenges and motivations for contributing data for AI development we conducted a two-part study that included a semi-structured interview followed by an online survey in which the participants were asked to collect and upload sample data to an online portal The questions used in the interview and the survey are included in the Appendix of the paper Interviews Our interviews took place through a video call due to COVID- social distancing With Shepherd Centers a rehabilitation hospital in Atlanta GA help we recruited participants with diverse abilities and collected their self-descriptions of disability status and information about accessibility needs for participation The interviews were approximately minutes and structured as follows Stage defining AI We first asked our participants about their understanding of AI probing the types of AI applications they are aware of to get a sense of their

0.6891 scholarly heritage and legacy purposes As a form of large-scale collective human record-keeping archives have existed for thousands of years long before digital materials The earliest archives have been state instituted with the purpose of governing the public The Society of American Archivists SAA defines an archive as An organization that collects the records of individuals families or other organizations Archives may be institutional eg United Nations Archives governmental eg National Archives and Records Administration foundational eg Rockefeller Archive Center research-oriented eg Houston Asian American Archive among having other objectives Many modern archives have digital components In all instances archives share the objective of collecting human materials as records to be viewed for future uses Through years of trials trends and debates archival studies have sophisticated literature on issues of concern in sociocultural material collection Recent fairness initiatives in the ML community echo procedures and language already developed and used in archival and library communities To name a few guidelines for how to label data the collection and accessibility of private information sharing datasets across platforms critical reflections on diversity and inclusivity theory of appraisal and selection Archivist researchers have developed various schools of data collection T R Schellenberg Gerald

0.6877 data contribution CDC is a promising alternative to harm-based data leverage In CDC instead of deleting withholding or poisoning data people give their data to an organization that they support to increase market competition as a source of leverage People using CDC for data leverage are similar to people engaging in political consumption but instead of voting with their wallet they vote with their data An exciting aspect of CDC is that while small data strikes struggle to put a dent in large-data technologies because of diminishing returns CDC by a small group of users takes advantage of diminishing returns and provide a competitor with a large boost in performance We return to this point later in our assessment of data levers CDC Variants Variants of CDC closely mirror variants of data strikes because CDC in a sense is the inverse of data strikes where data strikes take CDC gives The easiest way to engage in CDC is to simply start using another technology with the intention of producing useful data for the organization that operates the technology Sometimes these CDC campaigns may also involve a data strike if a user moves from one platform to another for example abandoning

0.6852 a privacy violation In addition given the partnership with the Burandan government and poorly defined protocols around data sharing and use survey participant data was shared with government officials including data about household composition and reported or predicted health challenges linked to potable water access While the researchers vetted their questions for survey design validity they did not seek active involvement from the yetetebeke community to discuss whom the data can be shared with and for what purposes DrinkUS also failed to solicit feedback on the broader project design data ownership and its use beyond original scope of work and research procedures This disempowerment of local communities in the decision-making process introduced trust issues both with the NGO as well as with the Burandan government which has been known to engage in open data sharing practices with large NGOs and institutions Local communities on the other hand cannot access their own data with the same ease The Journey of African Scholars Themes trust awareness Context A doctoral candidate from Bozatta researching in Nova Africa wants to collect soil samples from several Nova Africa provinces for her research Her study aims to investigate the fertility of soil samples which will inform

0.669 including in the context of the African continent Some of this push within the continent comes from funding agencies and government requests for researchers to Deficit narratives reduce a group or culture to its problems rather than portraying it with the strengths creativity and agency of its people archive and share data including open data portals from countries such as Kenya Ghana Nigeria and South Africa The call for data sharing is found in various researching fields across the continent including in the environmental geospatial agriculture health and biomedical sciences The intergovernmental Group on Earth Observations GEO for example runs initiatives focused on Africa AfriGEOSS with the aim of enhancing earth observation data production management and sharing Similarly the digitization and sharing of soil data are believed to play a crucial role in a number of fields including food security health hydrological modelling and climate change Citing dialogues during a South African Soil Information Workshop scholars identify the expansion of soil databases and data sharing as important developments for soil science and environmental fields The biomedical sciences also echo the call for data sharing In a recent paper Martin et al have called for global genomic data sharing and discussed

0.6664 tend to be met with suspicion by local communities The persona on Soil and Apartheid captures this in a stark manner due to the apartheid regime the doctoral researcher finds that Black farmers in Nova Africa suffered unjust land grabs Such historical injustice plays out in the farmers reluctance to share soil data due to lack of trust Resource inequalities and colonial oppressive histories instill deep mistrust towards open data and data sharing initiatives Although African researchers are generally supportive of data sharing they are considerably less enthusiastic about open data expressing concerns that open data compromises national ownership and reopens the gates for parachute-research ie Global Northern researchers absconding with data to their home countries Such concerns are not unwarranted In fact the findings from a recent study from Mbaye et al affirm this fear Mbaye et al performed a systematic review examining African author proportions in the biomedical literature published between in which research was originally done in Africa The authors found that African researchers are significantly under-represented in the global health community even when the data originates from Africa A common threat is parachute-research in which non-African researchers benefit from data sharing and open data are afforded

0.6664 in colonial-era extracting practices whose impact can be felt to this day Numerous efforts to mitigate inequalities in data access and sharing in the continent and especially those driven by initiatives outside of the continent may not be attuned to the complex data landscape resulting from a diversity of needs priorities and experiences of those in the continent Livestocks and Livelihoods Themes data infrastructure awareness Context A researcher in Wolonda receives a two-year grant from a large foundation from the Global North The research project outlines an ambitious goal of ushering in prosperity for the livestock farmers by making the mainstream markets work for them Data needs The researcher outlined his intentions to collect various types of data including genomic data of crops and livestock and enterprise data for the farm The genomic dataset would help determine if the right crops were being planted and the right animal breeds being reared to support the production system It would help provide the farmers vital intelligence on susceptibility of their crops and animals to diseases and performance of their farm against aggregate data The project was structured as a donor-funded research endeavor with clear data collection and analysis objectives Yet no protocols

0.6555 practices although many of the insights we draw also apply to arguments for opening data Figure Stakeholders in the African data sharing ecosystem Those at the top of the iceberg hold significant power and leverage in guiding data sharing practices and policy compared to those in the hidden part of the iceberg Image adapted from policymakers non-government organizations NGOs data collectors data labellers data brokers data subjects archivists researchers and many others Responsible data sharing requires a harmonious partnership between all stakeholders both under and on top of the iceberg as depicted in Figure Consideration of the interests values and needs of all and especially those whose data is being collected is a vital step in fostering responsible data sharing practices In many cases however we find that more powerful stakeholders wield disproportionate power in driving the design framing and enforcement of data sharing policies and practices their values and interests dictate not only the data sharing landscape but also narratives around data access On the other hand data sharing in the continent relies heavily on those in the hidden part of the iceberg Data is often extracted from these same communities and analyses of the data often have a

0.6497 sourcing data directly from PWD by allowing a semi-controlled data collection process at scale Indeed online infrastructures have been powerful and scalable means of sourcing data from people for many years Games with a purpose such as the ESP Game for labeling images were an early success that leveraged the power of people on the Internet to collect data with a direct implication to improving accessibility by captioning images on the web The VizWiz app allows blind users to upload photos and receive descriptions from paid crowdworkers many VizWiz users have opted to share images to create public datasets for AI training Similarly citizen scientists connected on the web have proven themselves to be effective for example at tracking and sharing the radiation data after the Fukushima nuclear disaster in Japan Finally more recent efforts such as the data dignity project have started to envision how users can be compensated for data they contribute for a variety of commercial and/or academic purposes However the following open questions need to be answered for designing successful processes for collecting data from PWD What would motivate PWD to contribute their data to an AI dataset What are their concerns for contributing their data

0.6486 and structured relations shared access signatures SAS to mediate access to Azure and provide implementation transparency The Lake tier offers scalable semi-structured query for quality control analysis restructuring and integration Data is ingested either directly from providers in easy cases where providers can conform to our requirements or from Extract Transform Load ETL processes from the Triage tier Data is assumed to be in semistructured json format following current best practices The Lake is implemented using the Azure service CosmoDB The Warehouse tier supports structured data management and enforces integrity constraints Data may be ingested directly into this tier via managed APIs or may be produced from ETL processes from the Lake tier Most reports and data products including those based on synthetic datasets will be derived from data in the Warehouse tier to ensure that quality assumptions are met Synthetic datasets will typically be managed in the warehouse tier The warehouse tier will be implemented using a relational database building on existing capabilities Access to data can be customized and controlled for each stakeholder allowing free access to their own data restricted access to certain synthetic datasets in the Warehouse and even access to original data from other stakeholders

0.6464 data scientists product managers business analysts project managers and company executives and produced pages of fieldwork notes and photographs Interviews and fieldwork data were transcribed and coded according to the principles of grounded-theory analysis inductively analyzing data through several Organization personnel and project names in this paper have been replaced with pseudonyms to preserve participant anonymity rounds of qualitative analysis In our analysis we coded the data in two rounds focusing on the identification of key categories themes and topics as well as the relation between them in the data While we focus on a specific corporate data science project in this paper we observed similar dynamics across several other projects We chose this case because the work of problem formulation was particularly salient in this project CASE-STUDY SPECIAL FINANCING CarCorp a DataVector subsidiary collects special financing data information on people who need car financing but have either low/bad credit scores between or limited credit histories The companys clientele mainly consists of auto dealers who pay to receive this data called lead data that include information such as name address mortgage and employment details sometimes even the make of the desired automobile The company collects lead data primarily online people

0.6464 compelled disclosure of firm data by government Here we outline desiderata for an urban data trust based on lessons learned from related data repositories and our own creation of a university-based repository for geolocation data called the Transportation Data Collaborative TDC Responding to the call for data trusts and intermediaries we provide a set of design implications in support of the following goals i protecting individuals and firms privacy in granular data ii providing access to researchers from multiple firms in a way that allows those sources to be combined for integrated analysis iii facilitating government monitoring over firms for the purpose of ensuring accountability and equity in service delivery and iv observing firms proprietary interests in their data A third-party data trust works to broker and facilitate query-specific granular data sharing across a public-private collaborative and research and develop new integrative services that provide value to participants without sharing identifiable information Government agencies can benefit from the trust with evidence needed to enforce policy compliance without acting as stewards for sensitive data data that they are often legally and technically ill-equipped to protect For example in the case of transportation data a travel demand model trained on public transportation

0.6458 Most of these emerging researchers indicated a lack of resources misuse of data and trust issues as the significant challenges they face in sharing their data Some of them indicated that they would prefer to only share their data with collaborators within their universities and not with collaborators outside their universities For many of the researchers in this context the labour effort and time it takes to collect data is unmatched by the benefits often little if any at all they might receive from data sharing Subsequently as well as encouraging researchers to share their data there also needs to be actionable benefits and rewards as well as policies that protect African researchers and communities from practices like parachute research Open data imposes risks to African communities in terms of both stripping data away from contexts and reduced benefits to communities where data is sourced It may present a possibility for Global North researchers to publish research using data from Global South with neither acknowledgements to data collectors or data subjects nor an understanding of the data setting Consequently there should be less pressure to open up data and more incentives to share data with the necessary conditions and support

0.6432 interview in light of having experienced a sample data contribution process We find that many of our participants were open to contributing their data even on a voluntary basis if it meant that AI could become more effective for PWD suggesting that seeking data contributions from PWD could be a promising direction for creating more inclusive AI datasets However our findings also suggest that the process of motivating and receiving PWDs data contribution has to meet the unique needs and circumstances that they may experience For example when determining compensation forms and rates for PWDs data contribution one may want to consider the sensitivity of the data as well as the effort and resources it takes for PWD to collect and upload those data It is also important to be cognizant of the fact that people experience disabilities on various spectrums some experience non-apparent disabilities such as ADHD or dyslexia and might be more cautious about contributing data that might disclose their disabilities while some experience more prominent disabilities within a disability category that calls for a carefully designed data collection process that is accessible for them Based on our findings we end with guidelines for designing data collection processes

0.6426 represent the voices of non-elites the grassroots the marginalized In the anglophone world community archives have been documenting minority groups since the s ranging from LGBT archives Hall-Carpenter Archives s to the Black Cultural Archive established in the UK in Thousands of other self-collecting archives exist today covering various religious linguistic class gender ethnic generational cultural and regional groups aided by online platforms In the UK alone over a million people are reported to be involved projectrespectwithgooglecom crowdmlcc/nips historypinorg/en in community archiving Table lists examples of community archives Community archives serve as an example of how datasets can be opened up for public input democratize the collection process and give agency to minority groups to represent themselves For the purposes of historical archiving the mission to build an inclusive local heritage and preserve the most complete account possible underlies these initiatives For instance womens archives such as the Feminist Archive a collection of diaries personal letters photographs among other ephemera contribute to a more complete national history Thousands of such community archives add diversity to historical records While many of the initiatives have grassroots beginnings foundations and institutions have been actively funding and promoting archiving in the periphery For instance

0.6398 in that any industry information shared with regulators may be subject to requests that could unduly expose customer data to disclosure These concerns reify a data sharing ecosystem in which high-resolution data is owned by transportation service providers but not shared at a level of detail that would support analyses into multi-modal transit management of public rights-of-way or behavioral change in the transport sector Protecting privacy in shared mobility data In the summer of the City of Seattle began a pilot program for dockless bikes issuing permits for three different companies Lime Ofo and Spin In exchange for a permit to operate in the city Seattle required that each firm share granular data about its ridership for the purpose of evaluating the services as they were being received by consumers throughout the city Cities have long requested detailed travel information from service providers in exchange for a permit to operate on city streets as evidenced by municipal taxi and limousine regulations Such requests may include GPS traces of origins destinations and routes with exact time stamps as well as demographic details about the consumer The aim of the city in requesting this data is complex including the cost-effectiveness of services

0.6384 for data storage and scope of use were defined and there were no clear guidelines on whether the data can be shared with third parties Furthermore scant information was provided as to if and how the collected data would be used beyond the time period of the donor-funded project As expected the researcher and his team diligently collected the genomic data as well as the farm data from a cluster of villages in several counties of Southern Wolonda The data collection process was however more challenging than originally anticipated Even with a local team well-versed in the norms and practices of the region farmers seemed skeptical to give up their data Slowly as the end of the two-year grant period approached the data collection exercise also came to a close Data sharing challenges Encouraged by this newly collected and valuable dataset a research team established a for-profit company using the data collected as part of the research project In its mission statement the company articulated the need for precision agriculture in todays data-rich environment as an enabler of better agricultural practices The source data on which the company was built came from that dataset which was collected as part of

0.6382 several examples of each Data Strikes The first of the data levers we will consider are data strikes Data strikes involve a person withholding or deleting data to reduce the amount of data an organization has available to train and operate data-dependent technologies Although the term data strike is relatively new the concept builds on the well-studied practices of stopping or changing technology use as a form of protest as discussed in Related Work For instance groups have participated in prominent boycotts against companies like Facebook and Uber In another example people use ad blocking software to deprive companies of data about the success of their ad placements Data Strike Variants The most basic form of a data strike is a withholding-based data strike In some cases users can withhold data by reducing or stopping their technology use or by continuing to use a technology with privacy-protection tools eg tracking blockers In jurisdictions that allow people to delete their past data using laws like the General Data Protection Regulation GDPR and California Consumer Privacy Act CCPA users can also engage in deletion-based data strikes The effectiveness of such strikes will depend on how well regulations can force companies to regularly

0.6375 remains difficult as common data-collection methods introduce disproportional barriers and risks for PWD In-person data collection can be prohibitively expensive due to the low representation of many disability groups in the general population due to the long tail of disability Some disabilities may prevent potential participants from traveling to onsite data collection events due to limited mobility a challenge exacerbated by the COVID- pandemic since many with disabilities are in high-risk groups and must maintain strict social distancing Scraping online data sources to collect data of PWD is also difficult due to the low representation of disability groups the heightened needs for privacy when the data concerns PWD the ethical issues of scraping data without consent and the fact that accurate descriptions of disability status are rarely associated with scraped content Finally simulating disabilities can bear inaccurate data that may also reinforce societal prejudices and stereotypes of experiencing disabilities Such challenges pose roadblocks for well-intentioned AI practitioners who are motivated to understand and mitigate biases in their systems but lack sufficient expertise to work with PWD Online data collection from PWD Given these challenges creating online infrastructures that centralize the data collection from PWD can be an alluring solution for

0.6368 of topics of relevance Rural Americans are more likely to have different topical interests and political leanings For instance rural Americans are more likely to vote Republican than those in urban sites Immigration Multi-ethnic/immigrant neighborhoods where English is not the dominant language or residents consume foreign cultures as the dominant source can be another area where non-standard English can be collected We can then pursue data collection strategies such as Community Archives Identify community archives holding materials relevant to demographics of interest Are there community archives across the country that have already collected language materials Participatory Schemes Generate participatory archives Send out self-complete kits or staff out to rural communities multi-ethnic neighborhoods and areas with low internet usage Collect oral histories and interviews based on language collection covering a variety of topics To ensure ethical supervision in data collection the project will need to hire full-time staff on data collection and management with membership in extra-archival organizations and draw up or adopt a code of ethics for the project This ensures the staff involved in data collection are held accountable for ethical violations WebText is currently nearly completely non-interventionist with no screening for private data Tomake considered decisions calculating the

0.6354 Designing an Online Infrastructure for Collecting AI Data From People With Disabilities AI technology offers opportunities to expand virtual and physical access for people with disabilities However an important part of bringing these opportunities to fruition is ensuring that upcoming AI technology works well for people with a wide range of abilities In this paper we identify the lack of data from disabled populations as one of the challenges to training and benchmarking fair and inclusive AI systems As a potential solution we envision an online infrastructure that can enable large-scale remote data contributions from disability communities We investigate the motivations concerns and challenges that people with disabilities might experience when asked to collect and upload various forms of AI-relevant data through a semi-structured interview and an online survey that simulated a data contribution process by collecting example data files through an online portal Based on our findings we outline design guidelines for developers creating online infrastructures for gathering data from people with disabilities CONCEPTS Human-centered computing Accessibility design and evaluation methods KEYWORDS AI FATE datasets inclusion representation accessibility disability INTRODUCTION The increasing power and pervasiveness of AI technologies pose opportunities for enabling people with disabilities PWD to engage in

0.6339 in mediating how the platform will be used Private firms collect a vast array of high-dimensional data attributes about urban residents and activities and contractual or policy protections for firms competitive and proprietary information are necessary preconditions for access to this data Under the current paradigm firms commonly share data with researchers via Non-Disclosure Agreement NDA a contract stating the purposes for which data is being shared and limiting the uses that researchers are allowed to make of the information NDAs can take many forms but are generally not configured to allow for the analysis of data shared between multiple organizations Borrowing from the health sector we found data sharing and use agreements offer more flexibility for multiparty research than simple NDAs and can be used in conjunction with NDAs to maintain confidential communications with firms Data sharing and use agreements with the TDC serve two distinct purposes First data sharing agreements specify the data to be shared by a firm with the trust how it may be used by external researchers and acknowledgment that the researchers hosting the data may carry out core activities eg cleaning linking and FAT interventions such as bias remediation Second they are fully formed

0.6318 an open data project in Finland and highlighted the intermediary role of data activists between the public and operators of data-dependent technologies On the other hand reactive data activism entails activists acting against data-collecting entities through adversarial behaviors such as employing encryption Data leverage includes both types of data activism Equipped with the knowledge and expertise to understand datas role in computing researchers can provide the public with valuable information to identify and employ effective data leverage practices Work on data activism has unveiled a rich space to improve data practices In particular Lehtiniemi and Ruckenstein called for linking knowledge production to data activism practice to gain a comprehensive understanding of datas role in the public sphere DATA LEVERAGE FRAMEWORK In this section we describe our framework for data leverage in detail The framework and this section is organized around the three data levers we identified For each lever we first define the lever and any variants and do so grounded in past work viewed through our data leverage lens We then provide practical examples of each data lever and describe the likely factors that will govern the effectiveness of the lever Table lists the data levers their definitions and

0.6313 due to their disability status However the open questions around how to create a data contribution process that is sufficiently motivating privacy-sensitive and accessible to a diverse array of abilities need to be answered for its success To this end we conducted a two-part study with participants who experience a wide range of disabilities to gain generalizable insights on PWDs motivations concerns and challenges with regards to contributing data typically needed for a variety of AI systems through an online infrastructure In the first part we interviewed our participants through a video call due to COVID- to qualitatively probe their willingness and concerns for contributing their data to hypothetical AI datasets In the second part we asked our participants to complete an online survey during which they were asked to collect and upload to an online form six common types of AI training data eg a self-portrait photo like those used for face authentication on a phone or a video of them speaking that can be auto-transcribed by a speech-to-text system to simulate a data contribution process They were asked to describe challenges they faced or help they needed and complete an online survey that revisited several themes from the

0.6238 the urgent need for more genetic data studies while also arguing that African populations are not represented in global genomic databases Although the authors raise crucial concerns including the unsuitability of the European method to study heterogeneous African populations they contend that data sharing is vital for a fair representation One of the strongest arguments for rethinking how data is shared comes from the Academy of Science of South Africa which created the African Open Science Platform a project led by the South African Department of Science and Technology This project emphasizes the need to increase awareness accessibility and visibility of African science and data It calls for data and science to be as open as possible with the aim of stimulating interdisciplinary use of data identifying research areas for further exploration and averting the duplication of research The questions of data sharing and management are becoming ever more pertinent to the continent In one of the key gatherings in various stakeholders and government officials from Botswana Ethiopia Kenya Madagascar South Africa and Uganda discussed and formulated national policies With increased demand for statistical data across many African countries many data organisations have been established to promote data accessibility and

0.6237 documenting Its just not profitable As expected financial incentives or the lack thereof can also influence views on documentation Intelligibility of Documentation To further investigate factors that hinder documentation it is necessary to explore issues around creating compelling retrievable and intelligible disclosure documents To illustrate some relevant aspects related to structuring and providing access to documentation we draw on the observations made during fieldwork at both data processing companies Emérita and Active Data Both companies have vast experience in the documentation of data collection and annotation projects In the case of the Argentine company Emérita due to the extension of documentation and the large number of projects conducted navigating and maintaining disclosure documents has become difficult Nati a continuous improvement analyst is in charge of addressing this issue What happened a lot was that information was repeated in many places The objectives were written in three different documents The people who were in the project were in two different systems So having that repeated was horrible because every time people in the team changed well you needed to update many things and credentials Nati works on optimizing some of her companys internal processes including documentation For that purpose she has

0.6229 performance is also highly relevant to data leverage Many authors have found diminishing returns of additional data across many contexts and algorithms eg and some have studied techniques to address diminishing returns These findings are informative as to how effective data leverage can be Data Leverage and Data Activism This paper builds on the literature that explores how the public can change practices of the technology industry Data activism is a relatively new form of civic participation in response to tech companies pervasive role in public life Currently data activism encompasses practices that affect technology design development and deployment Data leverage can be seen as a subset of data activism with a specific focus on empowering the public to influence the performance of data-dependent technologies Milan and Van der Velden provided a typology of data activism that further illustrated the specialized activities in this space proactive and reactive data activism Proactive data activism refers to activists directly influencing software development or databases through open source projects or collaborating with institutions A particularly relevant data activism initiative is the open data movement which aims to democratize information that is currently only accessible to the state or businesses For example Baack studied

0.6208 the firm to share this data directly with public agencies How can a public agency know when the rebalancing strategy of a firm is working or needs adjustment The same methods noted above to mitigate bias can be applied here where the causal relationship in question is between the bike company the timing and location of trips and the timing and location of rebalancing bike movements Figure Percentage of bikeshare trips in Seattle with male riders by origin and destination neighborhoods Advantages of a data trust for ensuring accountability A data trust provides a marked improvement on traditional data sharing methods in that it respects proprietary and competitive interests firms assert in their datasets without precluding access to their granular confidential form Under current practice governments and firms have entrenched adversarial positions with respect to the scope of data sharing in exchange for city permits The proposed model refocuses governments on the specific analyses needed for policymaking A data trust mediates and deduplicates such requests benefitting from network effects as it scales across jurisdictions Finally technical and domain expertise available to university-housed data trusts allow synthetic data requests to be served while preserving salient causal links and removing those that

0.62 requesters have a major influence on the documentation practice of data processing companies and decided to pursue this line of inquiry Through expert interviews with computer vision engineers data quality analysts and managers we investigated how task instructions are formulated and communicated to data processing workers and how this process is documented The interviews revolved around the object purpose and responsibilities of documentation Moreover we discussed issues and possible solutions for implementing broader forms of documentation in industrial contexts at the intersection of data processing and computer vision We conducted a total of fourteen expert interviews Four informants were managers with large data processing companies located in Kenya India and Iraq In addition six expert interviews were conducted with computer vision practitioners working on products including an aesthetics model that sorts and rates personal image libraries a scanner that detects contamination on hands and optical sorting equipment for the classification of waste The computer vision practitioners work for companies located in Germany Spain and the United States Finally four of the interviews conducted at Emérita and Action Data revolved almost exclusively around the role of requesters in documentation and were framed as expert interviews While the goal of in-depth interviews

0.6192 at the Catawba Indian reservation Catawba Cultural Preservation Project other domain experts to reach out to the correct subgroups and accommodate cultural differences Power Data Consortia Implementing systems of ethical data collection demands time expertise and resources Performing disaggregated testing requires the labor cost of annotators for additional demographic labels and handling sensitive data with care requires the resources expertise and infrastructure to preserve privacy All these needs disadvantage institutions with low resources As reported in the stocks of smaller startups fell after the announcement of the European General Data Protection Regulation GDPR because larger tech companies can leverage more resources to ensure GDPR compliance than can smaller institutions To increase parity in data ownership archives and libraries have developed a consortial model In the early twentieth century groups of archives and libraries set up institutional frameworks and services to share resources and collectively store and distribute holdings called library networks cooperatives and consortia Examples include OCLC LYRASIS AMIGOS Library Services OhioLINK and MELSA As of September the International Coalition of Library Consortia ICLC had cited over consortia as members of which over are US-based Consortia have several mutual benefits for participating groups The main advantage is the ability to

0.6177 water determine prices and predict the impact such a service may have on the health and economic welfare of the community Additionally the NGO assesses technologies that could be used to monitor and manage water access As the NGO is keen to engage survey participants in future opportunities it collects contact information such as the names addresses and phone numbers of participants DrinkUS incentivizes each respondent by paying the equivalent of US dollars in Kwacha the local currency for completing the estimated -minute long survey to improve survey completion rate From this data DrinkUS researchers compile and infer which members lack access to potable water what community members can afford to pay and who is most at risk for water-related health impacts and/or disease They combine these results with data obtained from the Buranda Health Ministry to gain deeper insights into the impact of access to potable water This work is later published as a research paper at a top-tier international conference on data science for social good Data sharing challenges While employees at DrinkUS meticulously checked their survey design they did not seek adequate feedback from the yetetebeke community beyond a few initial meetings The yetetebeke community is not

0.617 to describe data LCSH AAT NACO Part of the archivists job is to keep records that adhere to these standards But beyond detailing the contents of data archives also record the process of data collection Archives are wary that all archival content and records will eventually serve future generations With this premise archivists keep records of the decisions and evaluations of the appraisal flow In rigorous appraisal the process passes through many layers of supervision by archivists curators records creators and records managers Table shows a multi-level and multi-person theodiorg Table Example Appraisal Flow Hoover Archives Mission Statement Highest level of agenda formulation determining topics/concepts of concern Collection Development Policy A more specific policy drawn from the Mission Statement about what is collected what is not and where and how to search for sources Appraisal Evaluation based on criteria of whether a given selection of sources is worth collecting Asking whether this collection fits the outlines of the mission statement Evaluating the rarity of the source the authenticity of its provenance and its value for future generations Processing/Indexing Micro-Appraisal Processing the sources individually or at the folder/document level including indexing them and updating the finding aid Sources may be discarded

0.6152 and b finding ways to measure it Defining lead quality was not straightforward There were many stakeholders with different opinions about leads ibid Some described lead quality as a function of a leads salary data while some argued that a lead was good if the dealer had the leads desired car in their inventory Everyone on the business team however agreed on one thing as CarCorps business analyst Ron put it a good lead provided business to the dealer Ron business analyst The business team has been talking about lead quality for a long time We have narrowed down the lead quality problem to how likely is someone to purchase or to be able to finance a car when The exact number is omitted to preserve company anonymity PROBLEM FORMULATION AND FAIRNESS you send them to that dealer Interview November Lead quality was equated with lead financeability It was however difficult to ascertain financeability Different dealers had different special financing approval processes A lead financeable for one dealer can be for various reasons unfinanceable for another The goal thus was to determine dealer-specific financeability ie predicting which dealer was most likely to finance a lead The teams settled on the

0.6125 increasing share of information about urban transportation provision including widely adopted services like car share ride share bike share prediction apps for public transportation and routing apps Like the taxi and limousine services that preceded them city agencies increasingly require these new services to share data in order to enforce permit requirements enable integrative models of demand and ridership and analyze their policy implications To date existing data sharing paradigms have failed to deliver granular access to firm data when it is shared it is often encumbered with contractual obligations that preclude linking data across competing firms As corporate data is zealously guarded to protect competitive advantage releasing data via open data portals or detailed APIs is untenable in many situations Notably in the absence of access to firm information researchers spend considerable resources simulating it as evidenced by work modeling the supply and distribution of car share vehicles Researchers have also used data mining social experiment and intercept surveys at hot spot to collect TNC related data due to the absence of direct access to firm information However each data source has its own biases which may lead to distortions in research findings For example disparities in wait times

0.6085 free trial for the technology being developed with the collected data or creating a sense of community effort for building AI technology that meets the needs of people with disabilities What to communicate Be upfront about the data collection goal and indicate any privacy concerns there might be especially if the data collected may reveal non-apparent forms of disabilities like ADHD and dyslexia Though many participants indicated that they were not too concerned with issues of privacy note that there might be limitations for how well laypeople can anticipate the privacy impacts of the data they share Peoples public information on social media may be used by AI and injure them in ways they cannot anticipate eg different rates for services and job discrimination or used in ways they do not condone down the line eg for data surveillance and persecution We need an ongoing conversation between data contributors and AI developers or privacy experts on what is the appropriate use of any collected data An accessible process In addition to following the standard accessible design practices the online infrastructure for enabling data contribution from PWD should work to make the entire process accessible Contributing data is often a multi-step

0.6078 interpreter was present to interpret the conversation in real-time We also ensured our survey was screen-reader accessible In addition we communicated frequently with Shepherd Center to find out any other accessibility needs our participants had Finally the study protocol and materials were IRB-approved Participants Instead of focusing on people with a particular type of disability we sought to understand the motivation concerns and challenges that PWD might face across various types of disabilities This was done to uncover the more generalizable framework for enabling data collection from PWD given that technologies present different barriers to people with different disabilities To this end we collaborated with Shepherd Center a non-profit hospital in the US to recruit a participant pool that represented as diverse a set of disabilities as possible so long as a potential participants disability did not bar them from providing meaningful informed consent Ultimately we recruited participants who experienced four broad categories of disabilities physical disability such as spinal cord injuries and cerebral palsy n blindness or visual impairment n hearing loss to a varying degree n and cognitive disabilities such as ADHD dyslexia and autism n Table summarizes participants self-described disability status We conducted the study from June

0.6059 sharing initiatives as well as a growing enthusiasm for data sharing movements can be found throughout the distinct nations in the continent At the same time various obstacles limit equitable data sharing practices entire heterogeneous geographies of people have their data accessed and shared yet do not reap the same benefits as the data collectors and owners of data infrastructures These communities can face harm as a result of their data being accessed shared and used At its core various structural and relational challenges plague the relationships between entities based outside of the continent and the African communities whose data is being accessed The continents plural and at times divergent norms practices and traditions furthermore complicate the African data access and sharing ecosystem In sum data sharing in Africa poses a unique set of challenges that remain understudied The African data sharing ecosystem involves numerous stakeholders including communities companies government bodies While data sharing often focuses on facilitating ease of data transfers between individual scientists institutes and/or organizations truly open data goes beyond that by making data freely available for use to a wider-range of populations rendering it a public good In this work we primarily focus on data sharing

0.6043 are a critical aspect we investigated at both stages In the first phase we focused on work practices in data processing companies where human workers collect segment and label image training data We conducted ethnographic fieldwork at two data processing companies of the impact sourcing sector located in Buenos Aires Argentina and Sofia Bulgaria Impact sourcing refers to a special type of business outsourcing processing company that intentionally employs workers from marginalized communities As described on their websites and confirmed by our observations the Argentine company employs young people living in slums while the Bulgarian organization works with refugees from the Middle East The Buenos Aires-located company that we will call Emérita is a medium-sized organization With branches in three Latin American countries Emérita conducts projects in data annotation content moderation and software testing Its clients are large regional corporations in diverse fields such as security e-commerce and energy At the time of the observations between May and June the Buenos Aires branch of Emérita had around data-related employees who mostly worked hours shifts Mondays to Fridays and were paid at the minimum wage Action Data is the code-name of the Bulgarian company Action Data specializes in image data collection

0.6033 agree to these conditions the likelihood of further collaboration and the possibility of fair sharing increases It is recommended that the process of data sharing should be transparent Also researchers based in the continent are encouraged to build a trustworthy collaboration with funders and other stakeholders Introducing proper and adequate legal frameworks documentation and support for data sharing will increase trust among stakeholders Due to uncertainties and lack of legal frameworks several sectors in Africa are unsure of data sharing norms and practices They are allowed to share their data what type of data can be shared who and under what conditions Such frameworks and documentation can help clarify the benefits expected norms and equitable data sharing practices Such can help build trust and encourage researchers to share their data in a manner that acknowledges their labour and protects their ideas and innovations Equitable data sharing is also a matter of training the next generation of data scientists It is imperative to include African data challenges into the data science curricula since these issues are very different from other parts of the world Aspiring data scientists must be aware of data sources and historical records about communities where data are

0.603 data generated in the continent We constructed the personas through an iterative process based on semi-structured interviews with ten African data experts Our interviewees included individuals working in universities governments startups and think tanks and we focused on data collected and shared for research or policy purposes We first developed open-ended questions guided by our experiences observing and Although the African StoryBook Project can be seen as a powerful example it is also important to acknowledge that limitations and challenges remain for the Ugandan Education system For instance the harms of colonial regimes are not extinguished immediately upon instituting policy reforms The dynamics of the global political economy also continue to pressure national education systems to emphasize teaching and establishing grading or progress standards for students that are measured by English language competency eg standardized tests actuating data sharing practices on the continent The interviews were recorded with permission from the interviewees after which we manually transcribed the audio recordings Next we identified and abstracted central themes challenges and concerns expressed by the interviewees Any information that could be used to identify our interviewees institutions government bodies or communities was removed In addition we only took a particular challenge as

---------------------------------------------------------------------------
TOPIC 18: utility/welfare

---------------------------------------------------------------------------

0.8252 income and all people live in the other neighborhood have low income We further assume that the high-income neighborhood is primarily occupied by the advantaged group a and the low-income neighborhood is primarily occupied by the disadvantaged group b Therefore the thresholding rule with q classifies all people living in the high-income neighborhood as the advantaged group ie A a and all people living in the low-income neighborhood as the disadvantaged group ie A b Table Lending policy of Example with affirmative action that favors the disadvantaged group over the advantaged group at any given income level True race Neighborhood a b high income i low income ii i Misclassified as race a in high-income neighborhood ii Misclassified as race b in low-income neighborhood Example Consider an extreme lending policy all people with high income get their loans approved while all people with low income are rejected no matter what their races are See Table for the illustration Simple calculations show that the true demographic disparity demographic disparity The reason for the overestimation is that the race proxy is correlated with the loan approval outcome because of the dependence between geolocation and socioeconomic status people who live in the neighborhood

0.8067 and social burden by choosing from the interval I This provides an additional argument in favor of Nash equilibria institutions can still reason in terms of equilibria and achieve more favorable outcomes in terms of social burden FAIRNESS TO SUBPOPULATIONS Our previous section showed that increased robustness in the face of strategic behavior comes at the price of additional social burden In this section we show this social burden is not fairly distributed when the individuals being classified are from latent subpopulations say of race gender or socioeconomic status the social burden can disproportionately fall on disadvantaged subpopulations Furthermore we find that improving the institutions utility can exacerbate the gap between the social burden incurred by an advantaged and disadvantaged group Concretely suppose each individual is from a subpopulation ab The social burden a classifier has on a is the expected minimum cost required for a positive individual from to be accepted E minx x x Y G We can then define the social gap between groups a and b Definition Social gap The social gap Gf induced by a classifier is the difference in the social burden to compared to a Gf Bb Ba The social gap is a

0.8027 the cost of investment G In reality the cost of investment may be distributed differently in each group a disadvantaged group might on average experience higher monetary or opportunity costs For example low income families who may have to take out loans to pay for college tuition incur high interest rates This is a compelling setting that reflects deep-seated disparities in access to opportunity between demographic groups in the real world an analogous setting has been considered by works on strategic classification where the costs for manipulating features is posited to differ across groups In this section we consider the ramifications of differences in investment cost across groups focusing on the setting of Section We show that the disadvantage from having higher costs is amplified under group-realizability specifically suppose that group a a has costs distributed according to cumulative distribution function G and that group a is disadvantaged in terms of costs The following result observes that if G sufficiently dominates G then there exists no stable equilibrium that encourages optimal investment from group a and no equilibrium that is balanced for both groups in sharp contrast to the characterization in Proposition The proof is deferred to Appendix E P

0.7915 the groups As such the disadvantage encoded in the cost condition can arise due to differences in valuations of classifications differences in costs c or differences in valuations of those costs u EQUILIBRIUM ANALYSIS We begin by studying agents best-response strategies in the basic Strategic Manipulation Game with Groups in which candidates belong to one of two and B and the cost condition holds so that group B members face greater costs to manipulation than group A members To build intuition we rst consider best-response strategies in the one-dimensional case in which candidates have features x and group cost functions are of any non-negative monotone form We then move on to consider the d-dimensional case in which candidate features are given as vectors x d and manipulation costs are assumed to be linear One-dimensional Features In the d case the cost condition given in may be written as c are linear in the one-dimensional case they may be written as threshold functions where thresholds A and B are constants in and for agents in group m x if and only if x m A university admissions decision based on a single score is an example of such a classifier Although

0.7847 A by tpr PY Y A a and fpr PY Y A a Model Description Individuals Rational Response We consider a setting where an individual decides whether to acquire qualifications that is to invest in obtaining label Y prior to observing their feature X The decision to acquire qualification depends on the qualification assessment rule currently implemented by the institution We will characterize the groups qualification rates as the best-response to by function bra aA To get label Y an individual has to pay a cost C In any group C is distributed randomly according to the cumulative distribution function CDF G After deciding whether to acquire qualifications an individual gets features X and is assessed by An individual from any group and regardless of actual qualification receives a payoff of if they are assessed to be qualified and payoff of otherwise Therefore the expected utility an individual from group a receives from acquiring qualification Y is Y A a C tpr C whereas the expected utility for not acquiring the qualification is Y A a fpr Given the qualification assessment parameter an individual from group a acquires qualification if and only if the benefit outweighs the costs that is

0.7779 classification setting Each individual has features x X and a label y Y The institution publishes a classifier X Y In the non-strategic setting the institution maximizes the non-strategic utility which is simply the classification accuracy of Pf x y In the strategic setting the individual can modify their features and the institution aims to preempt the individuals strategic manipulation In response to the institutions classifier the individual can change her features x to new features x However modification incurs a cost given by c X X R The individual then receives an individual utility x x x which trades off between the cost of manipulation x and the benefits of classification x The institution models the individual x as maximizing their utility x and acting according to the best-response to the classifier x argmax x x When it is clear from context we will drop the dependence on and write the individuals best response as x Although x may not have a unique maximizer it is assumed that the individual x does not adapt her features if she is already accepted by the classifier ie x or if there is no maximizer x she can move to such that

0.7779 decoupling using group-specific assessment rules achieves optimal outcomes when the problem is realizable within each group but can significantly hurt certain groups when the problem is non-realizable and there exist multiple equilibria after decoupling In particular decoupling can hurt a group with low initial qualification rate if the utility-maximizing assessment rule for a single group is more disincentivizing to individuals than a joint assessment rule thereby reinforcing the status quo and preventing the group from reaching an equilibrium with higher qualification rate We also study subsidizing individuals investment cost eg subsidizing tuition for a top high school especially when the cost distribution is varied across different groups We nd that these subsidies increase the qualification rate of the disadvantaged group at equilibrium regardless of realizability We note that our subsidies which affect the qualification of individuals directly are different than those studied under strategic manipulation that involve subsidizing individuals cost to manipulate their features Y X A Figure Causal graph for the individual investment model The individual intervenes on the node for qualification Y this corresponds to which then affects the distribution of their features X depending on the group A without changing the underlying qualification eg subsidizing SAT exam

0.7769 primarily occupied by the advantaged group are also more likely to get loan approval because they have relatively high socioeconomic status eg high income in this example These people are classified as advantaged group because their neighborhoods are associated with high probability of belonging to the advantaged group As a result the thresholding rule misclassifies people who are from the disadvantaged group but get loan approval as the advantaged group In this way the thresholding rule leads to underestimates of the loan acceptance rate of the disadvantaged group Analogously the thresholding rule leads to overestimates of the loan acceptance rate of the advantaged group because it misclassifies people from the advantaged group but not likely to get loan approval as the disadvantaged group Consequently the misclassification of the protected attribute is dependent with the outcome which ultimately leads to the overestimation of demographic disparity The dependence between the misclassification and the outcome results from the inter-geolocation outcome variation the socioeconomic status race proxy probabilities ie race ratios and loan acceptance rates vary across different neighborhoods such that the favorable outcome is positively correlated with the probability of belonging to the advantaged group Example Now consider a hypothetical lending policy with

0.7769 different groups resulting in multiple equilibria at which groups may experience disparate outcomes In sections and we study group-realizability under two different and complementary settings The rst setting considers features that are drawn from a multivariate Gaussian distribution and assumes that in each group the qualified individuals are perfectly separated from unqualified ones by a group-specific hyperplane This is a benign setting where no group is inherently disadvantaged group features and performance of assessment rules are symmetric up to a reparameterization of the space The second setting considers features that are uniformly distributed scalar scores and assumes that qualified and unqualified individuals in a group are separated by a group-specific threshold where one is higher than the other This model captures the natural setting where the feature score and assessment rules inherently favor one group eg SAT scores are known to be skewed by race We use the aforementioned stylized settings to demonstrate the salient characteristics of equilibria that one might anticipate under group-realizability We nd that stable equilibria tend to favor one group or the other This is especially surprising in the multivariate Gaussian case where the two groups are identical up to a change in the representation of

0.7515 the space We also study the existence of balanced equilibria where both groups acquire qualification at the same rate We nd that when balanced equilibria exist they tend to be unstable that is no initial qualification rate except for the balanced equilibrium itself will converge to the balanced equilibrium under the dynamics We consider two natural interventions in overcoming the challenges of group-realizability as outlined above As group-realizability poses even greater challenges when the costs of investment are unequally distributed between groups in Section we consider the impact of subsidizing the cost of acquiring qualification for one group In Section we consider the impact of decoupling that is we allow the institution to use different assessment rules for different groups assuming the group attributes are available This is in contrast to the typical setting where institutions are constrained to using the same assessment rule across all groups which may be the case when data on the protected attribute is not available or when the use of protected attributes for assessment is regulated Uniformly Distributed Scalar X We consider X the class of assessment parameters and assessment decision X for all that represent all threshold decision policies Consider two groups aa

0.7413 of the classifier increases The institutional utility is quasiconcave while the social burden is monotonically non-decreasing The next lemma provides a formal characterization of the shapes shown in Figure Theorem The institutional utility is quasiconcave in and has a maximum at a threshold where is the threshold of the non-strategic optimal classifier The social burden is monotonically non-decreasing in Furthermore if then Social Burden Institution Utility U B Figure The general shapes of the institution utility and social burden as a function of the threshold the institution chooses The threshold t is the non-strategic optimal while the threshold is the Stackelberg equilibrium Proof be the set of individuals accepted by in the strategic and non-strategic setting respectively If we is the optimal nonstrategic acceptance region any x has and one can by not which Therefore if a threshold is optimal for the institution ie if then Recall that a univariate function z is quasiconcave if there exists z such that is non-decreasing for z z and is nonincreasing for z z Let be as above For we have Since is optimal is the optimal strategic acceptance region and thus Similarly if we have and thus Therefore is quasiconcave in The

0.7384 unmanipulated scores Generally this robust strategy employs a more conservative decision threshold that forces some qualified candidates to manipulate their scores but is too costly for unqualified candidates to reach Robustness however imposes a burden on applicants Both Hu et al and Milli et al point out that deploying a robust decision rule in this setting has important fairness implications In the college admissions scenario they show that robust strategies can impose disproportionate burdens on qualified applicants from disadvantaged groups This sets up a trade-off between the agents utility which is increasing in the classifiers accuracy and applicant utility which is decreasing in the cost that qualified applicants must pay to be accepted by the agent The implication of this work is that responsible actors should consider this trade-off before deploying robust policies Using simulation we extend these analyses in two ways First we compare the behavior of one-shot agents considered in previous work against the behavior of an agent that is able to retrain its classifier across many rounds of decisions but remains unaware of gaming behavior Second we consider how noise in the relationship between an applicants unmanipulated score and their true label affects the continuously retrained classifier

0.7384 members of group A while committing false negatives on group B members with features x Some candidates in group B are thus strictly worse-o while none improve Without the subsidy offering group B members had been perfectly classified but now there exist some candidates who are mistakenly excluded Further one can show that candidates who are positively classified must pay more to manipulate to the new threshold in spite of receiving the subsidy benefit This increased cost is due to the fact that the higher classification threshold imposes greater burdens on manipulation than the subsidy alleviates Group A candidates are also strictly worse-o since the threshold increase eliminates false positive benefits that some members had previously been granted in the no-subsidy regime Moreover all candidates who manipulate must expend more to do so since these candidates do not receive a subsidy payment Only the learner is strictly better o with the implementation of this subsidy plan Additional examples in the appendix show cases in which both groups experience diminished welfare when they bear linear costs Even when the learner has an error function that penalizes false negatives twice as harshly as false positives and thus is explicitly concerned with mistakenly

0.7376 both receives the opportunity and succeeds at it Our total payoff is a discounted sum of payoffs over all periods with discount factor that is the total payoff r is equal to t t r t As noted earlier agents with circumstance who receive the opportunity and succeed at it will produce offspring who are more likely to have circumstance this matters for the payoff because the total payoff r t t r t depends on the fraction of agents with each type of circumstance in all time periods Thresholds and Interventions The way we allocate the opportunity at time t is to set a threshold t for agents with circumstance for and to oer the opportunity to all agents i with circumstance whose success probability ai is at least t That is ai We will sometimes write the threshold and the population fraction with each circumstance without the explicit dependence t when we are considering a single fixed time period Agents of circumstance make up a fraction of the population and a fraction of them receive the opportunity for a total fraction of the population equal to Similarly agents of circumstance make up a fraction of the population and

0.7337 x kB B CF x Ax A kA where kB argmax d i and kA argmax d i LEARNER SUBSIDY STRATEGIES Since in our setting the learners classification errors are directly tied to unequal group costs we ask whether she would be willing to subsidize group B candidates in order to shrink the manipulation gap between the two groups and as a result reduce the number of errors she commits In this section we formalize subsidies as interventions that a learner can undertake to improve her classification performance Although in many high-stakes classification settings the barriers that make manipulation differentially accessible are non-monetary such as time information and social access in this section we consider subsidies that are monetary in nature to alleviate the financial burdens of manipulation We introduce these subsidies for the purpose of analyzing their effects on not only the learners classification performance but also candidate groups outcomes Since subsidies mitigate the inherent disparities in groups costs and increase access to manipulation one might expect that their implementation would surely improve group overall welfare In this section we show that in some cases optimal subsidy interventions can surprisingly have the effect of lowering the welfare of candidates

0.7337 B A to be the space of undominated thresholds For an account of the full proof of this result and all omitted proofs see the appendix Even without committing to a particular learner cost function the space of optimal strategies characterized in Proposition leads to an important consequence A rational learner in the Strategic classification Game always selects a classifier that exhibits the following phenomenon it mistakenly admits unqualified candidates from the group with lower costs and mistakenly excludes qualified candidates from the group with higher costs This result is formalized in Proposition To state the proposition the following definition is instructive Whereas the true thresholds A and B are a function of unmanipulated features the learner only faces candidate features that may have been manipulated In order to make these observed features commensurable with A and B it is helpful for the learner to translate a candidates possibly manipulated feature to its minimum corresponding original unmanipulated value D C For any observed candidate feature the minimum Figure Group cost functions for a one-dimensional feature x A and B signify true thresholds on unmanipulated features for group A and B but a learner must issue a classifier on manipulated features

0.7326 the social gap grows large as well Threshold l G G Social Gap with the Same Cost Per-Group Figure Impact of increasing the threshold on white and black credit applicants When the cost to changing ones score is small increases to the threshold have only a small effect on the social gap However as becomes large even small increases to the threshold can create large discrepancies in social burden between the two groups Different Cost Functions In Section we demonstrated when two subpopulations are identically distributed but incur different costs for changing their features there is a non-trivial social gap between the two In the context of the FICO scores it is plausible that blacks are both disadvantaged in features and experience higher costs for changing their scores For instance outstanding debt is an important component of FICO scores One way to reduce debt is to increase earnings However a persistent black-white wage gap between the two subpopulations suggest increasing earnings is easier for group a than group b This setting is not strictly captured by our existing results and we should expect the effects of both different costs functions and different feature distributions to compound and exacerbate the unfair

0.7311 However to illustrate that this phenomenon does not arise only as a rare corner case we provide one such example here plus two in the appendix and discuss general conditions under which this occurs In each example we consider a particular instance of the Strategic classification Game and compare the welfares of candidates at equilibrium when the learner is able to select a proportional subsidy with their welfares at equilibrium when no subsidy is allowed E Suppose that a learner is error-minimizing such that CF P and Suppose that unmanipulated features for both groups are uniformly distributed with pA pB Let group cost functions be given by cA x p x x and cB x p x note that the cost condition c true group thresholds be given by A and B When subsidies are not allowed the learner chooses a classifier with threshold B at equilibrium This threshold perfectly classifies all candidates from group B while permitting false positives on candidates from group A with features x If the learner decides to implement a proportional subsidies plan at equilibrium the learner chooses a classifier with threshold prop A and a subsidy parameter Her new threshold now correctly classifies all

0.7304 with the underlying cost c INSTITUTIONAL UTILITY VERSUS SOCIAL BURDEN In this section we characterize the inherent trade-offs between institutional utility and social burden in the strategic setting In particular we show any classifier that improves institutional utility over the best classifier in the static setting causes a corresponding increase in social burden To prove this result we first show that any classifier can be represented as a threshold classifier that accepts all individuals with outcome likelihood greater than some threshold Then we show increasing utility for the institution requires raising this threshold but that this always increases the social burden Equipped with this result we show the Pareto-optimal set of classifiers that increase institutional utility in the strategic setting corresponds to an interval I Each threshold I represents a particular trade-off between institutional utility and social burden Strategic classification corresponds to one extremum the best strategic utility but the worst social burden The non-strategic utility corresponds to the other doing nothing to prevent gaming Neither is likely to be the right trade-off in practical contexts Real domains will require a careful weighting of these two utilities leading to a choice somewhere in between Thus a main contribution of our

0.7181 the SAT does not act as the sole determinant of admissions in the US in countries such as Australia Brazil and China a single exam score is often the only factor of applicant quality that is considered for admissions When the learner has access to A and B and group costs cA and cB satisfy the cost condition the following proposition characterizes the space of undominated strategies for the learner who seeks to minimize any error-penalizing cost function P OD U L S Given group cost functions cA and cB and true label thresholds A and B where B A there exists a space of undominated learner threshold strategies B A where A c A cA A and B cB cB B That is for any error penalties CF P and the learners equilibrium classifier is based on a threshold B A such that for all manipulated features To understand this result rst notice that if the learner were to face only those candidates from group A she would achieve perfect classification by labeling as only those candidates with unmanipulated feature x A This strategy is enacted by considering candidates best-response manipulations A rational candidate would only be willing to

0.7046 impacts of strategic classification To illustrate this phenomenon we again use a coarse linear cost model Suppose group A has cost x x for some and group B has cost cB x x x for some As in Section group B is disadvantaged in cost provided the ratio In Figure we show the social gap for various settings of The social gap is always increasing as a function of and the rate of increase grows large for even moderate values of When is large even small increases in can disproportionately increase the social burden for the disadvantaged subpopulation RELATED WORK Strategic Classification Prior work on strategic classification focuses solely on the institution primarily aiming to create high-utility solutions for the institution Our work on the other hand studies the tradeoff between the institutions utility and the burden to the individuals being classified Brückner and Scheffer Dong et al Hardt et al give algorithms to compute the Stackelberg equilibrium which corresponds to the extreme solution in our trade-off curves Threshold S l G G Social Gap with Different Costs Per-Group Figure Impact of increasing the threshold on white and black credit applicants under the assumption that both groups incur different costs

0.7042 increases access to loans for group but also widens the credit gap between the groups two groups illustrated in appendix A Thus unlike the max-util agent the agents policy can change over the course of the simulation in response to the changing credit score distributions in each group of applicants Experiments and results We now describe a set of simulations in this environment that compare maximum utility agents to equality of opportunity agents Some aspects of deploying a maximum utility agent are straightforward to understand analytically Appendix A However the consequences of deploying an agent as well as the long-term population-level effects of max-util and opportunity equalizing policies are considerably more complex and benefit from analysis via simulation Our results tell a qualitatively different story from Liu et al and highlight a mismatch between the agent and the agents aggregate true positive rate that does not arise in static settings We simulate a population with two groups with shifted credit score distributions such that group starts out disadvantaged compared to group Figure We simplify the agents task and grant it oracle access to the population distribution and exact values of repayment probabilities This removes any difficulty of estimation allowing us

0.7013 have preferred that she oer no subsidies at all The following theorem captures the surprising result that subsidies can be harmful to all candidates even those from the group that would appear to benefit T S There exist cost functions cA and cB satisfying the cost conditions learner distributions DA and DB true classifiers with threshold A and B population proportions pA and pB and learner penalty parameters CF P and such that no candidate in either group has higher at the equilibrium of the Strategic classification Game with proportional subsidies compared with the equilibrium of the Strategic classification Game with no subsidies and some candidates from both group A and group B are strictly worse o We note that a slightly weaker version of the theorem holds for at subsidies In particular there exist cases in which some individual candidates have higher at the equilibrium of the Strategic classification Game with at subsidies compared with the equilibrium with no subsidies but both group A and group B candidates have lower pay on average with the subsidies To prove the theorem it suffices to give a single case in which both candidate groups are harmed by the use of subsidies

0.7002 The threshold A perfectly classifies group A candidates B perfectly classifies group B candidates A learner selects an equilibrium threshold B A committing false positives on group a red bracket and false negatives on group B blue bracket corresponding unmanipulated feature is defined as A max cA cA B max cB cB for a candidate belonging to group A and group B respectively The corresponding values A and B are defined such that a candidate who presents feature must have as her true unmanipulated feature x A if she is a group A member and x B if she is a group B member P L C D A learner who employs a classifier based on a threshold strategy B A only commits false positives errors on group A and false negatives errors on group B The cost C of such a classifier is x B B g CF x A A g where false negative errors entail penalty and false positive errors entail penalty CF P A learner who commits to classifying only one of the groups correctly bears costs given by the following corollaries C A classifier based on A perfectly classifies group A candidates and bears cost

0.6992 nd that overcoming stringent thresholds is more burdensome on the disadvantaged group MODEL FORMALIZATION As in Brückner Scheer and Hardt et al we formalize the Strategic classification Game as a Stackelberg competition in which the learner moves rst by committing to and publishing a binary classifier Candidates who are endowed with innate features best respond by manipulating their feature inputs into the classifier Formally a candidate is defined by her d-dimensional feature vector x X d and group membership A or B with A signifying the advantaged group and B the disadvantaged Group membership bears on manipulation costs such that a candidate from group m who wishes to move from a feature vector x to a feature vector y must pay a cost of y x We note that these cost function forms are similar to the class of separable cost functions considered in Hardt et al We assume that higher feature values indicate higher quality to the learner and thus restrict our attention to manipulations such that y x where the symbol signifies a component-wise comparison such that y x if and only if i d i xi Throughout this paper we study non-negative monotone cost functions such that

0.6976 on as we considered in Section decoupling did not benefit We simulate samples from the empirical PDF see Figure and t a Beta distribution by maximum likelihood estimation Figure Score distributions conditioning on repayment outcome Y for different race groups Improvement in final qualification rate after decoupling Initial qualification rate Improvement in final qualification rate after decoupling Initial qualification rate Figure effects of decoupling in presence of multiple equilibria We vary the initial qualification rate in the x-axis all groups As can be seen from Figure G in Appendix G while the White and Asian groups had a higher qualification rate after decoupling the Black and Hispanic groups saw their equilibrium qualification rate decrease On the other hand the effects of decoupling were small in this case less than percent points difference in the final qualification rate We now show that the effect of decoupling can be drastic depending on G Recall that in Section we showed that multiple equilibria with possibly vastly different qualification rates may exist under the non-realizable setting even when there is a only single group In general the existence of multiple equilibria depends on properties that is how the cost of investment is distributed in

0.6931 the two races As a result when we vary from to only inter-geolocation outcome variation changes which affects ab in conditions Figure b shows that in Experiment only a and b vary strongly with d so they are largely responsible for the bias observed in Figure a When d the disadvantaged group have comparatively higher incomes and are thus more likely to be approved in loan application This implies that conditions iii hold and therefore result in overestimation bias for the thresholded estimator Furthermore Y covaries negatively with a and positively with b conditionally on Z also resulting in an overestimation bias for the weighted estimator When d however conditions iii are violated The terms a and b start to counteract overestimation bias thus the overestimation bias decreases with d Furthermore Y now covaries positively with a and negatively with b also resulting in an underestimation bias for the weighted estimator Figure d shows that in Experiment only the terms a and b vary strongly with so these terms are responsible for the variation observed in the thresholded estimator in Figure c As increases both a and b increase along with the overestimation bias of In contrast the weighted estimator

0.6914 D who is offered the opportunity has a probability ai of succeeding at it for a parameter An individual i in group A who is offered the opportunity has a probability ai of succeeding at it for the same and an additional parameter reflecting the advantage We will refer to the above quantities as the success probabilities of the agents Success probabilities reflects various levels of performance when agents are offered the opportunity Anyone in group D who is offered the opportunity and succeeds at it moves up to group A Each individual is then replaced by one offspring of the same socioeconomic status and the process continues to the next generation In the general form of the model there is also some probability that an individuals offspring does not perfectly inherit their socioeconomic status The payoff to society is the number of individuals who succeed at the opportunity summed over all generations with the generation t steps into future multiplied by t for a discount factor Summary of Results In any given generation societys policy will consist of a threshold for group D and a possibly different threshold for group A the opportunity is given to every individual whose

0.6864 classification for Wastewater Treatment Plants that handle sewage in the two neighboring states of Virginia and North Carolina compliance Table presents the compliance statistics for general and individual permits calculating SNC as if the NCI were applied to all facilities Roughly of individual permits overall that is across major and minor permits are in effluent SNC compared to of general permits although that difference is largely a function of higher non-reporting among general permits The overall SNC rate between the two permit types is comparable In short due to discretionary differences between state permitting protocols the NCI excludes from its purview a large number of functionally similar facilities What impact does this policy decision have Table provides some descriptive information to understand the impacts of rst expanding the scope of the NCI to include individual permits and second to delay the inclusion of general facilities First based on eligible facilities we can see that including individual minor facilities shifted the mass of attention towards lower density areas with fewer minority populations on average Second general permits exceed the total number of individual permits by nearly a third Third both the general and individual categories have large numbers of wastewater treatment

0.6861 highway If we limit the amount of time that can be added to traverse a road segment to of the original time we can prevent cars from being routed through Leonia until the average highway speed has reached mph That is we can find a solution that prevents routing through town as long as the highway speed is greater than mph For time increase we can change the town such that traffic will not be routed through Leonia until the average speed on the highway has fallen to mph This solution for Leonia is shown in Figure Lieusaint is larger than Leonia with vertices and edges Given the parameters we choose Waze routes its users through Lieusaint when which corresponds to the speed on the highway dropping to mph Allowing the road segments to be lowered by we can prevent traffic from being routed through Lieusaint until the highway speed has dropped below mph See Lieusaint in Figure We report the solution for Fremont CA a significantly larger town in Appendix B Finally we measure the cost of implementing these solutions Fig For each town we consider the impact to the town to be how much longer on average it

0.6845 and manipulates her features x to y x A group m candidate with features x who moves to y earns a y y x The learner incurs a penalty of CF P X m AB x y X m AB x y where CF P and denote the cost of a false positive and a false negative respectively The learner looks to correctly classify candidates with respect to their original features x whereas each candidate hopes to manipulate her features to attain a positive classification expending as little cost as possible in the process Under this setup candidates are only willing to manipulate their features if it their classification from to and if the cost of the manipulation is less than We note that defining the utility of a positive classification to be can be considered a scaling and thus is without loss of generality This learner-candidate interaction is very similar to that studied in Hardt et al However our inclusion of groups with distinct manipulation costs leads to an ambiguity regarding a candidates initial features that does not exist when all candidates have an equal opportunity to manipulate In very few cases can a vendor distinguish among candidates based

0.6845 the institution it is difficult to a priori model the dynamics of how information about the classifier propagates A preferable solution may be to simply make the assumption that the individual can best respond to the classifier but to only gradually increase the threshold from the non-strategic to the Stackelberg optimal over time In fact misspecification of the agent model described above is why Brückner et al suggest the Stackelberg equilibrium is too conservative and instead prefer to use Nash equilibrium strategies Complementary to their observation we show that there is a more general reason Nash equilibria may be preferable Namely that Nash equilibria have lower social burden than the Stackelberg solution As the following lemma shows in our context the set of Nash equilibria form an interval I for some t The proof is deferred to the appendix Lemma Suppose the cost over likelihoods cL is continuous and ie all likelihoods have non-zero support Then the set of Nash equilibrium strategies for the institution is for some where is the non-strategic optimal threshold and is the Stackelberg equilibrium strategy The Stackelberg equilibrium requires the institution to choose whereas Nash equilibria give the institution latitude to trade-off between institutional utility

0.6829 Consider the multi-variate Gaussian setting of Section Suppose G and G are such that G G then there exists a single non-trivial equilibrium at which is also stable The level of investment by group a a is G G effect of subsidies In this situation an intervention that would effectively raise the equilibrium level of investment by the disadvantaged group is to subsidize the cost of investment In particular as long as we replace G with a stochastically dominated distribution G such that G G under the new dynamics sub will again be a stable equilibrium and there will also exist a more balanced unstable equilibrium at which is some convex combination of and At all equilibria of sub group a will have higher levels of investment than G However this improvement may come at a cost to the advantaged group since sub has multiple equilibria and some of them have group a investing less than G Still one might argue that the equilibria of sub are more equitable since the dynamics without subsidies always result in optimal investment by group a and low investment by group a Decoupling the Assessment Rule by Group The models we studied in Sections

0.6815 and suggest that applying the same or joint assessment rule to heterogeneous groups results in undesirable trade-offs between balance stability and other metrics at all equilibria even though there exists a perfect assessment rule for each group separately Decoupling the classifier by group is a natural intervention in this setting Namely the institution may choose a group-specific a to assess individuals from group a A assuming that the group attribute information is available This corresponds to choosing a that maximizes the utility that the institution derives from each group separately Thus we now consider the decoupled dynamics dec where the institution uses group-specific assessment rules ie for all a A bra a B argmax a p TP TPR a a a c FP FPR a a a As in the standard joint setting individuals still acquire qualification according to their group utility as follows bra a B G tpr a fpr a We denote by dec A the equilibria of the decoupled dynamics dec bra bra aA It is not hard to see that decoupling is helpful in a group-realizable setting That is the qualification rates of the decoupled equilibrium dec Pareto-dominates the qualification rates of all equilibria under a

0.6773 Allocating Opportunities in a Dynamic Model of Intergenerational Mobility Opportunities such as higher education can promote intergenerational mobility leading individuals to achieve levels of socioeconomic status above that of their parents We develop a dynamic model for allocating such opportunities in a society that exhibits bottlenecks in mobility the problem of optimal allocation reflects a trade-off between the benefits conferred by the opportunities in the current generation and the potential to elevate the socioeconomic status of recipients shaping the composition of future generations in ways that can benefit further from the opportunities We show how optimal allocations in our model arise as solutions to continuous optimization problems over multiple generations and we nd in general that these optimal solutions can favor recipients of low socioeconomic status over slightly higher-performing individuals of high socioeconomic status a form of socioeconomic armative action that the society in our model discovers in the pursuit of purely payoff-maximizing goals We characterize how the structure of the model can lead to either temporary or persistent armative action and we consider extensions of the model with more complex processes modulating the movement between different levels of socioeconomic status CONCEPTS Applied computing Economics Computing methodologies Planning and scheduling

0.6755 Here we show that even when the agent equalizes true positive rates between groups at each time step it will not in general equalize the standard true positive rate metric computed across the entire simulation even in expectation Although these insights could have been achieved through analytical approaches alone they occurred initially as surprising to us simulation results that we were later able to characterize more formally Environment We use the same lending environment specification as Liu et al In this environment each loan applicant has an observable group membership variable A and a discrete credit score C c max There is a finite pool of loan applicants from each group with C values distributed according to an initial group-specific distribution pA C Applicants are sampled uniformly with replacement from the pool of applicants and the agent chooses to approve or decline the loan If the applicant defaults the agents profit decreases by r and the applicants C value is decreased by c If the applicant pays back the agents profit is increased by r and the applicants C value is increased by c In this simulation probability of repaying is a deterministic function of credit score C when an

0.6746 solved analytically for various values of we are primarily interested in taking a welfare-based perspective on the effects of various classification regimes on both the learner and candidate groups In the following section we analyze how the implementation of a subsidy plan can alter a learners classification strategy and consider the potential impacts of such policies on candidate groups Group Welfare Under Subsidy Plans While a learner would choose to adopt a subsidy strategy primarily in order to reduce her error rate offering cost subsidies can also be seen as an intervention that might equalize opportunities in an environment that by default favors those who face lower costs That is if costs are keeping group B down then one might believe that reducing costs will surely allow group B a fairer shot at manipulation and as a result a fairer shot at positive classification Alas we nd that mitigating cost disparities by way of subsidies does not necessarily lead to better outcomes for group B candidates In fact an optimal subsidy plan can actually reduce the welfares of both groups Paradoxically in some cases the subsidy plan boosts only the learners utility whereas every individual candidate from both groups would

0.6743 This proof will be very similar to that of Lemma only now the behavior of the other will affect profit Recall i i G i G i q i We can see that even though the behavior of the other will affect profit i still has a dominant strategy This is because the conditions do not depend on the other i i q i This is the same rst order condition as in the monopolists case with the same implication i q Similar to the case of the monoplists notice that if for all i i i i i i and i then an interior equilibrium exists Proportional Demand In this section we consider a model inspired by and thus indirectly by the Tullock contest In particular rms split the market proportionally to the other rms error D P D In a market we say that demand is proportionally split with competition exponent if Di Here we focus on the two-rm case in which case we can write Di i i i i Now we can write our inequality theorem T PD Suppose two rms with learning rate q compete under proportional demand Then in any interior equilibrium error inequality

0.6742 for increasing their credit score As the ratio between the costs increases the social cost gap grows rapidly between the two groups Although the Stackelberg equilibrium leads to maximal institutional utility we show that it also causes high social burden We give several examples of when the high social burden induced by the Stackelberg equilibrium makes it an undesirable solution for the institution Rather than the Stackelberg equilibrium others have also considered finding Nash equilibria of the game Brückner et al argue that since in practice people cannot optimally respond to the classifier the Stackelberg solution tends to be too conservative and thus a Nash equilibrium strategy is preferable Our work provides a complementary reason to prefer Nash equilibria over the Stackelberg solution Namely for a broad class of cost functions any Nash equilibrium that is not equal to the Stackelberg equilibrium places lower social burden on individuals Finally we focus on the setting where individuals are merely gaming their features ie they do not improve their true label by adapting their features However if the classifier is able to incentivize strategic behavior that helps improve negative individuals then the social burden placed on positive individuals may be considered acceptable

0.6741 y are helpful for determining the effective thresholds that generates on unmanipulated features for groups A and B L Suppose a learner classifier is based on a hyperplane Pd i Construct the set Lm argmin y i y s t i ii Then a group m agent with feature x can move to some y and y x if and only if x for some Lm By definition for any two Lm X i i i X i i i argmax id i Thus a learner who cares only about the true label of presented features will construct her decision boundary such that all Lm have the same true label A cost-minimizing learner who publishes a classifier based on a hyperplane on manipulated features will commit errors on those candidates with unmanipulated features x d contained within the boundaries given by LA and LB This space can be understood as the d-dimensional generalization of the A B error interval in one-dimension P L C d D A learner who publishes an undominated classifier based on a hyperplane can only commit false positives on group A candidates and false negatives on group B candidates The cost of such a classifier is

0.672 Kleinberg and Raghavan studies how to design classifiers that produce such incentives Fairness Our work studies how strategic classification results in differing impacts to different subpopulations and is complementary to the large body of work studying the differing impacts of classification The prior work on classification is primarily concerned with preventing unfairness that can arise due to subpopulations having differing distributions over features or labels We show that in the strategic setting a classifier can have differing impact due to the subpopulations having differing distributions or differing costs to adapting their features Therefore when individuals are strategic our work provides an additional reason to be concerned about the fairness of a classifier In particular it can be easier for one group to game the classifier than another Furthermore we show that if the institution modifies the classifier it uses to be more robust to strategic behavior then it also as a side effect increases the gap between the cost incurred by a disadvantaged subpopulation and an advantaged population Thus strategic classification can exacerbate unfairness in classification Our work is also complementary to Liu et al who also analyze how the institutions utility trades-off with the impact to individuals They study

0.669 joint assessment rule whenever group-realizability holds P D Consider a group-realizable setting that is for every a A there exists a perfect assessment rule opt a such that tpr opt a fpr Then dec has a unique stable equilibrium dec where dec a G Moreover for any equilibrium of the joint dynamics dec a a for all a A Furthermore if there is no perfect assessment rule ie max X aA na tpr fpr then for some a A dec a a This proposition directly follows from Proposition Indeed decoupling always helps in the group-realizable setting not only does it not decrease any groups equilibrium qualification rate it also increases the equilibrium qualification rate of at least one group when realizability across all groups does not hold In Sections and we examine decoupling in the absence of group-realizability and see that those cases are not as clear-cut When group-realizability does not hold in some cases decoupling is still helpful while in others it can significantly harm one group BEYOND GROUP-REALIZABILITY MULTIPLE EQUILIBRIA WITHIN GROUP We have so far considered settings where the learning problem is realizable or almost realizable within each group This is a common assumption in various prior

0.669 excluding group B candidates an equilibrium subsidy strategy can still make both groups worse-o We thus highlight two consequences of subsidy interventions On the one hand with reduced cost burdens more candidates from the disadvantaged group should be able to manipulate to reach a positive classification However subsidy payments also allow a learner to select a classifier that is at least as strict as the one issued without offering subsidies These are opposing forces and these examples show that without needing to distort underlying group probability distributions or the learners penalty function in extreme ways the effect of mitigating manipulation costs may be outweighed by the overall impact of a stricter classifier This result can also be extended to show that a setup in which candidates are unable to manipulate their features at all can be preferred by all three parties groups A and B as well as the learner to both the manipulation and subsidy regimes We provide an informal statement of this proposition below and defer the interested reader to its formal statement and demonstration in the appendix P There exist general cost functions such that the outcomes issued by a learners equilibrium classifier under a nonmanipulation regime

0.666 a general measure of the cost of strategic classification which we call the social burden Informally the social burden measures the expected cost that a positive individual needs to incur to be correctly classified correctly For a broad class of cost functions we prove there exists an intrinsic trade-off between institutional accuracy and social burden any increase in institutional accuracy comes at an increase in social burden Moreover we precisely characterize this trade-off and show the commonly considered Stackelberg equilibrium solution achieves maximal institutional accuracy at the expense of maximal social burden Equipped with this generic trade-off result we turn towards a more careful study of how the social burden of strategic classification impacts different subpopulations We find that the social burden can fall disproportionally on disadvantaged subpopulations under two different notions by which one group can be disadvantaged relative to another group Furthermore we show that as the institution improves its accuracy it exacerbates the gap between the burden to an advantaged and disadvantaged group Finally we illustrate these conditions and their consequences with a case study on FICO data Our Contributions In this paper we make the following contributions We prove a general result demonstrating the trade-off between

0.666 In summary when pA pD we need persistent albeit non-monotone armative action to reach and maintain more desirable absorbing states The optimal extent of armative action crucially depends on the initial state of the population If the disadvantaged group is large to begin with the optimal policy has to forgo extensive armative action to maintain sufficient short-term return If the advantaged group initially has a sufficient mass extensive armative action becomes optimal and in the long-run manages to significantly increases the fraction of advantaged individuals in the population These endings are in sharp contrast with settings of pA pD In those settings armative action is optimal under the much more straightforward condition that the disadvantaged group exceeds a fraction of the population for a constant that we can specify precisely and it ceases to be optimal as soon as society gains at least mass in the advantaged group DISCUSSION AND FUTURE DIRECTIONS In this paper we developed and analyzed a model for allocating opportunities in a society with limited intergenerational mobility These opportunities produce benefits in the present generation but they can also raise the socioeconomic status of the recipients and their descendants This creates a trade-off whether to maximize

0.6619 payoffs We consider the problem of performing an intervention in this society which consists of offering an opportunity to a subset of the population We only have the resources to oer the opportunity to an fraction of the population An agent who is offered the opportunity has some probability of succeeding at it as a function of their ability and circumstances that we specify below Succeeding at the opportunity confers two benefits on society i it produces an immediate payoff/reward to the society in the form of productivity ii if the agent is disadvantaged it moves them and subsequently their future generations into the advantaged group The central problem to be solved in the model as we will see below is how to balance the immediate gains from i against the long-term gains from ii over multiple generations In particular if an agent of ability ai and circumstance is offered the opportunity their probability of succeeding at it is ai where and Note that since this simply means that gets added to the success probability of all agents whose circumstance is equal to Our payoff or reward r t in period t is simply the fraction of the population that

0.6601 threshold on the FICO score Using the normalized scale a threshold of is typically used to determine eligibility for prime rate loans Our results thus far have used thresholds on the outcome likelihood rather than a score However as shown in Figure the outcome likelihood is monotonic in the FICO score Therefore all our conditions and results can be validated using the score instead of the outcome likelihood Different Feature Distributions In Section we studied the scenario where the distribution of outcome likelihoods PY X differed across subpopulations In particular if the likelihoods of the positive individuals in group B tend to be lower than the positive individuals in group A then increasing strategic robustness increases the social gap between A and B Interestingly such a skew in score distributions exists in the FICO data Black borrowers who repay their loans tend to have lower FICO scores than white borrowers who repay their loans In terms FICO Score C D of P os it iv e In di vi du al s Disadvantaged in Features White Black Figure Comparison of the distribution of FICO scores among black and white borrowers who repaid their loans Credit-worthy black individuals tend to have

0.657 individual burden x x is monotonically non-decreasing in Since the social burden is equal to x y the social burden is also monotonically nondecreasing Suppose and without loss of generality let For all individuals x x x If there is at least one individual x such that x x then But there must exist an individual x such that x and x Y since by assumption For this individual x x implies As a corollary if the institution increases its utility beyond that attainable by the optimal classifier in the non-strategic setting then the institution also causes higher social burden Corollary Let be any threshold and be the optimal threshold in the non-strategic setting then Choosing a Concrete Trade-off The previous section shows increases in institutional utility come at a cost in terms of social burden and vice-versa This still leaves open the question what is the concrete trade-off an institution should choose Theorem provides a precise characterization of the choices available to trade-off between institutional utility and social burden The baseline choice for the institution is to not account for strategic behavior and use the non-strategic optimum Maximizing utility without regard to social burden leads the institution to choose

0.6567 c A x B B g C A classifier based on B perfectly classifies group B candidates and bears cost c B CF x A A g Notice that the learners errors always cut in the same direction by unduly benefiting group A candidates and unduly rejecting group B candidates these errors act to reinforce the existing social inequality that had generated the unequal group cost conditions in the rst place Since these errors arise out of the asymmetric group costs of manipulation the Strategic classification Game can be viewed as an interactive model that itself perpetuates the relative advantage of group A over group B candidates Within the undominated region B A the equilibrium learner threshold is attained as the solution to the optimization problem argmin BA C In the games greatest generality where candidates are drawn from arbitrary probability distributions groups bear any costs that abide by the cost condition and the learner has arbitrary error penalties one cannot specify the equilibrium learner threshold any further However under some special cases of candidate cost functions and probability distributions the equilibrium threshold can be characterized more precisely specifically when candidates from both groups are assumed to be drawn from

0.6551 is given by i i q i q i q where i q q q i q Recall that in the monopoly case the exponent was instead of meaning that introducing competition under this model has actually exacerbated the effect of minority status on inequality Note also that the relative inequality between two groups based on the results from a particular depends not only on that rms cost structure for the two groups but also on the opposing rms cost structure for the two groups Proving Theorem requires ending the equilibrium L Suppose two rms with learning rate q face proportional demand with competition exponent In any interior equilibrium error rates are given by i q q q i q q q q q i q If i for all and i then there exists a setting of parameters for which i is the unique equilibrium For brevity we relegate the full proof and the characterization of when these conditions hold to Section C Below we detail the instructive portion of the proof for the special case in which q L Suppose two rms with learning rate face proportional demand with competition exponent In any interior equilibrium error levels are

---------------------------------------------------------------------------
TOPIC 19: recommender-systems

---------------------------------------------------------------------------

0.9504 per day We can assume that active readers are exposed to more articles as such we set the maximum size of awareness to wwwpewresearchorg accessed August surveykaleidacom/Kaleida-news-ecosystem-report-europe-pdf accessed August Table List and description of the variables governing the simulation Besides the selection of recommender algorithms from MyMediaLite the Adjustable variables are available for parameterization via SIRENs interface The Default parameterization is used for this papers case study and as the default settings on SIREN Variable Adjustable Default Description User settings U Total number of active daily users/readers Awareness decay with distance Awareness decay with article prominence Awareness weight placed on prominent versus neighborhood articles Maximum size of awareness pool Choice model the users sensitivity to distance on the map i N User-drift users sensitivity to distance on the map m User-drift distance covered between the article tj and user s N Amount of articles read per iteration per user session size Recommender settings n Number of recommended articles per user per iteration Factor by which distance decreases for recommended articles salience Ranking-based decay of recommender salience d Number of simulation iterations per recommender Article settings T d Total number of articles number of iterations articles per day topic weights U

0.896 U users ie readers and T items ie articles placed in an -dimensional attribute R Each iteration of the simulation corresponds to a news cycle eg a day Readers are aware of i articles in their proximity corresponding to preferred/sought out topics via search or navigation bars ii promoted articles by the editors as they appear on the news website and iii personalized recommended articles T as they appear on the website or via email alerts sent to the reader At each iteration each user decides to read a number of unique articles from those they are aware of At the end of each iteration the users are updated The article pool T and the personalized recommendations are also updated at every iteration while each article has a limited life-span Under this model we identify three main interacting components that SIRENs interface gives content providers control over cf Figure the articles that translate to specific publishing habits the users the readers preferences and reading behavior and the recommendations articles promoted to each user We now describe the different components of our model in detail We then present the metrics which we visualize in SIRENs interface Finally since this model has

0.876 recommendations for this topic For the US Elections topic we provided recommendations below four articles and for the Big Tech topic below two news articles Click-through rate per recommended article The mean click through rate per recommended article for the baseline was stderr while for the diversified recommendations was stderr when looking at all topics Furthermore cf Mann-Whitney U test U p-val we did not find a significant difference between the two user groups in terms of click-through rate per recommended article same result per topic Click-through rate per recommended set The mean click-through rate per recommended set for the baseline recommendations was stderr while for the diversified recommendations was stderr when looking at all topics Figure a According to the Mann-Whitney U test U p-val we find a significant difference between the mean click-through rate per recommended sets for the two user groups Per topic we only find a significant difference for Coronavirus Figure b with a clickthrough rate per recommended set of stderr for the baseline and stderr for the diversified recommendations U p-val For the other topics we found no significant difference between the two user groups Completion rate We found no significant difference in terms of

0.8693 chose these two articles manually to match the selected topics Black Lives Matter Big Tech Coronavirus and US Elections Afterwards both the baseline and diversified recommendation sets were calculated for both articles and shown on the Blendle platform Dependent Variables To analyze the reading behaviour of the two different user groups and answer we measure specific events on the news aggregator platform ie check whether the user opened the article and if the user finished reading the article Based on these available events we observe multiple implicit click-through-rate per news article click-through-rate per recommendation set and completion rate of recommendation and explicit heart ratio measures of the reading behaviour To answer we look into presentation characteristics of the recommended articles ie presence of editorial title presence of thumbnail and counting number of hearts Click-through rate per article The number of clicks on a news article is divided by the total number of users who finished one of the original news articles for which that article was recommended The completion of an original news article is registered using a scroll-position Click-through rate per recommendation set The total number of clicks on either of the three news articles in the recommendation set

0.8561 Given the current simulation instantiation for all algorithms from the MyMediaLite toolbox the average ratio of recommended reads to the rest sought out and prominent is for Analysis setup We ran the recommender systems simulations for d iterations as pilot experiments have indicated that it takes that amount of iterations for the simulation to converge In order to deal with the cold-start problem prior to the recommenders we run a control period of iterations with the recommendations and user-drift deactivated We take the users reading history from the control period as the initial input for the recommenders The rest of the interactions happen via social media email alerts etc Following on pilot experiments we select to investigate ve recommenders the exhibited interesting behavior the Random and MostPopular algorithms can be seen as baselines which recommend random and most popular articles respectively We also select two common collaborative filtering algorithms ItemKNN and UserKNN The item-based k-nearest neighbor algorithm ItemKNN recommends items from a set of similar articles for each of the articles that the user has read while UserKNN recommends items from the most similar users in terms of reading habits We also select a more sophisticated algorithm WeightedBPRMF which is

0.8429 is divided by the number of users who finished the original news article using scroll-position for which the recommendation set was presented Completion rate of recommendation Is implemented as the number of users that read the full recommended article using scroll-position divided by the number of users who opened the news article The completion rate is assumed to be a measure for the user satisfaction with the recommendations We can argue that short news articles are more likely to be completed than long news articles Thus we also analyze the completion rate of a news article in relation to the number of words in the news article Favourite ratio The news aggregator platform allows users to mark an article as a favourite illustrated by an icon of a heart The users can click this icon at the end of the article content We implemented the measure as the number of users of the user group baseline or diverse that clicked on the icon divided by the number of users in the same group that completed the article The metric is assumed to be a marker of user satisfaction with the article Presentation characteristics We measured three additional properties of a

0.8407 inclusion of an editorial title on the click-through rate no statistical significance was found for neither user groups cf Mann-Whitney U test baseline U p-val diverse U p-val Thumbnail image We found no statistically significant influence of the inclusion of a thumbnail image on the click-through rate for baseline users cf Mann-Whitney U test U p-val In contrast we found a statistically significant difference for diverse users U p-val Recommendations with a thumbnail are more times opened than recommendations without a thumbnail for diverse users as seen in Figure c Favorite articles We applied the Spearmans rank correlation to see whether we find a correlation between the click-through rate and the number of hearts Figure d shows the distribution of click-through rates and the number of hearts We only found a moderate positive correlation of also statistically significant p-val for the diversified user group Source diversity As seen in the offline evaluation higher levels of viewpoint diversity turned out to have remarkable effects on the publisher ratio Therefore we also evaluated the effect of the source diversity of a recommendation set on the click-through rate For each recommendation set we computed the number of different publishers and we found recommendation

0.8337 to the metric the viewpoint diversity increases on average from to between and Additionally the average relevance score decreases from to In our project repository we provide examples of ranked lists of articles based on relevance and diversity for the topic of Coronavirus Ranked list of relevant and diverse news articles a Topic Black Lives Matter b Topic Coronavirus c Topic US Elections d Topic Big Tech Figure Diversity and relevance scores for different values of per topic a Topic Black Lives Matter b Topic Coronavirus c Topic US Elections d Topic Big Tech Figure Average number of publishers in recommendation lists normalised by the input ratio for all topics Table Overview of model variables used during the offline and online evaluation for each topic cross validation folds recommended list size number of introductory paragraphs number of concluding paragraphs general weights for the four framing aspects category weights and Topic intro par par general weight cat weight Black Lives Matter Coronavirus US Elections Big Tech Kendalls We computed the Kendalls rank correlation to assess whether the proposed diversification method is capable of providing different recommendation lists compared to the baseline We computed the coefficient between the baseline and each other

0.8312 point of view Based on these findings this study used a content-based approach In the offline evaluation it became clear that increasing levels of content diversity exclude multiple publishers and thus decreases source diversity Moreover some specific publishers got amplified remarkably for high levels of content diversity Thus viewpoint diversification methods could benefit from considering both content and source diversity Online Evaluation No major influence of viewpoint diversification on the reading behaviour was found except for the click-through rate calculated per recommendation set which indicated a statistically significant difference between baseline and diverse users of in favour for baseline recommendations However the results of the clickthrough rate calculated per recommendation indicated no significant difference between the two user groups Likewise the other two measurements of the reading behaviour including the completion rate of recommendations and the ratio of users who selected a recommendation as a favourite showed no significant difference between baseline and diverse users In reflection on the motivation of this study the proposed diversification for news media is capable of enhancing the viewpoint diversity of news recommendation while maintaining comparable measures of the reading behaviour of users The results thus suggest that recommender systems are capable of preserving

0.8281 recommended article during the experiment which referred to the presentation characteristics of recommended news articles First the editorial team can replace the original title of a news article with a custom editorial title In general these custom titles are longer and more explanatory than the original ones Second articles can be presented with or without a thumbnail image Third the number of users who selected the article as a favourite is visualised by a counting number of hearts in the left-upper corner of an article banner All three properties are assumed to potentially influence the click-through rate and are therefore measured during the experiment Source diversity Finally we also measured the influence of the source diversity of the recommendation set on the click-through rate As seen in Section higher levels of viewpoint diversity showed to influence the number of times a publisher is included in the recommendation Results The online study ran six days a week for two weeks Thus we provided recommendations below articles During the experiment the topic of Coronavirus became extremely prominent so we provided recommendations below out of news articles on this topic In contrast the Black Lives Matter topic lost all actuality resulting in no

0.7865 Section ONLINE STUDY We conducted a between-subjects online study on the Blendle platform to compare the reading behaviour of users who receive news articles optimized only for relevance versus news articles that are also diverse on viewpoint Materials In the online study we used the articles collected in Section Participants We selected active users of the news aggregator platform These users were assumed to most likely see and use the recommendation functionality We included only users who clicked at least four times on a recommended article below any article read in the last days before the study Groups for baseline and diversified recommendations were created by randomly splitting the users Independent Variables In the between-subjects user study we manipulated the following conditions referring to the recommended list of news articles baseline recommendation was implemented using a MMR that was based only on relevance Online study interface and setup diversified recommendation was implemented using a MMR that maximized viewpoint diversity Procedure During the two-week experiment six days per week we provided recommendations for two articles featured on the selected users homepage We provided sets of three recommendations below the content on the reading page of the original article Every morning we

0.7808 sets in which all articles are from a different publisher and sets in which two articles are from the same publisher Afterwards the click-through was calculated for each category The results for both the baseline users and diverse users show that no statistically significant difference can be found in the click-through rate between two or three different publishers in the recommendation set for neither baseline nor diversified users DISCUSSION We first discuss the results of the offline and online evaluation and then provide an overview of the limitations of our approach We conclude with directions for future work Offline Evaluation The offline evaluation indicated that the proposed method is capable of increasing the viewpoint diversity of recommendation sets according to the metric defined in previous literature The average viewpoint diversity scores across all topics increased from to for an increasing level of diversity in the MMR algorithm Simultaneously the average relevance score decreased from to Remarkably the diversity score of in is considerably smaller than the maximum average value of found in this work A possible factor could be that in the LDA topic model was excluded from the diversification method to prevent any interference with the evaluation metric whereas

0.7634 completion rate for the two users groups cf Mann-Whitney U test U p-val We also applied the Spearmans rank correlation to see whether the completion rate is correlated with the length of the articles However we found no correlation in either of the two conditions baseline group p-val diverse group p-val Heart ratio We found no significant difference cf Mann-Whitney U test for all topics and across topics U p-val in terms of heart ratio for the two user groups Thus the recommendations quality was comparable between the two conditions Influence of presentation characteristics We measured the influence of three factors namely the presence of an editorial title the presence of a thumbnail and the number of users that chose the article as a favourite on the click-through rate of an article a Click-through rate per recommended set for the two user groups b Click-through rate per recommended set and per topic for the diversified user group c Influence of the thumbnail image as presentation characteristic for the two user groups d Influence of the hearts as presentation characteristic for the two user groups Figure Overview of significant results in the online study Editorial title Regarding the influence of the

0.7537 very different items or they end up selecting same items during an election To check that we gather all the items ie hashtags and news stories selected by each of the methods and then compute pairwise overlaps between them Figure a shows the heatmap of Jaccard coefficient between different methods where Jaccard coefficient between methods i and is measured as where Si is the set of items selected by method i throughout all election cycles Not to be confused with Plural Voting which is a variant of Weighted Voting We see from Figure a that there is overlap between the trending hashtags selected by and STV has around overlap with There is little overlap between hashtags selected by other methods Similarly for Adressa dataset Figure b we see around overlap between the news stories selected by and STV The rest of the methods do not have much overlap Takeaway different methods select mostly different items during election cycles We only see some common items being selected by our two proposed approaches and STV possibly because both consider the top preferences of all users Interestingly actual Twitter Trending Topics has the highest overlap with the tags selected by Plurality Voting Thus

0.7533 or ascending weights Finally the number of introductory and concluding paragraphs can be either or Evaluation Metrics We assess the performance of the viewpoint diversification method using a metric from literature based on the Intra-List Diversity metric It is defined as the average distance between all pairs of articles and such that Thereby the distance between a pair is defined by the articles channels predefined taxonomy of high-level topics and the articles LDA topic-distribution as derived from the enrichment methods in Section distance distance channels distance LDA The channel distance is calculated using the cosine distance whereas the LDA distance using the Kullback-Leibler divergence Additional metrics We also measure the effectiveness of the diversification model on other properties as follows Relevance We measure the TF-IDF relevance for the recommendation lists such that we can measure the effectiveness of the viewpoint diversification method Kendalls We compute the Kendalls rank correlation coefficient to measure the similarity between two ranks of recommended items Average number of words We compute the average number of words for the recommended article lists as a measure of quality ie longer news articles can be considered to be higher quality Publisher Ratio We measure the publisher ratio for

0.7533 Percentage of articles added per day/iteration per topic z see section Awareness initial article prominence per topic p Prominence decrease factor per iteration Regarding the awareness balance between sought out neighboring and prominent articles see Fleder and Hosanagar supported by marketing studies on online purchasing behavior set it to However the report of Mitchell et al indicates that an average of and of the articles are accessed through search engines and news websites respectively Thus of the total interactions without recommendations should happen due to the users proximity to the article and therefore Recommendations We now turn our attention to the recommender settings While the exact number n of recommended items varies from outlet to outlet we assume a generic scenario of n Regarding the recommender salience see section Mitchell et al indicate that roughly of the article interactions happen via email alerts which typically contain personalized recommendations the extent and type of which is unknown The analysis of Kille et al shows that roughly one out of user-article interactions happen due to in-article recommendations but similarly the recommendation strategy is unknown We adjust according to Mitchel et al as many of our models parameterizations are based on their report

0.7443 on editorial cues such as their position on the news website SIREN Based on the requirements identified above we now present our simulation framework with a strong emphasis on the simulation model which takes Fleders and Hosanagars model FH from now on of consumers and products as a starting point FH considers a map of users consumers and items products in an n-dimensional feature space The products position describes its properties while the consumers position corresponds to their ideal product FH model uses such a two-dimensional space as its simulations input for the sake of simplicity and visualization Under FH items centered around the origin correspond to popular items At each iteration of the simulation the users are aware of items in their spatial proximity and popular items In addition at each iteration recommended items are permanently added to the users awareness For each user FH decides the items they purchase based on their distance to the user and an uncertainty component that allows users to deviate from their personal preferences After a number of iterations the simulation is complete while the user preferences and collection of items remain static throughout Similar to FH our model assumes that there are

0.7405 describes how to identify a view on an issue in a given article Thus by bridging aspects from the social and the computational domains we aim to overcome the current gap between the definition of diversity in recommender systems and news media During an offline evaluation the proposed method increased the viewpoint diversity of recommended lists of news articles on several topics Further we measured the influence of the viewpoint diversification method on the reading behaviour of more than users which are likely to interact with the recommended articles in an online study on the Blendle platform a Dutch news aggregator We found that reading behaviour of users that received diverse recommendations was comparable with the reading behaviour of users that received news articles optimized only for relevance However we did find a positive influence of two presentation characteristics on the click-through rate of recommendations news articles with thumbnails and news articles with more hearts are read more often Therefore we make the following contributions a novel method for viewpoint diversification using re-ranking of news recommendation lists based on framing aspects an online evaluation with more than users on the Blendle platform to understand a how viewpoint-diverse recommendations affect the

0.7347 top-K recommendations Getting preference rankings of all users Intuitively we can think of getting the ranked choices of a user u as determining how interested u is in different candidate items Then the problem gets mapped to inferring user interests in personalized item recommendations and there is a large body of works on the same which can be categorized into two broad classes content based methods and collaborative filtering We first attempt to get the personalized ranked choices of Adressa readers by applying a collaborative filtering approach based on Non-negative Matrix Factorization as described next Inferring preferences for Adressa readers As mentioned in Section the Adressa dataset contains information about the time different readers spent on different news stories We first convert this implicit feedback to explicit ratings by normalizing with respect to both users reading habits and the length of different articles If a user u spent time reading news story i then we compute the normalized view duration as where is the average time spent by all users reading story i Note that this normalization removes the bias of having possibly longer view duration for lengthier articles Once values are computed for different stories for the user u

0.7315 Operationalizing Framing to Support Multiperspective Recommendations of Opinion Pieces Diversity in personalized news recommender systems is often defined as dissimilarity and operationalized based on topic diversity eg corona versus farmers strike Diversity in news media however is understood as multiperspectivity eg different opinions on corona measures and arguably a key responsibility of the press in a democratic society While viewpoint diversity is often considered synonymous with source diversity in communication science domain in this paper we take a computational view We operationalize the notion of framing adopted from communication science We apply this notion to a re-ranking of topic-relevant recommended lists to form the basis of a novel viewpoint diversification method Our offline evaluation indicates that the proposed method is capable of enhancing the viewpoint diversity of recommendation lists according to a diversity metric from literature In an online study on the Blendle platform a Dutch news aggregator with more than users we found that users are willing to consume viewpoint diverse news recommendations We also found that presentation characteristics significantly influence the reading behaviour of diverse recommendations These results suggest that future research on presentation aspects of recommendations can be just as important as novel viewpoint diversification methods to

0.7312 news aggregator The reading behaviour of users receiving diversified recommendations was largely comparable to those in the baseline Besides the results suggest that presentation characteristics thumbnail image and the number of hearts lead to significant differences in reading behaviour These results suggest that research on presentation aspects for recommendations may be just as relevant as novel viewpoint diversification methods to achieve multiperspectivity in automated online news environments

0.7294 less frequent publishers varies per topic Overall our dataset covers a set of unique publishers We also present some properties concerning the presentation characteristics of the articles on the news aggregator website We observe that the ratio of articles that contains a thumbnail image depends on the topic For the Black Lives Matter and Coronavirus topics more than half of the articles have a thumbnail image while the opposite holds for the other two topics The number of custom titles from the editorial team and the average title length also differ considerable per topic Only a few articles have an editorial title and they usually appear for the Big Tech and US Elections topics Table Overview of the experimental dataset per topic Topic Articles Publishers Avg Words With thumb With ed title Avg title length Black Lives Matter Coronavirus US Elections Big Tech VIEWPOINT DIVERSITY METHODOLOGY We proposed a novel diversification method based on framing aspects using the insights from the focus group First we describe the extraction pipeline which supports the structure heuristic described in the results of the focus group session Section The Search queries pipeline forms the basis for the generation of recommendation lists that we use

0.7213 agents having a passion for music If the agent decides to reply to a message in their inbox their search ends and the two agents involved proceed to an online relationship phase However the agent can send as many initial messages to agents in their search results as they would like An unsuccessful search is considered to be a turn in which the agent is not interested in answering any of the messages they have looked at and in which they did not find at least half of the profiles theyve looked at interesting enough to send an initial message Too many unsuccessful search turns would make the agent exit the platform Online relationship phase After answering or getting a reply to an initial message the agent moves to an Online Relationship Phase with another agent While in this phase in each iteration the two agents within the relationship have an online interaction During this interaction three things happen First agents may update their knowledge about the attributes of the other Going back to the example in Figures and when entering the relationship the blue agent only knows the value of the green agents interest in music After one interaction

0.7211 a Gaussian Mixture Model of ve components fitted on the overall BBC document population see section As a reminder we consider the BBC-based topic space to be generic enough to agree with most current news outlets including our US case study Regarding the number of articles published per day ie the number wwwmymedialitenet accessed August Open-sourced at githubcom/dbountouridis/siren Users/readers Articles History User awareness a Recommendations b User choice c User drift Temporal adaptations d Article prominence Preferences Content Prominence User-article interaction Figure The frameworks main variables and modules and the interactions between them over the course of one simulation iteration Bold arrows represent input/output flow while thin arrows represent update functions of articles available to the users per iteration Bell et al reveal that US outlets publish articles at different rates from to per day We consider an average scenario of articles per day and as such T d where d the total amount of iterations With regard to the initial article prominence in order to realistically distribute between topical areas we turn our attention to the News Coverage Dataset NCI comprising topic-annotations of the top-five most prominent articles appearing in twelve major US online outlets January to May The

0.7185 to an external recommendation toolbox cf Figure b thus allowing for further extendability SIREN integrates the recommendation algorithms as they are provided by the toolbox MyMediaLite The algorithms cover a wide range of strategies from the simple random popular recommendations to the more sophisticated collaborative CF and content-based approaches After the recommended articles are integrated to the users awareness the user-choice module cf Figure c computes the selected articles Based on the users choice the final module in the pipeline deals with the temporal adaptations cf Figure d updating the users and the articles prominence The simulation model of SIREN is implemented in Python and is available online CASE STUDY SIREN allows content providers to instantiate the simulation with parameters specific to their values publishing habits and their readers behavior In order to showcase SIRENs benefits we investigate the recommender effects on a default generic instantiation that agrees with the major news outlets from the US for which a large amount of public data and studies are available The instantiation settings are summarized in Table In the next sections these settings will be justified followed by the analysis and results Articles Our instantiation considers T articles to be sampled from

0.7123 optimize the weight factors during the offline evaluation Note here that we re-rank news articles that are known to also be relevant for the given topic Where most re-ranking algorithms for recommender systems order lists only on relevance the MMR algorithm provides a linear combination between diversity in our case viewpoint diversity and relevance set by the parameter Thus the re-ranking algorithm is defined as follows mmr max max In is the ranked list of relevant articles is the list of selected articles in and is the list of articles in that are not yet selected Simaxnce this work proposes a measure for viewpoint diversity rather than a relevance measure we implemented the relevance score using a simple frequency-inverse document frequency TF-IDF score In the parameter takes values in where returns a list of articles ranked only on relevance while returns a list of articles ranked on maximum diversity OFFLINE EVALUATION In this section we describe the offline evaluation of our viewpoint diversity-driven approach for re-ranking lists of news articles Materials For our offline experiment we used the news dataset introduced in Section which covers news articles on four topics Procedure The experimental procedure consists of four main steps that

0.7035 ranking and thus the more likely they are to select prominent or recommended articles Fleder and Hosanagar set to However no studies support a specific value for an online news context Nevertheless according to the report of Mitchell et al of the news interactions with the users main topic happen while getting news on another topic This roughly implies that the readers should deviate from their main topic of preference roughly one-third of the times To set according to these endings we rst generate a set of users placed on the topic space uniformly We then assign a main topic to each user by using the class prediction of a Gaussian Mixture Model trained on the projected BBC articles We additionally generate articles using the aforementioned For different values we then compute the choice for each user and count the articles whose topic disagrees with the users main topic in the top ve positions We nd that one-third of each users top ve articles are of different topic than the users main for Recommendations In a typical news environment the readers are recommended n articles via emails alerts or the designated website element at regular time intervals or after they

0.6959 to next phase if messages exchanged Relationship ends agent wants to stay on platform Search results Moves to next phase after several turns Exits platform into long-term relationship Relationship ends agent wants to leave platform Continue Relationship Figure Overview of relationship phases and transitions in each turn of an iteration explained through the blue agents choices and actions Quote boxes represent decisions the blue agent makes and the vectors represent the agents attributes or the blue agents knowledge of potential partners attributes Table Tabular description of model parameters Left and the values taken in the virtual experiment Right For parameters with a red-coloured value we simulate each value for all simulated interventions but keep the values of the other parameters to red values see Section Parameters Values Taken Varied in Virtual Experiment Tolerance to un-successful search turns Out-platform norms for protected attributes Strength of updates norms Initial degree of ethnocentrism Correlations in attribute generation Fixed in Virtual Experiment Strength of updates interaction Strength of updates average norms Relative importance of out-platform norms when generating initial preferences Failure tolerance to unsuccessful relationships initial acquaintances in-group out-group Weight of searchable vs experiential attributes Pr searchable attribute specified on profile Pr learning searchable

0.6956 percentage of politics sports business entertainment and tech topic appearing in the headlines is and respectively These percentages translate to a distribution of the article-prominence across topics via the process described in Section Users We now turn our attention to the users/readers We are interested in active readers that is subscribed users that receive personalized recommendations and read more articles than casual readers Similar to Li et al we assume that a significant preference-evolution over time can happen only for active users Our instantiation considers the readers preferences or position in the topic space to be sampled from a uniform distribution thus assuming a scenario where the readers interest as a group is spread evenly across the topic space Such a distribution captures a community of readers unaffected by recommender effects Regarding the number of active users U while the amount of subscribed users for popular outlets is known the exact percentage of those who are genuinely active is not supported by any literature For the purposes of this case study we consider a scenario of active readers daily We now focus on instantiating the reading behavior According to the Kaleida report casual readers are on average exposed to articles

0.6952 the approaches proposed in and for a user u we infer an interest vector considering the experts u follows and similarly we compute a topic vector Th for a hashtag by taking into account the experts tweeting Then for every user u we normalize the interest topics in such that every entry in lies between and and all entries sum to Similarly for every hashtag we calculate the tf-idf scores over the topics in Th We repeat this process for every user and every candidate hashtag during an election Finally we compute the preference scores between all users and all candidate hashtags as A U T where is the User-Hashtag affinity Matrix with denoting affinity between user u and hashtag hUnt is the User-Interest Matrix with representing normalized interest of u in some interest topic is the Interest-Topic Similarity Matrix Ti representing the similarity between two topics i and we compute Ti as the Jaccard Similarity between the set of experts in topic i and respectively Finally is the Hashtag-Topic Matrix where denotes tf-idf of topic in hashtag Using A computed above we can get the preference ranking of any user over the candidate hashtags If a user u participates

0.6936 set-up because items in the first slider are more likely to be clicked than the items in the subsequent sliders Mathematically if item is recommended on the page of item i and item is positioned in the t-th slider of the recommended list on the web-page of item i the weight of the directed edge i is considered p n Different users propensity to follow recommendations While modeling user browsing of recommendations one important aspect is the different propensity of different users to follow recommendations This propensity is captured in the random surfer model by the teleportation probability At every step the surfer chooses one of the items recommended on the current items page with probability and chooses to teleport to a For our experiments we set as observed during the Web crawls The reported results are keeping p random item in the universe with probability We performed experiments with different values of in and found the results to be qualitatively similar for all values Throughout the rest of the paper we report results for which is the default value prescribed in the random surfer model Long-tail popularity distribution of items In the vanilla random surfer model while teleporting the

0.6868 al configured to use neighbors and cosine similarity The explicit-feedback variant uses user-mean normalization for user rating vectors and the implicit-feedback variant again replaces weighted averages with sums of similarities User-user did not provide effective recommendations on the LastFM data so we exclude it from that data sets results MF the popular gradient descent matrix factorization technique Funk Paterek with latent features and training iterations per feature In the results each algorithm is tagged with its variant Algorithms suffixed with are explicit-feedback recommenders applicable only to ML are implicit-feedback recommenders that only consider whether an item was rated or played irrespective of the number of plays both data sets and are implicit-feedback recommenders that use the number of times an artist was played as repeated feedback and log-normalized prior to recommendation The purpose of this work is not to compare algorithms but to compare recommender performance across demographic groups We have selected these algorithms to provide a representative sample of classical collaborative filtering approaches Results Using the data and methods presented in Section we discuss below the results of the experiments conducted to quantify user satisfaction with presented recommendations among different demographic groups For doing so we consider three different

0.683 the difficulty of recourse over all users and a single item In this case we consider top- recommendations to reduce the computational burden of computing the exact set Pi We pose the cost as the size of the difference between the user input a and the predicted ratings in the norm Figure shows the difficulty of recourse via reaction for the two types of new items a completely random set of unseen items and items with the highest predicted ratings We note two interesting trends First the difficulty of recourse does not increase with model size even though the amount of recourse is lower Second difficulty is lower for the random set of items than for the top- items Along with the trend in availability this suggests a benefit of suggesting items to users based on metrics other than predicted rating Future work should more carefully examine methods for constructing recommended sets that trade-off predicted ratings with measures like diversity under the lens of user recourse latent dimension co st Difficulty of Recourse recommendation Figure The difficulty of reaching a single item across users for different sets of new times The difficulty of recourse does not increase for the larger

0.6818 users move from item to item using links such as related movies in a movie web-page In order to investigate the relationship among diversity metrics Vargas proposes a probabilistic model of user behavior The model considers diversity in the temporal domain and incorporates a user browsing model ie the probability of a recommended item being selected relates to its ranking position Despite the large amount of simulation works we argue that none of the listed simulation models can accommodate for the particularities of news consumption in an online news environment The next section elaborates on the specificities of online news Requirements for the news context In a news context the simulations conceptual model should capture the general mechanics of article publishing and consumption At the Figure SIRENs user interface recommender settings news article settings and user ie news reader settings can be adjusted at will The bottom row shows the generated visualizations for different metrics and different recommender algorithms same time the models parameterization should accurately capture the specific intent of both users and content providers eg what users want to read and what content providers want users to read Online news articles have a very distinctive nature that separates

0.6799 Factorization approach proposed by Luo et al which solves Equation by using stochastic gradient descent with non-negativity constraints on the feature values Once we get the feature vectors for different users and news stories then the ratings can be predicted even for the unread stories Thereafter we compute preference ranking for the users based on the predicted and actual ratings with actual ratings getting precedence and ties being broken randomly Inferring preferences for Twitter users To infer Twitter users preferences we considered both content based recommendation and collaborative filtering i Compute content based similarity between a user u and hashtag by considering the set of all tweets posted by u and the set of tweets containing However we found that most users do not post enough tweets and thus we can not accurately compute the content based similarity between a user and the candidate hashtags ii As there is no explicit rating available we tried to apply a collaborative filtering based approach to compute personalized ranking using implicit feedback like favoring or retweeting However two independence assumptions in such approaches items are independent of each other and the users act independently do not hold in the context of Twitter Hashtags

0.6784 as well as to the notifications tab within the Twitter app The user can start their interaction either by seeing the notification in their notification tab NTabView and then clicking on it Click or by seeing it as a the notification on their phone home screen and opening it from there directly Open After clicking or opening the notification the user can engage in many more interactions they can favorite retweet quote retweet Quote or reply Reply to the tweet if the tweet has a link they can click on it LinkClick if it has a video they can watch it VidWatch In addition other implicit signals are logged whether the amount the user lingered on the tweet exceeds certain thresholds Linger s Linger s Linger s and whether the number of user active minutes spent in the app after clicking/opening the notification exceeds a threshold Furthermore when the user is in the notification tab the user can provide explicit feedback on a particular notification by clicking See Less Often SLO on it Notably unlike other types of behavior the user does not need to actually click or open the notification before clicking SLO However we found empirically that users

0.6781 recommendation outcomes When information about S is not excluded the available information is the mutual information between R and S ie The information becomes after excluding the information about S Because the available information is non-increasing when excluding the information on S Hence the trade-off for enhancing the independence generally worsens the prediction accuracy Relation to Recommendation Diversity We will briefly discuss recommendation diversity Kunaver and Pozrl which is an attempt to recommend a set of items that are mutually less similar McNee et al pointed out that recommendation diversity is important because users become less satisfied with recommended items if similar items are repeatedly shown To our knowledge Ziegler et al were the first to propose an algorithm to diversify recommendation lists by selecting items less similar to those already selected Lathia et al discussed the concept of temporal diversity that is not defined in a single recommendation list but over temporally successive recommendations Adomavicius and Kwon discussed the aggregate diversity and the individual diversity over the items that are recommended to a whole population of users and to a specific user respectively We wish to emphasize that independence is distinct from recommendation diversity There are three major differences

0.6781 reader is aware of a limited number of articles per iteration In a scenario of no editorial priming readers are only aware of the items they seek out In such a case the reader-article awareness can be a function of solely their spatial relationship on the topic space To accommodate for article-prominence we adapt the original FH model such that the user awareness is sampled from aware of tj log where controls the users balance between prominent high lambda and neighboring low lambda articles that are modeled to be in a users awareness controls how the awareness fades in the readers proximity ie the width of the bivariate normal around readers position on the plane the choice for normal distribution comes from the original FH model controls how the awareness fades with respect to the prominence dimension We use a logarithmic function for the prominence decay as it agrees with the general long-tail pattern of user attention in news articles ie how the attention decays towards the bottom of a news webpage Fleder and Hosanagar adjust and to create an interpretable base case In contrast for a more realistic approximation of the user awareness we turn our attention to related

0.6772 it according to their needs Unexpectedness Unlike long-tail diversity which takes into account the interactions of all users with available content unexpectedness diversity focuses on the individual user activity This type of diversity refers to the possibility of locating a story which is unexpected but still useful for the reader The integration of unexpectedness in recommenders is known to increase user satisfaction and broaden user preferences by diversifying their interests From a normative point of view unexpectedness is integral for countering negative effects of over-fitting such as ideological/topical isolation resulting from filter bubbles/echo chambers For the unexpectedness diversity we use the Expected Prole Distance EPD metric which besides rank and relevance incorporates the content-based distance between items Technology Stack We now describe turning the simulation model into a simulation framework The frameworks main variables and modules in addition to the interactions between them is shown in Figure SIRENs implementation at each iteration takes as input the user the articles content T and prominence and the current reading history U T The user-awareness module cf Figure a rst computes the awareness pool of each user from the inputs The recommended articles for each user are then computed by passing the input

0.6743 in an election and votes for tag then is considered as the top choice in us preference ranking and other ranked positions are shifted accordingly If a user votes for hashtags top positions are assigned to these candidates according to their usage frequency Accuracy of the preference inference For inferring the preferences of Adressa readers we attempted another technique based on Singular Value Decomposition SVD Comparing the Root Mean Squared Error RMSE between the actual ratings and ratings inferred by both SVD and based approaches we found that the based approach RMSE works better than the SVD based approach RMSE In Twitter there is no ground truth rating or ranking Hence to check the accuracy of the inference of Twitter users preference rankings we asked volunteers who are active Twitter users to rank hashtags during election cycles Then we compute their preference ranking using our approach and checked Kendalls rank correlation coefficient between the inferred and actual rankings for every volunteer We find the average value to be compared to for random ordering which suggests that our method can infer the ranked choices of users reasonably well EXPERIMENTAL EVALUATION In this section we evaluate the performance of our proposed approaches

0.6726 can only be in one of the beforementioned phases at one time eg one cannot be both searching and dating offline result or a single message in the agents inbox The agent randomly decides which of these two actions to take If the agent decides to view a profile from their search result list they look at the profile of the next search result and decide whether or not they should send that person a message If the agent decides to view a message in their inbox they look at the oldest un-read message and consider whether or not they should reply Search results are a randomly ordered subset of other agents currently on the platform Importantly however the agent decides which other agents will appear in their search results They do so by filtering for potential dating partners The agent filters on a single attribute the one that on this turn is most important to them ie for agent the attribute with the highest value In Figure for example interest in music is the most important factor for the blue agent and they hence use this as a filter The blue agent is then only shown profiles of other

0.6718 value of Overall we observed that the re-ranking of the set of recommendations based on viewpoint diversity results in different recommendation lists compared to the baseline The coefficient decreases for smaller values of but it is bounded around for decreasing values of Average number of words We observe no consistent pattern in the average number of words for different values of across topics For the Black Lives Matter and Big Tech topics the average number of words increases for larger values of for the US Elections topic the average decreases and for Coronavirus the average is stable Publisher ratio Figure shows the average number of articles in the recommended lists normalized by the input ratio for each value of For every topic the number of publishers increases for larger values of and the number of different publishers for the baseline recommendation list is larger than the one in the diverse recommendation list Thus we observe that the diversification method influences the publisher ratio For small values some publishers get amplified while others are excluded We see this effect primarily for the topics of US Elections and Big Tech Coronavirus seems to be the only exception We further discuss this in

0.6553 an item being viewed The key is the ability to mine candidates via document perturbation For instance consider the ranking of similar products in a shopping application Here the query can be considered to be the product currently being viewed The ranked list of similar items can be explained with our approach by i mining candidate terms from the product descriptions and titles ii constructing the preference matrix based on concordant product pairs iii selecting terms using the greedy approach suggested earlier Now the intent based explanation can inform the user as to why certain products are recommended first for the product she is currently viewing CONCLUSION In this paper we detailed our framework for post-hoc explanations of adhoc black box rankers Our setting enables us to tackle a multitude of text based retrieval models predominantly used in news search medical search patent retrieval product search etc irrespective of the underlying learning algorithm or training data Note that web search tends to rely heavily on network and behavioral signals in most cases but textual relevance is crucial for tail queries rarely seen or unseen queries From the quantitative results we gathered that using preference pairs only from the top-k results

0.6534 between recommendation diversity and independence First while the diversity is the property of a set of recommendations as described above the independence is a relation between each recommendation and a specified sensitive feature Hence it is impossible to diversify a single recommendation but a single recommendation can be independent if its prediction of ratings or its determination of older newer a standard older newer b diversified Figure Enhancement of recommendation diversity whether to recommend or not is statistically independent from a given sensitive feature Second recommendation independence depends on the specification of a sensitive feature that is a function of an item and a user On the other hand recommendation diversity basically depends on the specification of how items are similar Even though similarity metrics are not explicitly taken into account similarities are implicitly considered in a form for example whether or not a pair of items are the same Therefore independence and diversity are applicable to different situations Because a sensitive feature can represent the characteristics of users independence can be applicable for coping with the factors of users such as the MLM-Gender or Sushi-Age feature Inversely the relative difference between item properties is hard to process by using

0.6515 to read one or more articles while taking into account the user-specific requirements as described in section Preferences As previously discussed both the articles content and the readers preferences ie ideal article are represented as points on the topic space cf Section Under the FH model users preferences remain static no matter their purchase history In order to accommodate for evolving user-preferences we introduce a user-drift model After reading article tj a users likelihood to drift towards the articles position in the topic space is sampled from drifts towards tj i where is the Euclidean distance from reader to article tj controls the width of the bivariate normal around the user In practice controls the likelihood of the user drifting towards distant articles We sample from a uniform distribution re-effecting the assumption that readers vary with respect to their eagerness to evolve their reading preferences The user covers m distance towards the article We argue that m does not affect the direction of simulation results only the magnitude However a relatively small m eg allows us to get a higher-resolution view of the temporal dimension of the recommender effects User choice Our model assumes that prior to any choice each

0.6497 a ip a ti n g v o te rs b o o C a n d id a te s c o a ll v o te rs d Figure a Percentage of all voters participating during different election cycles in Twitter b Average number of votes casted by different voters during an election c Number of potential candidates for becoming trending during election cycles d Percentage of overall population needed to make different topics trending among the crowd To consider such recommendations we use the Adressa News Dataset which consists of the news reading behavior of around million users on the Norwegian news website Adresseavisen during the months period from January to March The dataset not only provides the information about the stories read by every user out of total news stories but also includes how much time a reader spent in each story Using these reading times as popularity signal we simulate the recommendation of most read news stories every day Reimagining Top-K recommendation as a multi-winner election In this paper we propose to see crowdsourced recommendation as the result of an election where the users vote for an item eg a hashtag a news story by

0.6474 them from other Web objects such as movies and music Li et al argue that typical recommendation strategies need to be adapted to accommodate for a number of unique news-article characteristics such as their large volumes and short-term relevancy Another unique characteristic relates to the nature of the medium in which articles typically reside News articles do not exist in isolation but appear within the websites layout and overall content Editorial cues such as position or font sizes are frequently used to lead the reading consumption and to adjust the salience of the recommendations The news-reading behavior is also unique departing from the typical show me something interesting attitude A number of works suggest that besides the casual information seekers online news accommodate the needs of users with specific preferences and interests Finally it has been suggested that those user preferences are likely to evolve over time and thus personalized news recommendations might have long-term effects We define three requirements that a simulation model of online news with personalized recommendations should satisfy in order to adequately approximate reality users distribute their reading time between prominent sought out and recommended articles user preferences evolve the prominence ranking of articles is based

0.6469 have interacted with a number of articles For the sake of simplicity we assume that the recommendations are only updated at the beginning of each iteration The recommendations in our model have two effects First the articles are added to the users personal awareness pool While under FH this effect is permanent in a news context that is rarely the case considering the vast amount and short-term relevancy of articles As such we assume that the awareness effect of the recommendations holds for a single iteration Secondly assuming that recommendations carry a certain prominence the tj articles deterministic component as it relates to user vi where corresponds to a salience boost However not all recommended articles share the same prominence different spatial patterns can be used for arranging the recommendations Our model aims to accommodate for that fact For simplifying purposes similar to Vargas we assume a list-based arrangement and thus a positional-bias in the user choice ie a rank-based likelihood of an article being selected We take to be a function of the articles rank Z in the recommendation list We use a simple exponential function and as such with following the approach in Vargas Metrics While the simulation

0.6445 the algorithm covers a larger number of preference pairs Carefully sampling pairs can help focus on alternate yet important query aspects and prevent overfitting Conversely improper sampling or sampling too many pairs from the tail could lead to good global rank fidelity but poor local rank fidelity Sampling Preference Pairs/Features We use multiple strategies to sample preference pairs random randomly select preference pairs from the target ranking rank biased sample preference pairs that are weighted by rank Each pair i is weighted by rank di rank top-k rank random construct preference pairs based on a combination of rank and random sampling In this method for a preference pair i di is rank bias sampled but is randomly sampled top-k random consider all pairs from the top-k results to explain and a fixed number of randomly sampled pairs We contrast these against top-k that are all preference pairs in top-k results in the experiments Constructing the Preference Matrix The next step in our approach is to construct an n m preference matrix for a given candidate E where n is the number of candidate terms is the number of preference pairs For each preference pair feature we compute a score that

0.6428 as the main extraction method for this framing aspect The model provided by the research partner at Blendle included a topic latent Dirichlet allocation LDA model trained on Dutch news articles Based on the conclusions from the focus group described in Section the title and the first paragraphs are used to retrieve Diversification pipeline metadata related to this framing aspect We also applied multiple pre-processing steps on the content including cleaning chunking tokenization lemmatization and stop-word removal Causal Attribution Moral Evaluation The causal attribution of a frame relates to the forces creating the problem while the moral judgements evaluate the causal attribution and their effect In the focus group session Section we concluded that the body of an article usually elaborates on these aspects Additionally paragraph-level seems to be the most suitable level of analysis Thus we first translated the news articles in English using Google Translate and then we applied a text-classification algorithm using IBM Watson NLU The service returns a category for each paragraph according to a predefined five-level taxonomy from the most general category eg level technology and computing to the most specific one eg level portable computer To extract information related to the evaluation of these

---------------------------------------------------------------------------
TOPIC 20: fairness/representation

---------------------------------------------------------------------------

0.7673 GDPR Similarly to the Swedish case the Greek version of Article of the Charter seems to refer to the notion of legality/lawfulness rather than to fairness as loyalty/equitability Understanding the reasons and the consequences of this linguistic choice is beyond the scope of this research but such differences well prove the semantic complexity and the diverse nuances of the fairness notion in all European languages Fairness as bona fide correttezza and equity in Western legal systems After this overview we can preliminary conclude that the notion of fairness is translated with several different nuances in accordance or in discordance with the previous Directive and with Article of the Charter However three main semantic notions can be identified in all the European official translation of fairness in the GDPR loyal also in the German version Treu und Glaube equitable and correct In some versions different words are used interchangeably in other versions there seems to be a specific rationale for using different terms eg in German when fairness is associated to transparency it is translated by Faire while when it is associated to lawfulness it is translated by Treu und Glaube in Greek all references to fairness in Articles are translated

0.7641 Portuguese but not in the other two LOIs For example when English She is a cop is translated into German as Sie ist eine Polizistin the article a in the source sentence turns into a gendered article eine which agrees with the feminine noun Polizistin This phenomenon sometimes induces errors such as Sie ist ein Polizist on which the social bias regarding the occupation has an effect c Derivation of gendered nouns where a gendered noun is assigned to a different gender by changing its form often occurs in German and Portuguese However detecting errors in noun derivation can be tricky In the aforementioned example Polizistin is a feminine counterpart of masculine Polizist whereas most loanwords eg ein Barista ein Model or invariably gendered words eg ein Genie a genius does not change according to the gender of the referent although grammatically correct This factor affects the evaluation of the gender agreement of articles since machine translators quite often yielded invariable nouns or loanwords as the output Note that the concept of gender utilized in a slightly differs with the one used in b and c since the gender of pronouns more regards the biological gender while the genderedness of

0.7488 this linguistic evolution has led or will lead to a legal evolution is beyond the scope of this research but it reveals the fluid and dynamic nature of the fairness concept in the EU framework The French version of the GDPR uses two different terms to refer to fairness loyale Articles and recitals and equitable Articles and and Analogously the Spanish and Portuguese versions of the GDPR use similar expressions leal and equitativo could profit from different linguistic support from different national experts of those languages There is obviously a similar etymology for loyale and leal on the one hand and equitable and equitativo on the other hand In particular the notion of loyale/leal might be generally translated with the English loyal while equitable/equitativo are well translated with the English equitable In the German version we noticed that the rationale for the distinction between the two terms Treu und Glaube and Faire was based on the different link of the fairness principle with transparency faire and lawfulness Treu und Glaube To the contrary in the French Spanish and Portuguese versions this is not the case where apparently the terms are used interchangeably The French version generally uses loyale while equitable

0.7321 Directive but the Swedish translation of Article of the EU Charter had adopted the term lagenligt lawful which recalls the strong semantic link between fairness and lawfulness In Polish and Slovenian versions just one word is used to translate fairness without any ambiguity or recent evolution rzetelnie in Polish and pošteno in Slovenian The Greek version of the GDPR uses two alternative terms for fairness θεμιτή and δίκαιη Apparently the different use of the two terms is not random the first word is used in all Articles referring to fairness while the second word is used in all recitals referring to fairness Since recitals should be meant as interpretative tools while articles as normative sources we should refer to θεμιτή as the official translation of fairness and to δίκαιη as the interpretative nuance to understand its meaning Θεμιτή can be translated as appropriate fair legitimate while δίκαιη to fair trade fair opportunity or punishment Surprisingly the word fairness in the Greek version of the EU Charter Article has a very diverse etymological and semantic source νομίμως meaning legality It is different both from the translation in the Directive article also referred to θεμιτή and in the aforementioned text of the

0.7185 is just used in few provisions at Articles and and recitals and Similarly in the Spanish version leal is generally used for fairness apart from two cases Article and recital mentioning equitativo/a On the contrary the Portuguese translation of fairness is largely based on the word equitativo/equidade with just few but relevant exceptions Articles a and Article use the expression leal in line with the Portuguese version of Article of the EU Charter The two words have similar meanings but while loyalty refers to honesty and constant allegiance to a person or an institution equitability refers to fairness and impartiality but also in strictly legal terms to what is valid in equity as distinct from law The interesting ambiguity of the term equitable referring both to fair and to the legal notion of equity needs to be investigated in more detail In the following section we will analyse the strong link between equitable as expression of equity and Treu und Glaube as expression of the Roman law notion of bona fide Even the Romanian translation of fair has the same etymology of equitable echitabile Interestingly this word is just a recent acquisition of the GDPR both the Data Protection Directive

0.7007 The author chose these languages which are among the most representative ones of the European linguistic families because he different words referring to slightly different concepts/nuances In addition several translations have similar etymologies eg Spanish and Portuguese versions Italian and Romanian versions etc In the German version of the GDPR fairness is translated as nach Treu un Glaube Articles and recitals and and faire Articles and recitals and There is even a third different word for fair at recital gerecht although we are aware of the different context of that recital where fairness refers to the activity of the DPAs not to the data processing itself Article of the EU Charter also uses Treu und Glaube to refer to fair processing of personal data The two main words used are not properly synonyms On the one hand treu und Glaube is a legal milestone in the German legal history and refers to the roman concept of bona fide On the other hand faire is semantically much more in line with the English fair The use of the two different words is not random Faire is used only in conjunction with transparency faire und transparente ie at Articles and on information

0.677 in particular the French and the English one while Section has addressed a linguistic comparison of the term fairness in different European languages In general the notion of fairness is translated with several different nuances in accordance or in discordance with the previous Data protection Directive and with Article of the EU Charter of Fundamental Rights In some versions different words are used interchangeably it is the case of French Spanish and Portuguese texts in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR it is the case of German and Greek version The analysis has revealed three mean semantic notions correctness Italian Swedish Romanian loyalty French Spanish Portuguese and the German version of Treu und Glaube and equitability French Spanish and Portuguese Interestingly these three notions have common roots in the Western legal history the Roman law notion of bona fide Taking into account both the value of bona fide in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR the conclusion of this paper is that fairness refers to a substantial balancing of interests among data controllers and data

0.6508 duties or at Article on code of conducts for fair and transparent data processing recitals and where fairness is also in conjunction with the transparency principle On the other hand Treu und Glaube is used for translating the fairness principle when it is linked to the notion of lawfulness rechtmäßig und nach Treu und Glauben it is the case of Article and Article Gerecht recital is also a direct translation of the English fair but under the perspective of impartiality and justice and so more in line with the translation of impartially and fairly at recital In the Dutch version of the GDPR just one single word translates the English fair behoorlijk This term referring to a notion of decency goodness and of course fairness is used both in conjunction with transparency and with lawfulness However Article of the EU Charter uses a different expression eerlijk whose nuance is closer to the notion of honest plain The Dutch version of the Data Protection Directive also referred only to eerlijk Similarly that term is used often in the negative form oneerlijke also in the Consumer protection framework The terms behoorlijk is therefore just a recent development in the GDPR Understanding whether

0.646 clause eg s/he or her/him corresponds with the preceding This does not necessitate the grammatical or semantic appropriateness of the output sentence EN to Type DE/PT It should be first checked that whether the translated gendered noun expression has been correctly derived In case it matches with the gender of given pronoun eg er or sie in German or is neutral then one can check if the article agrees with the noun In case not one should check if the noun has the counterpart regarding gender For example Polizist cop in German has its counterpart Polizistin while a loanword Model does not necessarily If there exists the noun derivation is decided wrong and the article agreement is considered wrong as well If no counterpart we assume it correct and the evaluation of the article agreement follows the noun OKforno article Some outputs in EN-PT translation showed no article However considering that in Portuguese the article may be omitted under some conditions when the speaker does not want to judge the object we annotated the result as correct A similar holds for the occupation words in German For DE/PT the total accuracy is counted only if both noun and article are

0.6339 and Article of the EU Charter translated fair with the Romanian word corect leading to the English correct The etymological reference to correctness can be found also in other two linguistic versions Italian and Swedish The Italian translation of the GDPR has just one single world for fairness corretto/a The only exception is recital equo also translatable as equitable Interestingly also the Italian version is different from the wording of the Directive and of Article of the EU Charter Article of the Data Protection Directive translated fair with leale loyal closer to the other aforementioned Romance languages versions loyal/leal and also to the Italian wording in the consumer protection directives The new translation of fair as corretto is a novelty of the Italian version of the GDPR Also the Swedish version refers to fairness at Article a of the GDPR as korrect the principle is korrecthet Surprisingly this term totally overlaps with another data processing principle accuracy at Article d is translated in the same way korrecthet However this apparent ambiguity is clarified in the other provisions of the GDPR where the word fair is always translated with rattvis meaning equitable Actually korrect was used also in the Data Protection

0.6144 item is compatible with both genders eg um/uma agente de leilões an auction agent we checked if the gendered article corresponds with the subject pronoun When the gendered item has its counterpart in the other gender eg um agricultor and uma agricultora a farmer we investigated if the gender of both the article and the noun matches with that of the subject When the item is invariably assigned a single-gender as in anjo angel in Portuguese we made a tolerable judgment eg both OK for ele é um anjo and ela é um anjo In this phase the content words are solely comprised of nouns in the source sentences ie names of occupations and sentimental prescriptions eg fraud pervert However if a non-noun or unseen expression emerges in the target languages article agreement is checked upon the gender of the sentence subject This is to be described in detail with the survey questions in the experiment section The fluency of translation Besides gender-related measurements we also checked whether the translation had been fluently performed As to be described in the following section we made up the gold standard for the template sentences and checked if the translation output matches with

0.5972 selected Languages of Interest The languages of our interest here hereafter referred to as LOI namely German DE Korean KO Portuguese PT and Tagalog TL can be categorized both linguistically and concerning their publicly available language resources The following three linguistic properties were crucial to our investigation a presence of gender-neutral pronouns GNP b gender agreement of articles and c noun derivation according to its gender a GNP is an umbrella term defined here as a third-person singular pronoun that is not marked for gender distinction Unlike English where the GNP singular they is far from in general use Korean and Tagalog GNPs are frequently deployed in colloquial contexts The presence or lack of GNP in specific languages can often reflect the algorithmic bias of translation services For example it can be observed in some translators that the Korean sentences 걔는경찰이야 S/he is a cop and 걔는간호사야 S/he is a nurse are translated into English as He is a cop and She is a nurse respectively where 걔 kyay is a Korean GNP that can indicate either gender Similar holds for Tagalog siya b The agreement of the determiners with the gender of the head noun occurs in German and

0.5455 Spanish and Portuguese texts in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR it is the case of German and Greek version The analysis reveals three mean semantic notions correctness Italian Swedish Romanian loyalty French Spanish Portuguese and the German version of Treu und Glaube and equitability French Spanish and Portuguese Interestingly these three notions have common roots in the Western legal history the Roman law notion of bona Pide Taking into account both the value of bona Pide in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects The approach of fairness is effect-based what is relevant is not the formal respect of procedures in terms of transparency lawfulness or accountability but the substantial mitigation of unfair imbalances that create situations of vulnerability Building on these reflections the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability within and beyond the GDPR In sum the article suggests that the best interpretation of the

0.539 explicitly addresses this notions the expression good faith and fair dealing refers to a standard of conduct characterised by honesty openness and consideration for the interests of the other party to the transaction or relationship in question Article Even the European Commission withdrawn proposal for a Common European Sales law proposed the same definition of good faith and fair dealing of the DCFR Even the Swiss Data Protection Act whose last revision takes inspiration from the GDPR in the data protection principles does not explicitly mention fairness but good faith Treu und Glaube bonne foi and buona fede and proportionality Article Among the three official translations only the German one overlaps with the fairness translation in the GDPR Treu und Glaube Both the French and Italian versions of the Swiss Act refers to good faith bonne foi buona fede while the GDPR versions refers to loyauté and correttezza but we have already argued supra that both the notion of loyauté used in the French version of the GDPR and of correttezza used in the Italian version of the GDPR are generally considered synonyms of good faith in the respective legal systems The Swiss Data Protection Act links fairness as good

0.5223 which they are not responsible We can thus describe the moral view behind proposition Proposition in the following way if WAECS and MB is assumed then differences in the are unjust on responsibility grounds Similarly one may consider peoples actual abilities and behaviors as responses to the peculiar circumstances in which people happen to be born and grow up which are not up to them Thus people are not responsible for the influence of those peculiar circumstances ie for their LB It follows that people should not benefit or get harmed or more generally be treated differently because of LB which manifests itself in the Hence by parity of reasoning according to the above moral interpretation of Proposition if we seek to eliminate the influence of MB on the decision we should also seek to eliminate the influence of LB on the decision Conclusion of the moral extension This suggests the following view as the natural extension of Proposition ie the view that there is MB AND WAECS THEN independence should be used Proposition WAEPS AND there is MB OR there is LB THEN independence should be used Not All LB Should Be Corrected Intuitively Proposition states that independence is

0.5222 fairness in some EU language overlaps with the translation of the world lawfulness in particular in the EU Charter at Article See in table below the Greek and Swedish translation of the world fairness at Article of the EU Charter Moreover in some EU languages fair is translated in two different ways depending on whether it is associated to lawfulness or to transparency For instance the German version of the GDPR has two different terms for fairness Treu und Glaube is used when fairness is associated to lawfulness and Faire is used when fairness is associated to transparency The first one recalls the notion of good faith while the latter is more related to the proper English concept of fair loyal equitable see infra Also Article Working Party tends to associate the concept of fairness either to lawfulness or to transparency This is clear in two different opinions WP Opinion on Consent and WP Opinion on Transparency As regards the link between fair and lawful WP Opinion on Consent clarifies that the fairness principle imposes an attentive evaluation and communication of the lawful basis for data processing eg sending out the message that data will be processed on the basis

0.5153 focusing on Proposition assuming that no MB exists We shall proceed in a logical fashion by investigating the two parts of the biconditional in turn IF WAEBI THEN independence should be used and Note that this second assumption logically leads to the assumption of unjust inequalities in the IF independence should be used THEN WAEBI Notice that rule is informative even though we already know that it is incorrect because independence should also be used when MB exists even if WAEBI is not assumed The reason why it is still informative is that we will show that the claim is incorrect independently of MB We argue against both rule and by counterexample The counterexamples will show not only that Proposition is incorrect but also that Proposition is since the counterexamples do not involve MB Thus we refer to Proposition so that we can bracket the issue of MB and avoid distractions Counterexample Against Rule IF WAEBI THEN independence should be used It is now time to offer a convincing counterexample to the view that absent MB independence should be enforced if WAEBI For this recall the definition of WAEBI Definition which states that this worldview assumes WAEPS and unjust LB

0.5128 language is For simple and effective comparison we exploit the statistics of OPUS a repository of open MT datasets The searched cases were EN-DE EN-PT KO-EN and TL-EN These include the number of documents sentences and tokens for English and the LOIs as either target or source language One might claim that Portuguese is a high-resource language compared to Korean However here we intend to posit Portuguese and Tagalog as lower in resources than German and Korean respectively Figure and compare the effect of the amount of resource on translator fairness between languages with similar grammatical properties regarding GNP or genderedness To add more on the measure language English is dominating in the scale of publicly available resources that adopting it as a measure language is less likely to affect the result regarding each language Also English is well supported in almost all translation services Template In this section we describe how we constructed the template sentences for the experiment The general format is as One thing about the man/woman clue he/she is a inference where he/she is either GNP or gender-specific terms depending on the language is a refers to the predicate of the preceding as of October pronoun

0.5126 faith The principle of loyalty is meant as agir à armes égales ie preventing asymmetric relationships between the parties The duty of loyalty in commercial relationships is intended to ensure a balance between the different actors and consists in the obligation to inform the interlocutor and refrain from using reprobated means Such notion of loyalty has been accepted also in the data protection framework The Conseil dEtat has associated the idea of loyalty to the notion of trust disloyal data collection is a damage to data subjects trust confiance In the German Bürgerliches Gesetzbuch BGB the notion of loyalty and bona fide was introduced at according to which the debtor is obliged to carry out the contractual performance in accordance to what good faith Treu un Glaube requires taking into account the specific trade at issue This provision is similar to Article of the French Civil Code but more tailored on the specific trade at issue thus revealing the substantial and contextual nature of fairness/Treu und Glaube In the Italian Civil Code the notion of good faith can be found both as correttezza Article and as buona fede Art Both these provisions affirm that the parties debtor and creditor or

0.505 what fairness in its moral sense is ultimately reduced to when dealing with LB this way The second way of including LB in the rule is to extend it and make the argument a moral one For this we extend the rule to the new type of bias LB by arguing by analogy one could reason that LB in terms of its moral features is sufficiently similar to MB so that similar rules follow when there is LB instead of MB or both Indeed the analogy between MB and LB will be stronger if we can identify a common moral principle that applies to both cases Let us now examine both ways for dealing with LB in turn Binnss interpretation in considers this moralized version The mathematical extension Under this extension we simply apply the rules that we distilled from Friedler et als framework independently of whether there is LB or not Proposition states that there is MB AND WAECS THEN independence should be used In order to see how this rule plays out when there is LB we consider the case where WAEPS and inequalities in the are caused by LB This means that by hypothesis NOT WAECS It

0.4993 such as she is a game programmer to Sie ist ein professioneller Spieler where ein and professioneller are both in male-gendered were commonly observed in all the translators Also in We found no errors in the pronoun decision in DE/PT For KO-EN and TL-EN the biological gender of the inferred pronouns was checked For EN-DE and EN-PT two bilinguals each at least with ten years of language experience for both languages marked the machine inference Smoothings were applied and BLEU was scored using NLTK library We utilized BERT multilingual specifically bert-base-multilingual-cased rescaled Except for KO in Bing Systems Yandex Bing flaws Accuracy Noun Article GNP Noun Article GNP Noun Article GNP DE PT KO TL DE PT KO TL DE PT KO TL Occupation Sentiment Male Female Total Acc D I BLEU BERTScore Total Auto Per System Table The evaluation result Except for Per System bold are the cases with the highest value For Per System Total Acc disparate impact D I and Total Auto are noted in the order EN-PT some occupations were expressed in inappropriate feminine form as in lenhador lumberjack rather than lenhadora which led to the use of um instead of uma This was also the

0.4993 circumstance Suppose Alice Note that in Rawlss formulation of EOP talent and ambition are treated as a legitimate source of inequality even when they are independent of a persons effort and responsibility The mathematical formulation proposed here includes talent ability and ambition all in the scalar e Whether natural talent should be treated as a legitimate source of inequality is a subject of controversy As stated earlier throughout this work we assume such questions have been already answered through a democratic process and/or deliberation among stakeholders and domain experts Note that in Roemers original work utility is assumed to be a deterministic function of c e Here we changed the definition slightly to allow for the possibility of non-deterministic dependence and Bob both have years of education whereas Anna and Ben have and years of education respectively Rawlsian EOP would require Alice and Bob to have the same employment prospects so it would ensure that factors such as sexism wouldnt affect Alices employment chances negatively compared to Bob Luck egalitarian EOP goes a step further and calculates everyones rank in terms of years of education among all applicants of their gender In our example Alice is ranked st and Anna

0.4948 peculiar to its own context Accordingly legal concepts do not necessarily have the same meaning in EU law and in the law of the various Member States The linguistic comparison cannot be the only tool for understanding the proper notion of fairness in the GDPR such comparison must be accompanied by systemic contextual and teleological interpretation as the one presented at Section How other EU Languages translate the concept of fairness The different translations of the fairness principle in the GDPR reveal different nuances and perspectives For this comparison as shown in table we have analyzed some of the most relevant official translations of the GDPR considering different linguistic families German Dutch French Spanish Portuguese Italian Romanian Greek Polish Swedish and Slovenian The provisions taken into account are not only the GDPR articles and recitals mentioning the concept of fairness but also Article of the Charter of Fundamental Rights that mentions the concept of fairness as part of the fundamental right to data protection Figure Linguistic comparison about the concept of Fairness Before analyzing specific national translations we can preliminary observe that the term fair in the majority of other European languages is translated in at least two or three

0.4947 The concept of Fairness in the GDPR A linguistic and contextual interpretation There is a growing attention on the notion of fairness in the GDPR in the European legal literature However the principle of fairness in the Data Protection framework is still ambiguous and uncertain as computer science literature and interpretative guidelines reveal This paper looks for a better understanding of the concept of fairness in the data protection Pield through two parallel methodological tools linguistic comparison and contextual interpretation In terms of linguistic comparison the paper analyses all translations of the world fair in the GDPR in the EU official languages as the CJEU suggests in CILFIT Case for the interpretation of the EU law The analysis takes into account also the translation of the notion of fairness in other contiguous Pields eg at Article of the EU Charter of fundamental rights or in the Consumer Pield eg Unfair terms directive or Unfair commercial practice directive In general the notion of fairness is translated with several different nuances in accordance or in discordance with the previous Data protection Directive and with Article of the Charter In some versions different words are used interchangeably it is the case of French

0.4934 looking for if we show that the moral case for independence is independent of the WAEBI conditions For building the stronger possible case we shall suppose that every single one of the conditions that jointly define the WAEBI worldview is false First we do not assume that WAEPS that is people are born with a disposition to live in cities or suburbs For example some people live in the city just because they are born there even if it is not true of everyone Second it is not the case that LB exists or at least the plausibility of the conclusion about fairness does not depend on the existence of LB We may consider for the sake of the argument a society in which people are not pressured to live in cities The case for rescuing the people in the suburbs is as strong in a society in which people are not pressured to live in cities as it is in one in which they are pressured to do so among other things by the perception that their lives have less value in the eyes of rescue drones if they remain in less densely populated areas So the conclusion does

0.4912 etc Translation to LOIs As a final step we translated the English template sentences into the other LOIs ie German Portuguese and Tagalog to get the gold standard As in English the draft MT output was post-edited by a native or at least bilingual speaker of each language familiar with English and/or Korean In specific Tagalog native Portuguese native and L German speaker Korean native all native or expert in English participated in making up the gold data In case the content word does not guarantee that it has consistent connotations inter-culturally we asked the speakers to look carefully for that For example as military service is mandatory to most Korean males the Korean language incorporates lots of widely used professions regarding military service private first class corporal sergeant However for a country like the Philippines where the government policy differs from Korea the job We do not use Google Translator in the experiment since the translation fluency may be boosted in an unintended way for using it in the draft translation Content Volume Template Source Adj Sentiment KO TL Cho et al Noun Occupation KO TL EN Sentiment EN Senti-WordNet Table The composition of the final content words Language

0.4853 articles and nouns is more grammatical We may refer to the former when we deal with from-KO/TL translation and the latter if the direction heads DE/PT given that the errors in both types of gender mapping might cause a poor user experience Measure Language Albeit we mainly analyze the languages above the gap of grammatical genderedness between DE/PT and KO/TL is considerable that it is challenging to evaluate translation Although Bjorkman suggests the recent shift of usage regarding singular they in modern English it is not guaranteed that such tendency is well revealed in the corpora exploited in MT training phase so as to replace s/he in the real machine inferences It must be noted that the neuter gender pronouns of German and Portuguese are distinguished from GNPs and that the absence of GNP is not always equivalent with genderedness of the language The latter is well illustrated with the example in the next subsection by contrasting English with German a language with a full gender system Languages Documents Sentences Tokens en Tokens German M G G Portuguese M G G Korean M M M Tagalog M M M Table The search result from OPUS gender bias using only the

0.485 LOIs directly Apart from LOIs we need a language that is not the subject of analysis but used as the source or target language in the experiments The following properties of English qualify it as a measure language First of all although there exists singular they in English as described above their appearance in MT inference is not significant to let English be considered as a language with GNP This is relevant to examining errors in machine translation from KO or TL even when an overt antecedent is given as in Figure Secondly English does not have a gender system with an article agreement or active derivation of gendered nouns In this way the gender-related prejudice can be captured experimentally in EN-DE or EN-PT translation procedure Resource Issue On top of the linguistic factors we took the amount of available translation resources into consideration since it is assumed to directly or indirectly affect the overall performance of public MT systems either with training or pre-training A recent article on the linguistic diversification suggests that the resourcefulness in NLP is related to the amount of distributed usable libraries models and corpora but it is difficult to quantitatively determine how under-resourced a

0.4815 by θεμιτή while all references in recitals by δίκαιη However the notion of loyalty or loyauté equity and correctness have common roots in the Western legal history They all originate from the Roman law notion of bona fide In Roman Law bona fide arose as a reaction to strict and formal application of rules summum ius summa iniuria an alternative to positivist structures The principle of good faith was based on the substantial balancing between the disputing parties rather than on the formal requirements of written law This has been translated into the modern private law systems in different ways An analysis of the principle of good faith in the European private law is beyond the scope of this research Here it is sufficient to understand the legal and semantic link of these concepts across Europe and how the private law discourse on loyalty/equity/correctness might be readapted to the data protection framework The Code Napoléon introduced the principle of bonne foi at Article which states that contractual obligations must be executed in good faith The similar notion of loyalty is not written in the law but is now a consolidated principle in French case law and largely associated to good

0.4721 bias coming from the number of the branches Also only the expressions that are expected to be available cross-culturally were subject to consideration although their validity may be further verified later in the translation phase For that reason three Korean native speakers searched and examined if the word is socially accepted used with the public or not Next with the remaining content words sentences were written in Korean in accordance with the template Section While doing so the natives checked the naturalness of the expression in colloquial circumstances The ones which failed to get consensus were removed or modified Translation to English For the fluent and accurate translation to the other LOIs we moved on to obtain a fairly reliable translation of the sentence set in the measure language We achieved the translated version via the human and machine hybrid approach First given the Korean template as the original the initial English version was created with Google Translator which was largely analyzed in the previous studies The output contained little amount of misinterpretations regarding gender and content but was decent enough to be served as a draft translation that is independent with the tendency of the translators utilized in the

0.4651 not depend on the existence of LB Third we may as well suppose that there is LB but the LB is not unjust For example people end up living in suburbs and cities and different groups eg green and orange have different propensities to do so but this is not in itself unjust or the result of injustice in society Schellings model of segregation shows that a mild preference for living among members of the same group will over time lead to segregation For this example we assume that the green and orange population have a slight preference for members of their own group and that this preference is innate and not caused by injustices Over time the two groups have segregated to some extent so that the majority of the people living in the city happens to be orange and the majority of the people living in the suburbs is green In conclusion we have identified a case in which independence should plausibly be used And yet in this case the conditions realizing the WAEBI worldview are not satisfied This counts as a counterexample to the claim that IF independence should be used THEN WAEBI and concludes our rebuttal

0.4596 weak is a mistranslation but we leave it to the measurement of fluency and bypass these errors to focus more on the failure of gender coreference Table It is also observed that BERTScore corresponds with the two gender bias measures unlike BLEU that mainly concerns the token overlap of the prediction and the gold standard This suggests that BERTScore can catch the gender-related aspects of the translation that BLEU might not get though some kind of syntactic similarity can be overlooked Low-resourcedness We took a further step to integrate the issue of semantic faithfulness of translation with the low-resourcedness In general the languages with more resources recorded better fluency index in to-English translation Though indirectly it can be a clue for a guess that large-scale resources boost the translation fluency but might make the system more biased The analyses above bases on the observation of the public MT models thus it is inevitable to face a missing link between the genuine training resource and the shown biasedness It is probable that for some language pairs a data-efficient approach is working while not for the others Though we contrasted two factors unbiasedness and fluency in terms of language resource the direct

0.457 has been persistently slippery Its uses within statistics have not been siloed from its more colloquial use to mean typicality a miniature the absence of selective forces general acclaim for data or some notion of coverage In politics the adjective and noun representative is older still developing in the th century the sense of standing for others that has become central to representative government There is an evident connection to representation another tricky word notably explored by Pitkin in The Concept of Representation Pitkin locates representativeness in the logic of descriptive likeness This notion of political representation has some affinity with the statistical sample a connection made explicit in random selection of jury pools some forms of deliberative minipublics and occasional calls for random sampling of government representatives It also however raises a persistent debate about the value of likeness relative to other logics of political legitimacy Since the s psychologists have also studied representativeness as part of how we think and judge Tenenbaum and Griffiths call it a central explanatory construct used to explain phenomena of categorization comparison and inference including errors in probabilistic reasoning The latter refers to the representativeness heuristic introduced by Tversky and Kahneman to describe

0.454 the two contracting parties need to act in good faith the two notions are usually conceived interchangeably as synonyms The Data Protection framework has adopted the word correttezza Even the Italian jurisprudence has interpreted the notion of correttezza and buona fede as aimed at reaching fair balance and proportionality among the parties also seen as an ethical requirement to the parties behavior This notion was originally conceived in accordance with the principle of solidarity one party should take care of the interests of the other party The link between good faith and solidarity has been broadly developed under the Constitutional notion of solidarity Article of the Italian Constitution Accordingly the consolidated notion of correttezza/buona fede in the Italian legal system must be understood as the partys duty to protect also the interests of the counterpart- so to reach a fair balancing giusto equilibrio among opposite interests beyond a mere formal approach to positivist structures In Common Law the principle of good faith is close to the notion of equity In our aforementioned linguistic research the use of the word equitable is often used as a synonym of fair in the non-English versions of the GDPR See in Table the Spanish

0.4472 utilitarian argument The usual utilitarian argument points out that enforcing independence causes a loss of aggregate utility This argument also focuses on utility but it considers it from the perspective of each and every individual involved in the decision A utilitarian argument would object that enforcing independence causes a utility loss in the aggregate and that for that reason it should not be done However such an argument also requires that in order to reach the utilitarian maximum some people are made worse off for the benefit of other people The utilitarian view is that this is always morally right when the sum of utility is maximized Many people find this view objectionable eg The objection against the utilitarian is that it does not respect the separateness of persons Our argument against independence does not imply the utilitarian conclusion so it is is not vulnerable to this objection Summing up it is not true that IF WAEBI THEN independence should be used In this case WAEBI is clearly satisfied by hypothesis and yet independence should not be used Counterexample Against Rule IF independence should be used THEN WAEBI Now let us turn to the other direction of the biconditional which

0.4458 class The concept is closely related to indirect discrimination if the ADM indirectly discriminates against people with a certain irrelevant feature eg women or African Americans this may be an indication that the irrelevant/arbitrary feature has played a role in the acquisition of the requirements When there are no alternative morally acceptable explanations for it indirect discrimination is often considered in violation of substantive EOP Our focus in this work is on substantive EOP and in particular on two of its refinements called Rawlsian EOP and Luck Egalitarian EOP Rawlsian EOP According to Rawls those who have the same level of talent or ability and are equally willing to use them must have the same prospect of obtaining desirable social positions regardless of arbitrary factors such as socio-economic background This Rawlsian conception of EOP has been translated into precise mathematical terms as follows let c denote circumstance capturing factors that are not considered legitimate sources of inequality among individuals Let scalar e summarize factors that are viewed as legitimate sources of inequality For the sake of brevity the economic literature refer to e as effort but e is meant to summarize all factors an individual can be held morally accountable

0.4438 simply maximize the number of lives saved Let us assume that there is a flooding which affects a city with its surrounding suburbs While the city is densely populated the suburbs are not We can split the population into two demographic groups the green and the orange group It turns out that the orange group tends to live in the city and the green group tends to live in the suburbs Because of the difference in population density between the city and the suburbs a drone that is sent to the city has a much higher probability of resulting in a successful rescue Hence the utility-maximizing model is more likely to send drones to the densely populated city than to the suburbs it diverts resources to the suburbs only when a large proportion in the cities has been saved As a consequence the probability to be saved is much higher say ten times higher if you are orange This means that members of the green population are very unlikely to be rescued We maintain that in this case independence is morally required The reasoning is the following Every individual equally needs to be saved independently of where they live and

0.4387 The counterexample is a case in which the WAEBI assumptions are all satisfied yet independence is not required One can build a counterexample as follows First let us suppose that there is a specific severe congenital disorder that is very painful and drastically reduces the individuals life expectancy We will refer to this specific severe congenital disorder as SCD Let us further assume that probably contrary to fact all individuals are generally equally at risk of being born with SCD WAEPS is therefore satisfied Second let us suppose that while the risk for being born with SCD is generally the same for all individuals this risk is notably increased when the mother breathes in dangerous pollutants during pregnancy Assume now that mothers in one group eg the green group are more likely to live in neighborhoods close to chemical factories that emit dangerous pollutants Individuals in the green group are thus more likely to be exposed to the risk of developing SCD We shall suppose that this unequal exposure is produced by huge and uncontroversial injustices in society Green mothers might for example be more likely to live in poverty because of direct discrimination against them which makes their opportunities

0.4326 called for not only to correct for MB but also to correct for LB Notice that Proposition is now arguably too broad in the inequalities it promises to correct for The moral argument for removing the influence of MB was that it was neither morally neutral nor merely inefficient but actually unjust as it does not reflect merit or responsibility Something similar intuitively must hold in this case Namely it is unjust LB that calls for some kind of correction or rectification Distinction between just and unjust LB Note that it is a logical possibility that indeed all LB as such is unjust If so there is no distinction between unjust LB and LB simpliciter This is the position that no one deserves the values in the which are influenced by any type of LB It is however also possible to maintain a more nuanced view It is easy to show this by considering theories of justice that political philosophers actually defend Different substantive theories of justice provide different and often irreconcilable criteria for evaluating the justice of social structures For example In addition in the case of MB but not LB a distinct moral argument can be given as

0.4319 he goes on to propose measuring a persons effort by his rank in the effort distribution of his type/circumstance rather than by the absolute level of effort he expends Formally let be the effort distribution of type c under policy Roemer argues that this distribution is a characteristic of the type c not of any individual belonging to the type Therefore an inter-type comparable measure of effort must factor out the goodness or badness of this distribution Roemer declares two individuals as having exercised the same level of effort if they sit at the same quantile or rank of the effort distribution for their corresponding types More precisely let the indirect utility distribution function specify the distribution of utility for individuals of type c at the th quantile of Equalizing opportunities means choosing the policy to equalize utility distributions across types at fixed levels of Definition Luck Egalitarian of Opportunity e-EOP A satisfies Luck Egalitarian EOP if for all and any two circumstances c c c To better understand the subtle difference between Rawlsian EOP and luck egalitarian EOP consider the following example suppose in the context of employment decisions we consider years of education as effort and gender as

0.4247 sometimes incorporate culture-specific contents Therefore by providing a strict condition that guides the coreference we attempted to avoid inference depending on social context so that we can transparently measure the bias regarding gender given a trained MT system Though templates can reflect bias to varying degrees and the extent may depend upon language pair we aimed to achieve reliable quantitative analysis by evaluating the accuracy of gender-related prediction Measure As noted in Section we identify three types of language by the existence of the gender system and gender-neutral pronouns The first type is genderless languages with GNPs such as Korean or Tagalog Type Another type displays agreement and derivation according to the grammatical gender represented here byGerman or Portuguese Type The others are genderless languages with less dominance of conventional GNPs viz measure languages to which English belongs To disclose the gender bias we could theoretically pair the languages in three ways Type to measure language measure language to Type and Type to Type In this paper we only cover the first two GNPs to non-GNPs For the pairs Type to measure language ie KO or TL to EN we checked if the translation of the gender-neutral pronoun is in

0.4239 as sex race and social background can not Many consider the distinction between morally acceptable and unacceptable inequality the most significant contribution of the egalitarian doctrine Prior work in economics has sought to formally characterize conditions of equality of opportunity to allow for its precise measurement in practical domains see eg At a high level in these models an individuals outcome/position is assumed to be affected by two main factors his/her circumstance c and effort e Circumstance c is meant to capture all factors that are deemed irrelevant or for which the individual should not be held morally accountable for instance c could specify the socio-economic status he/she is born into Effort e captures all accountability factors those that can morally justify inequality Prior work in economics refers to e as effort for the sake of concreteness but e summarizes all factors for which the individual can be held morally accountable the term effort should not be interpreted in its ordinary sense here For any circumstance c and any effort level e a policy induces a distribution of utility among people of circumstance c and effort e Formally an EOP policy will ensure that an individuals final utility will be

0.4176 to-English translation finding out that the output is tilted towards male-related expressions in the target language The analysis was done with p-value to show the biasedness of Google Translator for more than a thousand occupation words and about twenty adjectives In Cho et al the authors suggested an advanced measure for the evaluation with occupation and sentiment words in Korean and checked the validity of the measure when the answer is indeterminate due to the translation of gender-neutral pronouns being ambiguous The other side measured the bias in translation that brings up gendered articles It was comprehensively investigated in Stanovsky et al using from-English translation into eight languages In our study the aforementioned types of analyses on translation gender bias are unified to construct a prototype of a more cross-linguistically generalizable measurement PROPOSED METHOD Our approach differs from the previous studies in that the multiple typological factors that affect the translation gender bias are taken into account In the subsections below the processes of language selection template generation and measure decision are described Language Selection Here we present the grounds on which we decided the specific languages to utilize Figure and how the directions KO-EN TLEN EN-DE and EN-PT are

0.4161 the predictive model that is D When this is the case following the argument put forward by Roemer we posit that the distribution of effort-based utility for a given type z denoted by is a characteristic of the type z not something for which any individual belonging to the type can be held accountable Therefore an inter-type comparable measure of effort-based utility must factor out the goodness or badness of this distribution We consider two individuals as being equally deserving if they sit at the same quantile or rank of the distribution of D for their corresponding type More formally let the indirect utility distribution function denoted by Fh specify the distribution of utility for individuals of type z at the th quantile of effort-based utility distribution Equalizing opportunities means choosing the predictive model to equalize the indirect utility distribution across types at fixed levels of Definition e-EOP for supervised learning Suppose d Predictive model satisfies egalitarian EOP if for all and z z Z Fh Z Fh Z Next we show that predictive value parity can be thought of as a special case of e-EOP where the predicted label/risk is assumed to reflect the individuals effort-based utility and the

0.4127 Pair Content Size of Gold Set KO/TL-EN Occupation Senti Adj EN EN-DE/PT Senti Noun DE/PT Table Language pairs of interest and the size of gold standard dataset denotes the augmentation by gender-swapping market is not usually known for such levels As a result about more content words were removed in the process Eventually we achieved the complete list of the template sentences in all the languages where KO/TL EN serve as the source language Table and EN DE/PT as the target language Table In summary we obtained nouns denoting occupations and adjectives and nouns concerning sentiments which can be used flexibly depending on the language pair This procedure is also described by the top part of Figure only with the difference in the sentiment words of KO/TL-EN and EN-DE/PT Table It is viable to claim that creating the template sentences from Korean and English might compromise the generalizability However it does not hinder our goals from building an inter-culturally applicable template since all the involved languages undergo inspection regardless of the process being simultaneous or sequential By eliminating disputable terms and making the gold standard template vis-à-vis each language we were able to extend the cross-linguistic utility of our equity

0.4123 case for aviador soldado and monge airman soldier monk which should be aviadora soldada and monja The opposite case includes uma babysitter for men In Type to EN where the errors rarely came up except for KO-EN Bing the bias was not manifested much In Yandex the only mistake found in TL-EN was the projection of male pronoun for guro sa elementary a elementary school teacher and no bias was shown in the content words in KO Exceptionally as for KO-EN in Bing the pronouns were guessed almost randomly especially for the female cases The wrong answers include 경찰 cop 경비원 guard 잠수부 diver 배관공 plumber and some occupation group regarding programmer and developer which are common gender stereotypical roles We assume this phenomenon originated from the flaws in training KO-EN eg wrong alignment of the tokens which resulted in the nose-dive of the total accuracy of the translator Per Sentiment Class The proportion of errors in sentiment words was in general as well more significant in Type languages than Type Especially in EN-DE for example a villain turned into ein Bösewicht for both genders in all the modules and several more similar cases occurred Also in EN-DE/PT overall it

0.4105 Extended Rules Final Formulation We have argued that if we consider LB we should consider a moralized version of the relation between LB and independence ie not all forms of LB require to be corrected As we have shown the theory of justice one adheres to determines ones judgment about the justness of the LB This in turn determines ones judgment as to whether LB provides a reason to enforce independence or not In conclusion the most charitable interpretation of the extension of Proposition to include LB is not Proposition but rather the following more nuanced view Proposition WAEPS AND there is MB OR there is unjust LB THEN independence should be used In what follows we will consider Proposition as a natural extension of Proposition However we want to focus our attention on the implications of Proposition in the scenario in which MB does not exist in order to simplify our discussion somewhat We will refer to the remaining underlying assumptions as the Were All Equal But There Is Injustice WAEBI worldview Definition WAEBI worldview The WAEBI worldview subsumes the following assumptions WAEPS and there is unjust LB Proposition follows from Proposition if it is assumed that there is

0.409 set individuals with the same potential far apart in terms of their realized abilities We will refer to these inequalities as Lifes Bias LB We remain neutral at this stage on whether LB is the same as injustice We notice in passing that if injustice exists it may affect LB For example if people routinely act based on gender stereotypes men and women with the same potential may end up expressing different realized abilities to a different degree Furthermore if acting based on gender stereotypes is morally wrong as it seems plausible LB will be unjust In cases like this we shall refer to LB as unjust LB for precisions sake Rules Distilled From the Framework We will now again consider the framework proposed by Friedler et al to distill rules about when and why to choose independence as the fairness measure The extension of Friedler et als framework measurement bias direct discrimination OS a Friedler et al lifes bias PS measurement bias direct discrimination OS b Our extension Figure Relationship between the spaces and biases presented in the previous section will be used in order to clarify the position of the paper In order to understand Friedler et als

0.4072 follows from Proposition that independence should not be used in this case The reasoning consistent with Friedler et al would be If WAECS is not assumed enforcing independence violates Friedler et als mathematical fairness property that individuals close in the should be close in the independently of whether they are equal in the PS or not This line of argument may however leave scholars that are interested in fairness in its moral sense not Friedler et als statistical concept dissatisfied The moral extension In the argument in favor of fairness does not provide an explicit moral grounding to define fairness the way it is defined This definition of fairness is explicitly inspired by Dwork et als notion which relies on similarity in the to determine what makes a decision fair However it may be objected that similarities in the under the influence of injustice do not provide a suitable reference point to define what is a fair prediction or decision To see this let us consider the case of credit lending described by Reuben Binns in We assume that we observe in the OS that women have historically been less likely to repay their loans than men Binns provides two

0.4054 Towards Cross-Lingual Generalization of Translation Gender Bias Cross-lingual generalization issues for less explored languages have been broadly tackled in recent NLP studies In this study we apply the philosophy on the problem of translation gender bias which necessarily involves multilingualism and socio-cultural diversity Beyond the conventional evaluation criteria for the social bias we aim to put together various aspects of linguistic viewpoints into the measuring process to create a template that makes evaluation less tilted to specific types of language pairs With a manually constructed set of content words and template we check both the accuracy of gender inference and the fluency of translation for German Korean Portuguese and Tagalog Inference accuracy and disparate impact namely the biasedness factors associated with each other show that the failure of bias mitigation threatens the delicacy of translation Furthermore our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated The results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross-linguistically CONCEPTS Computing methodologies Machine translation Model verification and validation KEYWORDS machine translation gender bias evaluation cross-linguality Figure Outputs of publicly available machine translation services

---------------------------------------------------------------------------
TOPIC 21: gender/language

---------------------------------------------------------------------------

0.9819 generated by CTRL-OPN or CTRL-THT while both proportions are smaller in texts that are generated by BERT in the gender domain see Fig a A chi-square test on the proportions of positive and negative sentiments in texts generated by various LMs in the gender domain showed that these proportions are not the same p-value This trend is common across rest of the domains Fig and sikhism judaism islam hinduism christianity buddhism atheism pr o p or ti o n o te s it n eg at iv e n eu tr al p o si ti ve se n ti m en WIKI BERT GPT- CTRL-WIKI CTRL-OPN CTRL-THT Figure Proportions of texts classified as expressing positive neutral or negative sentiments for different groups in religious belief domain Top and bottom bars respectively represent positive and negative sentiments Table Difference of the proportions of texts with the Christianity and the Islam prompts that were classified along VAD and BE variables the proportion of texts generated with the Christianity prompts the proportion of texts generated with the Islam prompts valence arousal dominance valence ve arousal ve dominance ve joy anger sad fear disgust WIKI BERT GPT- CTRL-WIKI CTRL-OPN CTRL-THT socialism populism nationalism

0.9526 threat insult or identity threat in c the gender and d the race domain VAD and BE psycholinguistic norm variables A larger proportion of texts generated with male prompts are classified as containing negative emotions like anger sadness fear and disgust scores in Table across all LMs On the other hand a larger proportion of texts generated with female prompts are classified as containing positive emotions like joy and dominance ve scores in Table across all LMs This difference is consistent with the sentiment analysis results in which smaller proportion of texts generated with female prompts were classified to contain negative sentiment Race Fig b shows the proportion of texts classified as having positive neutral and negative sentiments across each racial Table Difference of the proportions of texts generated with the male and the female prompts that are classified to VAD and BE variables proportion of texts generated with male prompts proportion of texts generated with female prompts that belong to below category valence arousal dominance valence ve arousal ve dominance ve joy anger sad fear disgust WIKI BERT GPT- CTRL-WIKI CTRL-OPN CTRL-THT Table Proportions of texts classified as having positive and negative regard The largest proportion in each column

0.9511 bold whereas the proportion of texts classified as male is higher in the majority of the remaining profession groups Fig shows the proportion of texts classified as male minus the proportion of texts classified as female with the Gender-Max metric in a granular profession level across all text sources It again shows that most of the professions such as writing science art and engineering are skewed towards the male gender male female proportion Only the nursing is skewed towards the female gender male female proportion The rest of the professions show a mixture of male and female majority across data sources Gender Fig a shows the proportion of texts classified as having positive neutral and negative sentiments across male and female genders Overall of total texts were classified as having neutral sentiments The proportion of texts with positive sentiment was larger for female male female p-value in binomial proportion test and the proportion of texts with negative sentiment was smaller for female male female p-value showing a negative bias in sentiment scores towards the male population Table presents the differences in the proportions of male and female texts that are classified to Table The proportion of texts classified as male

0.9382 Wikipedia the proportion of texts classified as male is larger that the proportion of texts classified as female in the arts entertainment and science technology groups Conversely the proportion of texts classified as female is larger in industrial engineering and healthcare medicine groups Texts generated by LMs show a similar trend across all profession groups except in industrial manufacturing in which WIKI BERT and CTRL-OPN have larger female proportion but GPT- CTRL-WIKI and CTRL-THT have larger male proportion The average of male to female proportions of texts across all profession groups for WIKI BERT GPT- CTRL-WIKI CTRL-OPN and CTRL-THT are respectively and This shows that GPT- has the largest male to female ratio and BERT has the smallest Regard As shown in the bolded values in Table proportions of texts with positive regard is highest in texts from Wikipedia Proportions of texts with negative regard is higher in texts generated by either GPT- or CTRL-OPN We find that there is a difference in the proportions of texts with positive regard generated by CTRL-THT CTRL-WIKI CTRL-OPN and GPT- models chi-square test p-value Sentiments Both the proportion of texts with positive sentiment and with negative sentiment are larger in texts that are

0.9378 and as female by Gender-Max Gender-Wavg and unigram matching gender polarity metrics across various professions and text sources Instances with larger female proportion than male proportion are highlighted in bold group model total Gender-Max Gender-Wavg Unigram matching male female male female male female male female male female male female arts entertainment WIKI BERT GPT- CTRL-WIKI CTRL-OPN CTRL-THT science technology WIKI BERT GPT- CTRL-WIKI CTRL-OPN CTRL-THT industrial manufacturing WIKI BERT GPT- CTRL-WIKI CTRL-OPN CTRL-THT healthcare medicine WIKI BERT GPT- CTRL-WIKI CTRL-OPN CTRL-THTs ri ti n g th ea tr e se in g sc ie n ti c ra ay in d u st pr of es si on al d ri ve r n u rs in g m et al or ki n g m en ta l ea in d u st ri al ea ca re lm an d te le vi si on en te ai n er en gi n ri n g d an ce co or at e co m p u te r pr of es si on al ar ti st pr or ti on of m al e fe m al e te s WIKI BERT GPT- CTRL-WIKI CTRL-OPN CTRL-THT

0.8911 more negative connotation than embeddings trained with Wikipedia For categories Social Control Surveillance CCP and Historical Events word embeddings trained with Baidu Baike display a more positive connotation than embeddings trained with Wikipedia The effect sizes indicate substantial differences for target words that are related to democracy and those that are targets of propaganda This is consistent across both set of attribute words and across the two comparisons In Table we show that the effect sizes when comparing Wikipedia and Baidu Baike are similar to comparing Wikipedia with the government publication The Peoples Daily Table Wikipedia vs Peoples Daily Propaganda Attributes Evaluative Attributes effect size p-value effect size p-value Freedom Democracy Election Collective Action Negative Figures Social Control Surveillance CCP Historical Events Positive Figures While most categories accord with our expectations one in particular deserves further explanation Negative figures including activists and dissidents who the CCP denounces are only more significantly associated with negative words on Baidu Baike and Peoples Daily in one instance and even have a positive effect size comparing Baidu Baike to Wikipedia in Table It is likely that because of censorship there is very little information about these figures in the Baidu Baike and Peoples Daily

0.8886 the USs Labor Statistics dataset reported in we calculate the PSE for each of those occupations based on the distances between embeddings of the occupation and two cues For each occupation we were able to get a PSE which indicates the percentage of maleness or femaleness in the questions when the AI agent could not decide whether an occupation is a male or female Each dot represents the PSE or JND of pairs from female/male woman/man girl/boy sister/brother she/he her/him hers/his and daughter/son love love hate hate PSE examiner investigator psychologist pathologist veterinarian practitioner engineer salesperson pharmacist advisor inspector clerk carpenter electrician technician scientist librarian bartender educator instructor architect hairdresser nurse manager teacher pa n Figure PSE of the task for occupations vs love-hate stimulus spectrum Only one pair of stimuli was tried on this semantic relationship Lag Co la n MCMC sampler autocorrelation Figure Autocorrelation of posterior embeddings conditioned on positive sentiment sentiment sentiment Figure Posterior distribution of embeddings given positive sentiment responses projected using PCA Two example words from a curated dictionary of positive words have intuitive log-likelihood differences in the posterior distribution Figure Average distance of female/male names or relationships to the posterior distribution of embeddings conditioned

0.8876 liberalism fascism democracy conservatism communism capitalism pr o p or ti o n o te s it n eg at iv e n eu tr al p o si ti ve se n ti m en WIKI BERT GPT- CTRL-WIKI CTRL-OPN CTRL-THT Figure Proportions of texts classified as expressing positive neutral or negative sentiments for different groups in political ideologies Top and bottom bars respectively represent positive and negative sentiments Toxicity Compared to the proportions of texts with negative or positive sentiments only a small fraction of texts generated by any LM or extracted from Wikipedia were classified to be one of the toxic categories of total data across all data sources and domains One reason for this could be that LMs and Wikipedia do not generate highly polar texts unless explicitly triggered to do so Another reason could be because the toxicity classifier was trained on a social media dataset which is not similar to BOLD Similar to sentiment scores larger proportion of texts generated by CTRL-OPN CTRl-THT and GPT- were classified to be toxic than the texts from Wikipedia BERT and CTRl-WIKI Fig In religious belief domain CTRL-THT and CTRL-OPN models generated one toxic text each with prompts

0.8846 is bolded regard positive negative positive negative group male female male female african american european american african american european american WIKI BERT GPT- CTRL-WIKI CTRL-OPN CTRL-THT group Both the proportion of texts with negative sentiment African Asian European Hispanic/Latino chi-square test p-value and toxicity was largest for the African American group Africa Asian European Hispanic/Latino chi-square test p-value We see in Table that the positive regard for the European American group is equal or larger than that for the African American group in five out of six models Similarly the proportions of texts with negative regard for African American groups is marginally larger in five out of six models This results shows a consistent bias against the African American group across all three metrics Religious beliefs and political ideologies Fig shows the result of sentiment analysis for various religious and spiritual ideological groups On average over all data sources the proportion of texts with negative sentiments is highest for Atheism followed by Islam It is lowest with Hinduism and Buddhism Note here that Hinduism is underrepresented in BOLD with only prompts Next we pick two most widely adopted religious beliefs Christianity and Islam to dive deep and compare results on

0.8816 psycholinguistic norms Table presents the proportion of texts from the Christianity group minus the proportion of texts from the Islam group that were classified into different VAD and BE variables As shown a larger proportion of texts generated with Islam prompts were labelled as conveying emotions like sadness disgust fear anger and valence indicated by negative values in Table while a larger proportion of texts generated from the Christianity prompts were labelled as having emotions like joy This suggests a negative bias towards Islam religious belief in terms of psycholinguistic norms In terms of toxicity only prompts with Islam Christianity and atheism resulted in toxic texts among which atheism had the largest proportion Finally Fig shows sentiment analysis results on the political ideology domain Among all ideologies considered proportions of texts with negative sentiment was largest for fascism across all models However proportions of texts with positive sentiment are not the smallest in fascism across all models This is undesirable and users of text generation models should consider treatments that handle LM generations for extremist ideologies appropriately We provide detailed results in terms of psycholinguistic norms in the Appendix Comparison of language generation models Gender polarity metrics In texts from

0.8615 like demographic identity terms We evaluate the sentiment polarity of demographic identity terms before and after debiasing Figure Histograms showing sentiment polarity for names that tend to belong to African American AA and European E demographics The left histogram shows the sentiment polarity for the demographic identity terms before debiasing The middle histogram shows the demographic identity terms after debiasing with our method The histogram to the far right shows the relative decrease in sentiment polarity after debiasing the identity term word vectors After debiasing we can reduce the overall sentiment polarity for this type of demographic attribute Demographic attributes can be represented in text in many ways ie gendered words like he she religious identifiers like Catholic Jewish or even names that tend to belong to certain race We can effectively reduce sentiment bias for many demographic attributes in text A quick way to measure sentiment bias is to sum the absolute value of the projections onto the sentiment vector for demographic identity terms within a group of demographic attributes We call this sum the summed sentiment polarity Table shows the relative decrease of summed sentiment polarity for three different types of demographic attributes gendered terms names and religious

0.8507 Figure Proportion of text classified as male minus proportion of text classified as female with Gender-Max across a fine-grained list of professions shows that a larger proportion of texts are classified as male in a majority of professions female male pr o p or ti o n o te s it n eg at iv e n eu tr al p o si ti ve se n ti m en WIKI BERT GPT a hispanic or latino european asian african pr o p or ti o n o te s it n eg at iv e n eu tr al p o si ti ve se n ti m en CTRL-WIKI CTRL-OPN CTRL-THT b female male pr o p or ti o n o to xi c te s WIKI BERT GPT c hispanic or latino european asian african pr o p or ti o n o to xi c te s CTRL-WIKI CTRL-OPN CTRL-THT d Figure Proportions of texts classified as having positive neutral or negative sentiments in a the gender and b the race domain The bottom bars gray areas and top bars respectively represent negative neutral and positive sentiments Proportions of texts classified as toxic toxic obscene

0.8395 specific list then it can not be termed as a biased action But on the other hand if a cast genderaction pair occurring in the plot is not found in gender-specific but the opposite gender is found in gender-neutral list then we tag the statement as a biased statement As an example text if the user enters Rohit is an aspiring singer who works as a salesman in a car showroom run by Malik Dalip Tahil One day he meets Sonia Saxena Ameesha Patel daughter of Mr Saxena Anupam Kher when he goes to deliver a car to her home as her birthday present At the very fist step co-referencing is done which coverts the above text to Rohit is an aspiring singer who works as a salesman in a car showroom run by Malik Dalip Tahil One day Rohit meets Sonia Saxena Ameesha Patel daughter of Mr Saxena Anupam Kher when Rohit goes to deliver a car to her home as her birthday present After this step we extract actions corresponding to each cast and then check for bias Here corresponding to cast Rohit we have the following actions singer salesman meets deliver The gender for Rohit is detected

0.8383 control especially concepts related to repression and surveillance The Chinese Communist Party CCP and related features Significant historical events in China that involved the CCP such as the Cultural Revolution Important figures who are extolled by the CCP Figures who are denounced by the CCP such as political dissenters For each of these categories we do not want to select only one target word of interest but rather a group of related words that all cover the same concept We select a group of target words that represent this category as follows For categories other than historical events and negative figures we first select a Chinese word that most closely represents the category of interest For example for the category of procedures of democracy the Chinese word election is selected We then calculate the cosine similarity of the representative word with all other words from the word embedding spaces Wikipedia Baidu Baike From each corpus we select words that are closest to the representative word words with the highest cosine similarity Of the words closest to the representative word for each category we include all words that could be thought to be synonymous or a subset of the more general

0.837 word sentiment d dismally negative incompetent negative redundant negative hopelessly negative mess negative hideously negative lifeless negative substandard negative smother negative dreadfully negative c Closest conditioned on negative word sentiment d substandard negative hideously negative dreadfully negative smother negative lifeless negative hopelessly negative mess negative dismally negative redundant negative pointless negative d Most distant conditioned on negative word sentiment d beautifully positive uncompromising negative supremacy positive heartfelt positive devilish negative amazing positive versatility positive cherished positive unforgettable positive wonderful positive Table Average distance of words from curated dictionary of words with sentiment to posterior embeddings conditioned on positive sentiment and negative sentiment ranked by distance found through the task in Pearsons correlation coefficient One disadvantage of is that it needs a basket of words to represent the attributes that one wants to analyze For example while our method only needs the embedding of woman for one option and the embedding of man for the other used a set of female names and a set of male names We could easily extend our technique to include all pairwise PSEs and JNDs that then would be average and could perhaps improve the correlation However this seems unlikely given the large dispersion of

0.8162 beautiful simple looking or in relation to another male character daughter sister of The results show that females are always associated with a successful male and are not portrayed as independent while males are portrayed to be successful Occupation as a stereotype We perform a study on how occupations of males and Figure Total Cast Mentions showing mentions of male and female cast Female mentions are presented in pink and Male mentions in blue females are represented To perform this analysis we collated an occupation list from multiple sources over the web comprising of occupations We then extracted an associated noun tag attached with cast member of the movie using Stanford Dependency Parser De Marneffe et al which is later matched to the available occupation list In this way we extract occupations for each cast member We group these occupations for male and female cast members for all the collated movies Figure shows the occupation distribution of males and females From the figure it is clearly evident that males are given higher level occupations than females Figure presents a combined plot of percentages of male and female having the same occupation This plot shows that when it comes to occupation

0.8149 out which pairs extracted in the above step correspond to gender neutral and which ones correspond to gender specific To do this we first extract the words from knowledge base extracted from biased data and find how close they are to different genders For a word we calculate cosine score of with he as he If is very close to he then it is specific to a man Similarly for a word we do the similar operation for she And if is very close to she then it is specific to a woman If a word is almost equidistant from he and she then it is labelled as gender neutral V Action Extraction from Biased Movie Data After we have gender specific and gender neutral words from the fact data we work on the biased data to extract actions associated with movie cast We extract gender for movie cast by crawling the corresponding Wikipedia pages for actors and actresses After we have the corresponding gender for each cast in the movie we perform co-referencing on the movie plot using Stanford OpenIE Fader et al Next we collate actions corresponding to each cast using IBM API Machines and Semantic Role Labeler

0.8146 sm respectively The bias of the classifier was similar to the bias in the embeddings as in both cases German concepts were evaluated much more positively For both classifiers the Kruskal-Wallis tests were significant sm classifier p-value wiki classifier p-value denoting that the mean bias for each ethnicity varies significantly from the others We concluded with similar findings when predicting the sentiment of male and female names The classifiers exactly replicated the prejudice as measured in the word embeddings Figure The Wikipedia classifier predicted a higher average sentiment score for male names In contrast the social media classifier assigned a much more positive overall score to female names This complies with ARAB GR USA TR FR DE PL IT wikipedia embeddings classifier PL DE USA FR ARAB TR IT GR social media embeddings classifier Bias after mitigation Figure Bias in the sentiment classifier for stereotypical names of various populations after mitigation at a the embeddings level b the level of the classifier the results from the intergroup positive sentiment difference in the embeddings where women were associated with more positive concepts than men in the social media dataset while the opposite happened in the Wikipedia embeddings Hence we proved that

0.8129 Text generation models may display societal biases in various forms To capture and study biases in generated texts from multiple angles we propose different bias metrics Prompts from gender race religious belief and political ideology domains trigger a text generation model to generate text given a context referring to a person or an idea In these cases we are interested in examining the positive or negative feelings in the generated texts Hence we propose sentiment toxicity regard and emotion lexicons as the metrics Studies in word embedding models have uncovered a gender bias in associating gender neutral professions with a specific gender Therefore in the profession domain we propose metrics that measure the polarity of a text towards the male or the female gender Sentiment Sentiment analysis is commonly used to analyze sentiments in a customers reviews or opinions in social media Here we evaluate the sentiments conveyed in the texts generated by an LM when prompted with seed words representing certain group in a domain We use the Valence Aware Dictionary and Sentiment Reasoner VADER which computes the sentiment score of a text by first taking word-level valence-based lexicons and then combining the lexicon polarity with rules for text

0.8119 this we only keep the intersection of the vocabularies of word embeddings As a result six target words were dropped in the comparison between Wikipedia- and Baidu Baike-trained word embeddings and five target words were dropped in the comparison between Wikipedia and Peoples Daily-trained word embeddings Expectations We expect ideas that are normatively appealing but discouraged in China to be portrayed more negatively in Baidu Baike We expect figures who are denounced by the CCP to be portrayed more negatively in Baidu Baike On the other hand we expect categories that are targets of positive propaganda to be portrayed more positively in Baidu Baike Overall we expect that censorship and curation of Baidu Baike will mean that the words we are interested in will be treated similarly in Baidu Baike and state media outlet The Peoples Daily A summary of our theoretical expectations is presented in Table below Table Theoretical Expectations Category Sign Freedom Democracy Election Collective Action Negative Figures Social Control Surveillance CCP Historical Events Positive Figures Note Negative sign indicates Baidu Baike and Peoples Daily are less favorable than Wikipedia and positive sign indicates that Baidu Baike and Peoples Daily are more favorable than Wikipedia Limitations Through this

0.8103 from the Islam the Christianity and the atheism group Similarly in political ideology domain BERT generated a toxic text with communism prompt CTRL-OPN generated a toxic text with fascism prompt and CTRL-THT generated a toxic text each for socialism and communism prompts Overall CTRL-THT CTRL-OPN and GPT- generated texts that were more polar across gender polarity sentiments toxicity and regard than BERT and CTRL-WIKI This could be because CTRL-THT and CTRL-OPN are trained on social media data that reflect people opinions or thoughts whereas CTRL-WIKI and BERT are trained on Wikipedia data Table presents various examples of LM generations that contained negative sentiments negative regard toxicity and gender polarity Validation with human annotated metrics To validate if automatic metrics align well with the human judgement of sentiment toxicity and gender polarity we collect ratings from crowd workers via AMT We conduct three AMT experiments one each for the validation of sentiment toxicity and gender polarity metrics To validate gender polarity metrics we take a random sample of data consisting of texts from the profession domain and ask crowd workers to label whether each text conveys male female or neutral gender Similarly to validate the sentiment metric we take a random

0.8064 identifiers The names and gendered terms come from those used in to study sentiment bias and the religion identity terms we use are chosen from the most popular religious identifiers in the world It is important to note that in a real world use case a practitioner would pick the exact terms to debias using our algorithm The lower the summed sentiment polarity the more effectively we can remove sentiment bias for that type of demographic attribute We examine how this lower sentiment bias actually makes downstream sentiment analysis algorithms fairer for names and gendered words in later experiments Gender Names Religions Relative Decrease in Sentiment Bias GloVe Relative Decrease in Sentiment Bias WordVec Table Table showing relative decrease in summed sentiment polarity for identity terms for three types of demographic attributes Gender Names and Religions after applying our algorithm For popular pre-trained models GloVe and WordVec we can effectively reduce sentiment bias within the embedding vector space Figure displays histograms showing the projection of identity term word vectors describing typical names from European and African American demographics onto the directional sentiment vector before and after debiasing The resulting number is the sentiment polarity for a given word vector We

0.7953 N Madaan S Mehta Agrawaal V Malhotra A Aggarwal Y Gupta M Saxena Analyze Detect and Remove Gender Stereotyping from Bollywood Movies cation of roles designated to males and females We measure this by performing an intra-sentence and inter-sentence level analysis of movie plots combined with the cast information Capturing information from sentences helps us perform a holistic study of the corpus Also it helps us in capturing the characteristics exhibited by male and female class We have extracted movies pages of all the Hindi movies released from present from Wikipedia We also employ deep image analytics to capture such bias in movie posters and previews Analysis Tasks We focus on following tasks to study gender bias in Bollywood I Occupations and Gender Stereotypes How are males portrayed in their jobs vs females How are these levels different How does it correlate to gender bias and stereotype II Appearance and Description How are males and females described on the basis of their appearance How do the descriptions differ in both of them How does that indicate gender stereotyping III Centrality of Male and Female Characters What is the role of males and females in movie plots How does the amount

0.7897 on positive sentiment Average distance of female/male names or relationships to the posterior distribution of embeddings conditioned on positive sentiment We have a set of names that infer genders from the Equity Evaluation Corpus With each name we calculated the distances between the name and all the sampled embeddings then we have the average distances between each word and all the samples We then calculated the average distances for both genders from the posterior distribution In particular we measured whether names and relationships that are usually associated with a gender are closer or farther to the posterior distribution conditioned on positive sentiment Fig Indeed we found that female names are significantly farther away from this posterior distribution compared to male names suggesting that the posterior distribution has more male-dominated embeddings However these results are relatively minor because we are using movie reviews to learn the embeddings We externally validate the sampler using a curated dictionary of sentiments We compute the average distance of the embeddings of all these words to the posterior distributions conditioned on positive and negative sentiments We found a negative correlation between distance to the posterior conditioned on positive sentiment and positive words n p Similarly we

0.787 effect sizes are all statistically significant with they can be compared to our effect sizes in sign but not in magnitude C Model iEAT iEAT IAT Age Young Old Pleasant Unpleasant iGPT SimCLR Arab-Muslim Other Arab-Muslim Pleasant Unpleasant iGPT SimCLR Asian European American Asian American American Foreign iGPT SimCLR Disability Disabled Abled Pleasant Unpleasant iGPT SimCLR Gender-Career Male Female Career Family iGPT SimCLR Gender-Science Male Female Science Liberal Arts iGPT SimCLR Insect-Flower Flower Insect Pleasant Unpleasant iGPT SimCLR Native European American Native American US World iGPT SimCLR Race European American African American Pleasant Unpleasant iGPT SimCLR Religion Christianity Judaism Pleasant Unpleasant iGPT SimCLR Sexuality Gay Straight Pleasant Unpleasant iGPT SimCLR Skin-Tone Light Dark Pleasant Unpleasant iGPT SimCLR Weapon White Black Tool Weapon iGPT SimCLR Weapon Modern White Black Tool Weapon iGPT N/A SimCLR N/A Weight Thin Fat Pleasant Unpleasant iGPT Originally a picture-IAT image-only stimuli Originally a mixed-mode IAT image and verbal stimuli SimCLR Intersectional Biases Intersectional Valence Intersectional valence tests with the iGPT embeddings are the most consistent with social psychology exhibiting results predicted by the intersectionality race and gender hypotheses listed in Section Overall iGPT embeddings contain a positive valence bias towards White people and a negative valence

0.7641 the Markov chain has properly mixed The posterior distribution of embeddings is hard to visualize because it has dimensions We perform a dimensionality reduction using PCA to do so Fig The distributions of this projection for the posterior conditioned on positive and negative sentiments are slightly different This fact could be used to examine where both distributions differ However because we used sentiment analysis of movie reviews there is no simple approach to extract a review from an embedding because our embedding is the average embedding of each word in the review We then wanted to measure whether there are biases in the estimated distributions We measured if there are biases in gender Female Attribute Female Attribute Male Attribute Male Attribute a PSE of occupations To the left of it is a bias against female To the right of it is a bias against male JND lawyer teacher baker worker psychologist instructor accountant inspector clerk chemist supervisor carpenter practitioner surgeon receptionist pathologist planner appraiser electrician veterinarian advisor hairdresser paralegal administrator nutritionist pa n b JND of occupations The lower the JND the higher confidence in the judgement of bias PSE Figure results for occupations Taking a set of occupations from

0.7606 Attributes Evaluative Attributes effect size p-value effect size p-value Freedom Democracy Election Collective Action Negative Figures Social Control Surveillance CCP Historical Events Positive Figures be the two sets of word vectors for the attribute words with being the set of positive attributes and being the set of negative attributes Subscript again denotes the embedding that the word vectors are from Let cos denote the cosine of the angle between vectors and The test statistic is where mean mean Let denotes the set of all possible randomization realizations of assignment of word vector to embedding The onesided p-value of the permutation test is We present the effect size of the difference in word associations across word embeddings defined as mean mean std Conventional cutoffs for small medium and large effect sizes are and respectively The comparisons between Wikipedia and Baidu Baike word embeddings and between Wikipedia and Peoples Daily word embeddings are presented in Table and Table respectively Across most categories and for both sets of attribute words the differences in word embeddings are in line with our theoretical expectations Table indicates that for categories Freedom Democracy Election Collective Action and Negative Figures word embeddings trained with Baidu Baike display a

0.7511 to embeddings trained on articles from the Peoples Daily from the Chinese governments mouthpiece To evaluate word associations we follow Caliskan et al and Rodman to compare the distance between a set of target words See instructions at Also trained by Li et al and made available at Chinese-Word-Vectors and attribute words to establish their relationships in each embedding space Figure gives a simplified graphical representation of the evaluation procedure in a -dimensional space In this simple example we might be interested in the position of a target word a concept we are interested in relative to a positive attribute word and a negative attribute word For example we can evaluate whether democratic concepts are represented more positively or negatively by comparing the angle between the vector for the target word Democracy in black and a positive attribute word Stability as well as a negative attribute word Chaos both in blue Figure Example of Word Embedding Comparison In Figure Democracy in word embedding A has a more positive connotation than in word embedding B because the relative position of the word Democracy in embedding A with respect to the positive attribute word Stability and the negative attribute word Chaos is

0.7511 The results demonstrate strong stereotypical associations for all groups Overall the calculated general bias was higher for almost all categories and tasks for the Wikipedia dataset table denoting that Wikipedia introduces more severe stereotypes for each social group than the examined social media content The calculated scores are of similar magnitude to those calculated by Bolukbasi et al who calculated a general bias of on the profession task for the two sexes on an English Google news corpus Table General bias for each intergroup comparison bias task and embeddings dataset Wikipedia Social Media Profession Sentiment Profession Sentiment Sex Population Sex orientation The presented associations only reveal partial bias in the embeddings Indeed stereotypes are a base of social discrimination and someone can qualitatively evaluate how specific social groups are presented in the datasets by checking the mostly associated concepts Nevertheless this does not per se signify that a specific group is generally favored over another which would provide evidence of prejudice To achieve that we calculated the mean polarity score for the sentiment concepts being closer to each social group and then extracted the difference for each intergroup comparison The results are given in Figure For both Wikipedia and social

0.749 We tried this approach but unfortunately it does not work well for sentiment as the semantics for what defines a positive or negative concept is much looser than for gender Additionally corresponding positive and negative words do not have as clear pairwise mappings as gendered words ex male-female To mitigate these two issues we first take the most significant PCA component of a matrix of positive word vectors and do the same with negative word vectors from the Sentiment Lexicon dataset We then take the signed difference between these positive and negative principal components We found that the first significant component contains significantly more variance than any other component Figure displays the top principal components of the positive sentiment word matrix left and negative sentiment word matrix right One can see that most of the signal for capturing positive or negative sentiment can be captured in the first principal component We call the signed difference between these positive and negative principal components our directional sentiment vector as it connects the positive and negative subspaces Given a word vector we can now project it onto the directional sentiment vector to assess its sentiment polarity A visualization of the resulting sentiment polarity

0.7447 only around times Moreover there is a consistency of this ratio from to for almost years Cast Appearance in Movie Plot We analyze how male cast and female cast have been addressed This essentially involves extracting verbs and adjectives associated with male cast and female cast To extract verbs and adjectives linked to a particular cast we use Stanford Dependency Parser De Marneffe et al In Fig and we present the adjectives and verbs associated with males and females We observe that verbs like kills shoots occur with males while verbs like marries loves are associated with females Also when we look at adjectives males are often represented as rich and wealthy while females are represented as beautiful and attractive in movie plots Cast Introductions in Movie Plot We analyze how male cast and female cast have been introduced in the plot We use OpenIE Fader et al to capture such introductions by extracting relations corresponding to a cast Finally on aggregating the relations by gender we find that males are generally introduced with a profession like as a famous singer an honest police officer a successful scientist and so on while females are either introduced using physical appearance like

0.74 women play more central role than male Movie Preview Analysis We analyze all the frames extracted from the movie preview dataset and obtain information regarding the presence/absence of a male/female in the frame If any person is present in the frame we then find out the emotion displayed by the person The emotion displayed can be one of angry disgust fear happy neutral sad surprise Note that there can be more than one person detected in a single frame in that instance emotions of each person is detected We then aggregate the results to analyze the following tasks on the data Screen-On Time Figure shows the percentage distribution of screen-on time for males and female characters in movie trailers We see a consistent trend across the years where mean screen-on time for females is only a meagre compared to of the time for male characters Portrayal through Emotions In this task we analyze the emotions most commonly exhibited by male and female characters in movie trailers The most substantial difference is seen with respect to the Anger emotion Over the years anger constitutes of the emotions displayed by male characters as compared to the of emotions displayed by female characters

0.7346 training data is increased The key point is that since the gender bias is high the small training data has enough information to classify correctly of the cases Movie Poster and Plot Mentions We analyze images on Wikipedia movie pages for presence of males and females on publicity posters for the movie We use Dense CAP Johnson et al to extract male and female occurrences by checking our results in the top responses having a positive confidence score After the male and female extraction from posters we analyze the male and female mentions from the movie plot and co-relate them The intent of this analysis is to learn how publicizing a movie is biased towards a female on advertising material like posters and have a small or inconsequential role in the movie Pe nt e Percentage of female-centric movies over the years Percentage of female-centric movies over the years Highchartscom Figure Percentage of female-centric movies over the years Figure Percentage of female-centric movies over the years While of the movie plots have more male mentions than females surprisingly more than movie posters feature actresses Movies like GangaaJal Platform Raees have almost male mentions in plot but female mentions whereas in

0.7334 than Wikipedia Table Baidu Baike vs Wikipedia Naive Bayes SVM TextCNN estimate p-value estimate p-value estimate p-value Freedom Democracy Election Collective Action Negative Figures Social Control Surveillance CCP Historical Events Positive Figures The results are largely consistent with what we found in Section Overwhelmingly Wikipedia predicts headlines that contain target words in the categories of freedom democracy election and collective action to be more positive In contrast Baidu Baike predicts headlines that contain target words of figures that the CCP views positively to be more positive The exceptions to our expectations are the categories of social control surveillance CCP and historical events where we cannot reject the null of no difference between the two corpuses although they do not go against our expectations We find similar results for the comparison between Peoples Daily and Chinese language Wikipedia in Table Table Peoples Daily vs Wikipedia Naive Bayes SVM TextCNN estimate p-value estimate p-value estimate p-value Freedom Democracy Election Collective Action Negative Figures Social Control Surveillance CCP Historical Events Positive Figures To provide intuition Figure shows examples of headlines labeled differently between model trained with Baidu Baike pre-trained embeddings and model trained with Chinese language Wikipedia in our test set The model

0.7324 media Germans were depicted much more positively than foreigners The Woman Foreigners Homosexuality rc en ta ge o d er en ce Man German Heterosexuality Wikipedia Social Media Sexism Dataset Figure Intergroup positive sentiment difference in the embeddings same applies for heterosexuals in comparison to homosexuals Both results are in accordance to the sentiment task results as Germans and heterosexuals were associated with much more positive feelings and concepts confirming the existence of biases that favor privileged social groups In German Wikipedia men were generally depicted more positively On the other hand in the social media dataset women were associated with more positive words One explanation is that in Wikipedia men were described by stereotypical concepts like power attack and reinforcement which are labeled as positive in the polarity dictionary In contrast the social media data also related men to concepts like fascism and robbery ie words with highly negative sentiment That could also be rooted in the nature of German language which uses the male plural when making colloquial general claims Because negative statements about groups on social media were generated in a male form this bias could have been replicated by the model Furthermore the sentiment difference does

0.7324 Baidu Baike page For example when we attempted to create entries on Baidu Baike such as June Fourth Movement or Wuerkaixi we were automatically returned an error Perhaps because of the size difference between the two corpuses increasingly researchers developing cutting edge Chinese language NLP models are drawing on the Baidu Baike corpus Baidu Baike word embeddings have been shown to perform better on certain tasks Here we assess the downstream implications of this choice on the representation of democratic concepts social control and historical events and figures First we follow Caliskan et al to compare the distance between these concepts and a list of adjectives and sentiment words Then we show the downstream consequences of the choice of corpus on a predictive task of the sentiment of headlines DISTANCE FROM DEMOCRACY COMPARISON BETWEEN BAIDU BAIKE and wikipedia EMBEDDINGS In this section we consider the differences in word associations among word embeddings trained with Chinese language-wikipedia and Baidu Baike We use word embeddings made available by Li et al Li et al train -dimensional word embeddings on both Baidu Baike and Chinese language Wikipedia using the same algorithm WordVec For a benchmark we also compare these two sets of embeddings

0.7304 representative of various concepts In this case contains embeddings for verbal stimuli such as boy father and man while contains embeddings for verbal stimuli like oce and business These linguistic visual and sometimes auditory stimuli are proxies for the aggregate representation of a concept in cognition Embedding association tests use these unambiguous stimuli as semantic representations to study biased associations between the concepts being represented Since the stimuli are chosen by experts to most accurately represent concepts they are not polysemous or ambiguous tokens We use these expert-selected stimuli as the basis for our tests in the image domain The test statistic measures the differential association of the target concepts and with the attributes and B G B G B where B is the differential association of with the attributes quantified by the cosine similarity of vectors B mean mean We test the significance of this association with a permutation test over all possible equal-size partitions to generate a null hypothesis as if no biased associations existed The one-sided -value measures the unlikelihood of the null hypothesis A B B and the effect size a standardized measure of the separation between the relative association of and with and is mean

0.7292 in a sentence ie positive/negative anger sadness valence We focus on the task of regressing sentiment intensity or valence investigates the unfairness in this type of sentiment analysis task for over different models trained on the Task Affect in Tweets The authors create the Equity Evaluation Corpus EEC as a baseline to help measure differences in valence predictions between similar sentences that differ in the presence of a demographic identity term The authors measure unfairness in valence regression for race using names that tend to belong to African American demographics vs European demographics For example a sentence template from the EEC dataset looks like Name feels emotional state word For gender the authors measure unfairness in valence regression using gendered words like he or she Similar templates are used in this scenario He/she feels emotional state word The authors perform valence predictions on sentences in the EEC database with emotional state words and compare the average scores between different demographic groups We describe the comparison metrics below Avg score difference The average for only those pairs where the score for the African American noun phrase sentence is higher The greater the magnitude of this score the stronger the bias in

0.7278 for some national origin identity terms is displayed in Figure For our experiments the more positive the projection the more positive sentiment polarity More negative projections correspond to more negative sentiment polarity The exact reasons why identity terms have differing sentiment polarity is not clear but the effect of this inequality in downstream algorithms can cause discrimination We choose to equalize the sentiment between identity terms by decorrelating the word vectors with sentiment pushing the identity terms closer to or in vector terminology minimizing the projection onto the directional sentiment vector One can naively accomplish this goal by projecting all word vectors onto our sentiment subspace However this projection distorts the word vectors beyond usability We instead use an adversarial framework to balance word vector distortion with sentiment bias We explore this tradeoff in our experiments Validating the Directional Sentiment Vector We need to validate that the directional sentiment vector effectively captures sentiment polarity to ensure we can effectively decorrelate word vectors Figure Top principal components of the positive sentiment word matrix left and negative sentiment word matrix right In both cases the first principal component contains much of the signal for representing positive and negative sentiment in the embedding

0.7251 profession we take occupational categories from Wikipedia For gender we consider males and females To avoid the confounding effect of profession on gender we use only male and female actors for gender-based prompts In the race domain we consider European Americans African Americans Asian Americans and Latino Hispanic Americans Based on Wikipedias list of political ideologies we consider socialism populism nationalism liberalism fascism democracy conservatism communism anarchism left-wing and right-wing We include political ideology like fascism to understand how texts generated for political ideologies in the extreme end compare to texts generated for moderate political ideologies fascism group is not included to interpret negative generations with fascism prompt as a bias Similarly based on Wikipedias list of religious and spiritual beliefs we take the most commonly adopted religious beliefs in the world Sikhism Judaism Islam Hinduism Christianity Buddhism and Atheism Table shows the statistics of BOLD BOLD collection From here we collect English prompts from Wikipedia as follows For each domain we identify a list of Wikipedia pages corresponding to appropriate groups for that domain For instance we take groups for the profession domain from wiki/Lists_of_occupations Next for each group such as arts entertainment we scrape the Wikipedia pages for

0.7243 latency of response or the rate of classification error Stimuli may take the form of words pictures or even sounds and there are several IATs with picture-only stimuli Notably Caliskan et al adapt the heavily-validated IAT from social psychology to machines by testing for the mathematical association of word embeddings rather than response latency They present a systematic method for measuring language biases associated with social groups the Word Embedding Association Test WEAT Like the IAT the WEAT measures the effect size of bias in static word embeddings by quantifying the relative associations of two sets of target stimuli eg woman female and man male that represent social groups with two sets of evaluative attributes eg science mathematics and arts literature For validation two WEATs quantify associations towards flowers vs insects and towards musical instruments vs weapons both accepted baselines Greenwald et al Greenwald et al refer to these baseline biases as universally accepted stereotypes since they are widely shared across human subjects and are not potentially harmful to society Other WEATs measure social group biases such as sexist and racist associations or negative attitudes towards the elderly or people with disabilities In any modality implicit biases can potentially be

0.7241 BOLD Dataset and Metrics for Measuring Biases in Open-Ended Language Generation Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context While these models now empower many downstream applications from conversation bots to automatic storytelling they have been shown to generate texts that exhibit social biases To systematically study and benchmark social biases in open-ended language generation we introduce the Bias in Open-Ended Language Generation Dataset BOLD a large-scale dataset that consists of English text generation prompts for bias benchmarking across five domains profession gender race religion and political ideology We also propose new automated metrics for toxicity psycholinguistic norms and text gender polarity to measure social biases in open-ended text generation from multiple angles An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices CONCEPTS Computing methodologies Natural language generation KEYWORDS Fairness natural language generation INTRODUCTION

0.723 in each domain BOLD contains prompts that trigger text generation from various demographic groups that compose profession gender race religious belief and political ideology domains see Table In each domain some groups may be more frequently associated with negative emotions than others when an LM generates text In this section we examine biases in generated texts towards different demographic groups in each domain Profession Table shows the proportion of texts that were classified as male or as female with Gender-Max Gender-Wavg and unigram matching metrics across various professions and data sources This categorization of profession was obtained by merging a set of granular professions as follows arts entertainment includes dance film and television entertainer writing artistic and theater science technology includes engineering computer and scientific industrial manufacturing includes metal working industrial and railway industry and healthcare medicine includes healthcare nursing and mental health Only of total texts across all professions are classified as either male or female This is because the prompts were extracted from Wikipedia articles without any constraint that will force an LM to generate gender polar texts The proportion of texts classified as female is higher in healthcare medicine group across all metrics and data sources Table

0.7227 of male being central or female being central differ How does it present a male or female bias IV Mentions Image vs Plot How many males and females are the faces of the promotional posters How does this correlate to them being mentioned in the plot What results are conveyed on the combined analysis V Dialogues How do the number of dialogues differ between a male cast and a female cast in official movie script VI Singers Does the same bias occur in movie songs How does the distribution of singers with gender vary over a period of time for different movies VII Female-centric Movies- Are the movie stories and portrayal of females evolving Have we seen female-centric movies in the recent past VIII Screen Time Which gender if any has a greater screen time in movie trailers IX Emotions of Males and Females Which emotions are most commonly displayed by males and females in a movie trailer Does this correspond with the gender stereotypes which exist in society RELATED WORK While there are recent works where gender bias has been studied in different walks of life Soklaridis et al MacNell et al Carnes et al Terrell et al Saji

0.7122 anchored comparison on White males the group with the most representation and compared against White females Black males and Black females respectively Table Drawing on social psychology we pose three hypotheses about intersectional bias Intersectionality hypothesis tests at the intersection of gender and race will reveal emergent biases not explained by the sum of biases towards race and gender alone Race hypothesis biases between racial groups will be more similar to differential biases between the men than between the women Gender hypothesis biases between men and women will be most similar to biases between White men and White women Embedding Association Tests Though our stimuli are images rather than words we can use the same statistical method for measuring biased associations between image representations to quantify a standardized effect size of bias We follow Caliskan et al in describing the WEAT here Let and be two sets of target concepts embeddings of size C and let and be two sets of attribute embeddings of size For example the Gender-Career IAT tests for the differential association between the concepts male and female and the attributes career and family Generally experts in social psychology and cognitive science select stimuli that are typically

0.7088 neutral in gender polarity we compute the second set of metrics on a per-class basis and use the average of per-class scores weighted by the number of samples in each class Table summarizes the result in which we find a strong correlation between human annotations for male and female gender with both cosine similarly based gender polarity metrics Spearmans correlation coefficient and Among all three gender polarity metrics unigram matching has the lowest Spearmans correlation coefficient with and lowest f-score with As shown in Fig a and b with both Gender-Max and Gender-Wavg a larger proportion of mismatch is caused by a text that is annotators neutral blue curve but automated metrics male score By contrast a larger proportion of error with unigram matching occurs when an annotators male red curve is computed as a neutral score by the automatic metric see Fig c One reason for this error is that the classification to male or female class with unigram matching is reliant on the manually chosen list of tokens for male and female gender Automatic metrics for sentiment and toxicity are also positively correlated with human annotations of sentiment and toxicity however with a smaller value of Spearmans correlation

0.7076 tokens she her hers herself woman women shes girl and girls A text is identified as expressing male gender if the count of male words in the text is larger than the count of female words If both counts are zero the text is labelled as neutral While this metric can account for the direct presence of gendered words in the text it does not account for words that may be indirectly related to a gender We propose a second gender polarity metric to take into account the presence of words in the text that are indirectly related to a gender It is based on Bolukbasi et al which identifies that the normalized projection of a word vector into the gender direction defined by she he is closer to if the word is closer to she and closer to if the word is closer to he in the word embedding space and shows that a word-level gender classifier based on this metric has a good approximation with human annotations of word-level gender With this finding we define our second text-level gender polarity metric as follows To avoid inheriting the gender biases in professions existing in a word embedding we use

0.7007 the hard debiased WordVec embedding On this word embedding space we first compute the gender polarity of each word in the text as follows where she he If is female-aligned then is close if is male-aligned then is close to and if is neutral then Next we aggregate the word-level gender polarity scores and obtain a continuous score indicating the gender polarity of the entire text A simple approach to aggregate word-level scores is averaging However since a text in general has a larger number of neutral words than gender polar words it tends to skew the gender polarity of the text towards neutral Hence we propose two alternative ways to aggregate word level gender polarity scores that apply a larger weight to the scores from gender polar words First we propose to weight all word-level gender polarity scores by their magnitude and take a weighted average termed as Gender-Wavg Gender-Wavg Second we propose to take the score from the most gender polar word in the text termed as Gender-Max for the rest of the paper argmax Gender-Max Once a global score is computed we take a threshold of to classify a text as expressing the male gender and a

0.6991 First lexicons should be extended to cover a larger vocabulary of words Second word-level lexicons should be aggregated to obtain a text-level lexicon To extend lexicons to a larger vocabulary we use the method in that trains a multi-task learning feed-forward network with FASTTEXT word embedding vectors to predict lexicons of unknown words To aggregate lexicons of each word and compute text level norms we compute the weighted average as follows where represents the word-level lexicon value and is the number of words used during this aggregation During text-level psycholinguistic norm calculation we do not include lexicons from words that belong to certain parts of speech like pronoun preposition and conjunction that do not convey any emotion For ease of interpretation we scale variable in VAD to with representing neutral and BE to with representing neutral Gender polarity We propose two types of gender polarity metrics Our first gender polarity metric termed unigram matching counts the total number of male and female specific tokens in the text Following current literature that studies gender bias in models we obtain a list for male and female identifying tokens as male tokens he him his himself man men hes boy boys and female

0.6975 we caution users of BOLD against a comparison with Wikipedia sentences as a fair baseline Our experiments on comparing Wikipedia sentences with texts generated by LMs also show that the Wikipedia is not free from biases and the biases it exhibits resemble the biases exposed in the texts generated by LMs see Section CONCLUSION We presented a novel dataset BOLD and a set of metrics to evaluate fairness in open-ended language generation Our experiments on evaluating the biases in three different LMs and a comparison with Wikipedia texts show that LMs are prone to more frequently generating texts with negative connotations towards a particular group of people or an idea than others For instance these models more frequently generate texts with negative sentiments and toxicity towards the African American group and more frequently generate text containing male words when a profession context is provided We also show that GPT- CTRL-THT and CTRL-OPN conform more to social biases than BERT and CTRL-WIKI This shows a crucial need to study and benchmark social biases in open-ended language generation and prevent the reinforcement of detrimental biases in downstream tasks With these findings and the proposed dataset in this paper we provide a test-bed

